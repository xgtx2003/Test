"""
ä¼˜åŒ–ç‰ˆçš„Milvusæ“ä½œæ¨¡å— - æ”¯æŒæ‰¹é‡å¤„ç†å’Œå¤šçº¿ç¨‹
"""

from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility
import json
import logging
import requests
from typing import List, Dict, Any, Optional, Tuple
import numpy as np
import time
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class BatchInsertData:
    """æ‰¹é‡æ’å…¥æ•°æ®ç»“æ„"""
    collection_name: str
    data: List[List[Any]]
    texts_for_vectorization: List[str]

class OptimizedSiliconFlowEmbedder:
    """ä¼˜åŒ–çš„SiliconFlow APIè°ƒç”¨ç±»ï¼Œæ”¯æŒå¤§æ‰¹é‡å‘é‡åŒ–"""
    
    def __init__(self, api_key: str, base_url: str = "https://api.siliconflow.cn/v1"):
        self.api_key = api_key
        self.base_url = base_url
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        self.dimension = 1024
        self.max_batch_size = 64  # SiliconFlow APIæœ€å¤§æ‰¹é‡é™åˆ¶ä¸º64
        
    def encode_batch(self, texts: List[str], model: str = "BAAI/bge-m3") -> List[List[float]]:
        """
        æ‰¹é‡å‘é‡åŒ–æ–‡æœ¬ï¼Œè‡ªåŠ¨åˆ†æ‰¹å¤„ç†å¤§é‡æ–‡æœ¬
        """
        if not texts:
            return []
        
        all_embeddings = []
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(texts), self.max_batch_size):
            batch_texts = texts[i:i + self.max_batch_size]
            print(f"ğŸ”„ å‘é‡åŒ–æ‰¹æ¬¡ {i//self.max_batch_size + 1}/{(len(texts)-1)//self.max_batch_size + 1} ({len(batch_texts)} ä¸ªæ–‡æœ¬)")
            
            batch_embeddings = self._encode_single_batch(batch_texts, model)
            all_embeddings.extend(batch_embeddings)
            
            # ç®€å•çš„é€Ÿç‡é™åˆ¶
            if i + self.max_batch_size < len(texts):
                time.sleep(0.1)
        
        return all_embeddings
    
    def _encode_single_batch(self, texts: List[str], model: str) -> List[List[float]]:
        """å¤„ç†å•ä¸ªæ‰¹æ¬¡çš„å‘é‡åŒ–"""
        processed_texts = [text if text and text.strip() else " " for text in texts]
        
        payload = {
            "model": model,
            "input": processed_texts,
            "encoding_format": "float"
        }
        
        max_retries = 3
        for attempt in range(max_retries):
            try:
                response = requests.post(
                    f"{self.base_url}/embeddings",
                    headers=self.headers,
                    json=payload,
                    timeout=60  # å¢åŠ è¶…æ—¶æ—¶é—´
                )
                
                if response.status_code == 200:
                    result = response.json()
                    embeddings = [item["embedding"] for item in result["data"]]
                    return embeddings
                else:
                    logger.error(f"APIè°ƒç”¨å¤±è´¥ (å°è¯• {attempt+1}/{max_retries}): {response.status_code}, {response.text}")
                    if attempt < max_retries - 1:
                        time.sleep(1 * (attempt + 1))  # æŒ‡æ•°é€€é¿
                        
            except Exception as e:
                logger.error(f"å‘é‡åŒ–å¤±è´¥ (å°è¯• {attempt+1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(1 * (attempt + 1))
        
        # å¤±è´¥åè¿”å›é›¶å‘é‡
        logger.warning(f"æ‰¹æ¬¡å‘é‡åŒ–æœ€ç»ˆå¤±è´¥ï¼Œè¿”å›é›¶å‘é‡")
        return [[0.0] * self.dimension for _ in texts]

class OptimizedStandardClauseMatchingSystem:
    """
    ä¼˜åŒ–çš„æ ‡å‡†æ¡æ¬¾åŒ¹é…ç³»ç»Ÿ - æ”¯æŒæ‰¹é‡å¤„ç†å’Œå¤šçº¿ç¨‹
    """
    
    def __init__(self, uri: str, token: str, siliconflow_api_key: str, 
                 siliconflow_base_url: str = "https://api.siliconflow.cn/v1"):
        self.uri = uri
        self.token = token
        self.embedder = OptimizedSiliconFlowEmbedder(siliconflow_api_key, siliconflow_base_url)
        self.dimension = self.embedder.dimension
        self.connect()
        
        # é›†åˆåç§°å®šä¹‰
        self.collections = {
            'scope': 'standard_scope_collection',
            'parameters': 'standard_parameters_collection', 
            'topic_keywords': 'standard_topic_keywords_collection',
            'context_keywords': 'standard_context_keywords_collection',
            'table_headers': 'standard_table_headers_collection'
        }
        
        # åŸºç¡€æƒé‡é…ç½® - ç”¨äºåŠ¨æ€è°ƒæ•´
        self.base_weights = {
            'topic_keywords': 0.30,
            'scope': 0.20,
            'parameters': 0.20,
            'context_keywords': 0.10,
            'table_headers': 0.20
        }
        
        # é«˜ä»·å€¼å­—æ®µ - å½“ç›¸ä¼¼åº¦é«˜æ—¶ç»™äºˆé¢å¤–å¥–åŠ±
        self.high_value_fields = {'parameters', 'table_headers'}
        
        # ç›¸ä¼¼åº¦é˜ˆå€¼ - è¶…è¿‡æ­¤å€¼æ—¶å¯åŠ¨å¥–åŠ±æœºåˆ¶
        self.similarity_thresholds = {
            'parameters': 0.6,      # å‚æ•°ç›¸ä¼¼åº¦è¶…è¿‡0.6è®¤ä¸ºé«˜åº¦ç›¸å…³
            'table_headers': 0.7,  # è¡¨å¤´ç›¸ä¼¼åº¦è¶…è¿‡0.7è®¤ä¸ºé«˜åº¦ç›¸å…³
            'topic_keywords': 0.8,  # ä¸»é¢˜è¯éœ€è¦æ›´é«˜ç›¸ä¼¼åº¦
            'scope': 0.7,          # èŒƒå›´ç›¸ä¼¼åº¦é˜ˆå€¼
            'context_keywords': 0.75 # ä¸Šä¸‹æ–‡å…³é”®è¯é˜ˆå€¼
        }
        
        # ç¼“å­˜é›†åˆåŠ è½½çŠ¶æ€
        self._collections_loaded = False
    
    def connect(self):
        """è¿æ¥åˆ°Milvus"""
        try:
            connections.connect("default", uri=self.uri, token=self.token)
            logger.info("âœ… Successfully connected to Milvus")
        except Exception as e:
            logger.error(f"âŒ Failed to connect to Milvus: {e}")
            raise
    
    def batch_insert_clause_data(self, chapters_data: List[Dict[str, Any]], batch_size: int = 50):
        """
        æ‰¹é‡æ’å…¥ç« èŠ‚æ•°æ® - æ ¸å¿ƒä¼˜åŒ–æ–¹æ³•
        
        Args:
            chapters_data: ç« èŠ‚æ•°æ®åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« document_id, chapter, clause_data
            batch_size: æ‰¹é‡å¤„ç†å¤§å°
        """
        print(f"ğŸš€ å¼€å§‹æ‰¹é‡æ’å…¥ {len(chapters_data)} ä¸ªç« èŠ‚")
        
        # ç¬¬ä¸€æ­¥ï¼šæ”¶é›†æ‰€æœ‰éœ€è¦å‘é‡åŒ–çš„æ–‡æœ¬
        all_texts = []
        text_to_data_mapping = []  # è®°å½•æ¯ä¸ªæ–‡æœ¬å¯¹åº”çš„æ•°æ®ä¿¡æ¯
        
        print("ğŸ“ æ”¶é›†æ‰€æœ‰æ–‡æœ¬ç”¨äºå‘é‡åŒ–...")
        for i, chapter_info in enumerate(chapters_data):
            document_id = chapter_info['document_id']
            chapter = chapter_info['chapter']
            clause_data = chapter_info['clause_data']
            
            # æ”¶é›†scopeæ–‡æœ¬
            if clause_data.get('scope'):
                all_texts.append(clause_data['scope'])
                text_to_data_mapping.append({
                    'type': 'scope',
                    'index': i,
                    'document_id': document_id,
                    'chapter': chapter,
                    'text': clause_data['scope']
                })
            
            # æ”¶é›†topic keywordsæ–‡æœ¬
            if clause_data.get('topic_keywords'):
                keywords_text = ",".join(clause_data['topic_keywords'])
                all_texts.append(keywords_text)
                text_to_data_mapping.append({
                    'type': 'topic_keywords',
                    'index': i,
                    'document_id': document_id,
                    'chapter': chapter,
                    'text': keywords_text
                })
            
            # æ”¶é›†context keywordsæ–‡æœ¬
            if clause_data.get('context_keywords'):
                keywords_text = ",".join(clause_data['context_keywords'])
                all_texts.append(keywords_text)
                text_to_data_mapping.append({
                    'type': 'context_keywords',
                    'index': i,
                    'document_id': document_id,
                    'chapter': chapter,
                    'text': keywords_text
                })
            
            # æ”¶é›†parametersæ–‡æœ¬
            if clause_data.get('parameters'):
                for param in clause_data['parameters']:
                    # ç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½ä¸æ˜¯None
                    item = param.get('item', '') or ''
                    constraint = param.get('constraint', '') or ''
                    value = str(param.get('value', '') or '')
                    unit = param.get('unit', '') or ''
                    
                    param_text = f"{item} {constraint} {value} {unit}".strip()
                    all_texts.append(param_text)
                    text_to_data_mapping.append({
                        'type': 'parameters',
                        'index': i,
                        'document_id': document_id,
                        'chapter': chapter,
                        'param': param,
                        'text': param_text
                    })
            
            # æ”¶é›†table headersæ–‡æœ¬
            if clause_data.get('table_headers'):
                headers_text = ",".join(clause_data['table_headers'])
                all_texts.append(headers_text)
                text_to_data_mapping.append({
                    'type': 'table_headers',
                    'index': i,
                    'document_id': document_id,
                    'chapter': chapter,
                    'text': headers_text
                })
        
        print(f"ğŸ“Š æ”¶é›†äº† {len(all_texts)} ä¸ªæ–‡æœ¬éœ€è¦å‘é‡åŒ–")
        
        # ç¬¬äºŒæ­¥ï¼šæ‰¹é‡å‘é‡åŒ–æ‰€æœ‰æ–‡æœ¬
        print("ğŸ”¢ å¼€å§‹æ‰¹é‡å‘é‡åŒ–...")
        all_embeddings = self.embedder.encode_batch(all_texts)
        
        # ç¬¬ä¸‰æ­¥ï¼šç»„ç»‡æ•°æ®æŒ‰é›†åˆåˆ†ç»„
        collection_data = {
            'scope': [],
            'parameters': [],
            'topic_keywords': [],
            'context_keywords': [],
            'table_headers': []
        }
        
        print("ğŸ“¦ ç»„ç»‡æ•°æ®æŒ‰é›†åˆåˆ†ç»„...")
        for i, mapping in enumerate(text_to_data_mapping):
            embedding = all_embeddings[i]
            data_type = mapping['type']
            
            if data_type == 'scope':
                collection_data['scope'].append([
                    mapping['document_id'],
                    mapping['chapter'],
                    mapping['text'],
                    embedding
                ])
            elif data_type == 'topic_keywords':
                collection_data['topic_keywords'].append([
                    mapping['document_id'],
                    mapping['chapter'],
                    mapping['text'],
                    embedding
                ])
            elif data_type == 'context_keywords':
                collection_data['context_keywords'].append([
                    mapping['document_id'],
                    mapping['chapter'],
                    mapping['text'],
                    embedding
                ])
            elif data_type == 'parameters':
                param = mapping['param']
                collection_data['parameters'].append([
                    mapping['document_id'],
                    mapping['chapter'],
                    param.get('item', '') or '',
                    param.get('constraint', '') or '',
                    str(param.get('value', '') or ''),
                    param.get('unit', '') or '',  # ç¡®ä¿Noneè½¬æ¢ä¸ºç©ºå­—ç¬¦ä¸²
                    param.get('source_text', '') or '',
                    embedding
                ])
            elif data_type == 'table_headers':
                collection_data['table_headers'].append([
                    mapping['document_id'],
                    mapping['chapter'],
                    mapping['text'],
                    embedding
                ])
        
        # ç¬¬å››æ­¥ï¼šæ‰¹é‡æ’å…¥åˆ°å„ä¸ªé›†åˆ
        print("ğŸ’¾ å¼€å§‹æ‰¹é‡æ’å…¥æ•°æ®åˆ°Milvus...")
        
        def insert_to_collection(collection_name, data_list):
            if not data_list:
                return
            
            collection = Collection(self.collections[collection_name])
            
            # è½¬æ¢æ•°æ®æ ¼å¼ä¸ºMilvusæœŸæœ›çš„æ ¼å¼
            if collection_name == 'parameters':
                # parametersé›†åˆæœ‰æ›´å¤šå­—æ®µ
                transposed_data = [
                    [row[0] for row in data_list],  # document_id
                    [row[1] for row in data_list],  # chapter
                    [row[2] for row in data_list],  # item
                    [row[3] for row in data_list],  # constraint
                    [row[4] for row in data_list],  # value
                    [row[5] for row in data_list],  # unit
                    [row[6] for row in data_list],  # source_text
                    [row[7] for row in data_list],  # vector
                ]
            else:
                # å…¶ä»–é›†åˆæ ¼å¼ç›¸åŒ
                transposed_data = [
                    [row[0] for row in data_list],  # document_id
                    [row[1] for row in data_list],  # chapter
                    [row[2] for row in data_list],  # text
                    [row[3] for row in data_list],  # vector
                ]
            
            collection.insert(transposed_data)
            print(f"âœ… æ’å…¥äº† {len(data_list)} æ¡è®°å½•åˆ° {collection_name} é›†åˆ")
        
        # å¹¶è¡Œæ’å…¥å„ä¸ªé›†åˆ
        with ThreadPoolExecutor(max_workers=5) as executor:
            futures = []
            for collection_name, data_list in collection_data.items():
                if data_list:
                    future = executor.submit(insert_to_collection, collection_name, data_list)
                    futures.append((collection_name, future))
            
            for collection_name, future in futures:
                try:
                    future.result(timeout=60)
                except Exception as e:
                    logger.error(f"âŒ æ’å…¥åˆ° {collection_name} å¤±è´¥: {e}")
        
        print(f"ğŸ‰ æ‰¹é‡æ’å…¥å®Œæˆï¼å¤„ç†äº† {len(chapters_data)} ä¸ªç« èŠ‚")

    def create_collections(self):
        """åˆ›å»ºæ‰€æœ‰å¿…è¦çš„é›†åˆï¼ˆä¿æŒä¸åŸç‰ˆä¸€è‡´ï¼‰"""
        print("åˆ›å»º Milvus é›†åˆ...")
        
        # 1. Scope é›†åˆ
        scope_fields = [
            FieldSchema(name="document_id", dtype=DataType.VARCHAR, max_length=100, is_primary=True),
            FieldSchema(name="chapter", dtype=DataType.VARCHAR, max_length=200),
            FieldSchema(name="scope_text", dtype=DataType.VARCHAR, max_length=65535),
            FieldSchema(name="vector", dtype=DataType.FLOAT_VECTOR, dim=self.dimension)
        ]
        scope_schema = CollectionSchema(scope_fields, "æ ‡å‡†æ–‡ä»¶èŒƒå›´å‘é‡é›†åˆ")
        
        if utility.has_collection(self.collections['scope']):
            utility.drop_collection(self.collections['scope'])
            logger.info(f"Dropped existing collection: {self.collections['scope']}")
        Collection(self.collections['scope'], scope_schema)
        logger.info(f"Created collection: {self.collections['scope']}")
        
        # 2. Parameters é›†åˆ
        param_fields = [
            FieldSchema(name="document_id", dtype=DataType.VARCHAR, max_length=100),
            FieldSchema(name="chapter", dtype=DataType.VARCHAR, max_length=200),
            FieldSchema(name="item", dtype=DataType.VARCHAR, max_length=500),
            FieldSchema(name="constraint", dtype=DataType.VARCHAR, max_length=200),
            FieldSchema(name="value", dtype=DataType.VARCHAR, max_length=200),
            FieldSchema(name="unit", dtype=DataType.VARCHAR, max_length=100),
            FieldSchema(name="source_text", dtype=DataType.VARCHAR, max_length=2000),
            FieldSchema(name="param_id", dtype=DataType.INT64, is_primary=True, auto_id=True),
            FieldSchema(name="vector", dtype=DataType.FLOAT_VECTOR, dim=self.dimension)
        ]
        param_schema = CollectionSchema(param_fields, "æ ‡å‡†æ–‡ä»¶å‚æ•°å‘é‡é›†åˆ")
        
        if utility.has_collection(self.collections['parameters']):
            utility.drop_collection(self.collections['parameters'])
            logger.info(f"Dropped existing collection: {self.collections['parameters']}")
        Collection(self.collections['parameters'], param_schema)
        logger.info(f"Created collection: {self.collections['parameters']}")
        
        # 3. Topic Keywords é›†åˆ
        topic_fields = [
            FieldSchema(name="document_id", dtype=DataType.VARCHAR, max_length=100),
            FieldSchema(name="chapter", dtype=DataType.VARCHAR, max_length=200),
            FieldSchema(name="keywords_text", dtype=DataType.VARCHAR, max_length=2000),
            FieldSchema(name="keyword_id", dtype=DataType.INT64, is_primary=True, auto_id=True),
            FieldSchema(name="vector", dtype=DataType.FLOAT_VECTOR, dim=self.dimension)
        ]
        topic_schema = CollectionSchema(topic_fields, "æ ‡å‡†æ–‡ä»¶ä¸»é¢˜å…³é”®è¯å‘é‡é›†åˆ")
        
        if utility.has_collection(self.collections['topic_keywords']):
            utility.drop_collection(self.collections['topic_keywords'])
            logger.info(f"Dropped existing collection: {self.collections['topic_keywords']}")
        Collection(self.collections['topic_keywords'], topic_schema)
        logger.info(f"Created collection: {self.collections['topic_keywords']}")
        
        # 4. Context Keywords é›†åˆ
        context_fields = [
            FieldSchema(name="document_id", dtype=DataType.VARCHAR, max_length=100),
            FieldSchema(name="chapter", dtype=DataType.VARCHAR, max_length=200),
            FieldSchema(name="keywords_text", dtype=DataType.VARCHAR, max_length=2000),
            FieldSchema(name="keyword_id", dtype=DataType.INT64, is_primary=True, auto_id=True),
            FieldSchema(name="vector", dtype=DataType.FLOAT_VECTOR, dim=self.dimension)
        ]
        context_schema = CollectionSchema(context_fields, "æ ‡å‡†æ–‡ä»¶ä¸Šä¸‹æ–‡å…³é”®è¯å‘é‡é›†åˆ")
        
        if utility.has_collection(self.collections['context_keywords']):
            utility.drop_collection(self.collections['context_keywords'])
            logger.info(f"Dropped existing collection: {self.collections['context_keywords']}")
        Collection(self.collections['context_keywords'], context_schema)
        logger.info(f"Created collection: {self.collections['context_keywords']}")
        
        # 5. Table Headers é›†åˆ
        table_fields = [
            FieldSchema(name="document_id", dtype=DataType.VARCHAR, max_length=100),
            FieldSchema(name="chapter", dtype=DataType.VARCHAR, max_length=200),
            FieldSchema(name="headers_text", dtype=DataType.VARCHAR, max_length=2000),
            FieldSchema(name="header_id", dtype=DataType.INT64, is_primary=True, auto_id=True),
            FieldSchema(name="vector", dtype=DataType.FLOAT_VECTOR, dim=self.dimension)
        ]
        table_schema = CollectionSchema(table_fields, "æ ‡å‡†æ–‡ä»¶è¡¨æ ¼æ ‡é¢˜å‘é‡é›†åˆ")
        
        if utility.has_collection(self.collections['table_headers']):
            utility.drop_collection(self.collections['table_headers'])
            logger.info(f"Dropped existing collection: {self.collections['table_headers']}")
        Collection(self.collections['table_headers'], table_schema)
        logger.info(f"Created collection: {self.collections['table_headers']}")
    
    def create_indexes(self):
        """ä¸ºæ‰€æœ‰é›†åˆåˆ›å»ºç´¢å¼•"""
        index_params = {
            "metric_type": "COSINE",
            "index_type": "IVF_FLAT",
            "params": {"nlist": 128}
        }
        
        for collection_name in self.collections.values():
            collection = Collection(collection_name)
            collection.create_index("vector", index_params)
            logger.info(f"Created index for {collection_name}")
    
    def load_collections(self):
        """åŠ è½½æ‰€æœ‰é›†åˆåˆ°å†…å­˜ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        if self._collections_loaded:
            return
            
        for collection_name in self.collections.values():
            collection = Collection(collection_name)
            collection.load()
            logger.info(f"Loaded collection: {collection_name}")
        
        self._collections_loaded = True
    
    def _apply_field_reranker(self, query_text: str, search_results: List[Dict], top_k: int) -> List[Dict]:
        """
        å¯¹å•ä¸ªå­—æ®µçš„æœç´¢ç»“æœè¿›è¡Œrerankeré‡æ’
        """
        if not search_results or len(search_results) <= top_k:
            return search_results
        
        # æ„å»ºæ–‡æ¡£æ–‡æœ¬
        documents = []
        for result in search_results:
            content = result.get('content', '')
            if content:
                documents.append(content)
            else:
                # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨document_idå’Œchapter
                documents.append(f"Document: {result['document_id']} Chapter: {result['chapter']}")
        
        try:
            payload = {
                "model": "BAAI/bge-reranker-v2-m3",
                "query": query_text,
                "documents": documents,
                "top_k": top_k,
                "return_documents": True
            }
            
            response = requests.post(
                f"{self.embedder.base_url}/rerank",
                headers=self.embedder.headers,
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                rerank_results = response.json().get("results", [])
                reranked_search_results = []
                
                for rerank_item in rerank_results:
                    original_index = rerank_item.get('index', 0)
                    if 0 <= original_index < len(search_results):
                        result = search_results[original_index].copy()
                        # ç”¨rerankeråˆ†æ•°æ›¿æ¢åŸå§‹ç›¸ä¼¼åº¦
                        result['similarity'] = rerank_item.get('relevance_score', result['similarity'])
                        result['reranked'] = True
                        reranked_search_results.append(result)
                
                return reranked_search_results
                
        except Exception as e:
            logger.error(f"å­—æ®µé‡æ’å¤±è´¥: {e}")
        
        # å¤±è´¥æ—¶è¿”å›åŸå§‹ç»“æœ
        return search_results[:top_k]

    def batch_find_matching_clauses(self, queries: List[Dict[str, Any]], top_k: int = 10, use_reranker: bool = True) -> List[List[Dict]]:
        """
        æ‰¹é‡æŸ¥æ‰¾åŒ¹é…çš„æ¡æ¬¾ - å­—æ®µçº§æ‰¹é‡searchä¼˜åŒ–ç‰ˆï¼ˆå¸¦rerankerå‰ç½®ï¼‰
        """
        self.load_collections()
        print(f"ğŸ”„ å¼€å§‹æ‰¹é‡æŸ¥è¯¢ {len(queries)} ä¸ªç« èŠ‚...")

        # 1. æŒ‰å­—æ®µæ”¶é›†æ‰€æœ‰éœ€è¦å‘é‡åŒ–çš„æ–‡æœ¬å’Œç´¢å¼•
        field_vectors = {ft: [] for ft in self.collections.keys()}
        field_query_indices = {ft: [] for ft in self.collections.keys()}
        field_texts = {ft: [] for ft in self.collections.keys()}

        for i, query in enumerate(queries):
            clause_data = query['clause_data']
            if clause_data.get('scope'):
                print("=" * 20,"å‘ç°scope")
                field_texts['scope'].append(clause_data['scope'])
                field_query_indices['scope'].append(i)
            if clause_data.get('topic_keywords'):
                keywords_text = " ".join(clause_data['topic_keywords'])
                field_texts['topic_keywords'].append(keywords_text)
                field_query_indices['topic_keywords'].append(i)
            if clause_data.get('context_keywords'):
                keywords_text = " ".join(clause_data['context_keywords'])
                field_texts['context_keywords'].append(keywords_text)
                field_query_indices['context_keywords'].append(i)
            if clause_data.get('parameters'):
                for param in clause_data['parameters']:
                    param_text = f"{param.get('item', '')} {param.get('constraint', '')} {param.get('value', '')} {param.get('unit', '')}"
                    field_texts['parameters'].append(param_text)
                    field_query_indices['parameters'].append(i)
            if clause_data.get('table_headers'):
                headers_text = " | ".join(clause_data['table_headers'])
                field_texts['table_headers'].append(headers_text)
                field_query_indices['table_headers'].append(i)

        # 2. æ‰¹é‡å‘é‡åŒ–æ‰€æœ‰æ–‡æœ¬
        for ft in self.collections.keys():
            if field_texts[ft]:
                field_vectors[ft] = self.embedder.encode_batch(field_texts[ft])
            else:
                field_vectors[ft] = []

        # 3. æ‰¹é‡searchå¹¶åˆ†é…ç»“æœï¼ˆæ‰€æœ‰å­—æ®µéƒ½æ‰©å¤§æœç´¢èŒƒå›´ç”¨äºrerankerï¼‰
        field_search_results = {ft: [] for ft in self.collections.keys()}
        for ft in self.collections.keys():
            if field_vectors[ft]:
                collection = Collection(self.collections[ft])
                search_params = {"metric_type": "COSINE", "params": {"nprobe": 10}}
                vectors = field_vectors[ft]
                
                # æ‰€æœ‰å­—æ®µéƒ½æ‰©å¤§æœç´¢èŒƒå›´ç”¨äºreranker
                search_limit = (top_k * 3) if use_reranker else (top_k * 2)
                
                # è·å–æ›´å¤šå­—æ®µç”¨äºreranker
                output_fields = ["document_id", "chapter"]
                if ft == 'parameters':
                    output_fields.extend(["item", "constraint", "value", "unit"])
                elif ft in ['topic_keywords', 'context_keywords']:
                    output_fields.append('keywords_text')
                elif ft == 'table_headers':
                    output_fields.append('headers_text')
                elif ft == 'scope':
                    output_fields.append('scope_text')
                
                batch_size = 10  # Milvus nqé™åˆ¶
                all_results = []
                for start in range(0, len(vectors), batch_size):
                    batch_vectors = vectors[start:start+batch_size]
                    batch_results = collection.search(
                        data=batch_vectors,
                        anns_field='vector',
                        param=search_params,
                        limit=search_limit,
                        output_fields=output_fields
                    )
                    all_results.extend(batch_results)
                field_search_results[ft] = all_results
            else:
                field_search_results[ft] = []

        # 4. å¯¹æ‰€æœ‰å­—æ®µåº”ç”¨reranker
        if use_reranker:
            for ft in self.collections.keys():
                if field_search_results[ft] and field_texts[ft]:
                    print(f"ğŸ”„ å¯¹ {ft} å­—æ®µåº”ç”¨reranker...")
                    # åˆ†ç»„å¤„ç†æ¯ä¸ªæŸ¥è¯¢çš„reranker
                    reranked_results = []
                    for idx, (query_text, result_batch) in enumerate(zip(field_texts[ft], field_search_results[ft])):
                        if result_batch:
                            # æ„å»ºå€™é€‰æ–‡æ¡£å†…å®¹
                            search_results = []
                            for hit in result_batch:
                                result_data = {
                                    'document_id': hit.entity.get('document_id'),
                                    'chapter': hit.entity.get('chapter'),
                                    'similarity': hit.score,
                                    'collection_type': ft
                                }
                                
                                # æ„å»ºå†…å®¹æ–‡æœ¬
                                if ft == 'parameters':
                                    content = f"{hit.entity.get('item', '')} {hit.entity.get('constraint', '')} {hit.entity.get('value', '')} {hit.entity.get('unit', '')}"
                                elif ft in ['topic_keywords', 'context_keywords']:
                                    content = hit.entity.get('keywords_text', '')
                                elif ft == 'table_headers':
                                    content = hit.entity.get('headers_text', '')
                                elif ft == 'scope':
                                    content = hit.entity.get('scope_text', '')
                                else:
                                    content = f"Document: {result_data['document_id']} Chapter: {result_data['chapter']}"
                                
                                result_data['content'] = content
                                search_results.append(result_data)
                            
                            # åº”ç”¨reranker
                            reranked = self._apply_field_reranker(query_text, search_results, top_k * 2)
                            reranked_results.append(reranked)
                        else:
                            reranked_results.append([])
                    
                    field_search_results[ft] = reranked_results

        # 5. æŒ‰åŸæŸ¥è¯¢åˆ†ç»„ï¼Œåˆå¹¶ç»“æœ
        all_results = []
        for i, query in enumerate(queries):
            matching_results = {}
            # éå†æ‰€æœ‰å­—æ®µï¼Œå°†å±äºå½“å‰queryçš„ç»“æœåˆå¹¶
            for ft in self.collections.keys():
                # æ‰¾åˆ°å±äºå½“å‰queryçš„æ‰€æœ‰searchç»“æœ
                query_result_indices = [idx for idx, qidx in enumerate(field_query_indices[ft]) if qidx == i]
                
                for result_idx in query_result_indices:
                    if result_idx < len(field_search_results[ft]):
                        if use_reranker:
                            # å·²ç»rerankedçš„ç»“æœï¼ˆæ‰€æœ‰å­—æ®µéƒ½å¯èƒ½è¢«rerankäº†ï¼‰
                            search_results = field_search_results[ft][result_idx]
                        else:
                            # åŸå§‹searchç»“æœ
                            hits = field_search_results[ft][result_idx] if field_search_results[ft] else []
                            search_results = []
                            for hit in hits:
                                search_results.append({
                                    'document_id': hit.entity.get('document_id'),
                                    'chapter': hit.entity.get('chapter'),
                                    'similarity': hit.score,
                                    'collection_type': ft
                                })
                        
                        self._merge_results(matching_results, search_results, ft, query['document_id'])
            
            # è®¡ç®—åŠ æƒç›¸ä¼¼åº¦å¹¶æ’åº
            final_results = self._calculate_weighted_similarity(matching_results)
            all_results.append(final_results[:top_k])
        
        print(f"âœ… æ‰¹é‡æŸ¥è¯¢å®Œæˆ")
        return all_results
    
    def _search_collection_with_vector(self, collection_type: str, query_vector: List[float], top_k: int) -> List[Dict]:
        """ä½¿ç”¨é¢„è®¡ç®—çš„å‘é‡åœ¨æŒ‡å®šé›†åˆä¸­æœç´¢"""
        collection = Collection(self.collections[collection_type])
        
        # æ‰§è¡Œæœç´¢
        search_params = {"metric_type": "COSINE", "params": {"nprobe": 10}}
        results = collection.search(
            data=[query_vector],
            anns_field='vector',
            param=search_params,
            limit=top_k,
            output_fields=["document_id", "chapter"]
        )
        
        search_results = []
        for hits in results:
            for hit in hits:
                search_results.append({
                    'document_id': hit.entity.get('document_id'),
                    'chapter': hit.entity.get('chapter'),
                    'similarity': hit.score,
                    'collection_type': collection_type
                })
        
        return search_results
    
    def _search_collection(self, collection_type: str, query_text: str, top_k: int, use_reranker: bool = False) -> List[Dict]:
        """åœ¨æŒ‡å®šé›†åˆä¸­æœç´¢ï¼ˆå¯é€‰rerankerå‰ç½®ç­›é€‰ï¼‰"""
        collection = Collection(self.collections[collection_type])
        
        # ç¼–ç æŸ¥è¯¢æ–‡æœ¬
        query_vector = self.embedder.encode_batch([query_text])[0]
        
        # å‘é‡å­—æ®µå
        vector_field = 'vector'
        
        # å…ˆç”¨å‘é‡æ£€ç´¢è·å–æ›´å¤šå€™é€‰ï¼ˆå¦‚æœç”¨rerankeråˆ™æ‰©å¤§å€™é€‰æ± ï¼‰
        search_limit = top_k * 3 if use_reranker else top_k
        search_params = {"metric_type": "COSINE", "params": {"nprobe": 10}}
        
        # è·å–æ›´å¤šå­—æ®µç”¨äºreranker
        output_fields = ["document_id", "chapter"]
        if collection_type == 'parameters':
            output_fields.extend(["item", "constraint", "value", "unit"])
        elif collection_type in ['topic_keywords', 'context_keywords', 'table_headers']:
            field_name = 'keywords_text' if 'keywords' in collection_type else 'headers_text'
            output_fields.append(field_name)
        elif collection_type == 'scope':
            output_fields.append("scope_text")
        
        results = collection.search(
            data=[query_vector],
            anns_field=vector_field,
            param=search_params,
            limit=search_limit,
            output_fields=output_fields
        )
        
        search_results = []
        for hits in results:
            for hit in hits:
                result_data = {
                    'document_id': hit.entity.get('document_id'),
                    'chapter': hit.entity.get('chapter'),
                    'similarity': hit.score,
                    'collection_type': collection_type
                }
                
                # ä¿å­˜åŸå§‹å†…å®¹ç”¨äºreranker
                if collection_type == 'parameters':
                    result_data['content'] = f"{hit.entity.get('item', '')} {hit.entity.get('constraint', '')} {hit.entity.get('value', '')} {hit.entity.get('unit', '')}"
                elif collection_type == 'scope':
                    result_data['content'] = hit.entity.get('scope_text', '')
                elif collection_type in ['topic_keywords', 'context_keywords']:
                    result_data['content'] = hit.entity.get('keywords_text', '')
                elif collection_type == 'table_headers':
                    result_data['content'] = hit.entity.get('headers_text', '')
                
                search_results.append(result_data)
        
        # å¦‚æœå¯ç”¨rerankerä¸”æœ‰è¶³å¤Ÿå€™é€‰ï¼Œåˆ™è¿›è¡Œé‡æ’ï¼ˆæ‰€æœ‰å­—æ®µéƒ½ä½¿ç”¨ï¼‰
        if use_reranker and len(search_results) > top_k:
            search_results = self._apply_field_reranker(query_text, search_results, top_k)
        
        return search_results[:top_k]
    
    def _merge_results(self, matching_results: Dict, search_results: List[Dict], 
                      collection_type: str, exclude_document_id: str):
        """åˆå¹¶æœç´¢ç»“æœ"""
        for result in search_results:
            # æ’é™¤åŒä¸€æ–‡æ¡£çš„ç»“æœ
            if result['document_id'] == exclude_document_id:
                continue
                
            key = f"{result['document_id']}#{result['chapter']}"
            if key not in matching_results:
                matching_results[key] = {
                    'document_id': result['document_id'],
                    'chapter': result['chapter'],
                    'similarities': {}
                }
            
            # ä¿å­˜è¯¥ç±»å‹çš„æœ€é«˜ç›¸ä¼¼åº¦
            if (collection_type not in matching_results[key]['similarities'] or 
                result['similarity'] > matching_results[key]['similarities'][collection_type]):
                matching_results[key]['similarities'][collection_type] = result['similarity']
    
    def _calculate_weighted_similarity(self, matching_results: Dict) -> List[Dict]:
        """
        æ™ºèƒ½åŠ æƒç›¸ä¼¼åº¦è®¡ç®— - æ ¹æ®å®é™…ç›¸ä¼¼åº¦è¡¨ç°åŠ¨æ€è°ƒæ•´æƒé‡
        """
        final_results = []
        
        for key, result in matching_results.items():
            similarities = result['similarities']
            
            # ç¬¬ä¸€æ­¥ï¼šè®¡ç®—åŠ¨æ€æƒé‡
            dynamic_weights = self._calculate_dynamic_weights(similarities)
            
            # ç¬¬äºŒæ­¥ï¼šè®¡ç®—åŸºç¡€åŠ æƒåˆ†æ•°
            base_weighted_score = 0.0
            total_weight = 0.0
            
            for collection_type, similarity in similarities.items():
                weight = dynamic_weights.get(collection_type, 0.1)
                base_weighted_score += similarity * weight
                total_weight += weight
            
            # ç¬¬ä¸‰æ­¥ï¼šåº”ç”¨é«˜ä»·å€¼å­—æ®µå¥–åŠ±æœºåˆ¶
            final_score = self._apply_high_value_bonus(base_weighted_score, similarities, total_weight)
            
            # # ç¬¬å››æ­¥ï¼šåº”ç”¨ååŒæ•ˆåº”å¥–åŠ±
            # final_score = self._apply_synergy_bonus(final_score, similarities)
            
            final_results.append({
                'document_id': result['document_id'],
                'chapter': result['chapter'],
                'weighted_similarity': final_score,
                'detailed_similarities': similarities,
                'dynamic_weights': dynamic_weights,
                'has_high_value_match': any(similarities.get(field, 0) >= self.similarity_thresholds.get(field, 0.8) 
                                          for field in self.high_value_fields)
            })
        
        # æŒ‰æœ€ç»ˆåˆ†æ•°æ’åº
        final_results.sort(key=lambda x: x['weighted_similarity'], reverse=True)
        
        return final_results
    
    def _calculate_dynamic_weights(self, similarities: Dict[str, float]) -> Dict[str, float]:
        """
        æ ¹æ®ç›¸ä¼¼åº¦è¡¨ç°åŠ¨æ€è°ƒæ•´æƒé‡
        """
        dynamic_weights = self.base_weights.copy()
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é«˜ç›¸ä¼¼åº¦çš„é«˜ä»·å€¼å­—æ®µ
        high_value_boost = False
        for field in self.high_value_fields:
            if field in similarities:
                similarity = similarities[field]
                threshold = self.similarity_thresholds.get(field, 0.75)
                
                if similarity >= threshold:
                    # é«˜ç›¸ä¼¼åº¦æ—¶å¢åŠ è¯¥å­—æ®µæƒé‡
                    boost_factor = 1.0 + (similarity - threshold) * 2  # æœ€å¤§å¯å¢åŠ åˆ°2å€æƒé‡
                    dynamic_weights[field] *= boost_factor
                    high_value_boost = True
        
        # å¦‚æœæœ‰é«˜ä»·å€¼å­—æ®µè¡¨ç°ä¼˜ç§€ï¼Œé€‚å½“é™ä½å…¶ä»–å­—æ®µæƒé‡
        if high_value_boost:
            for field in dynamic_weights:
                if field not in self.high_value_fields:
                    dynamic_weights[field] *= 0.8  # è½»å¾®é™ä½å…¶ä»–å­—æ®µæƒé‡
        
        # æ ‡å‡†åŒ–æƒé‡ï¼Œç¡®ä¿æ€»å’Œåˆç†
        total_weight = sum(dynamic_weights.values())
        if total_weight > 0:
            for field in dynamic_weights:
                dynamic_weights[field] = dynamic_weights[field] / total_weight
        
        return dynamic_weights
    
    def _apply_high_value_bonus(self, base_score: float, similarities: Dict[str, float], total_weight: float) -> float:
        """
        ä¸ºé«˜ä»·å€¼å­—æ®µçš„ä¼˜ç§€è¡¨ç°æä¾›é¢å¤–å¥–åŠ±
        """
        bonus = 0.0
        
        for field in self.high_value_fields:
            if field in similarities:
                similarity = similarities[field]
                threshold = self.similarity_thresholds.get(field, 0.75)
                
                if similarity >= threshold:
                    # è®¡ç®—å¥–åŠ±ï¼šç›¸ä¼¼åº¦è¶Šé«˜ï¼Œå¥–åŠ±è¶Šå¤§
                    field_bonus = (similarity - threshold) * 0.3  # æœ€å¤§å¥–åŠ±0.3
                    bonus += field_bonus
                    
                    # ç‰¹åˆ«å¥–åŠ±ï¼šå¦‚æœå¤šä¸ªé«˜ä»·å€¼å­—æ®µéƒ½è¡¨ç°ä¼˜ç§€
                    if field == 'parameters' and 'table_headers' in similarities:
                        table_sim = similarities['table_headers']
                        table_threshold = self.similarity_thresholds.get('table_headers', 0.75)
                        if table_sim >= table_threshold:
                            # å‚æ•°å’Œè¡¨å¤´éƒ½åŒ¹é…è‰¯å¥½æ—¶çš„ååŒå¥–åŠ±
                            bonus += 0.15
        
        return base_score + bonus
    
    def _apply_synergy_bonus(self, score: float, similarities: Dict[str, float]) -> float:
        """
        åº”ç”¨å­—æ®µååŒæ•ˆåº”å¥–åŠ±
        """
        # æ£€æŸ¥å¤šå­—æ®µé«˜è´¨é‡åŒ¹é…çš„ååŒæ•ˆåº”
        high_quality_fields = 0
        total_high_sim = 0.0
        
        for field, similarity in similarities.items():
            threshold = self.similarity_thresholds.get(field, 0.75)
            if similarity >= threshold:
                high_quality_fields += 1
                total_high_sim += similarity
        
        # å¤šå­—æ®µååŒå¥–åŠ±
        if high_quality_fields >= 3:
            # 3ä¸ªæˆ–æ›´å¤šå­—æ®µéƒ½è¡¨ç°ä¼˜ç§€æ—¶çš„ååŒå¥–åŠ±
            synergy_bonus = 0.1 * (high_quality_fields - 2)  # æ¯å¤šä¸€ä¸ªé«˜è´¨é‡å­—æ®µå¢åŠ 0.1
            score += synergy_bonus
        elif high_quality_fields == 2:
            # 2ä¸ªå­—æ®µè¡¨ç°ä¼˜ç§€çš„é€‚åº¦å¥–åŠ±
            avg_high_sim = total_high_sim / high_quality_fields
            if avg_high_sim >= 0.85:  # å¹³å‡ç›¸ä¼¼åº¦å¾ˆé«˜
                score += 0.05
        
        # ç¡®ä¿åˆ†æ•°ä¸è¶…è¿‡1.0çš„åˆç†èŒƒå›´
        return min(score, 1.0)
        
        # æŒ‰åŠ æƒç›¸ä¼¼åº¦æ’åº
        final_results.sort(key=lambda x: x['weighted_similarity'], reverse=True)
        
        return final_results
    
    def _apply_reranker(self, query_data: Dict[str, Any], results: List[Dict]) -> List[Dict]:
        """
        ä½¿ç”¨BGE-Rerankerå¯¹ç»“æœè¿›è¡Œé‡æ’
        """
        if not results:
            return results
        
        # æ„å»ºæŸ¥è¯¢æ–‡æœ¬
        query_parts = []
        if query_data.get('scope'):
            query_parts.append(query_data['scope'])
        if query_data.get('topic_keywords'):
            query_parts.append(" ".join(query_data['topic_keywords']))
        if query_data.get('parameters'):
            for param in query_data['parameters']:
                param_text = f"{param.get('item', '')} {param.get('value', '')}"
                if param_text.strip():
                    query_parts.append(param_text)
        
        query_text = " ".join(query_parts)
        
        # æ„å»ºæ–‡æ¡£æ–‡æœ¬ï¼ˆç®€åŒ–è¡¨ç¤ºï¼‰
        documents = []
        for result in results:
            doc_text = f"Document: {result['document_id']} Chapter: {result['chapter']}"
            documents.append(doc_text)
        
        # è°ƒç”¨é‡æ’API
        try:
            payload = {
                "model": "BAAI/bge-reranker-v2-m3",
                "query": query_text,
                "documents": documents,
                "top_k": len(results),
                "return_documents": True
            }
            
            response = requests.post(
                f"{self.embedder.base_url}/rerank",
                headers=self.embedder.headers,
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                rerank_results = response.json().get("results", [])
                
                # æ ¹æ®é‡æ’ç»“æœé‡æ–°æ’åº
                reranked_final = []
                for rerank_item in rerank_results:
                    original_index = rerank_item.get('index', 0)
                    if 0 <= original_index < len(results):
                        result = results[original_index].copy()
                        result['rerank_score'] = rerank_item.get('relevance_score', 0.0)
                        # ç»“åˆåŸå§‹åŠ æƒåˆ†æ•°å’Œé‡æ’åˆ†æ•°
                        result['final_score'] = (result['weighted_similarity'] * 0.7 + 
                                               result['rerank_score'] * 0.3)
                        reranked_final.append(result)
                
                # æŒ‰æœ€ç»ˆåˆ†æ•°æ’åº
                reranked_final.sort(key=lambda x: x['final_score'], reverse=True)
                return reranked_final
        except Exception as e:
            logger.error(f"é‡æ’å¤±è´¥: {e}")
        
        return results

# è¾…åŠ©å‡½æ•°
def create_optimized_milvus_system(siliconflow_api_key: str):
    """åˆ›å»ºä¼˜åŒ–çš„Milvusç³»ç»Ÿå®ä¾‹"""
    MILVUS_URI = "https://in03-198809e1c756a88.serverless.aws-eu-central-1.cloud.zilliz.com"
    MILVUS_TOKEN = "922c1d2f5c907763b6db6489e9cd6d0f8258314a43c87b37ead9e429ee2c48de8a27ecb3531213809ee47705b8c3ba6bbe50a3bd"

    return OptimizedStandardClauseMatchingSystem(
        uri=MILVUS_URI,
        token=MILVUS_TOKEN,
        siliconflow_api_key=siliconflow_api_key
    )

def setup_optimized_database(system):
    """è®¾ç½®ä¼˜åŒ–çš„æ•°æ®åº“ï¼ˆæ¸…ç©ºå¹¶é‡æ–°åˆ›å»ºï¼‰"""
    try:
        system.create_collections()
        system.create_indexes() 
        system.load_collections()
        print("âœ… ä¼˜åŒ–æ•°æ®åº“è®¾ç½®å®Œæˆ")
        return True
    except Exception as e:
        print(f"âŒ æ•°æ®åº“è®¾ç½®å¤±è´¥: {e}")
        return False