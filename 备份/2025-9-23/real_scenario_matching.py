"""
çœŸå®åœºæ™¯çš„æ ‡å‡†æ¡æ¬¾åŒ¹é…
CELEXä½œä¸ºçŸ¥è¯†åº“ï¼ŒGSOä½œä¸ºæŸ¥è¯¢æºè¿›è¡ŒåŒ¹é…
"""

import json
import os
# å¯¼å…¥ä¼˜åŒ–ç‰ˆæœ¬çš„Milvusæ“ä½œæ¨¡å—
from milvus_op_optimized import create_optimized_milvus_system, setup_optimized_database
from typing import List, Dict, Any

# æ‚¨çš„SiliconFlow APIå¯†é’¥
SILICONFLOW_API_KEY = "sk-kgrvjrxrrnokraizppuidrfjucnlqcbynmnzskrlselyfuev"

def load_json_file(file_path: str) -> List[Dict]:
    """åŠ è½½JSONæ–‡ä»¶"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        print(f"âœ… æˆåŠŸåŠ è½½æ–‡ä»¶: {file_path}")
        return data
    except Exception as e:
        print(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {e}")
        return []

def extract_all_chapters_recursive(chapter_data: Dict, file_name: str, section_name: str, document_prefix: str) -> List[Dict]:
    """é€’å½’æå–æ‰€æœ‰ç« èŠ‚ä¿¡æ¯ï¼ŒåŒ…æ‹¬childrenä¸­çš„å­ç« èŠ‚"""
    chapters = []
    
    chapter_id = chapter_data.get('chapter_id', '')
    chapter_title = chapter_data.get('chapter_title', '')
    raw_text = chapter_data.get('raw_text', '')
    full_path = chapter_data.get('full_path', '')
    
    # æ„å»ºæ–‡æ¡£ID
    document_id = f"{document_prefix}_{file_name}_{section_name}"
    
    # æå–æ¡æ¬¾æ•°æ®
    clause_data = {
        "scope": chapter_data.get('scope', ''),
        "parameters": chapter_data.get('parameters', []),
        "topic_keywords": chapter_data.get('topic_keywords', []),
        "context_keywords": chapter_data.get('context_keywords', []),
        "table_headers": chapter_data.get('table_headers', [])
    }
    
    # æ·»åŠ åŸå§‹æ–‡æœ¬åˆ°scopeä¸­ä»¥å¢åŠ è¯­ä¹‰ä¿¡æ¯
    
    # if raw_text:
    #     if clause_data['scope']:
    #         clause_data['scope'] += f" | {raw_text[:200]}"  # é™åˆ¶é•¿åº¦
    #     else:
    #         clause_data['scope'] = raw_text[:200]
    
    # # å¦‚æœchapter_titleæ²¡æœ‰åœ¨scopeä¸­ï¼Œå°†å…¶æ·»åŠ åˆ°topic_keywords
    # if chapter_title and chapter_title not in str(clause_data['scope']):
    #     if not clause_data['topic_keywords']:
    #         clause_data['topic_keywords'] = []
    #     if isinstance(clause_data['topic_keywords'], list):
    #         clause_data['topic_keywords'].append(chapter_title)
    
    # æ·»åŠ å½“å‰ç« èŠ‚
    current_chapter = {
        'document_id': document_id,
        'file': file_name,
        'section': section_name,
        'chapter_id': chapter_id,
        'chapter_title': chapter_title,
        'full_path': full_path,
        'raw_text': raw_text,
        'clause_data': clause_data
    }
    chapters.append(current_chapter)
    
    # é€’å½’å¤„ç†children
    children = chapter_data.get('children', [])
    for child in children:
        child_chapters = extract_all_chapters_recursive(child, file_name, section_name, document_prefix)
        chapters.extend(child_chapters)
    
    return chapters

def extract_chapters_from_json(data: List[Dict], document_prefix: str) -> List[Dict]:
    """ä»JSONæ•°æ®ä¸­é€’å½’æå–æ‰€æœ‰ç« èŠ‚ä¿¡æ¯"""
    all_chapters = []

    section = data[0].get('sections', [])[0] if data and 'sections' in data[0] and data[0]['sections'] else {}

    file_name = data[0].get('file', 'unknown_file')
    section_name = section.get('section', 'MAIN')
    section_chapters = section.get('chapters', [])
    
    # é€’å½’å¤„ç†æ¯ä¸ªç« èŠ‚
    for chapter in section_chapters:
        chapter_list = extract_all_chapters_recursive(chapter, file_name, section_name, document_prefix)
        all_chapters.extend(chapter_list)
    
    # å­˜å‚¨æå–åˆ°çš„all_chaptersä»¥ä¾¿è°ƒè¯•
    with open('extracted_chapters_debug.json', 'w', encoding='utf-8') as debug_file:
        json.dump(all_chapters, debug_file, indent=2, ensure_ascii=False)

    return all_chapters

def import_celex_knowledge_base_optimized(milvus_system, right_file_path: str):
    """ä¼˜åŒ–ç‰ˆæœ¬ï¼šæ‰¹é‡å¯¼å…¥CELEXæ–‡ä»¶ä½œä¸ºçŸ¥è¯†åº“"""
    print("=== æ‰¹é‡å¯¼å…¥CELEXçŸ¥è¯†åº“ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰===")
    
    # åŠ è½½CELEXæ–‡ä»¶
    celex_data = load_json_file(right_file_path)
    if not celex_data:
        return False
    
    # é€’å½’æå–æ‰€æœ‰ç« èŠ‚
    celex_chapters = extract_chapters_from_json(celex_data, "CELEX")
    print(f"ğŸ“š ä»CELEXæ–‡ä»¶ä¸­é€’å½’æå–äº† {len(celex_chapters)} ä¸ªç« èŠ‚")
    
    # å‡†å¤‡æ‰¹é‡æ’å…¥çš„æ•°æ®
    chapters_for_batch_insert = []
    
    for i, chapter in enumerate(celex_chapters):
        try:
            # è·³è¿‡æ²¡æœ‰å®è´¨å†…å®¹çš„ç« èŠ‚
            clause_data = chapter['clause_data']
            if (not clause_data['scope'] and 
                not clause_data['parameters'] and 
                not clause_data['topic_keywords']):
                continue
            
            # æ„å»ºç« èŠ‚æ ‡è¯†ç¬¦
            chapter_identifier = f"{chapter['chapter_id']}_{chapter['chapter_title']}"[:50]
            if not chapter_identifier:
                chapter_identifier = f"ch_{i}"
            
            # æ·»åŠ åˆ°æ‰¹é‡æ’å…¥åˆ—è¡¨
            chapters_for_batch_insert.append({
                'document_id': chapter['document_id'],
                'chapter': chapter_identifier,
                'clause_data': clause_data
            })
                
        except Exception as e:
            print(f"âš ï¸  å‡†å¤‡ç« èŠ‚æ•°æ®å¤±è´¥: {chapter['chapter_id']}, é”™è¯¯: {e}")
            continue
    
    print(f"ğŸ“¦ å‡†å¤‡æ‰¹é‡æ’å…¥ {len(chapters_for_batch_insert)} ä¸ªæœ‰æ•ˆç« èŠ‚")
    
    # æ‰§è¡Œæ‰¹é‡æ’å…¥
    try:
        milvus_system.batch_insert_clause_data(chapters_for_batch_insert)
        print(f"âœ… CELEXçŸ¥è¯†åº“æ‰¹é‡å¯¼å…¥å®Œæˆï¼ŒæˆåŠŸå¯¼å…¥ {len(chapters_for_batch_insert)} ä¸ªç« èŠ‚")
        return True
    except Exception as e:
        print(f"âŒ æ‰¹é‡æ’å…¥å¤±è´¥: {e}")
        return False

def query_with_gso_optimized(milvus_system, left_file_path: str, top_k: int = 5):
    """ä½¿ç”¨GSOæ–‡ä»¶è¿›è¡Œæ‰¹é‡æŸ¥è¯¢åŒ¹é… - ä¼˜åŒ–ç‰ˆæœ¬"""
    print("\n=== ä½¿ç”¨GSOè¿›è¡Œæ‰¹é‡æŸ¥è¯¢åŒ¹é…ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰===")
    
    # åŠ è½½GSOæ–‡ä»¶
    gso_data = load_json_file(left_file_path)
    if not gso_data:
        return []
    
    # é€’å½’æå–æ‰€æœ‰GSOç« èŠ‚
    gso_chapters = extract_chapters_from_json(gso_data, "GSO")
    print(f"ğŸ” ä»GSOæ–‡ä»¶ä¸­é€’å½’æå–äº† {len(gso_chapters)} ä¸ªæŸ¥è¯¢ç« èŠ‚")
    
    # è¿‡æ»¤æœ‰æ•ˆç« èŠ‚å¹¶å‡†å¤‡æ‰¹é‡æŸ¥è¯¢æ•°æ®
    valid_queries = []
    chapter_index_mapping = []  # è®°å½•æœ‰æ•ˆç« èŠ‚åœ¨åŸåˆ—è¡¨ä¸­çš„ç´¢å¼•
    
    for i, gso_chapter in enumerate(gso_chapters):
        clause_data = gso_chapter['clause_data']
        
        # è·³è¿‡æ²¡æœ‰å®è´¨å†…å®¹çš„ç« èŠ‚
        if (not clause_data['scope'] and 
            not clause_data['parameters'] and 
            not clause_data['topic_keywords']):
            continue
        
        valid_queries.append({
            'document_id': gso_chapter['document_id'],
            'chapter': gso_chapter['chapter_id'], 
            'clause_data': clause_data
        })
        chapter_index_mapping.append(i)  # è®°å½•åŸå§‹ç´¢å¼•
    
    print(f"ğŸ“¦ å‡†å¤‡æ‰¹é‡æŸ¥è¯¢ {len(valid_queries)} ä¸ªæœ‰æ•ˆç« èŠ‚")
    
    if not valid_queries:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„æŸ¥è¯¢ç« èŠ‚")
        return []
    
    # æ‰§è¡Œæ‰¹é‡æŸ¥è¯¢
    batch_results = milvus_system.batch_find_matching_clauses(valid_queries, top_k=top_k, use_reranker=True)
    
    # æ„å»ºæœ€ç»ˆç»“æœï¼ˆæŒ‰ç”¨æˆ·è¦æ±‚çš„æ ¼å¼ï¼‰
    all_matches = []
    for i, (original_index, query_results) in enumerate(zip(chapter_index_mapping, batch_results)):
        gso_chapter = gso_chapters[original_index]
        
        # æ„å»ºå€™é€‰åˆ—è¡¨
        candidates = []
        if query_results:
            print(f"âœ… ç« èŠ‚ {gso_chapter['chapter_id']} æ‰¾åˆ° {len(query_results)} ä¸ªåŒ¹é…é¡¹")
            for j, match in enumerate(query_results, 1):
                # ä»document_idä¸­è§£æå‡ºfileä¿¡æ¯
                doc_parts = match['document_id'].split('_')
                file_name = doc_parts[1] if len(doc_parts) > 1 else 'regulation'
                section_name = doc_parts[2] if len(doc_parts) > 2 else 'MAIN'
                
                # ä½¿ç”¨final_scoreä½œä¸ºscoreï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨weighted_similarity
                score = match.get('final_score', match.get('weighted_similarity', 0))
                
                candidate = {
                    "score": score,
                    "chapter": [
                        file_name,
                        section_name,
                        match['chapter']
                    ],
                    "similarities": match.get("detailed_similarities", {})
                }
                candidates.append(candidate)
        else:
            print(f"âŒ ç« èŠ‚ {gso_chapter['chapter_id']} æœªæ‰¾åˆ°åŒ¹é…é¡¹")
        
        # æ„å»ºå½“å‰ç« èŠ‚çš„æŸ¥è¯¢ç»“æœ
        query_result = {
            "file": gso_chapter['file'],
            "section": gso_chapter['section'], 
            "chapter_id": gso_chapter['chapter_id'],
            "candidates": candidates
        }
        
        all_matches.append(query_result)
    
    print(f"\nâœ… æ‰¹é‡æŸ¥è¯¢å®Œæˆï¼Œå¤„ç†äº† {len(all_matches)} ä¸ªç« èŠ‚")
    return all_matches

def save_matching_results(matches: List[Dict], output_file: str):
    """ä¿å­˜åŒ¹é…ç»“æœåˆ°æ–‡ä»¶ï¼ˆç”¨æˆ·æœŸæœ›çš„æ ¼å¼ï¼‰"""
    print(f"\n=== ä¿å­˜åŒ¹é…ç»“æœåˆ° {output_file} ===")

    try:
        # 1. è‡ªåŠ¨åˆ›å»ºç¼ºå¤±ç›®å½•
        os.makedirs(os.path.dirname(output_file), exist_ok=True)

        # 2. å†å†™æ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(matches, f, indent=2, ensure_ascii=False)

        print(f"âœ… åŒ¹é…ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        print(f"ğŸ“Š æ€»è®¡ä¿å­˜äº† {len(matches)} ä¸ªæŸ¥è¯¢ç« èŠ‚çš„åŒ¹é…ç»“æœ")

    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±è´¥: {e}")

def print_summary_statistics(matches: List[Dict]):
    """æ‰“å°åŒ¹é…ç»Ÿè®¡ä¿¡æ¯ï¼ˆé€‚é…æ–°æ ¼å¼ï¼‰"""
    print("\n=== åŒ¹é…ç»Ÿè®¡æ‘˜è¦ ===")
    
    if not matches:
        print("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•åŒ¹é…ç»“æœ")
        return
    
    total_queries = len(matches)
    queries_with_matches = len([m for m in matches if m.get('candidates', [])])
    total_matches = sum(len(m.get('candidates', [])) for m in matches)
    
    print(f"ğŸ“Š æŸ¥è¯¢ç»Ÿè®¡:")
    print(f"   - æ€»æŸ¥è¯¢ç« èŠ‚æ•°: {total_queries}")
    print(f"   - æœ‰åŒ¹é…çš„æŸ¥è¯¢: {queries_with_matches}")
    print(f"   - æ€»å€™é€‰åŒ¹é…æ•°: {total_matches}")
    print(f"   - å¹³å‡æ¯æŸ¥è¯¢å€™é€‰æ•°: {total_matches/total_queries:.2f}")
    
    # åˆ†æ•°åˆ†å¸ƒç»Ÿè®¡
    all_scores = []
    for match in matches:
        for candidate in match.get('candidates', []):
            all_scores.append(candidate.get('score', 0))
    
    if all_scores:
        import statistics
        print(f"\nğŸ“ˆ å€™é€‰åˆ†æ•°åˆ†å¸ƒ:")
        print(f"   - æœ€é«˜åˆ†æ•°: {max(all_scores):.4f}")
        print(f"   - æœ€ä½åˆ†æ•°: {min(all_scores):.4f}")
        print(f"   - å¹³å‡åˆ†æ•°: {statistics.mean(all_scores):.4f}")
        print(f"   - ä¸­ä½æ•°åˆ†æ•°: {statistics.median(all_scores):.4f}")
        
        # åˆ†æ•°è´¨é‡åˆ†å¸ƒ
        high_quality = [s for s in all_scores if s > 0.7]
        medium_quality = [s for s in all_scores if 0.5 <= s <= 0.7]
        low_quality = [s for s in all_scores if s < 0.5]
        
        print(f"\nğŸ¯ åŒ¹é…è´¨é‡åˆ†å¸ƒ:")
        print(f"   - é«˜è´¨é‡åŒ¹é… (>0.7): {len(high_quality)} ä¸ª")
        print(f"   - ä¸­ç­‰è´¨é‡åŒ¹é… (0.5-0.7): {len(medium_quality)} ä¸ª")
        print(f"   - ä½è´¨é‡åŒ¹é… (<0.5): {len(low_quality)} ä¸ª")
    
    # æŒ‰ç« èŠ‚ç±»å‹ç»Ÿè®¡
    chapter_types = {}
    for match in matches:
        chapter_id = match.get('chapter_id', '')
        if '-' in chapter_id:
            chapter_type = chapter_id.split('-')[0] + '-'
        elif '.' in chapter_id:
            chapter_type = chapter_id.split('.')[0] + '.'
        else:
            chapter_type = 'other'
        
        if chapter_type not in chapter_types:
            chapter_types[chapter_type] = 0
        chapter_types[chapter_type] += 1
    
    print(f"\nğŸ“‚ ç« èŠ‚ç±»å‹åˆ†å¸ƒ:")
    for ch_type, count in sorted(chapter_types.items()):
        print(f"   - {ch_type}: {count} ä¸ªç« èŠ‚")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ çœŸå®åœºæ™¯æ ‡å‡†æ¡æ¬¾åŒ¹é…ç³»ç»Ÿ")
    print("=" * 60)
    
    # left_file = r"D:\Documents\çŸ¥è¯†å›¾è°±agent\ç¤ºä¾‹æ–‡ä»¶\temp\F2023C00147\F2023C00147.json"
    # right_file = r"D:\Documents\çŸ¥è¯†å›¾è°±agent\ç¤ºä¾‹æ–‡ä»¶\temp\R048r12e\R048r12e.json"

    # æ–‡ä»¶è·¯å¾„
    left_file = r"D:\æ±½è½¦æ£€æµ‹çŸ¥è¯†å›¾è°±Agent\æ±½è½¦æ£€æµ‹çŸ¥è¯†å›¾è°±Agent\ç¤ºä¾‹æ–‡ä»¶\temp\F2023C00147\F2023C00147.json"
    right_file = r"D:\æ±½è½¦æ£€æµ‹çŸ¥è¯†å›¾è°±Agent\æ±½è½¦æ£€æµ‹çŸ¥è¯†å›¾è°±Agent\ç¤ºä¾‹æ–‡ä»¶\temp\R048r12e\R048r12e.json"
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(right_file):
        print(f"âŒ CELEXæ–‡ä»¶ä¸å­˜åœ¨: {right_file}")
        return
    
    if not os.path.exists(left_file):
        print(f"âŒ GSOæ–‡ä»¶ä¸å­˜åœ¨: {left_file}")
        return
    
    try:
        # 1. åˆ›å»ºä¼˜åŒ–çš„ç³»ç»Ÿå®ä¾‹
        print("1. åˆ›å»ºä¼˜åŒ–çš„ç³»ç»Ÿå®ä¾‹...")
        milvus_system = create_optimized_milvus_system(SILICONFLOW_API_KEY)
        
        # 2. åˆå§‹åŒ–æ•°æ®åº“ï¼ˆæ¸…ç©ºä¹‹å‰çš„æ•°æ®ï¼‰
        print("2. åˆå§‹åŒ–æ•°æ®åº“...")
        setup_optimized_database(milvus_system)
        
        # 3. æ‰¹é‡å¯¼å…¥CELEXçŸ¥è¯†åº“ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
        success = import_celex_knowledge_base_optimized(milvus_system, right_file)
        if not success:
            print("âŒ CELEXçŸ¥è¯†åº“å¯¼å…¥å¤±è´¥")
            return
        
        # 4. ä½¿ç”¨GSOè¿›è¡Œæ‰¹é‡æŸ¥è¯¢åŒ¹é…ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
        matches = query_with_gso_optimized(milvus_system, left_file, top_k=10)
        
        # 5. ä¿å­˜åŒ¹é…ç»“æœ
        output_file = "./match_result/ADR60 VS ECER7.json"
        # output_file = "./match_result/GSO VS ECE.json"
        # output_file = "./match_result/GSO.json"
        save_matching_results(matches, output_file)
        
        # 6. æ‰“å°ç»Ÿè®¡æ‘˜è¦
        print_summary_statistics(matches)
        
        print("\nğŸ‰ åŒ¹é…å®Œæˆï¼")
        print(f"ğŸ“„ è¯¦ç»†ç»“æœè¯·æŸ¥çœ‹: {output_file}")
        print(f"ğŸ“‹ ç»“æœæ ¼å¼ï¼šæ¯ä¸ªGSOç« èŠ‚éƒ½åŒ…å«fileã€sectionã€chapter_idå’Œcandidatesï¼ˆtop5åŒ¹é…ï¼‰")
        
    except Exception as e:
        print(f"âŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()