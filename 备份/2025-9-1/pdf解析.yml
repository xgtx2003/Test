app:
  description: 'ä¿ç•™å¼•ç”¨ã€é¢å‘ä¸¤ä¸ªåœºæ™¯

    '
  icon: ğŸ¤–
  icon_background: '#FFEAD5'
  mode: advanced-chat
  name: pdfè§£æ
  use_icon_as_answer_icon: false
dependencies:
- current_identifier: null
  type: marketplace
  value:
    marketplace_plugin_unique_identifier: langgenius/deepseek:0.0.6@dd589dc093c8084925858034ab5ec1fdf0d33819f43226c2f8c4a749a9acbbb2
kind: app
version: 0.3.0
workflow:
  conversation_variables: []
  environment_variables: []
  features:
    file_upload:
      allowed_file_extensions:
      - .JPG
      - .JPEG
      - .PNG
      - .GIF
      - .WEBP
      - .SVG
      allowed_file_types:
      - image
      allowed_file_upload_methods:
      - local_file
      - remote_url
      enabled: false
      fileUploadConfig:
        audio_file_size_limit: 50
        batch_count_limit: 5
        file_size_limit: 15
        image_file_size_limit: 10
        video_file_size_limit: 100
        workflow_file_upload_limit: 10
      image:
        enabled: false
        number_limits: 3
        transfer_methods:
        - local_file
        - remote_url
      number_limits: 3
    opening_statement: ''
    retriever_resource:
      enabled: true
    sensitive_word_avoidance:
      enabled: false
    speech_to_text:
      enabled: false
    suggested_questions: []
    suggested_questions_after_answer:
      enabled: false
    text_to_speech:
      enabled: false
      language: ''
      voice: ''
  graph:
    edges:
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '1756557801310'
        sourceType: iteration-start
        targetType: llm
      id: 1756557801310start-source-1756557804693-target
      source: 1756557801310start
      sourceHandle: source
      target: '1756557804693'
      targetHandle: target
      type: custom
      zIndex: 1002
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '1756557801310'
        sourceType: llm
        targetType: code
      id: 1756557804693-source-1756557929661-target
      source: '1756557804693'
      sourceHandle: source
      target: '1756557929661'
      targetHandle: target
      type: custom
      zIndex: 1002
    - data:
        isInLoop: false
        sourceType: iteration
        targetType: code
      id: 1756557801310-source-1756563307317-target
      source: '1756557801310'
      sourceHandle: source
      target: '1756563307317'
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInLoop: false
        sourceType: start
        targetType: code
      id: 1756550268945-source-1756550411122-target
      source: '1756550268945'
      sourceHandle: source
      target: '1756550411122'
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '1756613329065'
        sourceType: iteration-start
        targetType: llm
      id: 1756613329065start-source-1756613344845-target
      source: 1756613329065start
      sourceHandle: source
      target: '1756613344845'
      targetHandle: target
      type: custom
      zIndex: 1002
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '1756613329065'
        sourceType: llm
        targetType: code
      id: 1756613344845-source-1756613456815-target
      source: '1756613344845'
      sourceHandle: source
      target: '1756613456815'
      targetHandle: target
      type: custom
      zIndex: 1002
    - data:
        isInIteration: false
        isInLoop: false
        sourceType: iteration
        targetType: code
      id: 1756613329065-source-1756618044071-target
      source: '1756613329065'
      sourceHandle: source
      target: '1756618044071'
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInIteration: false
        isInLoop: false
        sourceType: code
        targetType: code
      id: 1756618044071-source-1756629493937-target
      source: '1756618044071'
      sourceHandle: source
      target: '1756629493937'
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInIteration: false
        isInLoop: false
        sourceType: code
        targetType: answer
      id: 1756629493937-source-1756558073574-target
      source: '1756629493937'
      sourceHandle: source
      target: '1756558073574'
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInLoop: false
        sourceType: code
        targetType: iteration
      id: 1756550411122-source-1756557801310-target
      source: '1756550411122'
      sourceHandle: source
      target: '1756557801310'
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInLoop: false
        sourceType: code
        targetType: iteration
      id: 1756563307317-source-1756613329065-target
      source: '1756563307317'
      sourceHandle: source
      target: '1756613329065'
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '1756557801310'
        sourceType: code
        targetType: llm
      id: 1756557929661-fail-branch-1756698267184-target
      source: '1756557929661'
      sourceHandle: fail-branch
      target: '1756698267184'
      targetHandle: target
      type: custom
      zIndex: 1002
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '1756557801310'
        sourceType: code
        targetType: variable-aggregator
      id: 1756557929661-source-1756698812908-target
      source: '1756557929661'
      sourceHandle: source
      target: '1756698812908'
      targetHandle: target
      type: custom
      zIndex: 1002
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '1756557801310'
        sourceType: llm
        targetType: code
      id: 1756698267184-source-1756698848564-target
      source: '1756698267184'
      sourceHandle: source
      target: '1756698848564'
      targetHandle: target
      type: custom
      zIndex: 1002
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '1756557801310'
        sourceType: code
        targetType: variable-aggregator
      id: 1756698848564-source-1756698812908-target
      source: '1756698848564'
      sourceHandle: source
      target: '1756698812908'
      targetHandle: target
      type: custom
      zIndex: 1002
    nodes:
    - data:
        desc: ''
        selected: false
        title: å¼€å§‹
        type: start
        variables:
        - allowed_file_extensions: []
          allowed_file_types:
          - document
          allowed_file_upload_methods:
          - local_file
          - remote_url
          label: file
          max_length: 48
          options: []
          required: false
          type: file
          variable: file
        - label: context
          max_length: 100000
          options: []
          required: false
          type: paragraph
          variable: context
        - label: array
          max_length: 1000000
          options: []
          required: false
          type: paragraph
          variable: array
        - label: regulation
          max_length: 1000000
          options: []
          required: false
          type: paragraph
          variable: regulation
      height: 167
      id: '1756550268945'
      position:
        x: 30
        y: 459
      positionAbsolute:
        x: 30
        y: 459
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        answer: '{{#1756550411122.tree#}}







          {{#1756550411122.context#}}





          {{#1756550411122.array#}}'
        desc: ''
        selected: false
        title: ç›´æ¥å›å¤
        type: answer
        variables: []
      height: 122
      id: answer
      position:
        x: 30
        y: 666
      positionAbsolute:
        x: 30
        y: 666
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        code: "import fitz  # PyMuPDF\nimport re\nimport json\nimport os\nfrom typing\
          \ import List, Dict, Tuple\nfrom collections import defaultdict\n\n# chapter_patterns\
          \ = [\n#     re.compile(r'^(é™„\\s*å½•\\s*[A-Z])\\s+(.+)$'),\n#     re.compile(r'^([A-Z](?:\\\
          .\\d+)+)\\s+(.+)$'),\n#     re.compile(r'^(\\d+(?:\\.\\d+)*)(\\s+)(.+)$'),\n\
          # ]\n\n# chapter_patterns = [\n#     re.compile(r'^(APPENDIX\\s+[A-Z0-9]+)$',\
          \ re.I),          # APPENDIX A / APPENDIX 1\n#     re.compile(r'^([A-Z](?:\\\
          .\\d+)+)\\s+(.+)$'),\n#     re.compile(r'^(\\d+(?:\\.\\d+)*\\.?)\\s+(.+)$'),\
          \                     # 1.1. Title\n# ]\n\n# æ—§çš„ç« èŠ‚æ¨¡å¼ï¼ˆå·²æ³¨é‡Šï¼‰\n# chapter_patterns\
          \ = [\n#     re.compile(r'^(é™„\\s*å½•\\s*[A-Z0-9])$'), # é™„ å½• B\n#     re.compile(r'^((APPENDIX|ANNEX|ATTACHMENT)\\\
          s+(?:[A-Z0-9]+|\\([A-Z0-9]+\\)))$', re.I),  # ANNEX A / ANNEX 1\n#     re.compile(r'^([A-Z]\\\
          .)\\s+(.+)$'),                                # A. Title (å•ç‹¬å­—æ¯ç« èŠ‚)\n#   \
          \  re.compile(r'^([A-Z](?:\\.\\d+)+\\.?)\\s+(.+)$'),                   \
          \  # A.1. Title / A.1.1. Title\n#     re.compile(r'^(\\d+(?:\\.\\d+)*\\\
          .?)\\s+(.+)$'),                       # 1.1. Title\n#     re.compile(r'^(\\\
          d+(?:-\\d+)*-)\\s+(.+)$'),                          # 1- Title / 1-2- Title\n\
          # ]\n\n# æ–°çš„åˆå¹¶åçš„ç« èŠ‚æ¨¡å¼\nchapter_patterns = [\n    # 1. ä¸­æ–‡é™„å½•ï¼šé™„å½•A, é™„ å½• B\n  \
          \  re.compile(r'^(é™„\\s*å½•\\s*[A-Z0-9])$'),\n    \n    # 2. è‹±æ–‡é™„å½•ï¼šAPPENDIX\
          \ A, ANNEX A, ATTACHMENT A\n    re.compile(r'^((APPENDIX|ANNEX|ATTACHMENT)\\\
          s+(?:[A-Z0-9]+|\\([A-Z0-9]+\\)))$', re.I),\n    \n    # 3. å­—æ¯ç« èŠ‚ï¼ˆæ”¯æŒç‚¹å’Œæ¨ªçº¿åˆ†éš”ç¬¦ï¼‰ï¼šA.\
          \ Title, A.1. Title, A-1- Title\n    re.compile(r'^([A-Z](?:[.\\-]\\d+)*[.\\\
          -]?)\\s+(.+)$'),\n    \n    # 4. æ•°å­—ç« èŠ‚ï¼ˆæ”¯æŒç‚¹å’Œæ¨ªçº¿åˆ†éš”ç¬¦ï¼‰ï¼š1. Title, 1.1. Title, 1-\
          \ Title, 1-2- Title\n    re.compile(r'^(\\d+(?:[.\\-]\\d+)*[.\\-]?)\\s+(.+)$'),\n\
          ]\n\ndef detect_document_language(lines: List[str]) -> str:\n    \"\"\"\n\
          \    æ£€æµ‹æ–‡æ¡£è¯­è¨€ï¼šä¸­æ–‡æˆ–è‹±æ–‡\n    :param lines: æ–‡æ¡£çš„æ‰€æœ‰è¡Œ\n    :return: 'zh' è¡¨ç¤ºä¸­æ–‡ï¼Œ'en'\
          \ è¡¨ç¤ºè‹±æ–‡\n    \"\"\"\n    chinese_char_count = 0\n    total_chars = 0\n  \
          \  \n    # é‡‡æ ·å‰1000è¡Œæˆ–å…¨éƒ¨è¡Œ\n    sample_lines = lines[:1000] if len(lines) >\
          \ 1000 else lines\n    \n    for line in sample_lines:\n        for char\
          \ in line:\n            total_chars += 1\n            if '\\u4e00' <= char\
          \ <= '\\u9fff':  # ä¸­æ–‡å­—ç¬¦\n                chinese_char_count += 1\n    \n\
          \    # åªè¦æœ‰ä¸­æ–‡å­—ç¬¦å°±è®¤ä¸ºæ˜¯ä¸­æ–‡æ–‡æ¡£\n    if chinese_char_count > 0:\n        return 'zh'\n\
          \    else:\n        return 'en'\n\n# ä¸­æ–‡ç« èŠ‚max_chapter_num=50\n# å…¨æ–‡é¦–å…ˆæ£€æµ‹æ˜¯ä¸­æ–‡è¿˜æ˜¯è‹±æ–‡\n\
          def detect_chapter(line: str, max_chapter_num=1000, language='en', number_analysis=None):\n\
          \    clean_line = line.strip()\n    if not clean_line:\n        return None\n\
          \n    for pattern in chapter_patterns:\n        m = pattern.match(clean_line)\n\
          \        if m:\n            chapter_id = m.group(1).strip()\n          \
          \  chapter_title = m.group(len(m.groups())).strip() if m.group(len(m.groups()))\
          \ else \"\"\n            if re.match(r'^(é™„\\s*å½•\\s*[A-Z0-9])$', chapter_id):\n\
          \                # å»æ‰ä¸­é—´çš„ç©ºæ ¼\n                chapter_id = chapter_id.replace(\"\
          \ \", \"\")\n                # chapter_id = chapter_id[-1]\n           \
          \ # ---- åŸºç¡€è¿‡æ»¤ ----\n            first_num = None\n            if chapter_id.upper().startswith(\"\
          APPENDIX\"):\n                suffix = chapter_id[len(\"APPENDIX\"):].strip(\"\
          \ ()\")\n                if suffix.isdigit():\n                    first_num\
          \ = int(suffix)\n            else:\n                m_num = re.match(r'^(\\\
          d+)', chapter_id)\n                if m_num:\n                    first_num\
          \ = int(m_num.group(1))\n\n            if first_num is not None and number_analysis\
          \ is not None:\n                # ä½¿ç”¨æ™ºèƒ½æ•°å­—èŒƒå›´åˆ¤æ–­\n                min_reasonable\
          \ = number_analysis.get(\"min_reasonable\", 1)\n                max_reasonable\
          \ = number_analysis.get(\"max_reasonable\", max_chapter_num)\n         \
          \       \n                # ç‰¹æ®Šå¤„ç†æ³•è§„ç¼–å·æ¨¡å¼\n                if number_analysis.get(\"\
          regulation_mode\", False):\n                    regulation_number = number_analysis.get(\"\
          regulation_number\")\n                    if first_num != regulation_number:\n\
          \                        return None  # ä¸æ˜¯æ³•è§„ç¼–å·ï¼Œè¿‡æ»¤æ‰\n                else:\n\
          \                    # æ­£å¸¸ç« èŠ‚ç¼–å·èŒƒå›´æ£€æŸ¥\n                    if first_num < min_reasonable\
          \ or first_num > max_reasonable:\n                        return None  #\
          \ æ•°å­—èŒƒå›´ä¸åˆç†\n            elif first_num is not None:\n                # å…œåº•é€»è¾‘ï¼šä½¿ç”¨ä¼ ç»Ÿçš„max_chapter_num\n\
          \                if first_num < 1 or first_num > max_chapter_num:\n    \
          \                return None  # æ•°å­—èŒƒå›´ä¸åˆç†\n\n            # ---- å†…å®¹ç‰¹å¾è¿‡æ»¤ ----\n\
          \            # 1) æ ‡é¢˜å¿…é¡»åŒ…å«å­—æ¯æˆ–ä¸­æ–‡\n            if not re.search(r'[A-Za-z\\\
          u4e00-\\u9fff]', chapter_title):\n                return None\n\n      \
          \      # 2) å»æ‰çº¯æ•°å­—è¡¨æ ¼è¡Œ\n            if re.fullmatch(r'[\\d\\s\\.\\-]+', chapter_title):\n\
          \                return None\n\n            # 3) è¡¨æ ¼å†…å®¹è¿‡æ»¤ - æ£€æµ‹æ˜æ˜¾çš„è¡¨æ ¼æ•°æ®æ¨¡å¼\n\
          \            # å¦‚æœæ ‡é¢˜åŒ…å«å¤§é‡æ•°å­—ã€ç©ºæ ¼å’Œå°‘é‡å­—æ¯çš„ç»„åˆï¼Œå¯èƒ½æ˜¯è¡¨æ ¼æ•°æ®\n            if re.search(r'^\\\
          d+\\s+\\d+.*[A-Z]\\s+\\d+\\s+\\d+', chapter_title):  # å¦‚ \"10 0 E 0 16\"\
          \n                return None\n            \n            # æ£€æµ‹è¡¨æ ¼è¡Œæ¨¡å¼ï¼šå•ä¸ªå­—æ¯\
          \ + æ•°å­—ç»„åˆ\n            if re.fullmatch(r'[A-Z]\\s*\\d+.*', chapter_title)\
          \ and len(chapter_title.split()) >= 3:\n                # å¦‚æœæ ‡é¢˜æ˜¯ \"A 10 0\"\
          \ è¿™æ ·çš„æ ¼å¼ï¼Œå¾ˆå¯èƒ½æ˜¯è¡¨æ ¼æ•°æ®\n                parts = chapter_title.split()\n      \
          \          if len(parts) >= 3 and all(part.isdigit() or part in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\
          \ for part in parts[:3]):\n                    return None\n\n         \
          \   # 4) æ£€æµ‹åæ ‡ç‚¹æˆ–å‚æ•°è¡¨æ ¼ï¼šå¦‚ \"A15 0 E 0 3\"\n            if re.search(r'^[A-Z]\\\
          d+\\s+\\d+\\s+[A-Z]\\s+\\d+\\s+\\d+', chapter_title):\n                return\
          \ None\n\n            # 5) è¡Œå¤ªçŸ­\n            if len(clean_line) < 4 and not\
          \ chapter_id.upper().startswith(\"APPENDIX\") and not chapter_id.startswith(\"\
          é™„å½•\"):\n                return None\n\n            # 6) è¿‡æ»¤æ˜æ˜¾çš„è¡¨æ ¼æ ‡é¢˜ç»„åˆ\n  \
          \          if len(chapter_id) == 1 and chapter_id.isupper():\n         \
          \       # å•ä¸ªå¤§å†™å­—æ¯ä½œä¸ºç« èŠ‚IDï¼Œæ£€æŸ¥æ ‡é¢˜æ˜¯å¦åƒè¡¨æ ¼æ•°æ®\n                if re.search(r'\\d+.*\\\
          d+', chapter_title) and len(chapter_title.split()) <= 6:\n             \
          \       return None\n\n            return {\n                \"chapter_id\"\
          : chapter_id,\n                \"chapter_title\": chapter_title\n      \
          \      }\n\n    return None\n\ndef build_tree(chapter_list: List[Dict])\
          \ -> List[Dict]:\n    id_map = {}\n    root = []\n\n    # å…ˆæ³¨å†Œæ‰€æœ‰èŠ‚ç‚¹\n    for\
          \ chap in chapter_list:\n        chap[\"children\"] = []\n        # ç»Ÿä¸€å»æ‰æœ«å°¾ç‚¹å’Œæ¨ªçº¿ä½œä¸º\
          \ key\n        key = chap[\"chapter_id\"].rstrip('.-')\n        id_map[key]\
          \ = chap\n\n    # ä¸ºæ¯ä¸ªèŠ‚ç‚¹åˆ›å»ºç¼ºå¤±çš„çˆ¶èŠ‚ç‚¹ï¼ˆåªé’ˆå¯¹ä¸‰çº§åŠä»¥ä¸Šæ ‡é¢˜ï¼‰\n    for chap in chapter_list:\n\
          \        cid = chap[\"chapter_id\"].rstrip('.')\n        parts = cid.split('.')\n\
          \        \n        # åªæœ‰ä¸‰çº§åŠä»¥ä¸Šæ ‡é¢˜æ‰åˆ›å»ºä¸­é—´çˆ¶èŠ‚ç‚¹\n        if len(parts) >= 3:\n  \
          \          # åˆ›å»ºæ‰€æœ‰ç¼ºå¤±çš„ä¸­é—´çˆ¶çº§èŠ‚ç‚¹ï¼ˆä½†ä¸åŒ…æ‹¬é¡¶çº§çˆ¶èŠ‚ç‚¹ï¼‰\n            for i in range(2, len(parts)):\
          \  # ä»ç¬¬äºŒçº§å¼€å§‹åˆ›å»ºï¼Œè·³è¿‡é¡¶çº§\n                parent_key = '.'.join(parts[:i])\n \
          \               if parent_key not in id_map:\n                    # åˆ›å»ºç¼ºå¤±çš„çˆ¶èŠ‚ç‚¹\n\
          \                    parent_node = {\n                        \"chapter_id\"\
          : parent_key + \".\",\n                        \"chapter_title\": \"\",\n\
          \                        \"raw_text\": \"\",\n                        \"\
          children\": []\n                    }\n                    id_map[parent_key]\
          \ = parent_node\n\n    # æ„å»ºæ ‘ç»“æ„\n    for chap in chapter_list:\n        cid\
          \ = chap[\"chapter_id\"].rstrip('.')\n        parts = cid.split('.')\n\n\
          \        # æ ¹èŠ‚ç‚¹åˆ¤æ–­\n        if cid.startswith(\"APPENDIX\"):\n           \
          \ root.append(chap)\n        elif cid.startswith(\"é™„å½•\") or len(parts) ==\
          \ 1:\n            root.append(chap)\n        else:\n            parent_key\
          \ = '.'.join(parts[:-1])\n            parent = id_map.get(parent_key)\n\
          \            if parent:\n                parent[\"children\"].append(chap)\n\
          \            else:\n                # å¦‚æœçˆ¶èŠ‚ç‚¹ä¸å­˜åœ¨ï¼Œå¯¹äºäºŒçº§æ ‡é¢˜ï¼Œç›´æ¥ä½œä¸ºæ ¹èŠ‚ç‚¹\n        \
          \        if len(parts) == 2:\n                    root.append(chap)\n  \
          \              # ä¸‰çº§åŠä»¥ä¸Šæ ‡é¢˜æ²¡æœ‰çˆ¶èŠ‚ç‚¹æ—¶ï¼Œä¸åšå¤„ç†ï¼ˆå› ä¸ºå‰é¢å·²ç»åˆ›å»ºäº†çˆ¶èŠ‚ç‚¹ï¼‰\n\n\n    # å°†åˆ›å»ºçš„ä¸­é—´èŠ‚ç‚¹ä¹Ÿæ·»åŠ åˆ°æœ€ç»ˆçš„ç« èŠ‚åˆ—è¡¨ä¸­ï¼Œä½†åªæœ‰é‚£äº›æœ‰å­èŠ‚ç‚¹çš„\n\
          \    created_parents = []\n    for key, node in id_map.items():\n      \
          \  if node not in chapter_list and len(node[\"children\"]) > 0:\n      \
          \      created_parents.append(node)\n    \n    # å¯¹åˆ›å»ºçš„çˆ¶èŠ‚ç‚¹ä¹Ÿè¿›è¡Œæ ‘ç»“æ„æ„å»º\n    for\
          \ parent in created_parents:\n        cid = parent[\"chapter_id\"].rstrip('.')\n\
          \        parts = cid.split('.')\n        \n        if len(parts) == 1:\n\
          \            root.append(parent)\n        else:\n            parent_key\
          \ = '.'.join(parts[:-1])\n            grandparent = id_map.get(parent_key)\n\
          \            if grandparent and parent not in grandparent[\"children\"]:\n\
          \                grandparent[\"children\"].append(parent)\n            elif\
          \ len(parts) == 1:  # è¿™æ˜¯ä¸€çº§ç« èŠ‚\n                if parent not in root:\n \
          \                   root.append(parent)\n    \n    return root\n\ndef build_full_path(chapters:\
          \ List[Dict], path_prefix=\"\"):\n    for chap in chapters:\n        if\
          \ path_prefix:\n            chap[\"full_path\"] = f\"{path_prefix}/{chap['chapter_id']}\
          \ {chap['chapter_title']}\"\n        else:\n            chap[\"full_path\"\
          ] = f\"{chap['chapter_id']} {chap['chapter_title']}\"\n        if chap.get(\"\
          children\"):\n            build_full_path(chap[\"children\"], chap[\"full_path\"\
          ])\n\ndef fullwidth_to_halfwidth(text: str) -> str:\n    result = []\n \
          \   for char in text:\n        code = ord(char)\n        if 0xFF01 <= code\
          \ <= 0xFF5E:\n            result.append(chr(code - 0xFEE0))\n        else:\n\
          \            result.append(char)\n    return ''.join(result)\n\ndef build_term_dict(raw_text:\
          \ str) -> Dict[str, str]:\n    text = re.sub(r'\\n+', '\\n', raw_text.strip())\n\
          \    pattern = re.compile(\n        r'^\\d+\\.\\d+\\n'\n        r'(?P<cn>[^\\\
          n]*?)\\s*'\n        r'(?P<en>[A-Za-z].*?)\\s*(?=\\n)',\n        re.MULTILINE\n\
          \    )\n\n    term_map = {}\n    for m in pattern.finditer(text):\n    \
          \    cn = m.group(\"cn\").strip()\n        en = m.group(\"en\").strip()\n\
          \        if cn and en:\n            term_map[cn] = en\n    return term_map\n\
          \ndef extract_terms_with_abbr_from_terms_section(raw_text: str) -> Dict[str,\
          \ Dict[str, str]]:\n    \"\"\"\n    æå–æœ¯è¯­ç« èŠ‚ä¸­çš„ä¸­è‹±æ–‡æœ¯è¯­åŠç¼©å†™\n    è¿”å›æ ¼å¼ï¼š\n    {\n\
          \      \"ä¸­æ–‡æœ¯è¯­\": {\n         \"en\": \"è‹±æ–‡æœ¯è¯­\",\n         \"abbr\": \"ç¼©å†™ï¼ˆå¦‚æœ‰ï¼‰\"\
          \n      }\n    }\n    \"\"\"\n    term_map = {}\n    text = re.sub(r'\\\
          n+', '\\n', raw_text.strip())\n\n    pattern = re.compile(\n        r'(?P<cn>[\\\
          u4e00-\\u9fffï¼ˆï¼‰()Â·\\s]{2,})'        # ä¸­æ–‡éƒ¨åˆ†\n        r'\\s*'            \
          \                             # å¯é€‰ç©ºæ ¼\n        r'(?P<en>[A-Za-z][A-Za-z\\\
          s\\-/]*)'              # è‹±æ–‡æœ¯è¯­\n        r'(?:[;ï¼›:ï¼š]?\\s*(?P<abbr>[A-Z0-9Â·]+))?',\
          \       # å¯é€‰ç¼©å†™\n        re.MULTILINE\n    )\n\n\n    for m in pattern.finditer(text):\n\
          \        cn = m.group(\"cn\").strip()\n        en = m.group(\"en\").strip()\n\
          \        abbr = m.group(\"abbr\").strip() if m.group(\"abbr\") else \"\"\
          \n\n        term_map[cn] = {\"en\": en}\n        if abbr:\n            term_map[cn][\"\
          abbr\"] = abbr\n\n    return term_map\n\ndef extract_abbr_terms_from_symbols_section(raw_text:\
          \ str) -> Dict[str, Dict[str, str]]:\n    \"\"\"\n    æå–â€œç¬¦å·å’Œç¼©ç•¥è¯­â€ç« èŠ‚çš„ä¸­è‹±ç¼©å†™æ˜ å°„ï¼Œè¿”å›ä»¥ä¸­æ–‡ä¸ºé”®çš„ç»“æ„ï¼š\n\
          \    {\n        \"ä¸­æ–‡\": {\n            \"abbr\": \"ç¼©å†™\",\n            \"\
          en\": \"è‹±æ–‡é‡Šä¹‰\"\n        }\n    }\n    \"\"\"\n    abbr_map = {}\n    # æ¸…ç†æ–‡æœ¬\n\
          \    text = re.sub(r'\\n+', '\\n', raw_text.strip())\n\n    # åŒ¹é…æ¨¡å¼ï¼šACLR:\
          \ é‚»é“æ³„æ¼åŠŸç‡æ¯” (Adjacent Channel Leakage Power Ratio)\n    pattern = re.compile(\n\
          \        r'(?P<abbr>[A-Za-z0-9Â·\\-_]+)\\s*[:ï¼š]?\\s*'\n        r'(?P<cn>[\\\
          u4e00-\\u9fffÂ·]+)'\n        r'(?:[ï¼ˆï¼‰()]*\\s*(?P<en>[A-Za-z\\s/\\-]+)\\s*[ï¼ˆï¼‰()]*)?'\n\
          \    )\n\n    for m in pattern.finditer(text):\n        abbr = m.group(\"\
          abbr\").strip()\n        cn = m.group(\"cn\").strip(\"ï¼ˆï¼‰()\").strip()\n\
          \        en = m.group(\"en\").strip() if m.group(\"en\") else \"\"\n\n \
          \       if cn:\n            abbr_map[cn] = {}\n            if abbr:\n  \
          \              abbr_map[cn][\"abbr\"] = abbr\n            if en:\n     \
          \           abbr_map[cn][\"en\"] = en\n\n    return abbr_map\n\ndef should_merge_crossline(prev_text,\
          \ curr_text, prev_bbox, curr_bbox):\n    \"\"\"\n    åˆ¤æ–­æ˜¯å¦éœ€è¦æŠŠå½“å‰è¡Œåˆå¹¶åˆ°ä¸Šä¸€è¡Œ\n\
          \    \"\"\"\n    text_stripped = curr_text.strip()\n\n    # æ¨¡å¼åŒ¹é…ï¼šè¡¨æ ¼æ ‡é¢˜ã€ç¼–å·æ ‡é¢˜ç­‰\n\
          \    if re.match(r'^è¡¨\\s*\\d+', text_stripped):\n        return True\n\n\
          \    # å‚ç›´è·ç¦»å¾ˆå°ï¼ˆè¯´æ˜æ˜¯è§†è§‰ä¸Šçš„åŒä¸€è¡Œï¼‰\n    prev_y = prev_bbox[1]\n    curr_y = curr_bbox[1]\n\
          \    line_height = prev_bbox[3] - prev_bbox[1]\n    if abs(curr_y - prev_y)\
          \ < 0.3 * line_height:\n        return True\n\n    return False\n\ndef fix_broken_chapters(lines:\
          \ list[str]) -> list[str]:\n    def normalize_chapter_spaces(s: str) ->\
          \ str:\n        line = s.strip()\n        \n        # 1. ä¿ç•™åŸæ¥çš„é€»è¾‘ï¼šä¿®å¤ç‚¹åé¢çš„ç©ºæ ¼ï¼Œé€‚ç”¨äºæ‰€æœ‰æƒ…å†µ\
          \ (A. 1, 7. 1)\n        line = re.sub(r'\\.\\s+(?=\\d)', '.', line)\n  \
          \      \n        # 2. ä¿®å¤æ•°å­—/å­—æ¯å’Œç‚¹ä¹‹é—´çš„ç©ºæ ¼ï¼š7 .1 -> 7.1, A .1 -> A.1\n        line\
          \ = re.sub(r'([A-Za-z0-9]+)\\s+(\\.\\d+)', r'\\1\\2', line)\n        \n\
          \        # 3. ä¿®å¤å¤æ‚çš„å¤šçº§ç©ºæ ¼ï¼š7 . 1 . 2 -> 7.1.2\n        # éœ€è¦å¾ªç¯å¤„ç†ï¼Œç›´åˆ°æ²¡æœ‰æ›´å¤šå˜åŒ–\n\
          \        max_iterations = 10  # é˜²æ­¢æ— é™å¾ªç¯\n        iterations = 0\n       \
          \ prev_line = \"\"\n        while prev_line != line and iterations < max_iterations:\n\
          \            prev_line = line\n            # å¤„ç†å„ç§ç©ºæ ¼ç»„åˆï¼Œæ”¯æŒå­—æ¯å’Œæ•°å­—å¼€å¤´\n      \
          \      line = re.sub(r'([A-Za-z0-9]+)\\s*\\.\\s*(\\d+)', r'\\1.\\2', line)\n\
          \            iterations += 1\n        \n        # 4. ä¿®å¤OCRå¸¸è§é”™è¯¯ï¼šæ•°å­—å¼€å¤´çš„ç« èŠ‚\n\
          \        line = re.sub(r'(\\d+\\.\\d+)\\.\\s*l\\b', r'\\1.1', line)\n  \
          \      line = re.sub(r'([A-Za-z0-9]+)\\.l\\.(\\d+)', r'\\1.1.\\2', line)\n\
          \        line = re.sub(r'^l\\.(\\d+)', r'1.\\1', line)\n        \n     \
          \   # 5. ä¿®å¤å­—æ¯å¼€å¤´ç« èŠ‚çš„OCRé”™è¯¯ï¼šB.l -> B.1, A.O -> A.0, C.I -> C.1\n        line\
          \ = re.sub(r'^([A-Z])\\.l\\b', r'\\1.1', line)\n        line = re.sub(r'^([A-Z])\\\
          .l\\.(\\d+)', r'\\1.1.\\2', line)\n        line = re.sub(r'^([A-Z])\\.O\\\
          .(\\d+)', r'\\1.0.\\2', line)\n        line = re.sub(r'^([A-Z])\\.I\\.(\\\
          d+)', r'\\1.1.\\2', line)\n        \n        # 6. ä¿®å¤å…¶ä»–OCRé”™è¯¯ï¼šO -> 0, I ->\
          \ 1\n        line = re.sub(r'([A-Za-z0-9]+)\\.O\\.(\\d+)', r'\\1.0.\\2',\
          \ line)\n        line = re.sub(r'([A-Za-z0-9]+)\\.I\\.(\\d+)', r'\\1.1.\\\
          2', line)\n        \n        return line\n\n    lines = [normalize_chapter_spaces(line)\
          \ for line in lines]\n\n    return lines\n\ndef process_gb_terms_format(lines:\
          \ List[str]) -> List[str]:\n    \"\"\"\n    å¤„ç†å›½æ ‡æœ¯è¯­å®šä¹‰æ ¼å¼ï¼š\n    å°† \"3.1\" (ä¸‹ä¸€è¡Œ)\
          \ \"ä¸­æ–‡æœ¯è¯­ è‹±æ–‡æœ¯è¯­\" åˆå¹¶ä¸º \"3.1 ä¸­æ–‡æœ¯è¯­ è‹±æ–‡æœ¯è¯­\"\n    \"\"\"\n    result = []\n   \
          \ i = 0\n    \n    while i < len(lines):\n        current_line = lines[i].strip()\n\
          \        \n        # æ£€æµ‹æ˜¯å¦æ˜¯æœ¯è¯­å®šä¹‰ç¼–å·ï¼šçº¯æ•°å­—.æ•°å­—æ ¼å¼ï¼Œä¸”ä¸‹ä¸€è¡ŒåŒ…å«ä¸­æ–‡+è‹±æ–‡ï¼Œæˆ–è€…ç¬¬äºŒè¡Œæ˜¯ä¸­æ–‡ï¼Œç¬¬ä¸‰è¡Œæ˜¯è‹±æ–‡\n\
          \        if (i + 1 < len(lines) and \n            re.match(r'^\\d+\\.\\\
          d+$', current_line) and\n            current_line.startswith('3.')):  #\
          \ é€šå¸¸æœ¯è¯­ç« èŠ‚æ˜¯ç¬¬3ç« \n            \n            next_line = lines[i + 1].strip()\n\
          \            \n            # æ£€æŸ¥ä¸‹ä¸€è¡Œæ˜¯å¦ç¬¦åˆ: ä¸­æ–‡ + ç©ºæ ¼ + è‹±æ–‡ çš„æ¨¡å¼\n            if\
          \ re.search(r'[\\u4e00-\\u9fa5].*[A-Za-z]', next_line):\n              \
          \  # åˆå¹¶æˆæ ‡é¢˜æ ¼å¼\n                merged_line = f\"{current_line} {next_line}\"\
          \n                result.append(merged_line)\n                i += 2  #\
          \ è·³è¿‡ä¸‹ä¸€è¡Œ\n                continue\n\n            # æ£€æŸ¥ç¬¬äºŒè¡Œæ˜¯å¦æ˜¯ä¸­æ–‡ï¼Œç¬¬ä¸‰è¡Œæ˜¯å¦æ˜¯è‹±æ–‡\n\
          \            if (i + 2 < len(lines) and\n                re.search(r'[\\\
          u4e00-\\u9fa5]', lines[i + 1].strip()) and\n                re.search(r'[A-Za-z]',\
          \ lines[i + 2].strip())):\n                merged_line = f\"{current_line}\
          \ {lines[i + 1].strip()} {lines[i + 2].strip()}\"\n                result.append(merged_line)\n\
          \                i += 3  # è·³è¿‡åä¸¤è¡Œ\n                continue\n\n        result.append(current_line)\n\
          \        i += 1\n    \n    return result\n\ndef extract_full_text_with_filter(pdf_path:\
          \ str, top_crop=0.08, bottom_crop=0.08):\n    doc = fitz.open(pdf_path)\n\
          \    all_lines = []\n\n    prev_line_text = None\n    prev_bbox = None\n\
          \n\n\n    for page in doc:\n\n        h = page.rect.height\n        clip_rect\
          \ = fitz.Rect(0, h * top_crop, page.rect.width, h * (1 - bottom_crop))\n\
          \        page_dict = page.get_text(\"dict\", clip=clip_rect)\n\n       \
          \ for block in page_dict[\"blocks\"]:\n            if block[\"type\"] !=\
          \ 0:  # åªå¤„ç†æ–‡æœ¬\n                continue\n\n            for line in block[\"\
          lines\"]:\n                # 1. æŒ‰xåæ ‡åˆå¹¶åŒä¸€è¡Œçš„span\n                spans =\
          \ sorted(line[\"spans\"], key=lambda s: s[\"bbox\"][0])\n              \
          \  merged = \"\"\n                last_x = None\n                for sp\
          \ in spans:\n                    x0, x1 = sp[\"bbox\"][0], sp[\"bbox\"][2]\n\
          \                    width = max(1.0, x1 - x0)\n                    avg_char_w\
          \ = width / max(len(sp[\"text\"]), 1)\n\n                    if last_x is\
          \ not None:\n                        gap = x0 - last_x\n               \
          \         if gap > max(avg_char_w * 0.5, 3.0):\n                       \
          \     merged += \" \"\n                    merged += sp[\"text\"]\n    \
          \                last_x = x1\n\n                merged = merged.strip()\n\
          \                curr_bbox = line[\"bbox\"]\n\n                # 2. è·¨è¡Œæ™ºèƒ½åˆå¹¶åˆ¤å®š\n\
          \                if prev_line_text is not None:\n                    if\
          \ should_merge_crossline(prev_line_text, merged, prev_bbox, curr_bbox):\n\
          \                        prev_line_text += \" \" + merged\n            \
          \            prev_bbox = (\n                            prev_bbox[0],\n\
          \                            prev_bbox[1],\n                           \
          \ max(prev_bbox[2], curr_bbox[2]),\n                            max(prev_bbox[3],\
          \ curr_bbox[3])\n                        )\n                        continue\n\
          \                    else:\n                        all_lines.append(prev_line_text)\n\
          \n                prev_line_text = merged\n                prev_bbox = curr_bbox\n\
          \n    # æœ€åä¸€è¡Œ\n    if prev_line_text:\n        all_lines.append(prev_line_text)\n\
          \n    # è¿›è¡Œå…¨è§’å­—ç¬¦è½¬åŠè§’å­—ç¬¦\n    all_lines = [fullwidth_to_halfwidth(line.strip())\
          \ for line in all_lines]\n\n    # è¿›è¡Œç« èŠ‚ç¼–å·ä¿®å¤\n    normalized = fix_broken_chapters(all_lines)\n\
          \    \n    # \U0001F195 å›½æ ‡æœ¯è¯­å®šä¹‰æ ¼å¼å¤„ç†\n    normalized = process_gb_terms_format(normalized)\n\
          \n    # å†™å‡ºæ–‡ä»¶ä¸è¿”å›\n    with open('extracted_full_text.txt', \"w\", encoding=\"\
          utf-8\") as f:\n        f.write(\"\\n\".join(normalized))\n\n    return\
          \ normalized\n\ndef detect_chapter_pattern(chapters: List[Dict]) -> str:\n\
          \    \"\"\"\n    æ£€æµ‹æ–‡æ¡£çš„ç« èŠ‚æ¨¡å¼ï¼š\n    - 'alpha_first': å­—æ¯ç« èŠ‚åœ¨å‰ (A, A.1, A.2, B,\
          \ B.1, 1, 2, ...)\n    - 'numeric_first': æ•°å­—ç« èŠ‚åœ¨å‰ (1, 2, ..., A, A.1, A.2,\
          \ B, B.1, ...)\n    \"\"\"\n    alpha_indices = []\n    numeric_indices\
          \ = []\n    \n    for i, ch in enumerate(chapters):\n        chapter_id\
          \ = ch[\"chapter_id\"].strip()\n        if re.match(r'^[A-Z](\\.\\d+)*\\\
          .?$', chapter_id):\n            alpha_indices.append(i)\n        elif re.match(r'^\\\
          d+(\\.\\d+)*\\.?$', chapter_id):\n            numeric_indices.append(i)\n\
          \    \n    if not alpha_indices or not numeric_indices:\n        return\
          \ 'numeric_first'  # é»˜è®¤æ•°å­—ä¼˜å…ˆ\n    \n    # æ¯”è¾ƒç¬¬ä¸€ä¸ªå­—æ¯ç« èŠ‚å’Œç¬¬ä¸€ä¸ªæ•°å­—ç« èŠ‚çš„ä½ç½®\n    first_alpha\
          \ = min(alpha_indices)\n    first_numeric = min(numeric_indices)\n    \n\
          \    if first_alpha < first_numeric:\n        return 'alpha_first'\n   \
          \ else:\n        return 'numeric_first'\n\ndef parse_chapter_id(chapter_id:\
          \ str, pattern: str = 'numeric_first') -> List[int]:\n    \"\"\"\n    æ ¹æ®æ–‡æ¡£æ¨¡å¼è§£æç« èŠ‚ID\n\
          \    :param chapter_id: ç« èŠ‚IDå­—ç¬¦ä¸²\n    :param pattern: æ–‡æ¡£æ¨¡å¼ ('alpha_first'\
          \ æˆ– 'numeric_first')\n    \"\"\"\n    chapter_id = chapter_id.strip()\n\n\
          \    # å­—æ¯ç« èŠ‚æ ¼å¼ - æ”¯æŒç‚¹å’Œæ¨ªçº¿åˆ†éš”ç¬¦\n    if re.fullmatch(r'[A-Z](?:[.\\-]\\d+)*[.\\\
          -]?', chapter_id):\n        # ç»Ÿä¸€å¤„ç†ç‚¹å’Œæ¨ªçº¿åˆ†éš”ç¬¦\n        normalized = re.sub(r'[.\\\
          -]+', '.', chapter_id).rstrip('.')\n        parts = normalized.split('.')\n\
          \        letter = parts[0]\n        \n        if pattern == 'alpha_first':\n\
          \            # å­—æ¯åœ¨å‰æ¨¡å¼ï¼šA=1, B=2, C=3, ...\n            letter_value = ord(letter)\
          \ - ord('A') + 1\n        else:\n            # æ•°å­—åœ¨å‰æ¨¡å¼ï¼šå­—æ¯ç« èŠ‚æ”¾åœ¨æ•°å­—ç« èŠ‚ä¹‹å\n   \
          \         # å‡è®¾æœ€å¤šæœ‰100ä¸ªæ•°å­—ç« èŠ‚ï¼Œå­—æ¯ä»101å¼€å§‹\n            letter_value = ord(letter)\
          \ - ord('A') + 101\n        \n        try:\n            rest = [int(p) for\
          \ p in parts[1:]] if len(parts) > 1 else []\n            return [letter_value]\
          \ + rest\n        except ValueError:\n            return []\n\n    # æ•°å­—ç« èŠ‚æ ¼å¼\
          \ - æ”¯æŒç‚¹å’Œæ¨ªçº¿åˆ†éš”ç¬¦\n    elif re.fullmatch(r'\\d+(?:[.\\-]\\d+)*[.\\-]?', chapter_id):\n\
          \        try:\n            # ç»Ÿä¸€å¤„ç†ç‚¹å’Œæ¨ªçº¿åˆ†éš”ç¬¦\n            normalized = re.sub(r'[.\\\
          -]+', '.', chapter_id).rstrip('.')\n            parts = normalized.split('.')\n\
          \            numeric_parts = [int(p) for p in parts]\n            \n   \
          \         if pattern == 'alpha_first':\n                # å­—æ¯åœ¨å‰æ¨¡å¼ï¼šæ•°å­—ç« èŠ‚æ”¾åœ¨å­—æ¯ç« èŠ‚ä¹‹å\n\
          \                # å‡è®¾æœ€å¤šæœ‰26ä¸ªå­—æ¯ç« èŠ‚ï¼Œæ•°å­—ä»27å¼€å§‹\n                numeric_parts[0]\
          \ += 26\n            # æ•°å­—åœ¨å‰æ¨¡å¼ï¼šä¿æŒåŸæœ‰æ•°å­—\n            \n            return numeric_parts\n\
          \        except ValueError:\n            return []\n\n    return []\n\n\
          def is_chapter_a_before_b(a: list[int], b: list[int]) -> bool:\n    for\
          \ i in range(min(len(a), len(b))):\n        if a[i] < b[i]:\n          \
          \  return True\n        elif a[i] > b[i]:\n            return False\n  \
          \  return len(a) < len(b)\n\ndef is_reasonable_chapter_jump(prev_id: List[int],\
          \ curr_id: List[int]) -> bool:\n    \"\"\"\n    åˆ¤æ–­ç« èŠ‚è·³è·ƒæ˜¯å¦åˆç†ï¼Œæ›´å®½æ¾çš„ç­–ç•¥ï¼š\n   \
          \ ä¸»è¦è¿‡æ»¤æ‰æ˜æ˜¾ä¸åˆç†çš„è·³è·ƒï¼Œä½†å…è®¸æ­£å¸¸çš„ç« èŠ‚ç»“æ„\n    å¯¹å­—æ¯é“¾å’Œæ•°å­—é“¾éƒ½è¿›è¡Œåˆç†æ€§åˆ¤æ–­\n    \"\"\"\n    if not\
          \ prev_id or not curr_id:\n        return True  # å¦‚æœæ— æ³•è§£æï¼Œé»˜è®¤å…è®¸\n    \n  \
          \  # å¦‚æœæ˜¯ä¸åŒå±‚çº§ï¼Œä¸€èˆ¬éƒ½æ˜¯åˆç†çš„ï¼ˆå¦‚ 1. -> 1.1 æˆ– 1.1 -> 2.ï¼‰\n    if len(prev_id) != len(curr_id):\n\
          \        return True\n    \n    # åŒå±‚çº§çš„æƒ…å†µä¸‹ï¼Œæ£€æŸ¥è·³è·ƒå¹…åº¦\n    if len(prev_id) ==\
          \ 1:  # ä¸€çº§ç« èŠ‚\n        prev_num = prev_id[0]\n        curr_num = curr_id[0]\n\
          \        diff = curr_num - prev_num\n        \n        # åˆ¤æ–­æ˜¯å¦ä¸ºå­—æ¯ç« èŠ‚ï¼ˆç¼–ç èŒƒå›´101-126å¯¹åº”A-Zï¼‰\n\
          \        if prev_num >= 101 and curr_num >= 101:  # å­—æ¯ç« èŠ‚\n            #\
          \ å­—æ¯è·³è·ƒæ£€æŸ¥ï¼šä¸å…è®¸è·¨è¶Šè¶…è¿‡2ä¸ªå­—æ¯ï¼ˆå¦‚Bè·³åˆ°Eä»¥ä¸Šï¼‰\n            return 1 <= diff <= 2\n     \
          \   else:  # æ•°å­—ç« èŠ‚\n            return 1 <= diff <= 5  # å…è®¸è·³è·ƒ1-5ç« ï¼ˆè¿‡æ»¤æ‰ä»5è·³åˆ°100è¿™ç§æ˜æ˜¾é”™è¯¯çš„ï¼‰\n\
          \    \n    elif len(prev_id) == 2:  # äºŒçº§ç« èŠ‚\n        # å¦‚æœç¬¬ä¸€çº§ç›¸åŒï¼Œæ£€æŸ¥ç¬¬äºŒçº§çš„è·³è·ƒ\n\
          \        if prev_id[0] == curr_id[0]:\n            prev_num = prev_id[1]\n\
          \            curr_num = curr_id[1]\n            diff = curr_num - prev_num\n\
          \            \n            # åˆ¤æ–­ç¬¬ä¸€çº§æ˜¯å¦ä¸ºå­—æ¯ç« èŠ‚\n            if prev_id[0] >=\
          \ 101:  # å­—æ¯ç« èŠ‚çš„å­çº§\n                return 1 <= diff <= 5  # å­—æ¯ç« èŠ‚çš„å­çº§è·³è·ƒç¨å¾®å®½æ¾ä¸€äº›\n\
          \            else:  # æ•°å­—ç« èŠ‚çš„å­çº§\n                return 1 <= diff <= 10  #\
          \ äºŒçº§ç« èŠ‚å…è®¸æ›´å¤§è·³è·ƒ\n        else:\n            # ä¸åŒçš„ä¸€çº§ç« èŠ‚ï¼Œéƒ½åˆç†\n            return\
          \ True\n    \n    else:  # ä¸‰çº§åŠä»¥ä¸Šç« èŠ‚\n        # å¯¹äºæ·±å±‚æ¬¡ç« èŠ‚ï¼Œæ›´å®½æ¾ä¸€äº›\n        return\
          \ True\n\ndef analyze_chapter_number_distribution(chapters: List[Dict])\
          \ -> Dict[str, int]:\n    \"\"\"\n    åˆ†æç« èŠ‚ç¼–å·çš„æ•°å­—åˆ†å¸ƒï¼Œç¡®å®šåˆç†çš„æ•°å­—èŒƒå›´\n    è¿”å›: {\"\
          min_reasonable\": æœ€å°åˆç†æ•°å­—, \"max_reasonable\": æœ€å¤§åˆç†æ•°å­—, \"primary_range\"\
          : ä¸»è¦æ•°å­—èŒƒå›´}\n    \"\"\"\n    first_numbers = []\n    \n    for ch in chapters:\n\
          \        chapter_id = ch[\"chapter_id\"].strip()\n        # æå–ç¬¬ä¸€ä¸ªæ•°å­—\n  \
          \      m_num = re.match(r'^(\\d+)', chapter_id)\n        if m_num:\n   \
          \         first_numbers.append(int(m_num.group(1)))\n        # å¤„ç†APPENDIXåè·Ÿæ•°å­—çš„æƒ…å†µ\n\
          \        elif chapter_id.upper().startswith(\"APPENDIX\"):\n           \
          \ suffix = chapter_id[len(\"APPENDIX\"):].strip(\" ()\")\n            if\
          \ suffix.isdigit():\n                first_numbers.append(int(suffix))\n\
          \    \n    if not first_numbers:\n        return {\"min_reasonable\": 1,\
          \ \"max_reasonable\": 50, \"primary_range\": (1, 50)}\n    \n    first_numbers.sort()\n\
          \    \n    # åˆ†ææ•°å­—åˆ†å¸ƒæ¨¡å¼\n    from collections import Counter\n    counter\
          \ = Counter(first_numbers)\n    \n    # å¦‚æœå¤§å¤šæ•°ç« èŠ‚éƒ½æ˜¯åŒä¸€ä¸ªæ•°å­—å¼€å¤´ï¼ˆå¦‚60.1, 60.2, 60.3...ï¼‰ï¼Œè¿™å¯èƒ½æ˜¯æ³•è§„ç¼–å·\n\
          \    most_common = counter.most_common(1)[0]\n    most_common_num, most_common_count\
          \ = most_common\n    \n    # å¦‚æœæŸä¸ªæ•°å­—å‡ºç°æ¬¡æ•°è¶…è¿‡æ€»æ•°çš„60%ï¼Œä¸”è¿™ä¸ªæ•°å­—å¤§äº30ï¼Œå¯èƒ½æ˜¯æ³•è§„ç¼–å·æ¨¡å¼\n  \
          \  if most_common_count > len(first_numbers) * 0.6 and most_common_num >\
          \ 30:\n        print(f\"æ£€æµ‹åˆ°å¯èƒ½çš„æ³•è§„ç¼–å·æ¨¡å¼: {most_common_num}.x (å‡ºç°{most_common_count}æ¬¡)\"\
          )\n        # åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå…è®¸è¿™ä¸ªç‰¹å®šçš„æ³•è§„ç¼–å·\n        return {\n            \"min_reasonable\"\
          : most_common_num, \n            \"max_reasonable\": most_common_num, \n\
          \            \"primary_range\": (most_common_num, most_common_num),\n  \
          \          \"regulation_mode\": True,\n            \"regulation_number\"\
          : most_common_num\n        }\n    \n    # æ­£å¸¸çš„ç« èŠ‚ç¼–å·æ¨¡å¼\n    min_num = min(first_numbers)\n\
          \    max_num = max(first_numbers)\n    \n    # å¦‚æœæ•°å­—èŒƒå›´å¾ˆå°ï¼ˆ<= 50ï¼‰ï¼Œè®¤ä¸ºæ˜¯æ­£å¸¸ç« èŠ‚\n\
          \    if max_num <= 50:\n        return {\n            \"min_reasonable\"\
          : max(1, min_num), \n            \"max_reasonable\": min(50, max_num + 5),\
          \  # å…è®¸å°‘é‡è¶…å‡º\n            \"primary_range\": (min_num, max_num)\n       \
          \ }\n    \n    # å¦‚æœæ•°å­—èŒƒå›´å¾ˆå¤§ï¼Œå¯èƒ½åŒ…å«é¡µç ç­‰å¹²æ‰°ï¼Œé‡‡ç”¨æ›´ä¿å®ˆç­–ç•¥\n    # æ‰¾åˆ°æœ€å¯†é›†çš„æ•°å­—åŒºé—´\n    gaps\
          \ = []\n    for i in range(len(first_numbers) - 1):\n        gaps.append(first_numbers[i\
          \ + 1] - first_numbers[i])\n    \n    # å¦‚æœæœ‰æ˜æ˜¾çš„å¤§è·³è·ƒï¼ˆ>20ï¼‰ï¼Œå¯èƒ½å‰é¢æ˜¯æ­£å¸¸ç« èŠ‚ï¼Œåé¢æ˜¯é¡µç ç­‰\n\
          \    large_gap_idx = -1\n    for i, gap in enumerate(gaps):\n        if\
          \ gap > 20:\n            large_gap_idx = i\n            break\n    \n  \
          \  if large_gap_idx != -1:\n        # å–è·³è·ƒå‰çš„æ•°å­—ä½œä¸ºåˆç†èŒƒå›´\n        reasonable_max\
          \ = first_numbers[large_gap_idx]\n        return {\n            \"min_reasonable\"\
          : max(1, min_num), \n            \"max_reasonable\": reasonable_max,\n \
          \           \"primary_range\": (min_num, reasonable_max)\n        }\n  \
          \  \n    # é»˜è®¤ä¿å®ˆç­–ç•¥\n    return {\"min_reasonable\": 1, \"max_reasonable\"\
          : 50, \"primary_range\": (1, 50)}\n\ndef find_longest_chapter_chain_with_append(chapters:\
          \ List[Dict], language: str = 'en') -> Tuple[List[Dict], str]:\n    # å…ˆæ£€æµ‹ç« èŠ‚æ¨¡å¼\n\
          \    pattern = detect_chapter_pattern(chapters)\n    print(f\"æ£€æµ‹åˆ°ç« èŠ‚æ¨¡å¼: {pattern}\"\
          )\n    \n    # \U0001F195 åˆ†æç« èŠ‚æ•°å­—åˆ†å¸ƒ\n    number_analysis = analyze_chapter_number_distribution(chapters)\n\
          \    print(f\"ç« èŠ‚æ•°å­—åˆ†æç»“æœ: {number_analysis}\")\n    \n    # ç”¨æ£€æµ‹åˆ°çš„æ¨¡å¼é‡æ–°è§£æç« èŠ‚ID\n\
          \    parsed_ids = [parse_chapter_id(ch[\"chapter_id\"], pattern) for ch\
          \ in chapters]\n    # print(f'ç¬¬ä¸€ä¸ªç« èŠ‚: {chapters[0]}')\n    n = len(chapters)\n\
          \n    # ç¬¬ä¸€æ­¥ï¼šè¿‡æ»¤æ‰æ˜æ˜¾ä¸åˆç†çš„ç« èŠ‚ï¼ˆå¦‚è¯¯è¯†åˆ«çš„æ•°å­—ï¼‰\n    valid_indices = []\n    for i in range(n):\n\
          \        if not parsed_ids[i]:\n            continue\n            \n   \
          \     # æ£€æŸ¥æ˜¯å¦æ˜¯æ˜æ˜¾çš„è¯¯è¯†åˆ«\n        chapter_text = chapters[i][\"chapter_id\"]\
          \ + \" \" + chapters[i][\"chapter_title\"]\n        \n        # è¿‡æ»¤æ˜æ˜¾çš„æµ‹é‡å•ä½ã€é¢‘ç‡èŒƒå›´ã€çº¯æ•°å­—ç­‰\n\
          \        if re.search(r'\\b\\d+\\s*(MHz|GHz|Hz|kHz|dB|V|mV|ÂµV|A|mA|ÂµA|W|mW|Î©|%|Â°C|Â°F|mm|cm|m|km|kg|g|mg|ms|s|min|h|rpm|bar|Pa|kPa|MPa)\\\
          b', chapter_text, re.I):\n            continue\n        if re.search(r'\\\
          d+\\s*MHz\\s*[~-]\\s*\\d+\\s*MHz', chapter_text, re.I):\n            continue\n\
          \        if re.match(r'^\\d+\\s*$', chapters[i][\"chapter_title\"].strip()):\
          \  # æ ‡é¢˜æ˜¯çº¯æ•°å­—\n            continue\n        if len(chapters[i][\"chapter_title\"\
          ].strip()) < 2:  # æ ‡é¢˜å¤ªçŸ­\n            continue\n            \n        # \U0001F195\
          \ è¡¨æ ¼æ•°æ®ç‰¹å¾è¿‡æ»¤\n        chapter_title = chapters[i][\"chapter_title\"].strip()\n\
          \        chapter_id = chapters[i][\"chapter_id\"].strip()\n        \n  \
          \      # æ£€æµ‹è¡¨æ ¼è¡Œæ¨¡å¼ï¼šå•ä¸ªå­—æ¯ + ä¸»è¦æ˜¯æ•°å­—çš„æ ‡é¢˜\n        if (len(chapter_id) == 1 and chapter_id.isupper()\
          \ and \n            re.search(r'^\\d+.*\\d+', chapter_title) and \n    \
          \        len([x for x in chapter_title.split() if x.isdigit()]) >= 2):\n\
          \            continue\n            \n        # æ£€æµ‹åæ ‡ç‚¹æ ¼å¼ï¼šå¦‚ \"10 0 E 0 16\"\
          \n        if re.match(r'^\\d+\\s+\\d+\\s+[A-Z]\\s+\\d+\\s+\\d+', chapter_title):\n\
          \            continue\n            \n        # æ£€æµ‹å‚æ•°è¡¨æ ¼æ ¼å¼ï¼šå¦‚ \"34 65 F 25 77\"\
          \n        title_parts = chapter_title.split()\n        if (len(title_parts)\
          \ >= 4 and \n            sum(1 for part in title_parts if part.isdigit())\
          \ >= 3 and\n            sum(1 for part in title_parts if len(part) == 1\
          \ and part.isupper()) >= 1):\n            continue\n            \n     \
          \   # æ£€æµ‹å›¾è¡¨æ ‡æ³¨è¯´æ˜ï¼šå•ä¸ªå­—æ¯ + ä»¥ç ´æŠ˜å·å¼€å¤´çš„æ ‡é¢˜\n        if (len(chapter_id) == 1 and chapter_id.isupper()\
          \ and \n            chapter_title.startswith('â€”â€”â€”')):\n            continue\n\
          \            \n        valid_indices.append(i)\n    \n    # ç¬¬äºŒæ­¥ï¼šéªŒè¯å­—æ¯ç« èŠ‚çš„åˆç†æ€§ï¼ˆé’ˆå¯¹ä¸­è‹±æ–‡å·®å¼‚åŒ–å¤„ç†ï¼‰\n\
          \    if valid_indices:\n        # æ£€æŸ¥æ˜¯å¦åŒ…å«å­—æ¯ç« èŠ‚\n        alpha_chapters = []\n\
          \        for i, idx in enumerate(valid_indices):\n            chapter_id\
          \ = chapters[idx][\"chapter_id\"].strip()\n            if re.match(r'^[A-Z](?:\\\
          .\\d+)*\\.?$', chapter_id):\n                alpha_chapters.append((i, idx,\
          \ chapter_id[0]))  # (åœ¨valid_indicesä¸­çš„ä½ç½®, åŸå§‹ç´¢å¼•, é¦–å­—æ¯)\n        \n       \
          \ # å¦‚æœæœ‰å­—æ¯ç« èŠ‚ï¼Œè¿›è¡Œåˆç†æ€§éªŒè¯\n        if alpha_chapters:\n            if language\
          \ == 'en':\n                # è‹±æ–‡æ–‡æ¡£ï¼šè¦æ±‚å­—æ¯ç« èŠ‚å¿…é¡»ä»Aå¼€å¤´\n                first_alpha_letter\
          \ = alpha_chapters[0][2]\n                if first_alpha_letter != 'A':\n\
          \                    print(f\"è‹±æ–‡æ–‡æ¡£å­—æ¯ç« èŠ‚ä¸ä»¥Aå¼€å¤´ï¼Œè·³è¿‡: ç¬¬ä¸€ä¸ªå­—æ¯ç« èŠ‚æ˜¯ {alpha_chapters[0][2]}\"\
          )\n                    # ç§»é™¤æ‰€æœ‰å­—æ¯ç« èŠ‚\n                    alpha_indices_set\
          \ = {item[1] for item in alpha_chapters}\n                    valid_indices\
          \ = [idx for idx in valid_indices if idx not in alpha_indices_set]\n   \
          \         else:\n                # ä¸­æ–‡æ–‡æ¡£ï¼šæ£€æŸ¥å­—æ¯ç« èŠ‚çš„ä¸€è‡´æ€§ï¼ˆåŒä¸€é™„å½•åº”è¯¥ä»¥åŒä¸€å­—æ¯å¼€å¤´ï¼‰\n    \
          \            from collections import Counter\n                alpha_letters\
          \ = [item[2] for item in alpha_chapters]\n                letter_counter\
          \ = Counter(alpha_letters)\n                most_common_letter, most_common_count\
          \ = letter_counter.most_common(1)[0]\n                \n               \
          \ # å¦‚æœæŸä¸ªå­—æ¯å‡ºç°æ¬¡æ•°è¶…è¿‡60%ï¼Œè®¤ä¸ºè¿™æ˜¯ä¸»è¦çš„é™„å½•å­—æ¯\n                if most_common_count >\
          \ len(alpha_chapters) * 0.6:\n                    print(f\"ä¸­æ–‡æ–‡æ¡£æ£€æµ‹åˆ°ä¸»è¦é™„å½•å­—æ¯:\
          \ {most_common_letter} (å‡ºç°{most_common_count}æ¬¡)\")\n                   \
          \ # ä¿ç•™ä¸ä¸»è¦å­—æ¯ä¸€è‡´çš„ç« èŠ‚ï¼Œç§»é™¤å…¶ä»–å­—æ¯ç« èŠ‚\n                    keep_alpha_indices = {item[1]\
          \ for item in alpha_chapters if item[2] == most_common_letter}\n       \
          \             remove_alpha_indices = {item[1] for item in alpha_chapters\
          \ if item[2] != most_common_letter}\n                    valid_indices =\
          \ [idx for idx in valid_indices if idx not in remove_alpha_indices]\n  \
          \                  if remove_alpha_indices:\n                        removed_letters\
          \ = {chapters[idx][\"chapter_id\"].strip()[0] for idx in remove_alpha_indices}\n\
          \                        print(f\"ç§»é™¤ä¸ä¸€è‡´çš„å­—æ¯ç« èŠ‚: {removed_letters}\")\n   \
          \             else:\n                    # å¦‚æœæ²¡æœ‰æ˜æ˜¾çš„ä¸»è¦å­—æ¯ï¼Œä¿æŒåŸæœ‰é€»è¾‘ï¼ˆå¯èƒ½æ˜¯æ··åˆæƒ…å†µï¼‰\n\
          \                    print(f\"ä¸­æ–‡æ–‡æ¡£å­—æ¯ç« èŠ‚åˆ†å¸ƒè¾ƒå‡åŒ€: {dict(letter_counter)}\")\n\
          \                    # ä¸åšç‰¹æ®Šå¤„ç†ï¼Œä¿ç•™æ‰€æœ‰å­—æ¯ç« èŠ‚\n\n    # ç¬¬ä¸‰æ­¥ï¼šä»åå¾€å‰æ„å»ºæœ€é•¿é“¾\n    dp =\
          \ [1] * len(valid_indices)\n    next_link = [-1] * len(valid_indices)  #\
          \ æ”¹ä¸ºè®°å½•ä¸‹ä¸€ä¸ªèŠ‚ç‚¹\n    max_len = 0\n    max_idx = -1\n\n    # ä»åå¾€å‰éå†\n    for\
          \ i in range(len(valid_indices) - 1, -1, -1):\n        curr_idx = valid_indices[i]\n\
          \        curr_parsed = parsed_ids[curr_idx]\n        \n        # æ‰¾åœ¨å½“å‰èŠ‚ç‚¹ä¹‹åçš„æ‰€æœ‰èŠ‚ç‚¹\n\
          \        for j in range(i + 1, len(valid_indices)):\n            next_idx\
          \ = valid_indices[j]\n            next_parsed = parsed_ids[next_idx]\n \
          \           \n            # æ£€æŸ¥å½“å‰èŠ‚ç‚¹æ˜¯å¦å¯ä»¥è¿åˆ°ä¸‹ä¸€ä¸ªèŠ‚ç‚¹\n            if (is_chapter_a_before_b(curr_parsed,\
          \ next_parsed) and \n                is_reasonable_chapter_jump(curr_parsed,\
          \ next_parsed)):\n                if dp[j] + 1 > dp[i]:\n              \
          \      dp[i] = dp[j] + 1\n                    next_link[i] = j\n       \
          \ \n        if dp[i] > max_len:\n            max_len = dp[i]\n         \
          \   max_idx = i\n\n    # ç¬¬å››æ­¥ï¼šå¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆç†çš„é“¾ï¼Œé€€å›åˆ°ç®€å•çš„é¡ºåºè¿‡æ»¤\n    if max_len < 2:\n\
          \        # ç®€å•æŒ‰ç« èŠ‚ç¼–å·é¡ºåºè¿‡æ»¤\n        filtered_chapters = simple_chapter_filter(chapters)\n\
          \        \n        # å¦‚æœè¿‡æ»¤åè¿˜æ˜¯æ²¡æœ‰ç« èŠ‚ï¼Œå°†æ‰€æœ‰å†…å®¹æ”¾å…¥è·³è¿‡çš„å†…å®¹ä¸­\n        if not filtered_chapters:\n\
          \            all_content = []\n            for ch in chapters:\n       \
          \         content = f\"{ch['chapter_id']} {ch['chapter_title']}\"\n    \
          \            if ch.get('raw_text'):\n                    content += \" \"\
          \ + ch['raw_text']\n                all_content.append(content)\n      \
          \      skipped_text = \"\\n\".join(all_content)\n            return [],\
          \ skipped_text\n        \n        return filtered_chapters, \"\"\n\n   \
          \ # å›æº¯å‡ºä¸»é“¾ç´¢å¼•ï¼ˆä»å‰å¾€åçš„æ­£ç¡®é¡ºåºï¼‰\n    chain_indices = []\n    idx = max_idx\n    while\
          \ idx != -1:\n        chain_indices.append(valid_indices[idx])\n       \
          \ idx = next_link[idx]\n    \n    print(f\"ä»åå¾€å‰ç”Ÿæˆçš„æœ€é•¿é“¾: é•¿åº¦={len(chain_indices)},\
          \ ä½ç½®={chain_indices[:5]}{'...' if len(chain_indices)>5 else ''}\")\n   \
          \ \n    # æœ€é•¿é“¾çš„ç¬¬ä¸€ä¸ªç« èŠ‚ç´¢å¼•\n    first_chain_idx = chain_indices[0]\n    \n  \
          \  # ç”Ÿæˆè·³è¿‡çš„å†…å®¹ï¼ˆæœ€é•¿é“¾ç¬¬ä¸€ä¸ªç« èŠ‚ä¹‹å‰çš„æ‰€æœ‰å†…å®¹ï¼‰\n    skipped_chapters = chapters[:first_chain_idx]\n\
          \    skipped_text = \"\\n\".join([f\"{ch['chapter_id']} {ch['chapter_title']}\
          \ {ch.get('raw_text','')}\" for ch in skipped_chapters])\n    \n    chain_set\
          \ = set(chain_indices)\n\n    # æœ€ç»ˆç»“æœæ„å»º\n    result = []\n    last_valid\
          \ = None\n    for i, chap in enumerate(chapters):\n        if i in chain_set:\n\
          \            result.append(chap)\n            last_valid = chap\n      \
          \  elif i >= first_chain_idx:  # åªå¤„ç†æœ€é•¿é“¾å¼€å§‹ä¹‹åçš„ç« èŠ‚\n            if last_valid:\n\
          \                content_to_add = \"\\n\" + chap[\"chapter_id\"] + chap[\"\
          chapter_title\"]\n                if chap.get(\"raw_text\"):\n         \
          \           content_to_add += \" \" + chap[\"raw_text\"]\n             \
          \   last_valid[\"raw_text\"] += content_to_add\n\n    # åˆ¤æ–­ç« èŠ‚æ ‡é¢˜æ˜¯å¦åº”è¯¥åˆå¹¶åˆ°æ­£æ–‡ä¸­\n\
          \    for chap in result:\n        should_merge = False\n        \n     \
          \   # ä¸­æ–‡å¤„ç†ï¼šåŒ…å«ä¸­æ–‡ä¸”æœ‰ä¸­æ–‡é€—å·ï¼Œå¥å·ï¼Œå†’å·ï¼Œæˆ–è€…é•¿åº¦å¤§äº30å­—ç¬¦\n        if re.search(r'[\\u4e00-\\\
          u9fa5]', chap[\"chapter_title\"]):\n            # è·å–ç« èŠ‚ç¼–å·çš„ç¬¬ä¸€ä¸ªæ•°å­—ï¼Œå‰ä¸‰ç« è·³è¿‡åˆå¹¶åˆ¤æ–­\n\
          \            first_num = None\n            chapter_id = chap[\"chapter_id\"\
          ].strip('.-')\n            if re.match(r'^\\d+', chapter_id):\n        \
          \        first_num = int(re.match(r'^\\d+', chapter_id).group())\n     \
          \       # å‰ä¸‰ç« è·³è¿‡åˆå¹¶åˆ¤æ–­\n            if first_num is not None and first_num\
          \ <= 3:\n                continue          \n            if re.search(r'[ï¼Œã€‚ï¼š,:]',\
          \ chap[\"chapter_title\"]) or len(chap[\"chapter_title\"]) > 30:\n     \
          \           should_merge = True\n        \n        # è‹±æ–‡å¤„ç†ï¼šæ›´æ™ºèƒ½çš„åˆ¤æ–­é€»è¾‘\n   \
          \     else:\n            # å¦‚æœå…¨å¤§å†™ï¼Œåˆ™è‚¯å®šæ˜¯æ ‡é¢˜\n            if chap[\"chapter_title\"\
          ].isupper():\n                continue\n            # 1. å¦‚æœraw_textä»¥å°å†™å­—æ¯å¼€å¤´ï¼Œå¯èƒ½æ˜¯æ ‡é¢˜çš„å»¶ç»­\n\
          \            if len(chap[\"raw_text\"]) and chap[\"raw_text\"][0].islower():\n\
          \                should_merge = True\n            # 2. å¦‚æœchapter_titleåŒ…å«å®Œæ•´å¥å­çš„ç‰¹å¾\n\
          \            elif re.search(r'[,;!?]', chap[\"chapter_title\"]):\n     \
          \           should_merge = True\n            # 3. å¦‚æœchapter_titleå¾ˆé•¿ï¼ˆè¶…è¿‡50ä¸ªå­—ç¬¦ï¼‰ï¼Œå¯èƒ½æ˜¯æ®µè½æ–‡æœ¬\n\
          \            elif len(chap[\"chapter_title\"]) > 50:\n                should_merge\
          \ = True\n        \n        if should_merge:\n            chap[\"raw_text\"\
          ] = chap[\"chapter_title\"] + ' ' + chap[\"raw_text\"]\n            chap[\"\
          chapter_title\"] = \"\"\n\n    return result, skipped_text\n\ndef simple_chapter_filter(chapters:\
          \ List[Dict]) -> List[Dict]:\n    \"\"\"\n    ç®€å•çš„ç« èŠ‚è¿‡æ»¤ç­–ç•¥ï¼šå½“æœ€é•¿é“¾ç®—æ³•å¤±æ•ˆæ—¶çš„å¤‡ç”¨æ–¹æ¡ˆ\n\
          \    \"\"\"\n    # æ£€æµ‹ç« èŠ‚æ¨¡å¼\n    pattern = detect_chapter_pattern(chapters)\n\
          \    \n    result = []\n    parsed_ids = [parse_chapter_id(ch[\"chapter_id\"\
          ], pattern) for ch in chapters]\n    \n    for i, chap in enumerate(chapters):\n\
          \        parsed_id = parsed_ids[i]\n        \n        # åŸºæœ¬åˆç†æ€§æ£€æŸ¥\n      \
          \  if not parsed_id:\n            # æ— æ³•è§£æçš„ç« èŠ‚ï¼Œè¿½åŠ åˆ°ä¸Šä¸€ä¸ªæœ‰æ•ˆç« èŠ‚\n            if result:\n\
          \                content_to_add = \"\\n\" + chap[\"chapter_id\"] + chap[\"\
          chapter_title\"]\n                if chap.get(\"raw_text\"):\n         \
          \           content_to_add += \" \" + chap[\"raw_text\"]\n             \
          \   result[-1][\"raw_text\"] += content_to_add\n            continue\n \
          \       \n        # æ£€æŸ¥ç« èŠ‚ç¼–å·æ˜¯å¦åœ¨åˆç†èŒƒå›´å†…\n        first_num = parsed_id[0]\n \
          \       \n        # æ ¹æ®æ¨¡å¼è°ƒæ•´åˆç†æ€§æ£€æŸ¥\n        if pattern == 'alpha_first':\n\
          \            # å­—æ¯åœ¨å‰ï¼šA=1, B=2, ..., 1=27, 2=28, ...\n            if 1 <=\
          \ first_num <= 50:  # åˆç†èŒƒå›´ï¼š26ä¸ªå­—æ¯ + 20ä¸ªæ•°å­—ç« èŠ‚\n                result.append(chap)\n\
          \            else:\n                # ä¸åˆç†çš„ç« èŠ‚ï¼Œè¿½åŠ åˆ°ä¸Šä¸€ä¸ªæœ‰æ•ˆç« èŠ‚\n              \
          \  if result:\n                    content_to_add = \"\\n\" + chap[\"chapter_id\"\
          ] + chap[\"chapter_title\"]\n                    if chap.get(\"raw_text\"\
          ):\n                        content_to_add += \" \" + chap[\"raw_text\"\
          ]\n                    result[-1][\"raw_text\"] += content_to_add\n    \
          \    else:\n            # æ•°å­—åœ¨å‰ï¼š1, 2, ..., A=101, B=102, ...\n          \
          \  if (1 <= first_num <= 20) or (101 <= first_num <= 126):  # æ•°å­—ç« èŠ‚æˆ–å­—æ¯ç« èŠ‚\n\
          \                result.append(chap)\n            else:\n              \
          \  # ä¸åˆç†çš„ç« èŠ‚ï¼Œè¿½åŠ åˆ°ä¸Šä¸€ä¸ªæœ‰æ•ˆç« èŠ‚\n                if result:\n                   \
          \ content_to_add = \"\\n\" + chap[\"chapter_id\"] + chap[\"chapter_title\"\
          ]\n                    if chap.get(\"raw_text\"):\n                    \
          \    content_to_add += \" \" + chap[\"raw_text\"]\n                    result[-1][\"\
          raw_text\"] += content_to_add\n    \n    return result\n    \n    return\
          \ result\n\ndef split_sections_by_attachment(chapters: List[Dict]) -> List[Dict]:\n\
          \    \"\"\"\n    å°†æ•´ä¸ªæ–‡æ¡£æŒ‰é™„ä»¶ï¼ˆANNEXï¼‰åˆ‡åˆ†ã€‚\n    é¡¶å±‚ file: regulation / ANNEX n\n\
          \    æ”¹è¿›ï¼šåˆå¹¶è¿ç»­çš„ç›¸åŒé™„ä»¶æ ‡é¢˜\n    \"\"\"\n    sections = []\n    current_section\
          \ = {\n        \"section\": \"regulation\",  # é»˜è®¤ä¸»æ–‡æ¡£\n        \"chapters\"\
          : []\n    }\n\n    annex_pattern = re.compile(r'^(ANNEX|ATTACHMENT)\\s+([A-Z0-9]+)',\
          \ re.I)\n\n    for chap in chapters:\n        match = annex_pattern.match(chap['chapter_id'])\n\
          \        if match:\n            annex_name = match.group(1).upper() + \"\
          \ \" + match.group(2)  # æ ‡å‡†åŒ–åç§°ï¼Œå¦‚ \"ANNEX 1\"\n            \n           \
          \ # æ£€æŸ¥æ˜¯å¦ä¸å½“å‰ section çš„åç§°ç›¸åŒ\n            if current_section[\"section\"] !=\
          \ \"regulation\" and current_section[\"section\"].upper() == annex_name:\n\
          \                # ç›¸åŒçš„é™„ä»¶ï¼Œç›´æ¥æ·»åŠ åˆ°å½“å‰ sectionï¼Œè·³è¿‡é‡å¤çš„æ ‡é¢˜ç« èŠ‚\n                if chap.get('chapter_title')\
          \ or chap.get('raw_text', '').strip():\n                    current_section[\"\
          chapters\"].append(chap)\n                # å¦‚æœæ˜¯ç©ºçš„é‡å¤æ ‡é¢˜ç« èŠ‚ï¼ˆåªæœ‰chapter_idæ²¡æœ‰å†…å®¹ï¼‰ï¼Œåˆ™è·³è¿‡\n\
          \            else:\n                # ä¸åŒçš„é™„ä»¶ï¼Œä¿å­˜å½“å‰å—å¹¶æ–°å»º\n                if\
          \ current_section[\"chapters\"]:\n                    sections.append(current_section)\n\
          \                # æ–°å»ºé™„ä»¶å—\n                current_section = {\n        \
          \            \"section\": annex_name,\n                    \"chapters\"\
          : [chap] if (chap.get('chapter_title') or chap.get('raw_text', '').strip())\
          \ else []\n                }\n        else:\n            current_section[\"\
          chapters\"].append(chap)\n\n    if current_section[\"chapters\"]:\n    \
          \    sections.append(current_section)\n\n    return sections\n\ndef split_sections_by_appendix(chapters):\n\
          \    sections = []\n    current_section = {\"section\": \"MAIN\", \"chapters\"\
          : []}\n\n    for ch in chapters:\n        # æ£€æµ‹ APPENDIX å¼€å¤´çš„é¡¶å±‚æ ‡é¢˜ï¼Œæˆ–è€…é™„å½•\n \
          \       appendix_match = re.match(r'^(APPENDIX\\s+(?:[A-Z0-9]+|\\([A-Z0-9]+\\\
          )))$', ch['chapter_id'], re.IGNORECASE)\n        annex_match = ch[\"chapter_id\"\
          ].startswith(\"é™„å½•\")\n        \n        if appendix_match or annex_match:\n\
          \            # æ ‡å‡†åŒ–é™„å½•åç§°\n            if appendix_match:\n               \
          \ appendix_name = appendix_match.group(1).upper()\n            else:\n \
          \               appendix_name = ch['chapter_id'].strip()\n            \n\
          \            # æ£€æŸ¥æ˜¯å¦ä¸å½“å‰ section çš„åç§°ç›¸åŒ\n            if current_section[\"\
          section\"] != \"MAIN\" and current_section[\"section\"].upper() == appendix_name:\n\
          \                # ç›¸åŒçš„é™„å½•ï¼Œç›´æ¥æ·»åŠ åˆ°å½“å‰ sectionï¼ˆå¦‚æœæœ‰å®é™…å†…å®¹ï¼‰\n                if ch.get('chapter_title')\
          \ or ch.get('raw_text', '').strip():\n                    current_section[\"\
          chapters\"].append(ch)\n                # å¦‚æœæ˜¯ç©ºçš„é‡å¤æ ‡é¢˜ç« èŠ‚ï¼Œåˆ™è·³è¿‡\n            else:\n\
          \                # ä¸åŒçš„é™„å½•ï¼Œå…ˆä¿å­˜å½“å‰å—\n                if current_section[\"chapters\"\
          ]:\n                    sections.append(current_section)\n             \
          \   # æ–°å»ºé™„å½•å—\n                current_section = {\n                    \"\
          section\": appendix_name,\n                    \"chapters\": [ch] if (ch.get('chapter_title')\
          \ or ch.get('raw_text', '').strip()) else []\n                }\n      \
          \  else:\n            current_section[\"chapters\"].append(ch)\n\n    #\
          \ æœ«å°¾å—åŠ å…¥\n    if current_section[\"chapters\"]:\n        sections.append(current_section)\n\
          \n    # # æ‰“å°æå–çš„æ‰€æœ‰ç« èŠ‚æ ‡é¢˜\n    # for sec in sections:\n    #     print(f\"Section:\
          \ {sec['section']}\")\n    #     for chap in sec[\"chapters\"]:\n    # \
          \        print(f\"  Chapter ID: {chap['chapter_id']}, Title: {chap['chapter_title']}\"\
          )\n\n    return sections\n\ndef process_sections_with_lis(chapters, language='en'):\n\
          \    # å…ˆæ‹†åˆ†æˆæ­£æ–‡å’Œå¤šä¸ªé™„å½•\n    sections = split_sections_by_appendix(chapters)\n\
          \n    # æ¯ä¸ªéƒ¨åˆ†å†…éƒ¨å•ç‹¬è·‘æœ€é•¿é“¾\n    processed_sections = []\n    for sec in sections:\n\
          \        valid_chaps, skipped_content = find_longest_chapter_chain_with_append(sec[\"\
          chapters\"], language)\n        processed_sections.append({\n          \
          \  \"section\": sec[\"section\"],\n            \"context\": skipped_content,\
          \  # æ·»åŠ è¢«è·³è¿‡çš„å†…å®¹\n            \"chapters\": valid_chaps\n        })\n\n   \
          \ return processed_sections\n\ndef filter_start_of_main(chapters: List[Dict])\
          \ -> Tuple[List[Dict], str]:\n    \"\"\"\n    æ‰¾åˆ°ç¬¬ä¸€ä¸ªæ­£æ–‡ç« èŠ‚ä½œä¸ºèµ·ç‚¹ï¼Œè·³è¿‡ç›®å½•\n    \"\
          \"\"\n    start_index = 0\n    for i, chap in enumerate(chapters):\n   \
          \     chapter_id = chap.get(\"chapter_id\", \"\").strip()\n        # æ­£æ–‡ä¸»é“¾æˆ–é™„ä»¶å†…éƒ¨ç« èŠ‚ï¼šæ•°å­—å¼€å¤´æˆ–å­—æ¯å¼€å¤´\n\
          \        if chapter_id in {\"1\", \"1-\", \"1.\", \"A\", \"A.\", \"A.1\"\
          }:\n            # SCOPE / GENERAL / INTRO ç­‰éƒ½ç®—æ­£æ–‡èµ·ç‚¹\n            title_upper\
          \ = chap.get(\"chapter_title\", \"\").upper()\n            if any(k in title_upper\
          \ for k in [\"SCOPE\", \"GENERAL\", \"INTRO\", \"æ€»åˆ™\", \"èŒƒå›´\", \"LEGISLATIVE\"\
          , \"FUNCTION\"]):\n                start_index = i\n                break\n\
          \                \n        # ä¹Ÿæ£€æŸ¥æ ‡å‡†çš„ç« èŠ‚å¼€å¤´æ¨¡å¼\n        if re.match(r'^[A-Z](\\\
          .\\d+)*\\.?$', chapter_id) or re.match(r'^\\d+(\\.\\d+)*\\.?$', chapter_id):\n\
          \            title_upper = chap.get(\"chapter_title\", \"\").upper()\n \
          \           if any(k in title_upper for k in [\"SCOPE\", \"GENERAL\", \"\
          INTRO\", \"æ€»åˆ™\", \"èŒƒå›´\", \"LEGISLATIVE\", \"FUNCTION\"]):\n            \
          \    start_index = i\n                break\n                \n    # print(f'chapters[str]:\
          \ {chapters[start_index]}')\n    filtered_chapters = chapters[start_index:]\n\
          \    skipped_content = chapters[:start_index]\n    skipped_text = \"\\n\"\
          .join([f\"{ch['chapter_id']} {ch['chapter_title']} {ch.get('raw_text','')}\"\
          \ for ch in skipped_content])\n\n    return filtered_chapters, skipped_text\n\
          \n\ndef smart_paragraph_join(lines: List[str]) -> str:\n    \"\"\"\n   \
          \ æ™ºèƒ½æ®µè½åˆå¹¶ï¼šåªåœ¨æ®µè½ç»“æŸæ—¶æ¢è¡Œ\n    \"\"\"\n    if not lines:\n        return \"\"\n\
          \    \n    result = []\n    current_paragraph = []\n    \n    for i, line\
          \ in enumerate(lines):\n        line = line.strip()\n        if not line:\
          \  # ç©ºè¡Œç›´æ¥è·³è¿‡\n            continue\n            \n        # æ£€æŸ¥æ˜¯å¦æ˜¯æ®µè½ç»“æŸçš„æ ‡å¿—\n\
          \        is_paragraph_end = False\n        \n        # 1. ä»¥æ ‡ç‚¹ç¬¦å·ç»“å°¾ï¼ˆä¸­è‹±æ–‡ï¼‰\n\
          \        if re.search(r'[ã€‚ï¼ï¼Ÿï¼›ï¼š.!?;:]$', line):\n            is_paragraph_end\
          \ = True\n            \n        # 2. æ£€æŸ¥ä¸‹ä¸€è¡Œæ˜¯å¦æ˜¯æ–°æ®µè½çš„å¼€å§‹\n        if i + 1 <\
          \ len(lines):\n            next_line = lines[i + 1].strip()\n          \
          \  # ä¸‹ä¸€è¡Œæ˜¯ç« èŠ‚æ ‡é¢˜ã€åˆ—è¡¨é¡¹ã€æˆ–æ˜æ˜¾çš„æ®µè½å¼€å§‹\n            if (detect_chapter(next_line) or\n\
          \                re.match(r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\\d]+[ã€\\.\\)]', next_line) or \
          \ # åˆ—è¡¨é¡¹\n                re.match(r'^[ï¼ˆ(]\\d+[ï¼‰)]', next_line) or  # ç¼–å·é¡¹\n\
          \                re.match(r'^[â€”â€”\\-â€”]+', next_line)):  # ç ´æŠ˜å·å¼€å¤´\n       \
          \         is_paragraph_end = True\n        \n        # 3. è¡¨æ ¼ç›¸å…³å†…å®¹ä¿æŒåŸæœ‰æ¢è¡Œ\n\
          \        if ('è¡¨' in line and re.search(r'è¡¨\\s*[A-Z0-9]', line)) or \\\n\
          \           re.match(r'^[|\\s]*[A-Za-z0-9\\u4e00-\\u9fa5]+[|\\s]*$', line):\
          \  # ç®€å•è¡¨æ ¼è¡Œæ£€æµ‹\n            current_paragraph.append(line)\n            is_paragraph_end\
          \ = True\n        else:\n            current_paragraph.append(line)\n  \
          \      \n        # å¦‚æœæ˜¯æ®µè½ç»“æŸï¼Œå°†å½“å‰æ®µè½åˆå¹¶å¹¶åŠ å…¥ç»“æœ\n        if is_paragraph_end:\n\
          \            if current_paragraph:\n                paragraph_text = ''.join(current_paragraph).strip()\n\
          \                if paragraph_text:\n                    result.append(paragraph_text)\n\
          \                current_paragraph = []\n    \n    # å¤„ç†æœ€åå‰©ä½™çš„æ®µè½\n    if current_paragraph:\n\
          \        paragraph_text = ' '.join(current_paragraph).strip()\n        if\
          \ paragraph_text:\n            result.append(paragraph_text)\n    \n   \
          \ return '\\n'.join(result)\n\ndef parse_pdf_to_chapter_tree(pdf_path: str)\
          \ -> Tuple[List[Dict], Dict[str, str]]:\n    \"\"\"\n    ä» PDF ä¸­æå–ç« èŠ‚æ ‘å’Œæœ¯è¯­æ˜ å°„\n\
          \    :param pdf_path: PDF æ–‡ä»¶è·¯å¾„\n    :return: (ç« èŠ‚æ ‘, æœ¯è¯­æ˜ å°„)\n    \"\"\"\n \
          \   cleaned_lines = extract_full_text_with_filter(pdf_path)\n\n    # \U0001F195\
          \ æ£€æµ‹æ–‡æ¡£è¯­è¨€\n    language = detect_document_language(cleaned_lines)\n    max_chapter_num\
          \ = 50 if language == 'zh' else 1000\n    print(f\"æ£€æµ‹åˆ°æ–‡æ¡£è¯­è¨€: {'ä¸­æ–‡' if language\
          \ == 'zh' else 'è‹±æ–‡'}, max_chapter_num={max_chapter_num}\")\n\n    # \U0001F195\
          \ ç¬¬ä¸€è½®ï¼šç²—ç•¥æå–æ‰€æœ‰å¯èƒ½çš„ç« èŠ‚ï¼Œç”¨äºåˆ†ææ•°å­—åˆ†å¸ƒ\n    preliminary_chapters = []\n    current =\
          \ {\n        \"chapter_id\": \"\",\n        \"chapter_title\": \"\",\n \
          \       \"raw_text\": \"\"\n    }\n    buffer = []\n\n    for line in cleaned_lines:\n\
          \        # ç¬¬ä¸€è½®ä½¿ç”¨å®½æ¾çš„æ•°å­—èŒƒå›´è¿›è¡Œç²—æå–\n        chapter_info = detect_chapter(line,\
          \ max_chapter_num=1000, language=language, number_analysis=None)\n\n   \
          \     if chapter_info:\n            if current:\n                current[\"\
          raw_text\"] = smart_paragraph_join(buffer)\n                preliminary_chapters.append(current)\n\
          \                buffer = []\n            current = {\n                \"\
          chapter_id\": chapter_info[\"chapter_id\"],\n                \"chapter_title\"\
          : chapter_info[\"chapter_title\"],\n                \"raw_text\": \"\"\n\
          \            }\n        else:\n            buffer.append(line)\n\n    if\
          \ current:\n        current[\"raw_text\"] = smart_paragraph_join(buffer)\n\
          \        preliminary_chapters.append(current)\n\n    # \U0001F195 åˆ†æç« èŠ‚æ•°å­—åˆ†å¸ƒ\n\
          \    number_analysis = analyze_chapter_number_distribution(preliminary_chapters)\n\
          \    print(f\"æ•°å­—åˆ†å¸ƒåˆ†æ: {number_analysis}\")\n\n    # \U0001F195 ç¬¬äºŒè½®ï¼šä½¿ç”¨åˆ†æç»“æœé‡æ–°ç²¾ç¡®æå–ç« èŠ‚\n\
          \    chapters = []\n    current = {\n        \"chapter_id\": \"\",\n   \
          \     \"chapter_title\": \"\",\n        \"raw_text\": \"\"\n    }\n    buffer\
          \ = []\n\n    for line in cleaned_lines:\n        chapter_info = detect_chapter(line,\
          \ max_chapter_num=max_chapter_num, language=language, number_analysis=number_analysis)\n\
          \n        if chapter_info:\n            if current:\n                # ä½¿ç”¨æ™ºèƒ½æ®µè½åˆå¹¶è€Œä¸æ˜¯ç®€å•çš„\
          \ \\n è¿æ¥\n                current[\"raw_text\"] = smart_paragraph_join(buffer)\n\
          \                chapters.append(current)\n                buffer = []\n\
          \            current = {\n                \"chapter_id\": chapter_info[\"\
          chapter_id\"],\n                \"chapter_title\": chapter_info[\"chapter_title\"\
          ],\n                \"raw_text\": \"\"\n            }\n        else:\n \
          \           buffer.append(line)\n\n    if current:\n        current[\"raw_text\"\
          ] = smart_paragraph_join(buffer)\n        chapters.append(current)\n\n \
          \   # 1ï¸âƒ£ å…ˆæŒ‰é™„ä»¶åˆ‡åˆ†é¡¶å±‚\n    attachment_sections = split_sections_by_attachment(chapters)\n\
          \n    tree = []\n\n    for top_sec in attachment_sections:\n        \n \
          \       # 2ï¸âƒ£ æ¯ä¸ªé¡¶å±‚å—å†æŒ‰é™„å½•åˆ‡åˆ†\n        sections = split_sections_by_appendix(top_sec[\"\
          chapters\"])\n        section_tree_list = []\n\n        for sec in sections:\n\
          \            # filtered_chapters, skipped_text = filter_start_of_main(sec[\"\
          chapters\"])\n            # 3ï¸âƒ£ å¯¹æ¯ä¸ªéƒ¨åˆ†å†…éƒ¨ä¿ç•™æœ€é•¿é“¾\n            valid_chaps_in_sec,\
          \ skipped_text = find_longest_chapter_chain_with_append(sec[\"chapters\"\
          ], language)\n            tree_in_sec = build_tree(valid_chaps_in_sec)\n\
          \            build_full_path(tree_in_sec)\n            # æ’å…¥é”®å€¼å¯¹ section\n\
          \            section_tree_list.append({\n                \"section\": sec[\"\
          section\"],\n                \"context\": skipped_text,\n              \
          \  \"chapters\": tree_in_sec,\n            })\n\n        # 4ï¸âƒ£ æ„å»ºé¡¶å±‚æ ‘\n \
          \       tree.append({\n            \"file\": top_sec[\"section\"],  # regulation\
          \ æˆ– ANNEX n\n            \"sections\": section_tree_list,\n        })\n\n\
          \    term_map = {}\n\n    for chap in chapters:\n        title = chap.get(\"\
          chapter_title\", \"\")\n        if \"æœ¯è¯­\" in title:\n            # æå–æœ¯è¯­\n\
          \            terms = extract_terms_with_abbr_from_terms_section(chap[\"\
          chapter_title\"])\n            term_map.update(terms)\n            for child\
          \ in chap.get(\"children\", []):\n                terms = extract_terms_with_abbr_from_terms_section(child[\"\
          chapter_title\"])\n                term_map.update(terms)\n        elif\
          \ \"ç¼©ç•¥\" in title:\n            # æå–ç¼©ç•¥è¯­\n            abbr_terms = extract_abbr_terms_from_symbols_section(chap[\"\
          chapter_title\"] + chap[\"raw_text\"])\n            term_map.update(abbr_terms)\n\
          \            for child in chap.get(\"children\", []):\n                abbr_terms\
          \ = extract_abbr_terms_from_symbols_section(child[\"chapter_title\"] + child[\"\
          raw_text\"])\n                term_map.update(abbr_terms)\n\n    return\
          \ tree, term_map\n\nimport tempfile\nimport requests\nfrom urllib.parse\
          \ import urlparse\n\ndef resolve_pdf_path(pdf_path: str) -> str:\n    #\
          \ å¦‚æœæ˜¯ URLï¼Œå°±ä¸‹è½½åˆ°ä¸´æ—¶æ–‡ä»¶å¤¹\n    if pdf_path.startswith(\"http://\") or pdf_path.startswith(\"\
          https://\"):\n        response = requests.get(pdf_path)\n        response.raise_for_status()\n\
          \        suffix = os.path.splitext(urlparse(pdf_path).path)[-1]\n      \
          \  with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp_file:\n\
          \            tmp_file.write(response.content)\n            return tmp_file.name\n\
          \    else:\n        return pdf_path\n\ndef count_leaf_nodes(chapters):\n\
          \    \"\"\"é€’å½’è®¡ç®—ç« èŠ‚æ ‘ä¸­çš„å¶å­èŠ‚ç‚¹æ•°é‡\"\"\"\n    total = 0\n    for chapter in chapters:\n\
          \        if chapter.get(\"children\") and len(chapter[\"children\"]) > 0:\n\
          \            # æœ‰å­èŠ‚ç‚¹ï¼Œé€’å½’è®¡ç®—\n            total += count_leaf_nodes(chapter[\"\
          children\"])\n        else:\n            # å¶å­èŠ‚ç‚¹\n            total += 1\n\
          \    return total\n\ndef extract_chapters_by_id(tree, chapter_ids):\n  \
          \  \"\"\"ä»æ ‘ç»“æ„ä¸­æå–æŒ‡å®šIDçš„ç« èŠ‚ï¼Œä¿æŒåŸæœ‰å±‚çº§ç»“æ„\"\"\"\n    result = []\n    \n    for file_item\
          \ in tree:\n        # åªå¤„ç† file ä¸º regulation çš„å†…å®¹\n        if file_item[\"\
          file\"] != \"regulation\":\n            continue\n            \n       \
          \ new_file = {\n            \"file\": file_item[\"file\"],\n           \
          \ \"sections\": []\n        }\n        \n        for section in file_item[\"\
          sections\"]:\n            new_section = {\n                \"section\":\
          \ section[\"section\"],\n                \"context\": section[\"context\"\
          ],\n                \"chapters\": []\n            }\n            \n    \
          \        # æå–åŒ¹é…çš„ç« èŠ‚\n            for chapter in section[\"chapters\"]:\n\
          \                if chapter[\"chapter_id\"] in chapter_ids:\n          \
          \          new_section[\"chapters\"].append(chapter)\n            \n   \
          \         # åªæœ‰å½“sectionæœ‰ç« èŠ‚æ—¶æ‰æ·»åŠ \n            if new_section[\"chapters\"]:\n\
          \                new_file[\"sections\"].append(new_section)\n        \n\
          \        # åªæœ‰å½“fileæœ‰sectionsæ—¶æ‰æ·»åŠ \n        if new_file[\"sections\"]:\n  \
          \          result.append(new_file)\n    \n    return result\n\ndef get_first_three_chapters_from_main(tree):\n\
          \    \"\"\"ä» MAIN section ä¸­è·å–å‰ä¸‰ç« çš„ chapter_id\"\"\"\n    for file_item in\
          \ tree:\n        if file_item[\"file\"] != \"regulation\":\n           \
          \ continue\n            \n        for section in file_item[\"sections\"\
          ]:\n            if section[\"section\"] == \"MAIN\":\n                #\
          \ è·å–å‰ä¸‰ç« çš„ chapter_id\n                chapter_ids = []\n                for\
          \ i, chapter in enumerate(section[\"chapters\"]):\n                    if\
          \ i < 3:  # å‰ä¸‰ç« \n                        chapter_ids.append(chapter[\"chapter_id\"\
          ])\n                    else:\n                        break\n         \
          \       return set(chapter_ids)\n    return set()\n\ndef group_main_chapters_by_leaf_count(tree,\
          \ max_leaf_nodes=30):\n    \"\"\"å°† MAIN section çš„ç« èŠ‚æŒ‰å¶å­èŠ‚ç‚¹æ•°é‡åˆ†ç»„\"\"\"\n   \
          \ groups = []\n    current_group_chapters = []\n    current_leaf_count =\
          \ 0\n    \n    # æ‰¾åˆ° regulation file çš„ MAIN section\n    main_chapters =\
          \ []\n    for file_item in tree:\n        if file_item[\"file\"] != \"regulation\"\
          :\n            continue\n            \n        for section in file_item[\"\
          sections\"]:\n            if section[\"section\"] == \"MAIN\":\n       \
          \         # è·³è¿‡å‰ä¸‰ç« ï¼ˆcontextéƒ¨åˆ†ï¼‰\n                for i, chapter in enumerate(section[\"\
          chapters\"]):\n                    # if i >= 3:  # ä»ç¬¬å››ç« å¼€å§‹\n            \
          \        main_chapters.append(chapter)\n                break\n        break\n\
          \    \n    for chapter in main_chapters:\n        chapter_leaf_count = count_leaf_nodes([chapter])\n\
          \        \n        # å¦‚æœå•ä¸ªç« èŠ‚å°±è¶…è¿‡max_leaf_nodesï¼Œå•ç‹¬æˆç»„\n        if chapter_leaf_count\
          \ > max_leaf_nodes:\n            # å…ˆå¤„ç†å½“å‰ç§¯ç´¯çš„ç« èŠ‚ç»„\n            if current_group_chapters:\n\
          \                groups.append(current_group_chapters)\n               \
          \ current_group_chapters = []\n                current_leaf_count = 0\n\
          \            \n            # å•ç‹¬æˆç»„\n            groups.append([chapter[\"\
          chapter_id\"]])\n        else:\n            # æ£€æŸ¥åŠ å…¥åæ˜¯å¦è¶…è¿‡é™åˆ¶\n            if\
          \ current_leaf_count + chapter_leaf_count <= max_leaf_nodes:\n         \
          \       current_group_chapters.append(chapter[\"chapter_id\"])\n       \
          \         current_leaf_count += chapter_leaf_count\n            else:\n\
          \                # è¶…è¿‡é™åˆ¶ï¼Œå…ˆä¿å­˜å½“å‰ç»„ï¼Œå¼€å§‹æ–°ç»„\n                if current_group_chapters:\n\
          \                    groups.append(current_group_chapters)\n           \
          \     current_group_chapters = [chapter[\"chapter_id\"]]\n             \
          \   current_leaf_count = chapter_leaf_count\n    \n    # å¤„ç†æœ€åä¸€ç»„\n    if\
          \ current_group_chapters:\n        groups.append(current_group_chapters)\n\
          \    \n    return groups\n\ndef get_non_main_sections(tree):\n    \"\"\"\
          è·å–æ‰€æœ‰é MAIN çš„ sections\"\"\"\n    non_main_sections = []\n    \n    for file_item\
          \ in tree:\n        if file_item[\"file\"] != \"regulation\":\n        \
          \    continue\n            \n        for section in file_item[\"sections\"\
          ]:\n            if section[\"section\"] != \"MAIN\":\n                non_main_sections.append(section[\"\
          section\"])\n    \n    return non_main_sections\n\ndef create_section_tree(tree,\
          \ section_name):\n    \"\"\"åˆ›å»ºåŒ…å«æŒ‡å®š section çš„å®Œæ•´æ ‘ç»“æ„\"\"\"\n    result = []\n\
          \    \n    for file_item in tree:\n        if file_item[\"file\"] != \"\
          regulation\":\n            continue\n            \n        new_file = {\n\
          \            \"file\": file_item[\"file\"],\n            \"sections\": []\n\
          \        }\n        \n        for section in file_item[\"sections\"]:\n\
          \            if section[\"section\"] == section_name:\n                new_file[\"\
          sections\"].append(section)\n                break\n        \n        if\
          \ new_file[\"sections\"]:\n            result.append(new_file)\n       \
          \     break\n    \n    return result\n\ndef main(url: str) -> dict:\n  \
          \  pdf_path = resolve_pdf_path(url)  # å…¼å®¹ URL å’Œæœ¬åœ°è·¯å¾„\n\n    # ç« èŠ‚æ ‘\n    tree,\
          \ term_map = parse_pdf_to_chapter_tree(pdf_path)\n    \n    # æå–å‰ä¸‰ç« çš„ chapter_idï¼ˆä»\
          \ MAIN sectionï¼‰\n    first_three_chapter_ids = get_first_three_chapters_from_main(tree)\n\
          \    \n    # æå–å‰ä¸‰ç« ä½œä¸ºcontext\n    context_tree = extract_chapters_by_id(tree,\
          \ first_three_chapter_ids)\n    \n    # æŒ‰å¶å­èŠ‚ç‚¹æ•°é‡åˆ†ç»„ MAIN section çš„å…¶ä½™ç« èŠ‚\n \
          \   main_chapter_groups = group_main_chapters_by_leaf_count(tree, max_leaf_nodes=20)\n\
          \    \n    # è·å–æ‰€æœ‰é MAIN çš„ sections\n    non_main_sections = get_non_main_sections(tree)\n\
          \    \n    # ä¸ºæ¯ä¸ª MAIN ç« èŠ‚ç»„å’Œé MAIN section åˆ›å»ºå®Œæ•´çš„æ ‘ç»“æ„\n    array_items = []\n\
          \    \n    # å¤„ç† MAIN section çš„ç« èŠ‚ç»„\n    for group_chapter_ids in main_chapter_groups:\n\
          \        group_tree = extract_chapters_by_id(tree, set(group_chapter_ids))\n\
          \        if group_tree:  # ç¡®ä¿ç»„ä¸ä¸ºç©º\n            array_items.append(json.dumps(group_tree,\
          \ ensure_ascii=False))\n    \n    # å¤„ç†é MAIN sectionsï¼ˆæ¯ä¸ªé™„å½•ä½œä¸ºä¸€ä¸ªç‹¬ç«‹çš„ itemï¼‰\n\
          \    for section_name in non_main_sections:\n        section_tree = create_section_tree(tree,\
          \ section_name)\n        if section_tree:  # ç¡®ä¿sectionä¸ä¸ºç©º\n            array_items.append(json.dumps(section_tree,\
          \ ensure_ascii=False))\n\n    return {\n        \"tree\":json.dumps(tree,\
          \ ensure_ascii=False),\n        \"context\": json.dumps(context_tree, ensure_ascii=False),\n\
          \        \"array\": array_items\n    }"
        code_language: python3
        desc: ''
        outputs:
          array:
            children: null
            type: array[string]
          context:
            children: null
            type: string
          tree:
            children: null
            type: string
        selected: false
        title: ä»£ç æ‰§è¡Œ
        type: code
        variables:
        - value_selector:
          - '1756550268945'
          - file
          - url
          value_type: file
          variable: url
      height: 53
      id: '1756550411122'
      position:
        x: 334
        y: 459
      positionAbsolute:
        x: 334
        y: 459
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        desc: ''
        error_handle_mode: continue-on-error
        height: 236
        is_parallel: true
        iterator_input_type: array[string]
        iterator_selector:
        - '1756550411122'
        - array
        output_selector:
        - '1756698812908'
        - output
        output_type: array[string]
        parallel_nums: 10
        selected: false
        start_node_id: 1756557801310start
        title: è¿­ä»£
        type: iteration
        width: 1664
      height: 236
      id: '1756557801310'
      position:
        x: 638
        y: 459
      positionAbsolute:
        x: 638
        y: 459
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 1664
      zIndex: 1
    - data:
        desc: ''
        isInIteration: true
        selected: false
        title: ''
        type: iteration-start
      draggable: false
      height: 48
      id: 1756557801310start
      parentId: '1756557801310'
      position:
        x: 60
        y: 100.5
      positionAbsolute:
        x: 698
        y: 559.5
      selectable: false
      sourcePosition: right
      targetPosition: left
      type: custom-iteration-start
      width: 44
      zIndex: 1002
    - data:
        context:
          enabled: true
          variable_selector:
          - '1756550411122'
          - context
        desc: ''
        isInIteration: true
        isInLoop: false
        iteration_id: '1756557801310'
        model:
          completion_params: {}
          mode: chat
          name: deepseek-reasoner
          provider: langgenius/deepseek/deepseek
        prompt_template:
        - id: b8df94d2-1037-41d9-8649-be6e2790a7d5
          role: system
          text: "# è§’è‰²\n\nä½ æ˜¯ä¸€åæŠ€æœ¯æ ‡å‡†çŸ¥è¯†å·¥ç¨‹ä¸“å®¶ï¼Œä¸“æ³¨äºå°†æ±½è½¦åŠç›¸å…³é¢†åŸŸçš„æ ‡å‡†å’Œæ³•è§„æ–‡æ¡£è½¬åŒ–ä¸ºå¯ç»“æ„åŒ–è§£æçš„æ•°æ®èµ„äº§ã€‚\n\nä½ çš„ä»»åŠ¡æ˜¯åŸºäºè¾“å…¥çš„æ ‡å‡†æ–‡æ¡£\
            \ JSON æ ‘ç»“æ„ï¼Œç²¾å‡†æŠ½å–æŠ€æœ¯æ¡æ¬¾ã€å®éªŒè¦æ±‚ã€å‚æ•°çº¦æŸå’Œå¼•ç”¨ä¿¡æ¯ï¼Œç”Ÿæˆç»Ÿä¸€çš„æœºå™¨å¯è¯»æ•°æ®ï¼Œä»¥æ”¯æŒè·¨æ ‡å‡†æ¡æ¬¾æ¯”å¯¹ã€æ³•è§„ä¸€è‡´æ€§éªŒè¯ã€è®¾å¤‡èƒ½åŠ›è¯„ä¼°ç­‰ä¸‹æ¸¸åº”ç”¨ã€‚\n\
            \n---\n\n# è¾“å…¥å†…å®¹\n\nä½ å°†æ”¶åˆ°ä¸€ä¸ª JSON æ•°ç»„ï¼ŒåŒ…å«å¤šä¸ªå¯¹è±¡ï¼Œæ¯ä¸ªå¯¹è±¡çš„ç»“æ„å¦‚ä¸‹ï¼š\n\n- `file`: æ–‡ä»¶æ ‡è¯†ï¼ˆå¦‚\
            \ \"regulation\"ã€\"ANNEX 1\"ï¼‰\n- `sections`: å—æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼š\n  - `section`:\
            \ æ ‡è¯†å½“å‰å—ï¼ˆå¦‚ \"MAIN\", \"APPENDIX 1\"ï¼‰\n  - `context`: å½“å‰å—æ ‡é¢˜ä¸ç¬¬ä¸€ä¸ªç« èŠ‚é—´çš„æ–‡æœ¬ï¼ˆæ€»è¿°ï¼‰\n\
            \  - `chapters`: ç« èŠ‚æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼š\n    - `chapter_id`: ç« èŠ‚ç¼–å·ï¼ˆä¿æŒåŸæ ·ï¼‰\n    - `chapter_title`:\
            \ ç« èŠ‚æ ‡é¢˜\n    - `raw_text`: è¯¥èŠ‚çš„çº¯æ–‡æœ¬å†…å®¹\n    - `children`: å­æ¡æ¬¾æ•°ç»„ï¼ˆç»“æ„ä¸çˆ¶çº§ç›¸åŒï¼‰\n\
            \    - `full_path`: å®Œæ•´è·¯å¾„ï¼ˆå¯é€‰ï¼‰\n\n> åŒæ—¶æä¾›èƒŒæ™¯ä¸Šä¸‹æ–‡ï¼ˆ{{#context#}}ï¼‰ï¼ŒåŒ…å«èŒƒå›´ã€æœ¯è¯­å®šä¹‰ç­‰ç« èŠ‚ï¼Œä¾¿äºç†è§£ã€‚\n\
            \n---\n\n# è¾“å‡ºæ ¼å¼\n\nè¯·è¾“å‡ºä¸€ä¸ª JSON å¯¹è±¡ï¼ˆä¸å¾—åŒ…å«ä»»ä½•è§£é‡Šæ€§æ–‡å­—ï¼‰ï¼Œå¿…é¡»å®Œå…¨ç¬¦åˆä»¥ä¸‹ç»“æ„ï¼š\n\n```json\n\
            \    {\n      \"file\": \"string\",\n      \"section\": \"string\",\n\
            \      \"experiment_root_ids\": [\"string\", \"...\"],\n       \"chapters\"\
            :[\n        {      \n          \"chapter_id\": \"string\",\n\n       \
            \   \"paramaters\": [\n            {\n              \"item\": \"string\"\
            ,\n              \"constraint\": \"<=|<|=|>=|>|range_closed|range_open|enum|boolean\"\
            ,\n              \"value\": \"string|array|null\",\n              \"unit\"\
            : \"string|null\",\n              \"source_text\": \"string\"\n      \
            \      }\n          ],\n\n          \"topic_keywords\": [\"string\", \"\
            ...\"],\n          \"context_keywords\": [\"string\", \"...\"],\n\n  \
            \        \"refs\": [\n            {\n              \"ref_type\": \"internal|external\"\
            ,\n              \"doc_id\": \"string|null\",\n              \"target_id\"\
            : \"string\",\n              \"anchor_text\": \"string\"\n           \
            \ }\n          ],\n\n          \"table_headers\": [\"string\", \"...\"\
            ]}\n        ]\n\n    }\n```\n\n# å¤„ç†é€»è¾‘ï¼ˆé“¾å¼æ€è€ƒï¼‰\n\n1. **éå†ç« èŠ‚æ ‘**\n    éå†æ¯ä¸ª\
            \ `chapter_id` åŠå…¶å­ç« èŠ‚ï¼Œåˆ†åˆ«å¤„ç†ï¼Œç¡®ä¿è¾“å‡ºä¸­æ¯ä¸ªç« èŠ‚éƒ½ç‹¬ç«‹æˆæ¡ã€‚\n2. **å‚æ•°æå– (`paramaters`)**\n\
            \   - æå–æ ‡å‡†ä¸­çš„å®šé‡æŒ‡æ ‡æˆ–çº¦æŸæ¡ä»¶ï¼Œæ¯æ¡å‚æ•°ç‹¬ç«‹æˆå¯¹è±¡ã€‚\n   - `constraint`ï¼š\n     - å•å€¼çº¦æŸï¼š`<`,\
            \ `<=`, `>`, `>=`, `=`\n     - åŒºé—´é—­åˆï¼š`range_closed`\n     - åŒºé—´å¼€åŒºé—´ï¼š`range_open`\n\
            \     - æšä¸¾ï¼š`enum`\n     - å¸ƒå°”å€¼ï¼š`boolean`\n   - `value`ï¼šä¿æŒå­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²æ•°ç»„ï¼Œä¸­æ–‡æ•°å­—éœ€è½¬é˜¿æ‹‰ä¼¯æ•°å­—ã€‚\n\
            \   - `source_text`ï¼šä¿ç•™åŸæ–‡å®Œæ•´ç‰‡æ®µï¼Œç¬¦å·ä¸åšæ›¿æ¢ã€‚\n3. **å…³é”®è¯æå–**\n   - `topic_keywords`ï¼šæ ¸å¿ƒä¸»é¢˜å…³é”®è¯ï¼Œçªå‡ºå½“å‰ç« èŠ‚å›´ç»•ä»€ä¹ˆä¸»é¢˜å±•å¼€ï¼Œå¯¹ä»€ä¹ˆå†…å®¹è¿›è¡Œè§„å®šï¼Œåè¯çŸ­è¯­ï¼Œ1-6ä¸ªã€‚\n\
            \   - `context_keywords`ï¼šè¾…åŠ©ä¸Šä¸‹æ–‡å…³é”®è¯ï¼Œæè¿°å®éªŒå¯¹è±¡ã€ç¯å¢ƒã€æ¡ä»¶ç­‰ï¼Œ0-6ä¸ª\n4. **å¼•ç”¨æå– (`refs`)**\n\
            \   - æå–å†…éƒ¨å¼•ç”¨ï¼ˆæœ¬æ ‡å‡†å†…ç« èŠ‚/è¡¨æ ¼/å›¾ç‰‡ï¼‰ä¸å¤–éƒ¨å¼•ç”¨ï¼ˆå¤–éƒ¨æ ‡å‡†ç¼–å·ï¼‰ï¼Œå¹¶åŒºåˆ† `ref_type`ã€‚\n   - `doc_id`\
            \ å¡«å†™æ ‡å‡†ç¼–å·ï¼ˆå¦‚æœ‰ï¼‰ï¼Œå†…éƒ¨å¼•ç”¨å¡« `null`ã€‚\n   - `target_id` ä¿ç•™å¼•ç”¨ç›®æ ‡ç¼–å·ï¼ˆå¦‚â€œB.2.1.1â€ã€â€œè¡¨B.1â€ï¼‰ã€‚\n\
            \   - `anchor_text` åœ¨ä¸Šä¸‹æ–‡æå–ç®€è¦æ–‡æœ¬ï¼Œè¯´æ˜å¼•ç”¨çš„å†…å®¹ï¼Œç¡®ä¿å¯ç²¾ç¡®å®šä½ã€‚\n   - åŸæ–‡ç›¸åŒä½ç½®å‡ºç°çš„å¼•ç”¨ï¼Œå¯åªç”Ÿæˆä¸€æ¡å†…å®¹ï¼ŒåŒæ ·ä¿æŒå¹¶åˆ—å³å¯ã€‚ä¾‹å¦‚ï¼šæŒ‰Aã€Bè¿›è¡Œå®éªŒï¼Œæå–æ—¶å¯å°†Aã€Bå¹¶åˆ—ï¼Œè€Œä¸ç”¨ç”Ÿæˆä¸¤æ¡å†…å®¹ã€‚\n\
            5. **è¡¨æ ¼è¡¨å¤´ (`table_headers`)**\n   - å¦‚æœå½“å‰ç« èŠ‚åŒ…å«è¡¨æ ¼å¼•ç”¨ï¼Œæå–è¡¨æ ¼çš„è¡¨å¤´å­—æ®µï¼ŒæŒ‰é¡ºåºè¾“å‡ºã€‚\n6.\
            \ **å®éªŒç« èŠ‚è¯†åˆ« (`experiment_root_ids`)**\n   - åˆ¤æ–­ç« èŠ‚å†…å®¹æ˜¯å¦ä¸ºå®éªŒç« èŠ‚ï¼Œæ ‡è®°å…¶ `chapter_id`\
            \ ä¸ºå®éªŒæ ¹èŠ‚ç‚¹ã€‚æœ‰ä»¥ä¸‹ä¸‰ç§æƒ…å†µï¼š\n     - æ ‡é¢˜ä¸­ç›´æ¥åŒ…å«äº†å…³é”®å­—ä¾‹å¦‚å®éªŒã€testç­‰ï¼Œä½†éœ€æ³¨æ„åŒºåˆ†å®éªŒå’Œå®éªŒçš„éƒ¨åˆ†ï¼Œæ¯”å¦‚â€œD.1\
            \ å®éªŒæ¡ä»¶â€ã€â€œD.2 å®éªŒæ–¹æ³•â€ã€â€œé™„å½•D è‡ªæ£€è¯•éªŒæ–¹æ³•â€ï¼Œåˆ™â€œé™„å½•Dâ€åº”è¯¥ä½œä¸ºå®éªŒæ ¹èŠ‚ç‚¹ï¼Œå°½ç®¡D.1/D.2éƒ½åŒ…å«äº†å…³é”®è¯å®éªŒ\n \
            \    - æ­£æ–‡ä¸­åŒ…å«äº†â€œæŒ‰XXå®éªŒâ€ç­‰å†…å®¹\n     - æ ¹æ®æ¨æ–­ï¼Œè¯¥ç« èŠ‚åŠå…¶å­ç« èŠ‚å‡å›´ç»•æŸä¸ªå®éªŒå±•å¼€\n   - è‹¥ä¸€ä¸ªç« èŠ‚æ˜¯å®éªŒç« èŠ‚ï¼Œåˆ™æ‰€æœ‰å­ç« èŠ‚éƒ½ç®—å®éªŒå†…å®¹ï¼Œæ— éœ€å•ç‹¬é‡å¤æ ‡æ³¨ã€‚ä½†è‹¥æ˜¯å­ç« èŠ‚åŒ…å«äº†ä¸åŒçš„å®éªŒï¼Œåˆ™éœ€è¦å¯¹æ¯ä¸ªå®éªŒè¿›è¡Œæ ‡è®°ï¼Œè€Œä¸æ˜¯å½“å‰ç« èŠ‚\n\
            \   - è‹¥ç»¼åˆåˆ¤å®šä¸€ä¸ªsectionä»…å›´ç»•ä¸€ä¸ªå®éªŒå±•å¼€ï¼ˆä¾‹å¦‚ï¼šé™„å½•D è‡ªæ£€å®éªŒæ–¹æ³•ï¼‰ï¼Œåˆ™ä»¥ç‰¹æ®Šæ ‡è®°ALLä½œä¸ºç»“æœï¼ˆä¾‹å¦‚ï¼šexperiment_root_idsï¼š[\"\
            ALL\"]ï¼Œç¦æ­¢ä»…ä¿ç•™â€œDâ€å¯¼è‡´ä»£ç æ— æ³•åŒ¹é…ï¼‰\n7. **è¾“å‡ºè§„èŒƒ**\n   - å¿…é¡»è¾“å‡ºæ‰€æœ‰ç« èŠ‚çš„ç»“æ„åŒ–ä¿¡æ¯ã€‚\n   - `experiment_root_ids`\
            \ åˆ—è¡¨ä¸­ä»…åŒ…å«æœ€é¡¶å±‚å®éªŒèŠ‚ç‚¹ã€‚\n\n------\n\n# æ³¨æ„äº‹é¡¹\n\n- è¾“å‡ºå¿…é¡»æ˜¯ä¸¥æ ¼ JSONï¼Œä¸å¾—æ·»åŠ è§£é‡Šè¯´æ˜ã€æ³¨é‡Šæˆ– Markdown\
            \ ç¬¦å·ã€‚\n- ä¸å…è®¸æ¨æ–­ç« èŠ‚ç¼–å·æˆ–è™šæ„å€¼ï¼Œåªèƒ½æ ¹æ®è¾“å…¥å†…å®¹æå–ã€‚\n- ç¼ºå¤±ä¿¡æ¯æ—¶è¿”å›ç©ºæ•°ç»„`[]`æˆ–`null`ï¼Œä¸çœç•¥å­—æ®µã€‚\n-\
            \ æ•°å€¼ç»Ÿä¸€ç”¨é˜¿æ‹‰ä¼¯æ•°å­—ï¼Œä¸è¦æ·»åŠ å•ä½åˆ° `value` å­—æ®µã€‚\n- å¯¹äºè¡¨æ ¼å†…å®¹æ— éœ€æå–ä¸ºtable_headerså¤–çš„ä»»ä½•å­—æ®µ\n-\
            \ topic_keywordså¹¶ä¸æ˜¯è¶Šå¤šè¶Šå¥½ï¼Œè€Œæ˜¯æœåŠ¡äºåç»­å‘é‡åŒ–-å¬å›æ­¥éª¤ã€‚èŒƒå›´ã€å¼•ç”¨æ€§æ–‡ä»¶ç­‰çº²é¢†æ€§å†…å®¹ï¼Œä»…éœ€å¯¹ç« èŠ‚é¡¶å±‚ç¼–å·æå–ä¸€ä¸ªtopic_keywordsï¼Œæœ¯è¯­ç­‰ç« èŠ‚ï¼Œä»…éœ€å¯¹æ¯ä¸ªé˜é‡Šæœ¯è¯­çš„ç« èŠ‚æå–ä¸€ä¸ªtopic_keywords\n\
            - `topic_keywords` å’Œ `context_keywords`ä¿ç•™åŸæ–‡ï¼Œå¯ä¸ºä¸­æ–‡æˆ–è‹±æ–‡\n- æ‰€æœ‰å¼•ç”¨å¿…é¡»èƒ½åœ¨åŸæ–‡ä¸­ç²¾å‡†å®šä½ï¼Œä¸”å¿…é¡»æ˜¯å®é™…å¼•ç”¨æ‰èƒ½æå–refså­—æ®µï¼Œå³ä¸Šä¸‹æ–‡æœ‰ç±»ä¼¼äºâ€œå‚è§XXâ€æˆ–è€…â€œæŒ‰XXå®éªŒâ€ç­‰è¡¨è¿°ï¼Œè‹¥æ˜¯å­¤ç«‹ã€çªå…€å‡ºç°åˆ™å¯ä»¥ç†è§£ä¸ºé¡µçœ‰è¢«é”™è¯¯è§£æï¼Œæˆ–è€…â€œè§„èŒƒæ€§å¼•ç”¨æ–‡ä»¶â€ç« èŠ‚ä¸­å¯¹å¼•ç”¨æ–‡ä»¶çš„ç½—åˆ—\n\
            \n------\n\n# Few-shot ç¤ºä¾‹\n\n## è¾“å…¥ç¤ºä¾‹1\n\n```\n[\n  {\n    \"file\": \"\
            regulation\",\n    \"sections\": [\n      {\n        \"section\": \"é™„å½•B\"\
            ,\n        \"context\": \"(è§„èŒƒæ€§)è‡ªåŠ¨è§¦å‘è¯•éªŒæ–¹æ³•\",\n        \"chapters\": [\n\
            \          {\n          {\n            \"chapter_id\": \"B.2\",\n    \
            \        \"chapter_title\": \"è¯•éªŒé¡¹ç›®\",\n            \"raw_text\": \"\"\
            ,\n            \"children\": [\n              {\n                \"chapter_id\"\
            : \"B.2.1\",\n                \"chapter_title\": \"æ­£é¢ç¢°æ’\",\n         \
            \       \"raw_text\": \"\",\n                \"children\": [\n       \
            \           {\n                    \"chapter_id\": \"B.2.1.1\",\n    \
            \                \"chapter_title\": \"æ»‘å°æ­£é¢ç¢°æ’è¯•éªŒ\",\n                  \
            \  \"raw_text\": \"\",\n                    \"children\": [\n        \
            \              {\n                        \"chapter_id\": \"B.2.1.1.1\"\
            ,\n                        \"chapter_title\": \"\",\n                \
            \        \"raw_text\": \"å°†ç™½è½¦èº«æˆ–å·¥è£…å›ºå®šåœ¨ç¢°æ’è¯•éªŒæ»‘å°ä¸Š,å®‰è£…æ–¹å‘æ¨¡æ‹Ÿæ­£é¢ç¢°æ’ã€‚ \",\n         \
            \               \"children\": [],\n                        \"full_path\"\
            : \"B.2 è¯•éªŒé¡¹ç›®/B.2.1 æ­£é¢ç¢°æ’/B.2.1.1 æ»‘å°æ­£é¢ç¢°æ’è¯•éªŒ/B.2.1.1.1 \"\n              \
            \        },\n                      {\n                        \"chapter_id\"\
            : \"B.2.1.1.2\",\n                        \"chapter_title\": \"\",\n \
            \                       \"raw_text\": \"æ»‘å°æŒ‰ç…§ä»¥ä¸‹åŠ é€Ÿåº¦æ³¢å½¢ä¹‹ä¸€è¿›è¡Œç¢°æ’è¯•éªŒã€‚ a) ä½¿ç”¨åˆ¶é€ å•†æŒ‡å®šçš„åŠ é€Ÿåº¦æ³¢å½¢è¿›è¡Œè¯•éªŒ,æŒ‡å®šçš„åŠ é€Ÿåº¦æ³¢å½¢åº”ä¸ºåœ¨B.2.1.2ä¸­æè¿°çš„å®è½¦ç¢°æ’è¯•éªŒæ¡ä»¶ä¸­,è½¦èº«éå˜å½¢åŒºåŸŸé‡‡é›†çš„åŠ é€Ÿåº¦-æ—¶é—´æ›²çº¿,å¹¶ç»è¿‡æ»¤æ³¢ç­‰çº§CFC60\
            \ æ»¤æ³¢æˆ–100Hzä½é€šæ»¤æ³¢ã€‚å®é™…è¯•éªŒç»“æœæ³¢å½¢çš„ç§¯åˆ†é€Ÿåº¦å˜åŒ–é‡Î”vs( t)åº”åœ¨ä»»æ„æ—¶åˆ»,ä¸è¶…è¿‡æŒ‡å®šæ³¢å½¢çš„ç§¯åˆ†é€Ÿåº¦å˜åŒ–é‡[Î”vt( t)Â±1]km/hçš„èŒƒå›´ã€‚\\\
            nb) æŒ‰å›¾B.1 çš„æ ‡å‡†åŠ é€Ÿåº¦é€šé“èŒƒå›´å’Œè¡¨B.1 çš„å‚æ•°è¿›è¡ŒåŠ é€Ÿæˆ–å‡é€Ÿ,å…¶é€Ÿåº¦å˜åŒ–é‡Î”v ä¸º\\n(25Â±1)km/hã€‚\\nGB45672â€”2025å›¾B.1\
            \ æ­£é¢ç¢°æ’è‡ªåŠ¨è§¦å‘åŠ é€Ÿåº¦é€šé“è¡¨B.1 æ­£é¢ç¢°æ’è‡ªåŠ¨è§¦å‘åŠ é€Ÿåº¦å‚æ•°\\nç‚¹\\næ—¶é—´t\\nms\\nåŠ é€Ÿåº¦ä¸‹é™(Ã—g) ç‚¹ æ—¶é—´tms\\\
            nåŠ é€Ÿåº¦ä¸Šé™(Ã—g) A 15 0 E 0 3 B 45 10 F 40 17 C 60 10 G 63 17 D 85 0 H 105 0\"\
            ,\n                        \"children\": [],\n                       \
            \ \"full_path\": \"B.2 è¯•éªŒé¡¹ç›®/B.2.1 æ­£é¢ç¢°æ’/B.2.1.1 æ»‘å°æ­£é¢ç¢°æ’è¯•éªŒ/B.2.1.1.2 \"\n\
            \                      }\n                    ],\n                   \
            \ \"full_path\": \"B.2 è¯•éªŒé¡¹ç›®/B.2.1 æ­£é¢ç¢°æ’/B.2.1.1 æ»‘å°æ­£é¢ç¢°æ’è¯•éªŒ\"\n          \
            \        },\n                ],\n              },\n            ],\n  \
            \        }\n        ]\n      }\n    ]\n  }\n]\n```\n\n## è¾“å‡ºç¤ºä¾‹1\n\n```\n\
            \  {\n      \"file\": \"regulation\",\n      \"section\": \"é™„å½•B\",\n \
            \     \"experiment_root_ids\": [\"B.2.1.1\"]\n      \"chapters\":[\n \
            \         {\n      \"chapter_id\": \"B.2\",\n      \"topic_keywords\"\
            : [\"è¯•éªŒé¡¹ç›®\", \"è‡ªåŠ¨è§¦å‘è¯•éªŒæ–¹æ³•\"],\n    },\n        {\n      \"chapter_id\":\
            \ \"B.2.1\",\n      \"topic_keywords\": [\"æ­£é¢ç¢°æ’\"],\n    },\n        {\n\
            \      \"chapter_id\": \"B.2.1.1\",\n      \"topic_keywords\": [\"æ»‘å°æ­£é¢ç¢°æ’è¯•éªŒ\"\
            ],\n    },\n        {\n      \"chapter_id\": \"B.2.1.1.1\",\n      \"\
            topic_keywords\": [\"å®‰è£…æ–¹å‘\", \"æ­£é¢ç¢°æ’\"],\n      \"context_keywords\": [\"\
            ç™½è½¦èº«\", \"å·¥è£…\", \"ç¢°æ’è¯•éªŒæ»‘å°\"],\n    },\n     {\n      \"chapter_id\": \"\
            B.2.1.1.2\",\n      \"paramaters\": [\n        {\n          \"item\":\
            \ \"é€Ÿåº¦å˜åŒ–é‡Î”v\",\n          \"constraint\": \"=\",\n          \"value\"\
            : \"25\",\n          \"unit\": \"km/h\",\n          \"source_text\": \"\
            å…¶é€Ÿåº¦å˜åŒ–é‡Î”vä¸º(25Â±1)km/h\"\n        }\n      ],\n      \"topic_keywords\":\
            \ [\"æ»‘å°æ­£é¢ç¢°æ’è¯•éªŒ\",\"æ»‘å°\",\"åŠ é€Ÿåº¦æ³¢å½¢\", \"ç¢°æ’è¯•éªŒ\", \"åŠ é€Ÿåº¦-æ—¶é—´æ›²çº¿\"],\n      \"context_keywords\"\
            : [\"CFC60æ»¤æ³¢\", \"100Hzä½é€šæ»¤æ³¢\"],\n      \"refs\": [\n        {\n      \
            \    \"ref_type\": \"internal\",\n          \"doc_id\": null,\n      \
            \    \"target_id\": \"B.2.1.2\",\n          \"anchor_text\": \"å®è½¦ç¢°æ’è¯•éªŒæ¡ä»¶-åŠ é€Ÿåº¦æ³¢å½¢\"\
            \n        },\n        {\n          \"ref_type\": \"internal\",\n     \
            \     \"doc_id\": null,\n          \"target_id\": \"å›¾B.1\",\n        \
            \  \"anchor_text\": \"\"\n        },\n        {\n          \"ref_type\"\
            : \"internal\",\n          \"doc_id\": null,\n          \"target_id\"\
            : \"è¡¨B.1\",\n          \"anchor_text\": \"\"\n        },\n      ],\n \
            \     \"table_headers\": [\"ç‚¹\", \"æ—¶é—´t(ms)\", \"åŠ é€Ÿåº¦ä¸‹é™(Ã—g)\", \"åŠ é€Ÿåº¦ä¸Šé™(Ã—g)\"\
            ]\n    }\n      ]\n  }\n\n```\n\n## è¾“å…¥ç¤ºä¾‹2\n\n```\n[\n  {\n    \"file\"\
            : \"regulation\",\n    \"sections\": [\n      {\n        \"section\":\
            \ \"MAIN\",\n        \"context\": \"\",\n        \"chapters\": [\n   \
            \       {\n            \"chapter_id\": \"1\",\n            \"chapter_title\"\
            : \"èŒƒå›´\",\n            \"raw_text\": \"æœ¬æ–‡ä»¶è§„å®šäº†è½¦è½½äº‹æ•…ç´§æ€¥å‘¼å«ç³»ç»Ÿçš„æŠ€æœ¯è¦æ±‚ã€åŒä¸€å‹å¼åˆ¤å®šè¦æ±‚,æè¿°äº†ç›¸åº”çš„è¯•éªŒæ–¹æ³•ã€‚\\\
            næœ¬æ–‡ä»¶é€‚ç”¨äºM1 ç±»åŠN1 ç±»è½¦è¾†çš„è½¦è½½äº‹æ•…ç´§æ€¥å‘¼å«ç³»ç»Ÿã€‚\",\n            \"children\": [],\n   \
            \         \"full_path\": \"1 èŒƒå›´\"\n          },\n          {\n       \
            \     \"chapter_id\": \"2\",\n            \"chapter_title\": \"è§„èŒƒæ€§å¼•ç”¨æ–‡ä»¶\"\
            ,\n            \"raw_text\": \"ä¸‹åˆ—æ–‡ä»¶ä¸­çš„å†…å®¹é€šè¿‡æ–‡ä¸­çš„è§„èŒƒæ€§å¼•ç”¨è€Œæ„æˆæœ¬æ–‡ä»¶å¿…ä¸å¯å°‘çš„æ¡æ¬¾ã€‚å…¶ä¸­,æ³¨æ—¥æœŸçš„å¼•ç”¨æ–‡ä»¶,ä»…è¯¥æ—¥æœŸå¯¹åº”çš„ç‰ˆæœ¬é€‚ç”¨äºæœ¬æ–‡ä»¶;ä¸æ³¨æ—¥æœŸçš„å¼•ç”¨æ–‡ä»¶,å…¶æœ€æ–°ç‰ˆæœ¬(åŒ…æ‹¬æ‰€æœ‰çš„ä¿®æ”¹å•)é€‚ç”¨äºæœ¬æ–‡ä»¶ã€‚\\\
            nGB11551â€”2014 æ±½è½¦æ­£é¢ç¢°æ’çš„ä¹˜å‘˜ä¿æŠ¤ GB/T15089 æœºåŠ¨è½¦è¾†åŠæŒ‚è½¦åˆ†ç±» GB16735 é“è·¯è½¦è¾† è½¦è¾†è¯†åˆ«ä»£å·(VIN)\
            \ GB20071â€”2025 æ±½è½¦ä¾§é¢ç¢°æ’çš„ä¹˜å‘˜ä¿æŠ¤ GB20072â€”2024 ä¹˜ç”¨è½¦åç¢°æ’å®‰å…¨è¦æ±‚ GB/T20913â€”2007 ä¹˜ç”¨è½¦æ­£é¢åç½®ç¢°æ’çš„ä¹˜å‘˜ä¿æŠ¤\
            \ GB34660â€”2017 é“è·¯è½¦è¾† ç”µç£å…¼å®¹æ€§è¦æ±‚å’Œè¯•éªŒæ–¹æ³• GB39732â€”2020 æ±½è½¦äº‹ä»¶æ•°æ®è®°å½•ç³»ç»Ÿ GB/T43187â€”2023\
            \ è½¦è½½æ— çº¿é€šä¿¡ç»ˆç«¯ GB/T45086.1â€”2024 è½¦è½½å®šä½ç³»ç»ŸæŠ€æœ¯è¦æ±‚åŠè¯•éªŒæ–¹æ³• ç¬¬1éƒ¨åˆ†:å«æ˜Ÿå®šä½ GB/T45314â€”2025 é“è·¯è½¦è¾†å…æé€šè¯å’Œè¯­éŸ³äº¤äº’æ€§èƒ½è¦æ±‚åŠè¯•éªŒæ–¹æ³•\"\
            ,\n            \"children\": [],\n            \"full_path\": \"2 è§„èŒƒæ€§å¼•ç”¨æ–‡ä»¶\"\
            \n          },\n          {\n            \"chapter_id\": \"3\",\n    \
            \        \"chapter_title\": \"æœ¯è¯­å’Œå®šä¹‰\",\n            \"raw_text\": \"ä¸‹åˆ—æœ¯è¯­å’Œå®šä¹‰é€‚ç”¨äºæœ¬æ–‡ä»¶ã€‚\"\
            ,\n            \"children\": [\n              {\n                \"chapter_id\"\
            : \"3.1\",\n                \"chapter_title\": \"è½¦è½½äº‹æ•…ç´§æ€¥å‘¼å«ç³»ç»Ÿ on-boardaccidentemergencycallsystem;AECS\"\
            ,\n                \"raw_text\": \"é€šè¿‡è½¦è¾†å†…éƒ¨ç­–ç•¥åœ¨å‘ç”Ÿäº‹æ•…æ—¶è‡ªåŠ¨æ¿€æ´»,æˆ–ç”±è½¦å†…äººå‘˜è¿›è¡Œæ‰‹åŠ¨è§¦å‘å,å°†è½¦è¾†çš„ä½ç½®åŠè½¦è¾†ç›¸å…³çŠ¶æ€ä¿¡æ¯åŒæ­¥å‘é€ç»™ç´§æ€¥å‘¼å«æœåŠ¡å¹³å°å¹¶å»ºç«‹è¯­éŸ³é€šè¯çš„ç³»ç»Ÿã€‚\"\
            ,\n                \"children\": [],\n                \"full_path\": \"\
            3 æœ¯è¯­å’Œå®šä¹‰/3.1 è½¦è½½äº‹æ•…ç´§æ€¥å‘¼å«ç³»ç»Ÿ on-boardaccidentemergencycallsystem;AECS\"\n  \
            \            },\n              {\n                \"chapter_id\": \"3.2\"\
            ,\n                \"chapter_title\": \"è½¦è½½æ— çº¿é€šä¿¡ç»ˆç«¯ on-boardwirelesscommunicationterminal\"\
            ,\n                \"raw_text\": \"å®‰è£…åœ¨è½¦è¾†ä¸Š,èƒ½å¤Ÿé€šè¿‡èœ‚çªç§»åŠ¨é€šä¿¡ç­‰æŠ€æœ¯å»ºç«‹è¿æ¥å¹¶è¿›è¡Œä¿¡æ¯äº¤äº’çš„ç”µå­è®¾å¤‡ã€‚\\\
            næ³¨:è½¦è½½æ— çº¿é€šä¿¡ç»ˆç«¯é€šå¸¸ç”±å‘å°„æœºã€æ¥æ”¶æœºã€å¤©çº¿ã€æ§åˆ¶å™¨å’Œçº¿ç¼†ç­‰éƒ¨ä»¶æ„æˆã€‚\\n[æ¥æº:GB/T43187â€”2023,3.1]\",\n \
            \               \"children\": [],\n                \"full_path\": \"3\
            \ æœ¯è¯­å’Œå®šä¹‰/3.2 è½¦è½½æ— çº¿é€šä¿¡ç»ˆç«¯ on-boardwirelesscommunicationterminal\"\n       \
            \       }\n            ],\n            \"full_path\": \"3 æœ¯è¯­å’Œå®šä¹‰\"\n  \
            \        }\n        ]\n      }\n    ]\n  }\n]\n```\n\n## è¾“å‡ºç¤ºä¾‹2\n\n```\n\
            \  {\n      \"file\": \"regulation\",\n      \"section\": \"MAIN\",\n\
            \      \"experiment_root_ids\": []\n      \"chapters\":[\n          {\n\
            \            \"chapter_id\": \"1\",\n            \"topic_keywords\": [\n\
            \              \"èŒƒå›´\"\n            ],\n          },\n          {\n   \
            \         \"chapter_id\": \"2\",\n            \"parameters\": [],\n  \
            \          \"topic_keywords\": [\n              \"è§„èŒƒæ€§å¼•ç”¨æ–‡ä»¶\"\n        \
            \    ],\n          },\n          {\n            \"chapter_id\": \"3\"\
            ,\n            \"topic_keywords\": [\n              \"æœ¯è¯­å’Œå®šä¹‰\"\n      \
            \      ],\n          },\n          {\n            \"chapter_id\": \"3.1\"\
            ,\n            \"topic_keywords\": [\n              \"è½¦è½½äº‹æ•…ç´§æ€¥å‘¼å«ç³»ç»Ÿ\"\n \
            \           ],\n          },\n          {\n            \"chapter_id\"\
            : \"3.2\",\n            \"topic_keywords\": [\n              \"è½¦è½½æ— çº¿é€šä¿¡ç»ˆç«¯\"\
            \n            ],\n            \"refs\": [\n              {\n         \
            \       \"ref_type\": \"external\",\n                \"doc_id\": \"GB/T43187â€”2023\"\
            ,\n                \"target_id\": \"3.1\",\n                \"anchor_text\"\
            : \"è½¦è½½æ— çº¿é€šä¿¡ç»ˆç«¯å®šä¹‰\"\n              }\n            ],\n          },\n    \
            \  ]\n  }\n\n```\n\n"
        - id: 2e89206c-ded1-462d-9db7-2832e587c8ad
          role: user
          text: '{{#1756557801310.item#}}'
        selected: false
        title: LLM 2
        type: llm
        variables: []
        vision:
          enabled: false
      height: 89
      id: '1756557804693'
      parentId: '1756557801310'
      position:
        x: 204
        y: 80
      positionAbsolute:
        x: 842
        y: 539
      selected: true
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
      zIndex: 1002
    - data:
        code: "import json\nimport re\n\nimport json\nimport ast\nimport re\nfrom\
          \ typing import Dict, List, Any\n\ndef parse_input(src: str) -> List[Dict[str,\
          \ Any]]:\n    \"\"\"é¢„å¤„ç†å¹¶è§£æè¾“å…¥å­—ç¬¦ä¸²ä¸ºåˆ—è¡¨å­—å…¸ç»“æ„\"\"\"\n    try:\n        # 1. ç›´æ¥æŒ‰\
          \ </think> åˆ‡åˆ†å¹¶å–ååŠæ®µ\n        src = src.rsplit('</think>', 1)[-1].strip()\n\
          \n        try:\n            data = json.loads(src)\n        except json.JSONDecodeError:\n\
          \            try:\n                if src.startswith('\\ufeff'):\n     \
          \               src = src[1:]\n                data = ast.literal_eval(src)\n\
          \            except (SyntaxError, ValueError):\n                try:\n \
          \                   processed = src.replace('\\\\\"', '\"').replace('\\\\\
          n', '\\n')\n                    data = json.loads(processed)\n         \
          \       except Exception as e:\n                    raise ValueError(f\"\
          æ— æ³•è§£æï¼š{str(e)}\")\n\n        # ä¿è¯ä¸º list\n        if isinstance(data, dict):\n\
          \            return [data]\n        elif isinstance(data, list):\n     \
          \       return data\n        else:\n            return []\n    except Exception\
          \ as e:\n        raise ValueError(f\"é¢„å¤„ç†å¤±è´¥: {str(e)}\")\n        # return\
          \ []\n\n\ndef main(arg1: str) -> dict:\n    data = parse_input(arg1)\n\n\
          \    return {\n        \"result\":json.dumps(data, ensure_ascii=False),\n\
          \    }"
        code_language: python3
        desc: ''
        error_strategy: fail-branch
        isInIteration: true
        isInLoop: false
        iteration_id: '1756557801310'
        outputs:
          result:
            children: null
            type: string
        selected: false
        title: ä»£ç æ‰§è¡Œ 2
        type: code
        variables:
        - value_selector:
          - '1756557804693'
          - text
          value_type: string
          variable: arg1
      height: 89
      id: '1756557929661'
      parentId: '1756557801310'
      position:
        x: 508
        y: 80
      positionAbsolute:
        x: 1146
        y: 539
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
      zIndex: 1002
    - data:
        answer: '{{#1756613329065.output#}}'
        desc: ''
        selected: false
        title: ç›´æ¥å›å¤ 2
        type: answer
        variables: []
      height: 104
      id: '1756558073574'
      position:
        x: 4206
        y: 459
      positionAbsolute:
        x: 4206
        y: 459
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        code: "import json\nimport re\nimport os\n\nimport json\nimport ast\nimport\
          \ re\nfrom typing import Dict, List, Any\n\ndef parse_input(src: str) ->\
          \ List[Dict[str, Any]]:\n    \"\"\"é¢„å¤„ç†å¹¶è§£æè¾“å…¥å­—ç¬¦ä¸²ä¸ºåˆ—è¡¨å­—å…¸ç»“æ„\"\"\"\n    try:\n\
          \        # å»é™¤ LLM æ€ç»´é“¾æ ‡è®°\n        try: \n            src = re.sub(r'<think>.*?</think>',\
          \ '', src, flags=re.DOTALL).strip()\n        except:\n            pass\n\
          \n        try:\n            data = json.loads(src)\n        except json.JSONDecodeError:\n\
          \            try:\n                if src.startswith('\\ufeff'):\n     \
          \               src = src[1:]\n                data = ast.literal_eval(src)\n\
          \            except (SyntaxError, ValueError):\n                try:\n \
          \                   processed = src.replace('\\\\\"', '\"').replace('\\\\\
          n', '\\n')\n                    data = json.loads(processed)\n         \
          \       except Exception as e:\n                    raise ValueError(f\"\
          æ— æ³•è§£æï¼š{str(e)}\")\n\n        # ä¿è¯ä¸º list\n        if isinstance(data, dict):\n\
          \            return [data]\n        elif isinstance(data, list):\n     \
          \       return data\n        else:\n            return []\n    except Exception\
          \ as e:\n        raise ValueError(f\"é¢„å¤„ç†å¤±è´¥: {str(e)}\")\n        # return\
          \ []\n\ndef find_chapters_and_children_by_ids(tree, target_file, target_section,\
          \ experiment_root_ids):\n    \"\"\"\n    æ ¹æ® experiment_root_ids æ‰¾åˆ°å¯¹åº”çš„ç« èŠ‚åŠå…¶æ‰€æœ‰å­ç« èŠ‚\n\
          \    \n    Args:\n        tree: å®Œæ•´çš„ç« èŠ‚æ ‘ç»“æ„\n        target_file: ç›®æ ‡æ–‡ä»¶å (å¦‚\
          \ \"regulation\")\n        target_section: ç›®æ ‡sectionå (å¦‚ \"é™„å½•B\")\n    \
          \    experiment_root_ids: è¦æŸ¥æ‰¾çš„ç« èŠ‚IDåˆ—è¡¨ (å¦‚ [\"B.2.1.1\",\"B.2.4.1\"])\n   \
          \ \n    Returns:\n        List[Dict]: æ¯ä¸ªitemåŒ…å«æ‰¾åˆ°çš„ç« èŠ‚åŠå…¶æ‰€æœ‰å­ç« èŠ‚\n    \"\"\"\n\
          \    \n    def get_chapter_with_all_children(chapter):\n        \"\"\"é€’å½’è·å–ç« èŠ‚åŠå…¶æ‰€æœ‰å­ç« èŠ‚\"\
          \"\"\n        result = {\n            \"chapter_id\": chapter[\"chapter_id\"\
          ],\n            \"chapter_title\": chapter[\"chapter_title\"],\n       \
          \     \"raw_text\": chapter.get(\"raw_text\", \"\"),\n            \"full_path\"\
          : chapter.get(\"full_path\", \"\"),\n            \"children\": []\n    \
          \    }\n        \n        # å¦‚æœæœ‰å­ç« èŠ‚ï¼Œé€’å½’è·å–\n        if \"children\" in chapter\
          \ and chapter[\"children\"]:\n            for child in chapter[\"children\"\
          ]:\n                result[\"children\"].append(get_chapter_with_all_children(child))\n\
          \        \n        return result\n    \n    def find_chapter_by_id(chapters,\
          \ target_id):\n        \"\"\"åœ¨ç« èŠ‚åˆ—è¡¨ä¸­é€’å½’æŸ¥æ‰¾æŒ‡å®šIDçš„ç« èŠ‚\"\"\"\n        for chapter\
          \ in chapters:\n            if chapter[\"chapter_id\"] == target_id:\n \
          \               return chapter\n\n            # åœ¨å­ç« èŠ‚ä¸­é€’å½’æŸ¥æ‰¾\n            if\
          \ \"children\" in chapter and chapter[\"children\"]:\n                found\
          \ = find_chapter_by_id(chapter[\"children\"], target_id)\n             \
          \   if found:\n                    return found\n        \n        return\
          \ None\n    \n    result = []\n    \n    # éå†æ ‘ç»“æ„ï¼Œæ‰¾åˆ°ç›®æ ‡æ–‡ä»¶å’Œsection\n    for\
          \ file_item in tree:\n        if file_item[\"file\"] != target_file:\n \
          \           continue\n            \n        for section in file_item[\"\
          sections\"]:\n            if section[\"section\"] != target_section:\n \
          \               continue\n            \n            # åœ¨ find_chapters_and_children_by_ids\
          \ é‡Œï¼Œæ‰¾åˆ°ç›®æ ‡ section ååŠ ï¼š\n            if experiment_root_ids == [\"ALL\"]:\n\
          \                # æŠŠç¬¬ä¸€å±‚æ‰€æœ‰ chapter_id å–å‡ºæ¥\n                experiment_root_ids\
          \ = [ch[\"chapter_id\"] for ch in section.get(\"chapters\", [])]\n\n   \
          \         # åœ¨è¯¥sectionä¸­æŸ¥æ‰¾æ¯ä¸ªexperiment_root_id\n            for root_id in\
          \ experiment_root_ids:\n                found_chapter = find_chapter_by_id(section[\"\
          chapters\"], root_id)\n                if found_chapter:\n             \
          \       # è·å–è¯¥ç« èŠ‚åŠå…¶æ‰€æœ‰å­ç« èŠ‚\n                    chapter_with_children = get_chapter_with_all_children(found_chapter)\n\
          \                    result.append(chapter_with_children)\n            \
          \    else:\n                    print(f\"è­¦å‘Š: æœªæ‰¾åˆ°ç« èŠ‚ID '{root_id}' åœ¨ {target_file}/{target_section}\
          \ ä¸­\")\n    \n    return result\n\ndef extract_experiment_chapters(tree,\
          \ experiment_info):\n    \"\"\"\n    æ ¹æ®å®éªŒä¿¡æ¯æå–å¯¹åº”çš„ç« èŠ‚\n    \n    Args:\n  \
          \      tree: å®Œæ•´çš„ç« èŠ‚æ ‘\n        experiment_info: åŒ…å« file, section, experiment_root_ids\
          \ çš„å­—å…¸\n    \n    Returns:\n        List[Dict]: å®éªŒç›¸å…³çš„ç« èŠ‚åˆ—è¡¨\n    \"\"\"\n \
          \   return find_chapters_and_children_by_ids(\n        tree=tree,\n    \
          \    target_file=experiment_info[\"file\"],\n        target_section=experiment_info[\"\
          section\"],\n        experiment_root_ids=experiment_info[\"experiment_root_ids\"\
          ]\n    )\n\ndef main(arg1: list[str], arg2: str) -> dict:\n    \"\"\"\n\
          \    å¤„ç†å®éªŒæ•°æ®çš„ä¸»å‡½æ•°\n    \n    Args:\n        arg1: JSONå­—ç¬¦ä¸²åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å« file, section,\
          \ experiment_root_ids\n        arg2: ç« èŠ‚æ ‘çš„JSONå­—ç¬¦ä¸²\n    \n    Returns:\n \
          \       dict: å¤„ç†åçš„å®éªŒç« èŠ‚æ•°æ®\n    \"\"\"\n    import json\n    from collections\
          \ import defaultdict\n    \n    # è§£æè¾“å…¥æ•°æ®\n    experiment_data = []\n   \
          \ solved_tree = []\n    for t in arg1:\n        items = json.loads(t)\n\
          \        for item in items:\n            experiment_data.append({\n    \
          \            'file': item['file'],\n                'section': item['section'],\n\
          \                'experiment_root_ids': item['experiment_root_ids']\n  \
          \          })\n            solved_tree.append({\n                'file':\
          \ item['file'],\n                'section': item['section'],\n         \
          \       'chapters':item['chapters']\n            })\n    \n    # æŒ‰ file\
          \ å’Œ section åˆå¹¶ç›¸åŒçš„é¡¹ç›®\n    merged_experiments = defaultdict(list)\n    for\
          \ item in experiment_data:\n        key = (item['file'], item['section'])\n\
          \        merged_experiments[key].extend(item['experiment_root_ids'])\n \
          \   \n    # å»é‡å¹¶ç”Ÿæˆæœ€ç»ˆçš„ experiment_info åˆ—è¡¨\n    experiment_infos = []\n   \
          \ for (file, section), root_ids in merged_experiments.items():\n       \
          \ # å»é‡ä½†ä¿æŒé¡ºåº\n        unique_root_ids = []\n        seen = set()\n      \
          \  for root_id in root_ids:\n            if root_id not in seen:\n     \
          \           unique_root_ids.append(root_id)\n                seen.add(root_id)\n\
          \        if len(unique_root_ids):\n            experiment_infos.append({\n\
          \                'file': file,\n                'section': section,\n  \
          \              'experiment_root_ids': unique_root_ids\n            })\n\
          \    \n    # 1. å…ˆæŠŠ solved_tree æŒ‰ (file, section) åšåˆ†ç»„\n    merged_chapters\
          \ = defaultdict(list)\n    for node in solved_tree:\n        key = (node['file'],\
          \ node['section'])\n        merged_chapters[key].extend(node['chapters'])\n\
          \n    # 2. åœ¨æ¯ä¸ªåˆ†ç»„å†…å¯¹ chapters å»é‡ä¸”ä¿æŒåŸé¡ºåº\n    final_tree = []\n    for (file,\
          \ section), chapters in merged_chapters.items():\n        seen_id = set()\n\
          \        unique_chapters = []\n        for c in chapters:\n            cid\
          \ = c.get(\"chapter_id\")\n            if cid not in seen_id:\n        \
          \        unique_chapters.append(c)\n                seen_id.add(cid)\n \
          \       if unique_chapters:\n            final_tree.append({\n         \
          \       \"file\": file,\n                \"section\": section,\n       \
          \         \"chapters\": unique_chapters\n            })\n\n    # è§£æç« èŠ‚æ ‘\n\
          \    regulation_tree = parse_input(arg2)\n    \n    # print(experiment_infos)\n\
          \n\n    # å¤„ç†æ¯ä¸ªå®éªŒä¿¡æ¯ï¼Œæå–å¯¹åº”çš„ç« èŠ‚\n    result = []\n    for i, experiment_info\
          \ in enumerate(experiment_infos):\n        chapters_with_hierarchy = extract_experiment_chapters(regulation_tree,\
          \ experiment_info)\n        \n        result.append(json.dumps(chapters_with_hierarchy,\
          \ ensure_ascii=False))\n    \n    # 2. ç›®æ ‡è·¯å¾„\n    path = \"/tmp/mydata/final_tree.json\"\
          \n\n    os.makedirs(os.path.dirname(path), exist_ok=True)\n    with open(path,\
          \ \"w\", encoding=\"utf-8\") as f:\n        json.dump(final_tree, f, ensure_ascii=False,\
          \ indent=2)\n\n    return {\n        \"chapters\":result,\n        \"experiment_infos\"\
          :json.dumps(experiment_infos, ensure_ascii=False),\n        # \"regulation_tree\"\
          :str(regulation_tree),\n        \"final_tree\":json.dumps(final_tree, ensure_ascii=False),\n\
          \    }"
        code_language: python3
        desc: ''
        outputs:
          chapters:
            children: null
            type: array[string]
          experiment_infos:
            children: null
            type: string
          final_tree:
            children: null
            type: string
        selected: false
        title: ä»£ç æ‰§è¡Œ 3
        type: code
        variables:
        - value_selector:
          - '1756557801310'
          - output
          value_type: array[string]
          variable: arg1
        - value_selector:
          - '1756550411122'
          - tree
          value_type: string
          variable: arg2
      height: 53
      id: '1756563307317'
      position:
        x: 2422
        y: 459
      positionAbsolute:
        x: 2422
        y: 459
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        code: "\ndef main(arg1: str) -> dict:\n    return {\n        \"array\": [arg1],\n\
          \    }\n"
        code_language: python3
        desc: ''
        outputs:
          array:
            children: null
            type: array[string]
        selected: false
        title: ä»£ç æ‰§è¡Œ 4
        type: code
        variables:
        - value_selector:
          - '1756550268945'
          - array
          value_type: string
          variable: arg1
      height: 53
      id: '1756567749405'
      position:
        x: 30
        y: 828
      positionAbsolute:
        x: 30
        y: 828
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        code: "\ndef main(arg1: list[str]) -> dict:\n    return {\n        \"result\"\
          :json.dumps(arg1, ensure_ascii=False)\n    }\n"
        code_language: python3
        desc: ''
        outputs:
          result:
            children: null
            type: string
        selected: false
        title: ä»£ç æ‰§è¡Œ 5
        type: code
        variables:
        - value_selector:
          - '1756557801310'
          - output
          value_type: array[string]
          variable: arg1
      height: 53
      id: '1756568067080'
      position:
        x: 30
        y: 921
      positionAbsolute:
        x: 30
        y: 921
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        desc: ''
        error_handle_mode: continue-on-error
        height: 209
        is_parallel: true
        iterator_input_type: array[string]
        iterator_selector:
        - '1756563307317'
        - chapters
        output_selector:
        - '1756613456815'
        - result
        output_type: array[string]
        parallel_nums: 10
        selected: false
        start_node_id: 1756613329065start
        title: è¿­ä»£ 2
        type: iteration
        width: 812
      height: 209
      id: '1756613329065'
      position:
        x: 2726
        y: 459
      positionAbsolute:
        x: 2726
        y: 459
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 812
      zIndex: 1
    - data:
        desc: ''
        isInIteration: true
        selected: false
        title: ''
        type: iteration-start
      draggable: false
      height: 48
      id: 1756613329065start
      parentId: '1756613329065'
      position:
        x: 60
        y: 80.5
      positionAbsolute:
        x: 2786
        y: 539.5
      selectable: false
      sourcePosition: right
      targetPosition: left
      type: custom-iteration-start
      width: 44
      zIndex: 1002
    - data:
        context:
          enabled: true
          variable_selector:
          - '1756550411122'
          - context
        desc: ''
        isInIteration: true
        isInLoop: false
        iteration_id: '1756613329065'
        model:
          completion_params: {}
          mode: chat
          name: deepseek-reasoner
          provider: langgenius/deepseek/deepseek
        prompt_template:
        - id: 8e7bb797-e961-48ec-9029-47c638adcc67
          role: system
          text: "# è§’è‰²ï¼ˆRoleï¼‰\nä½ æ˜¯ä¸€å **æ ‡å‡†æµ‹è¯•ç”¨ä¾‹è®¾è®¡ä¸“å®¶**ï¼Œç²¾é€šæ±½è½¦åŠç›¸å…³é¢†åŸŸçš„æ ‡å‡†å’Œæ³•è§„æ–‡æ¡£ã€‚  \nä½ çš„èŒè´£æ˜¯å°†æ ‡å‡†æˆ–æ³•è§„æ–‡ä»¶ä¸­çš„æ¡æ¬¾å’Œè¯•éªŒæ–¹æ³•\
            \ **ä¸¥æ ¼æ‹†è§£ä¸ºç»“æ„åŒ–æµ‹è¯•ç”¨ä¾‹**ï¼Œè¾“å‡ºç»“æœç”¨äºï¼š  \n- æ ‡å‡†å’Œæ³•è§„çš„æ¯”å¯¹åˆ†æ  \n- æµ‹è¯•æœºæ„è®¾å¤‡èƒ½åŠ›éªŒè¯å’Œè¦†ç›–ç‡è¯„ä¼°  \n-\
            \ è‡ªåŠ¨åŒ–æµ‹è¯•ç”¨ä¾‹ç®¡ç†ç³»ç»Ÿ  \nè¦æ±‚ä¿¡æ¯å®Œæ•´ã€æ¡ç†æ¸…æ™°ã€å­—æ®µè§„èŒƒåŒ–ï¼Œç¡®ä¿ç»“æ„åŒ–ç»“æœå…·å¤‡å¯è¿½æº¯æ€§å’Œå¯æ‰§è¡Œæ€§ã€‚\n\n---\n\n# è¾“å…¥æ ¼å¼ï¼ˆInput\
            \ Formatï¼‰\nè¾“å…¥ä¸ºä¸€ä¸ª JSON æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ è¡¨ç¤ºä¸€ä¸ªç« èŠ‚ï¼š\n```json\n[\n  {\n    \"chapter_id\"\
            : \"string\",      // ç« èŠ‚ç¼–å·\n    \"chapter_title\": \"string\",   // ç« èŠ‚æ ‡é¢˜\n\
            \    \"raw_text\": \"string\",        // æœ¬ç« èŠ‚æ­£æ–‡\n    \"children\": [  \
            \              // å­ç« èŠ‚æ•°ç»„ï¼ˆé€’å½’ç»“æ„ï¼‰\n      { ...åŒç»“æ„... }\n    ],\n    \"full_path\"\
            : \"string\"        // ä»æ ¹åˆ°è¯¥ç« èŠ‚çš„è·¯å¾„\n  }\n]\n```\nè¯´æ˜ï¼š\n- ä½ ä¹Ÿå¯ä»¥ä» {{#context#}}\
            \ ä¸­ç†è§£æ­¤æ ‡å‡†çš„ç›¸å…³ä¿¡æ¯\n- `raw_text` å¯èƒ½ä¸ºç©ºï¼Œæ­£æ–‡å¯èƒ½åœ¨ `children` å†…ã€‚\n- åŒä¸€ç« èŠ‚å¯åŒ…å«å¤šä¸ªå®éªŒæ–¹æ³•ï¼›æ¯ä¸ªå®éªŒæ–¹æ³•éœ€å•ç‹¬æ‹†è§£æˆå¯¹è±¡ã€‚ä¸åŒç« èŠ‚é—´å„è‡ªç‹¬ç«‹ï¼Œä¸è¦ç›¸äº’æ¨æ–­\n\
            - ä¸€ä¸ªå®éªŒæ–¹æ³•ä¸­å­˜åœ¨å¤šç§è¯•éªŒæ–¹æ¡ˆæ—¶ï¼Œä¹Ÿè¦åˆ†åˆ«æ‹†è§£ä¸ºå¤šä¸ªå¯¹è±¡ï¼ˆå‘½åæ—¶åœ¨ `test_name` åæ·»åŠ æ–¹æ¡ˆæ ‡è¯†ï¼Œå¦‚â€œ(XXæ–¹æ¡ˆ)â€ï¼‰ã€‚\n\n\
            ---\n\n# è¾“å‡ºæ ¼å¼ï¼ˆOutput Formatï¼‰\nè¾“å‡ºä¸ºä¸€ä¸ª JSON æ•°ç»„ï¼Œå¯¹åº”è¾“å…¥çš„ç« èŠ‚æ•°ç»„ï¼Œæ¯ä¸ªç« èŠ‚å¯¹åº”ä¸€ä¸ªæ•°ç»„ï¼Œæ•°ç»„ä¸­çš„æ¯ä¸ªå¯¹è±¡ä»£è¡¨ä¸€ä¸ªç‹¬ç«‹çš„å®éªŒæˆ–å®éªŒæ–¹æ¡ˆï¼š\n\
            \n```json\n[\n  [\n    {\n      \"chapter_id\": \"string\",    // æ¥æºç« èŠ‚çš„chapter_idï¼Œä¿è¯å¯è¿½æº¯æ€§\n\
            \      \"test_name\": \"string\",     // æµ‹è¯•åç§°ï¼Œä»ç« èŠ‚æ ‡é¢˜æˆ–è·¯å¾„æç‚¼ï¼Œç®€æ´ä¸”å”¯ä¸€\n     \
            \ \"conditions\": [            // æµ‹è¯•æ¡ä»¶ï¼šè¯•ä»¶çŠ¶æ€ã€å®‰è£…ã€ç¯å¢ƒç­‰ï¼Œå®Œæ•´åˆ†ç‚¹åˆ—å‡º\n        \"\
            string\"\n      ],\n      \"criteria\": [              // åˆ¤å®šæ ‡å‡†ï¼šæ˜ç¡®æ£€æµ‹é¡¹+è¾¾æ ‡æ¡ä»¶ï¼Œåˆ†ç‚¹åˆ—å‡º\n\
            \        \"string\"\n      ],\n      \"equipment\": [             // æ‰€éœ€è®¾å¤‡åŠè§„æ ¼ï¼Œä¾¿äºè®¾å¤‡èƒ½åŠ›éªŒè¯\n\
            \        {\n          \"name\": \"string\",          // è®¾å¤‡åç§°\n       \
            \   \"specification\": \"string\"  // è®¾å¤‡è§„æ ¼/æ€§èƒ½\n        }\n      ],\n \
            \     \"parameters\": [            // æå–çš„å…³é”®å‚æ•°\n        {\n          \"\
            item\": \"string\",          // å‚æ•°é¡¹\n          \"constraint\": \"string\"\
            ,    // çº¦æŸï¼š<=|<|=|>=|>|range_closed|range_open|enum|boolean\n        \
            \  \"value\": \"string|array|null\", // å‚æ•°å€¼ï¼Œå¯ä¸ºæ•°å­—ã€èŒƒå›´æˆ–æšä¸¾\n          \"unit\"\
            : \"string|null\",     // å•ä½ï¼Œè‹¥æ— åˆ™null\n          \"source_text\": \"string\"\
            \    // åŸæ–‡å®Œæ•´ç‰‡æ®µï¼Œä¿ç•™ç¬¦å·ï¼Œä¾¿äºè¿½æº¯\n        }\n      ],\n      \"refs\": [     \
            \             // å¼•ç”¨ä¿¡æ¯\n        {\n          \"ref_type\": \"internal|external\"\
            , // å†…éƒ¨/å¤–éƒ¨å¼•ç”¨\n          \"doc_id\": \"string|null\",         // å¤–éƒ¨æ ‡å‡†ç¼–å·ï¼›å†…éƒ¨å¼•ç”¨å¡«null\n\
            \          \"target_id\": \"string\",           // å¼•ç”¨ç›®æ ‡ç¼–å·ï¼ˆå¦‚â€œB.2.1.1â€ã€â€œè¡¨B.1â€ï¼‰\n\
            \          \"anchor_text\": \"string\"          // å¼•ç”¨çš„ç®€è¦ä¸Šä¸‹æ–‡æè¿°\n      \
            \  }\n      ]\n    }\n  ]\n]\n```\n\n---\n\n# å­—æ®µè¯´æ˜\n- **chapter_id**ï¼šæ ‡è®°æ¥æºç« èŠ‚ï¼Œä¿è¯å¯è¿½æº¯æ€§ã€‚\
            \  \n- **test_name**ï¼šå”¯ä¸€æ ‡è¯†è¯¥å®éªŒæ–¹æ³•ï¼Œè‹¥åŒä¸€å®éªŒæœ‰å¤šæ–¹æ¡ˆï¼Œåœ¨ååŠ æ‹¬å·æ ‡æ˜æ–¹æ¡ˆï¼Œå¦‚â€œ(æ–¹æ¡ˆA)â€ã€‚  \n- **conditions**ï¼šå®Œæ•´åˆ†ç‚¹åˆ—å‡ºæµ‹è¯•å‰ç½®æ¡ä»¶ã€å®‰è£…çŠ¶æ€ã€ç¯å¢ƒè¦æ±‚ç­‰ã€‚\
            \  \n- **criteria**ï¼šæ¯æ¡å¿…é¡»åŒ…å«æ£€æµ‹å¯¹è±¡ä¸åˆ¤å®šæ¡ä»¶ï¼Œé¿å…ç¬¼ç»Ÿæè¿°ã€‚  \n- **equipment**ï¼šå®Œæ•´åˆ—å‡ºæ‰€æœ‰è®¾å¤‡åŠå…³é”®è§„æ ¼ã€‚\
            \  \n- **parameters**ï¼šæå–æ‰€æœ‰å®šé‡æŒ‡æ ‡ï¼Œä¸­æ–‡æ•°å­—è½¬é˜¿æ‹‰ä¼¯æ•°å­—ã€‚  \n- **refs**ï¼šæå–ç« èŠ‚å†…çš„æ ‡å‡†å¼•ç”¨ï¼š\n\
            \  - å†…éƒ¨å¼•ç”¨ï¼šæŒ‡å‘æœ¬æ ‡å‡†çš„ç« èŠ‚ã€è¡¨ã€å›¾ï¼Œ`ref_type=internal`ï¼Œ`doc_id=null`ã€‚\n  - å¤–éƒ¨å¼•ç”¨ï¼šæŒ‡å‘å¤–éƒ¨æ ‡å‡†ï¼Œ`ref_type=external`ï¼Œå¡«å†™æ ‡å‡†ç¼–å·ã€‚\n\
            \  - å¤šä¸ªå¼•ç”¨å¯åˆå¹¶æˆä¸€æ¡è®°å½•ï¼ˆå¦‚â€œè¡¨B.1ã€å›¾B.1â€å¯ä¸ºä¸€ä¸ªæ¡ç›®ï¼‰ã€‚\n\n---\n\n# å¤„ç†é€»è¾‘ï¼ˆChain-of-Thoughtï¼‰\n\
            1. **è§£æç« èŠ‚å†…å®¹**ï¼šé€’å½’éå†`children`ï¼Œåˆå¹¶æ‰€æœ‰æ­£æ–‡ã€‚  \n2. **æ‹†åˆ†å®éªŒæ–¹æ³•ä¸æ–¹æ¡ˆ**ï¼šæ ¹æ®æè¿°é€»è¾‘ã€åºå·ã€a)/b)åˆ†é¡¹ç­‰æ‹†æˆå¤šä¸ªå¯¹è±¡ã€‚\
            \  \n3. **æå–ç« èŠ‚ID**ï¼šå†™å…¥æ¯ä¸ªå¯¹è±¡çš„`chapter_id`å­—æ®µã€‚  \n4. **æå–å®éªŒåç§°**ï¼šä¼˜å…ˆç« èŠ‚æ ‡é¢˜ï¼Œç»“åˆè·¯å¾„ç®€åŒ–ï¼Œå¿…è¦æ—¶æ ‡æ˜æ–¹æ¡ˆã€‚\
            \  \n5. **è§£æå®éªŒæ¡ä»¶**ï¼š\n   - åˆ—å‡ºå®‰è£…æ–¹å¼ã€è¯•ä»¶çŠ¶æ€ã€ç¯å¢ƒæ¡ä»¶ã€ç”µæºçŠ¶æ€ç­‰ã€‚\n   - å³ä½¿ä¸å‚æ•°é‡å¤ï¼Œä¹Ÿéœ€å®Œæ•´ä¿ç•™ï¼Œä¿è¯ä¸Šä¸‹æ–‡å®Œæ•´æ€§ã€‚\
            \  \n6. **æå–åˆ¤å®šæ ‡å‡†**ï¼š\n   - åˆ†ç‚¹æè¿°æ£€æµ‹é¡¹+åˆ¤å®šæ¡ä»¶ï¼Œé¿å…â€œç¬¦åˆè¦æ±‚â€è¿™ç§æ¨¡ç³Šè¡¨è¿°ã€‚  \n7. **æå–è®¾å¤‡**ï¼š\n\
            \   - æ•æ‰æ‰€æœ‰æåŠçš„è®¾å¤‡ä¸å…³é”®è§„æ ¼ã€‚  \n8. **æå–å‚æ•°**ï¼š\n   - æ‰€æœ‰å®šé‡å€¼ã€èŒƒå›´ã€ç²¾åº¦è¦æ±‚ç­‰ï¼Œä¸¥æ ¼æ ‡æ³¨çº¦æŸç±»å‹å’Œå•ä½ã€‚\n\
            \   - ä¸­æ–‡æ•°å­—éœ€è½¬é˜¿æ‹‰ä¼¯æ•°å­—ã€‚  \n9. **æå–å¼•ç”¨**ï¼š\n   - å°†å†…éƒ¨å’Œå¤–éƒ¨å¼•ç”¨å•ç‹¬è®°å½•åœ¨`refs`æ•°ç»„ä¸­ã€‚  \n10.\
            \ **è¾“å‡ºæ ‡å‡†åŒ–**ï¼š\n    - ç¼ºå¤±é¡¹ä¿æŒå­—æ®µä½†å¡«ç©ºå€¼ï¼ˆå¦‚ç©ºæ•°ç»„æˆ–nullï¼‰ã€‚  \n    - ä¸¥æ ¼JSONç»“æ„ï¼Œé”®åå…¨éƒ¨å°å†™è‹±æ–‡ã€‚\
            \  \n\n---\n\n# Few-shotç¤ºä¾‹\n\n### è¾“å…¥\n```json\n[\n  {\n    \"chapter_id\"\
            : \"B.2.1.1\",\n    \"chapter_title\": \"æ»‘å°æ­£é¢ç¢°æ’è¯•éªŒ\",\n    \"raw_text\"\
            : \"\",\n    \"children\": [\n      {\n        \"chapter_id\": \"B.2.1.1.1\"\
            ,\n        \"chapter_title\": \"\",\n        \"raw_text\": \"å°†ç™½è½¦èº«æˆ–å·¥è£…å›ºå®šåœ¨ç¢°æ’è¯•éªŒæ»‘å°ä¸Š,å®‰è£…æ–¹å‘æ¨¡æ‹Ÿæ­£é¢ç¢°æ’ã€‚\"\
            ,\n        \"children\": [],\n        \"full_path\": \"B.2 è¯•éªŒé¡¹ç›®/B.2.1\
            \ æ­£é¢ç¢°æ’/B.2.1.1 æ»‘å°æ­£é¢ç¢°æ’è¯•éªŒ/B.2.1.1.1 \"\n      },\n      {\n        \"chapter_id\"\
            : \"B.2.1.1.2\",\n        \"chapter_title\": \"\",\n        \"raw_text\"\
            : \"æ»‘å°æŒ‰ç…§ä»¥ä¸‹åŠ é€Ÿåº¦æ³¢å½¢ä¹‹ä¸€è¿›è¡Œç¢°æ’è¯•éªŒã€‚ a) ä½¿ç”¨åˆ¶é€ å•†æŒ‡å®šçš„åŠ é€Ÿåº¦æ³¢å½¢è¿›è¡Œè¯•éªŒ,æŒ‡å®šçš„åŠ é€Ÿåº¦æ³¢å½¢åº”ä¸ºåœ¨B.2.1.2ä¸­æè¿°çš„å®è½¦ç¢°æ’è¯•éªŒæ¡ä»¶ä¸­,è½¦èº«éå˜å½¢åŒºåŸŸé‡‡é›†çš„åŠ é€Ÿåº¦-æ—¶é—´æ›²çº¿,å¹¶ç»è¿‡æ»¤æ³¢ç­‰çº§CFC60\
            \ æ»¤æ³¢æˆ–100Hzä½é€šæ»¤æ³¢ã€‚å®é™…è¯•éªŒç»“æœæ³¢å½¢çš„ç§¯åˆ†é€Ÿåº¦å˜åŒ–é‡Î”vs(t)åº”åœ¨ä»»æ„æ—¶åˆ»,ä¸è¶…è¿‡æŒ‡å®šæ³¢å½¢çš„ç§¯åˆ†é€Ÿåº¦å˜åŒ–é‡[Î”vt(t)Â±1]km/hçš„èŒƒå›´ã€‚\\\
            nb) æŒ‰å›¾B.1 çš„æ ‡å‡†åŠ é€Ÿåº¦é€šé“èŒƒå›´å’Œè¡¨B.1 çš„å‚æ•°è¿›è¡ŒåŠ é€Ÿæˆ–å‡é€Ÿ,å…¶é€Ÿåº¦å˜åŒ–é‡Î”v ä¸º(25Â±1)km/hã€‚\",\n      \
            \  \"children\": [],\n        \"full_path\": \"B.2 è¯•éªŒé¡¹ç›®/B.2.1 æ­£é¢ç¢°æ’/B.2.1.1\
            \ æ»‘å°æ­£é¢ç¢°æ’è¯•éªŒ/B.2.1.1.2 \"\n      }\n    ],\n    \"full_path\": \"B.2 è¯•éªŒé¡¹ç›®/B.2.1\
            \ æ­£é¢ç¢°æ’/B.2.1.1 æ»‘å°æ­£é¢ç¢°æ’è¯•éªŒ\"\n  }\n]\n```\n\n### è¾“å‡º\n```json\n[\n  [\n  \
            \  {\n      \"chapter_id\": \"B.2.1.1\",\n      \"test_name\": \"æ»‘å°æ­£é¢ç¢°æ’è¯•éªŒ(æ–¹æ¡ˆA)\"\
            ,\n      \"conditions\": [\n        \"å°†ç™½è½¦èº«æˆ–å·¥è£…å›ºå®šåœ¨ç¢°æ’è¯•éªŒæ»‘å°ä¸Šï¼Œå®‰è£…æ–¹å‘æ¨¡æ‹Ÿæ­£é¢ç¢°æ’ã€‚\"\
            ,\n        \"è¯•éªŒæ»‘å°æŒ‰åˆ¶é€ å•†æŒ‡å®šçš„åŠ é€Ÿåº¦æ³¢å½¢è¿›è¡Œè¯•éªŒã€‚\",\n        \"åŠ é€Ÿåº¦æ³¢å½¢æ¥æºï¼šå®è½¦ç¢°æ’è¯•éªŒæ¡ä»¶ä¸‹ï¼Œè½¦èº«éå˜å½¢åŒºåŸŸé‡‡é›†çš„åŠ é€Ÿåº¦-æ—¶é—´æ›²çº¿ã€‚\"\
            \n      ],\n      \"criteria\": [\n        \"æ£€æŸ¥ç¢°æ’è§¦å‘ä¿¡å·ï¼Œç¡®ä¿ä¸MSDä¸­è®°å½•çš„è§¦å‘ç±»å‹ä¸€è‡´ã€‚\"\
            ,\n        \"ç§¯åˆ†é€Ÿåº¦å˜åŒ–é‡Î”vs(t)åœ¨ä»»æ„æ—¶åˆ»ä¸å¾—è¶…å‡ºæŒ‡å®šæ³¢å½¢Î”vt(t)Â±1 km/hã€‚\"\n      ],\n  \
            \    \"equipment\": [\n        {\n          \"name\": \"ç¢°æ’è¯•éªŒæ»‘å°\",\n  \
            \        \"specification\": \"å¯æ¨¡æ‹Ÿæ­£é¢ç¢°æ’ï¼›æ”¯æŒè‡ªå®šä¹‰åŠ é€Ÿåº¦æ³¢å½¢è¾“å…¥\"\n        }\n    \
            \  ],\n      \"parameters\": [\n        {\n          \"item\": \"ç§¯åˆ†é€Ÿåº¦å˜åŒ–é‡Î”vs(t)\"\
            ,\n          \"constraint\": \"range_closed\",\n          \"value\": [\"\
            Î”vt(t)-1\", \"Î”vt(t)+1\"],\n          \"unit\": \"km/h\",\n          \"\
            source_text\": \"ç§¯åˆ†é€Ÿåº¦å˜åŒ–é‡Î”vs(t)åº”åœ¨ä»»æ„æ—¶åˆ»,ä¸è¶…è¿‡æŒ‡å®šæ³¢å½¢çš„ç§¯åˆ†é€Ÿåº¦å˜åŒ–é‡[Î”vt(t)Â±1]km/h\"\n\
            \        },\n        {\n          \"item\": \"æ»¤æ³¢ç­‰çº§\",\n          \"constraint\"\
            : \"enum\",\n          \"value\": [\"CFC60\", \"100Hzä½é€š\"],\n        \
            \  \"unit\": null,\n          \"source_text\": \"å¹¶ç»è¿‡æ»¤æ³¢ç­‰çº§CFC60æ»¤æ³¢æˆ–100Hzä½é€šæ»¤æ³¢\"\
            \n        }\n      ],\n      \"refs\": [\n        {\n          \"ref_type\"\
            : \"internal\",\n          \"doc_id\": null,\n          \"target_id\"\
            : \"B.2.1.2\",\n          \"anchor_text\": \"å®è½¦ç¢°æ’è¯•éªŒæ¡ä»¶\"\n        }\n \
            \     ]\n    },\n    {\n      \"chapter_id\": \"B.2.1.1\",\n      \"test_name\"\
            : \"æ»‘å°æ­£é¢ç¢°æ’è¯•éªŒ(æ–¹æ¡ˆB)\",\n      \"conditions\": [\n        \"å°†ç™½è½¦èº«æˆ–å·¥è£…å›ºå®šåœ¨ç¢°æ’è¯•éªŒæ»‘å°ä¸Šï¼Œå®‰è£…æ–¹å‘æ¨¡æ‹Ÿæ­£é¢ç¢°æ’ã€‚\"\
            ,\n        \"æŒ‰å›¾B.1æ ‡å‡†åŠ é€Ÿåº¦é€šé“å’Œè¡¨B.1å‚æ•°è¿›è¡Œæ»‘å°åŠ é€Ÿæˆ–å‡é€Ÿã€‚\"\n      ],\n      \"criteria\"\
            : [\n        \"æ£€æŸ¥ç¢°æ’è§¦å‘ä¿¡å·ï¼Œç¡®ä¿ä¸MSDä¸­è®°å½•çš„è§¦å‘ç±»å‹ä¸€è‡´ã€‚\",\n        \"é€Ÿåº¦å˜åŒ–é‡Î”våº”ä¸º25Â±1\
            \ km/hã€‚\"\n      ],\n      \"equipment\": [\n        {\n          \"name\"\
            : \"ç¢°æ’è¯•éªŒæ»‘å°\",\n          \"specification\": \"æ»¡è¶³å›¾B.1ã€è¡¨B.1åŠ é€Ÿåº¦æ›²çº¿å‚æ•°\"\n \
            \       }\n      ],\n      \"parameters\": [\n        {\n          \"\
            item\": \"é€Ÿåº¦å˜åŒ–é‡Î”v\",\n          \"constraint\": \"range_closed\",\n  \
            \        \"value\": [\"24\", \"26\"],\n          \"unit\": \"km/h\",\n\
            \          \"source_text\": \"é€Ÿåº¦å˜åŒ–é‡Î”vä¸º(25Â±1)km/h\"\n        },\n     \
            \ ],\n      \"refs\": [\n        {\n          \"ref_type\": \"internal\"\
            ,\n          \"doc_id\": null,\n          \"target_id\": \"å›¾B.1\",\n \
            \         \"anchor_text\": \"æ ‡å‡†åŠ é€Ÿåº¦é€šé“\"\n        },\n        {\n      \
            \    \"ref_type\": \"internal\",\n          \"doc_id\": null,\n      \
            \    \"target_id\": \"è¡¨B.1\",\n          \"anchor_text\": \"æ ‡å‡†åŠ é€Ÿåº¦å‚æ•°\"\n\
            \        }\n      ]\n    }\n  ]\n]\n```"
        - id: 8ee6a7aa-9eb1-44fd-8cf5-1d32d1a65fea
          role: user
          text: '{{#1756613329065.item#}}'
        selected: false
        title: LLM 2
        type: llm
        variables: []
        vision:
          enabled: false
      height: 89
      id: '1756613344845'
      parentId: '1756613329065'
      position:
        x: 204
        y: 60
      positionAbsolute:
        x: 2930
        y: 519
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
      zIndex: 1002
    - data:
        code: "import json\nimport re\n\nimport json\nimport ast\nimport re\nfrom\
          \ typing import Dict, List, Any\n\ndef parse_input(src: str) -> List[Dict[str,\
          \ Any]]:\n    \"\"\"é¢„å¤„ç†å¹¶è§£æè¾“å…¥å­—ç¬¦ä¸²ä¸ºåˆ—è¡¨å­—å…¸ç»“æ„\"\"\"\n    try:\n        # 1. ç›´æ¥æŒ‰\
          \ </think> åˆ‡åˆ†å¹¶å–ååŠæ®µ\n        src = src.rsplit('</think>', 1)[-1].strip()\n\
          \n        try:\n            data = json.loads(src)\n        except json.JSONDecodeError:\n\
          \            try:\n                if src.startswith('\\ufeff'):\n     \
          \               src = src[1:]\n                data = ast.literal_eval(src)\n\
          \            except (SyntaxError, ValueError):\n                try:\n \
          \                   processed = src.replace('\\\\\"', '\"').replace('\\\\\
          n', '\\n')\n                    data = json.loads(processed)\n         \
          \       except Exception as e:\n                    raise ValueError(f\"\
          æ— æ³•è§£æï¼š{str(e)}\")\n\n        # ä¿è¯ä¸º list\n        if isinstance(data, dict):\n\
          \            return [data]\n        elif isinstance(data, list):\n     \
          \       return data\n        else:\n            return []\n    except Exception\
          \ as e:\n        # raise ValueError(f\"é¢„å¤„ç†å¤±è´¥: {str(e)}\")\n        return\
          \ []\n\n\ndef main(arg1: str) -> dict:\n    data = parse_input(arg1)\n\n\
          \    return {\n        \"result\":json.dumps(data, ensure_ascii=False),\n\
          \    }"
        code_language: python3
        desc: ''
        isInIteration: true
        isInLoop: false
        iteration_id: '1756613329065'
        outputs:
          result:
            children: null
            type: string
        selected: false
        title: ä»£ç æ‰§è¡Œ 6
        type: code
        variables:
        - value_selector:
          - '1756613344845'
          - text
          value_type: string
          variable: arg1
      height: 53
      id: '1756613456815'
      parentId: '1756613329065'
      position:
        x: 508
        y: 78
      positionAbsolute:
        x: 3234
        y: 537
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
      zIndex: 1002
    - data:
        code: "import os, json, traceback, subprocess, platform\n\ndef main(arg1:\
          \ list[str], arg2: str) -> dict:\n    \"\"\"\n    å°†è¯•éªŒæ•°æ®ä¸ç« èŠ‚ä¿¡æ¯ä¸€ä¸€å¯¹åº”ï¼Œç”Ÿæˆç›®æ ‡æ ¼å¼\n\
          \    [\n        {\n            \"file\": \"regulation\",\n            \"\
          section\": \"é™„å½•B\",\n            \"experiments\": [...]\n        },\n  \
          \      ...\n    ]\n    \"\"\"\n    # 1. å…ˆæŠŠ arg2 çš„å­—ç¬¦ä¸²ååºåˆ—åŒ–æˆåˆ—è¡¨\n    sec_list\
          \ = json.loads(arg2)\n\n    # 2. ä¾æ¬¡å¤„ç† arg1 ä¸­çš„æ¯ä¸ªå…ƒç´ \n    result = []\n   \
          \ for sec, exp_str in zip(sec_list, arg1):\n        # æŠŠå½“å‰è¯•éªŒå­—ç¬¦ä¸²ååºåˆ—åŒ–æˆåˆ—è¡¨\n\
          \        experiments = json.loads(exp_str)\n\n        # 3. æ‹¼æˆç›®æ ‡å­—å…¸\n    \
          \    result.append({\n            \"file\": sec[\"file\"],\n           \
          \ \"section\": sec[\"section\"],\n            \"experiments\": experiments\n\
          \        })\n        \n\n    # 2. ç›®æ ‡è·¯å¾„\n    path = \"/tmp/mydata/result.json\"\
          \n\n    # 3. è°ƒè¯•ä¿¡æ¯\n    log = []\n    log.append(f\"[PWD] å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}\"\
          )\n    log.append(f\"[PATH] ç»å¯¹è·¯å¾„: {os.path.abspath(path)}\")\n    log.append(f\"\
          [DIR] çˆ¶ç›®å½•æ˜¯å¦å­˜åœ¨: {os.path.exists(os.path.dirname(path))}\")\n    log.append(f\"\
          [VOLUMES] æŒ‚è½½ä¿¡æ¯: {subprocess.getoutput('mount | grep mydata')}\")\n\n   \
          \ # 4. å†™æ–‡ä»¶\n    try:\n        os.makedirs(os.path.dirname(path), exist_ok=True)\n\
          \        with open(path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(result,\
          \ f, ensure_ascii=False, indent=2)\n        log.append(\"[SUCCESS] æ–‡ä»¶å·²å†™å…¥\"\
          )\n    except Exception as e:\n        log.append(f\"[ERROR] {type(e).__name__}:\
          \ {e}\")\n        log.append(traceback.format_exc())\n\n    # 5. å†æ£€æŸ¥ä¸€æ¬¡\n\
          \    log.append(f\"[EXISTS] æ–‡ä»¶æ˜¯å¦å­˜åœ¨: {os.path.exists(path)}\")\n    if os.path.exists(path):\n\
          \        log.append(f\"[SIZE] æ–‡ä»¶å¤§å°: {os.path.getsize(path)} å­—èŠ‚\")\n\n  \
          \  return {\"debug\": \"\\n\".join(log), \"result\": json.dumps(result,\
          \ ensure_ascii=False)}"
        code_language: python3
        desc: ''
        outputs:
          debug:
            children: null
            type: string
          result:
            children: null
            type: string
        selected: false
        title: ä»£ç æ‰§è¡Œ 7
        type: code
        variables:
        - value_selector:
          - '1756613329065'
          - output
          value_type: array[string]
          variable: arg1
        - value_selector:
          - '1756563307317'
          - experiment_infos
          value_type: string
          variable: arg2
      height: 53
      id: '1756618044071'
      position:
        x: 3598
        y: 459
      positionAbsolute:
        x: 3598
        y: 459
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        code: "import json\nimport os\nfrom collections import defaultdict\n\ndef\
          \ build_chapter_index(tree_data):\n    \"\"\"æ„å»ºchapterç´¢å¼•ï¼Œkeyä¸º(file, section,\
          \ chapter_id)ï¼Œvalueä¸ºchapterå¯¹è±¡çš„å¼•ç”¨\"\"\"\n    chapter_index = {}\n    section_index\
          \ = {}\n    \n    def add_chapter_recursive(chapter, file_name, section_name):\n\
          \        \"\"\"é€’å½’æ·»åŠ ç« èŠ‚åŠå…¶å­ç« èŠ‚åˆ°ç´¢å¼•ä¸­\"\"\"\n        if 'chapter_id' in chapter:\n\
          \            chapter_id = chapter['chapter_id']\n            chapter_key\
          \ = (file_name, section_name, chapter_id)\n            chapter_index[chapter_key]\
          \ = chapter\n        \n        # é€’å½’å¤„ç†å­ç« èŠ‚\n        if 'children' in chapter:\n\
          \            for child_chapter in chapter['children']:\n               \
          \ add_chapter_recursive(child_chapter, file_name, section_name)\n    \n\
          \    for file_data in tree_data:\n        file_name = file_data['file']\n\
          \        for section in file_data['sections']:\n            section_name\
          \ = section['section']\n            section_key = (file_name, section_name)\n\
          \            section_index[section_key] = section\n            \n      \
          \      # å¤„ç†æ¯ä¸ªé¡¶çº§ç« èŠ‚\n            for chapter in section['chapters']:\n   \
          \             add_chapter_recursive(chapter, file_name, section_name)\n\
          \    \n    return chapter_index, section_index\n\ndef merge_final_tree_data(chapter_index,\
          \ final_tree_data):\n    \"\"\"å°†final_treeçš„æ•°æ®èåˆåˆ°treeä¸­\"\"\"\n    merged_count\
          \ = 0\n    not_found_count = 0\n    \n    for final_section in final_tree_data:\n\
          \        file_name = final_section['file']\n        section_name = final_section['section']\n\
          \        \n        for final_chapter in final_section['chapters']:\n   \
          \         if 'chapter_id' not in final_chapter:\n                continue\n\
          \                \n            chapter_id = final_chapter['chapter_id']\n\
          \            chapter_key = (file_name, section_name, chapter_id)\n     \
          \       \n            # O(1)æŸ¥æ‰¾\n            tree_chapter = chapter_index.get(chapter_key)\n\
          \            \n            if tree_chapter:\n                # ç›´æ¥ä¿®æ”¹å¼•ç”¨çš„å¯¹è±¡\n\
          \                tree_chapter['parameters'] = final_chapter.get('paramaters',\
          \ [])\n                tree_chapter['topic_keywords'] = final_chapter.get('topic_keywords',\
          \ [])\n                tree_chapter['context_keywords'] = final_chapter.get('context_keywords',\
          \ [])\n                tree_chapter['refs'] = final_chapter.get('refs',\
          \ [])\n                tree_chapter['table_headers'] = final_chapter.get('table_headers',\
          \ [])\n                merged_count += 1\n            else:\n          \
          \      not_found_count += 1\n    \n    return merged_count, not_found_count\n\
          \ndef merge_result_data(chapter_index, result_data):\n    \"\"\"å°†resultçš„experimentsæ•°æ®èåˆåˆ°treeä¸­\"\
          \"\"\n    merged_count = 0\n    not_found_count = 0\n    \n    for result_section\
          \ in result_data:\n        file_name = result_section['file']\n        section_name\
          \ = result_section['section']\n        experiments_groups = result_section.get('experiments',\
          \ [])\n        \n        # éå†æ¯ä¸ªå®éªŒç»„\n        for experiments_group in experiments_groups:\n\
          \            if not experiments_group:\n                continue\n     \
          \           \n            # è·å–ç¬¬ä¸€ä¸ªå®éªŒçš„chapter_idä½œä¸ºè¯¥ç»„çš„å®šä½æ ‡è¯†\n            first_experiment\
          \ = experiments_group[0]\n            chapter_id = first_experiment.get('chapter_id')\n\
          \            \n            if not chapter_id:\n                not_found_count\
          \ += 1\n                continue\n            \n            chapter_key\
          \ = (file_name, section_name, chapter_id)\n            # O(1)æŸ¥æ‰¾å¯¹åº”çš„ç« èŠ‚\n \
          \           chapter = chapter_index.get(chapter_key)\n            \n   \
          \         if chapter:\n                # å°†è¯¥ç»„å®éªŒæ·»åŠ åˆ°å¯¹åº”ç« èŠ‚çš„experimentså­—æ®µ\n  \
          \              if 'experiments' not in chapter:\n                    chapter['experiments']\
          \ = []\n                chapter['experiments'].extend(experiments_group)\n\
          \                merged_count += 1\n            else:\n                not_found_count\
          \ += 1\n    \n    return merged_count, not_found_count\n\ndef main(result:\
          \ str, tree: str, final_tree: str, file_name) -> dict:\n    # åŠ è½½JSONæ•°æ®\n\
          \    tree_data = json.loads(tree)\n    final_tree_data = json.loads(final_tree)\n\
          \    result_data = json.loads(result)\n    \n    # æ„å»ºç´¢å¼•\n    chapter_index,\
          \ section_index = build_chapter_index(tree_data)\n    \n    # èåˆæ•°æ®\n   \
          \ chapter_merged, chapter_not_found = merge_final_tree_data(chapter_index,\
          \ final_tree_data)\n    experiment_merged, experiment_not_found = merge_result_data(chapter_index,\
          \ result_data)\n    \n    # ç»Ÿè®¡ä¿¡æ¯\n    total_chapters = len(chapter_index)\n\
          \    chapters_with_params = 0\n    chapters_with_keywords = 0\n    chapters_with_experiments\
          \ = 0\n    \n    for chapter in chapter_index.values():\n        if chapter.get('parameters'):\n\
          \            chapters_with_params += 1\n        if chapter.get('topic_keywords'):\n\
          \            chapters_with_keywords += 1\n        if chapter.get('experiments'):\n\
          \            chapters_with_experiments += 1\n    \n    # åˆå¹¶åçš„æ•°æ®å°±æ˜¯ä¿®æ”¹åçš„tree_data\n\
          \    merged_tree = tree_data\n\n    # å»æ‰æ‰©å±•åï¼Œå†æ‹¼ .json\n    base_name = os.path.splitext(file_name)[0]\
          \          # å¾—åˆ°æ— åç¼€çš„çº¯æ–‡ä»¶å\n    path = f\"/tmp/mydata/{base_name}.json\"\n\
          \    os.makedirs(os.path.dirname(path), exist_ok=True)\n    with open(path,\
          \ \"w\", encoding=\"utf-8\") as f:\n        json.dump(merged_tree, f, ensure_ascii=False,\
          \ indent=2)\n    \n    return {\n        \"result\": f\"èåˆå®Œæˆï¼ç»Ÿè®¡ä¿¡æ¯: æ€»ç« èŠ‚æ•°:\
          \ {total_chapters}, å‚æ•°ç« èŠ‚: {chapters_with_params}, å…³é”®è¯ç« èŠ‚: {chapters_with_keywords},\
          \ è¯•éªŒç« èŠ‚: {chapters_with_experiments}, èåˆæˆåŠŸ: ç« èŠ‚æ•°æ®{chapter_merged}ä¸ª, è¯•éªŒæ•°æ®{experiment_merged}ä¸ª\"\
          \n    }"
        code_language: python3
        desc: ''
        outputs:
          result:
            children: null
            type: string
        selected: false
        title: ä»£ç æ‰§è¡Œ 8
        type: code
        variables:
        - value_selector:
          - '1756618044071'
          - result
          value_type: string
          variable: result
        - value_selector:
          - '1756550411122'
          - tree
          value_type: string
          variable: tree
        - value_selector:
          - '1756563307317'
          - final_tree
          value_type: string
          variable: final_tree
        - value_selector:
          - '1756550268945'
          - file
          - name
          value_type: file
          variable: file_name
      height: 53
      id: '1756629493937'
      position:
        x: 3902
        y: 459
      positionAbsolute:
        x: 3902
        y: 459
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        context:
          enabled: false
          variable_selector: []
        desc: ''
        isInIteration: true
        isInLoop: false
        iteration_id: '1756557801310'
        model:
          completion_params: {}
          mode: chat
          name: deepseek-reasoner
          provider: langgenius/deepseek/deepseek
        prompt_template:
        - id: fb74cb72-3859-4cad-ac1e-cdbb9834f569
          role: system
          text: "# è§’è‰²ï¼ˆRoleï¼‰\nä½ æ˜¯ä¸€åä¸¥æ ¼çš„ JSON è¾“å‡ºä¿®å¤å™¨ï¼ˆJSON Fixerï¼‰ã€‚  \nä½ çš„èŒè´£æ˜¯ï¼š\n- æ¥æ”¶ä¸€ä¸ªå¯èƒ½æ ¼å¼é”™è¯¯çš„\
            \ JSON å­—ç¬¦ä¸²å’Œé”™è¯¯ä¿¡æ¯ã€‚\n- ä¸¥æ ¼ä¿®å¤å…¶ä¸­çš„è¯­æ³•é—®é¢˜æˆ–ç»“æ„é—®é¢˜ã€‚\n- è¾“å‡ºå®Œå…¨ç¬¦åˆé¢„æœŸæ ¼å¼çš„ JSON å¯¹è±¡ã€‚\n\n---\n\
            \n# è¾“å…¥ï¼ˆInputï¼‰\nä½ å°†æ”¶åˆ°ï¼š\n1. **error_type**ï¼šè§£æå¤±è´¥çš„ç±»å‹\n2. **error_message**ï¼šå…·ä½“é”™è¯¯åŸå› \n\
            2. **raw_output**ï¼šæ¨¡å‹åŸå§‹è¾“å‡ºï¼Œä½ éœ€è¦å¿½ç•¥ä»â€<think>â€œåˆ°\"</think>\"çš„éƒ¨åˆ†ï¼Œåªä¿ç•™é”™è¯¯ JSONã€‚\n\
            \nç¤ºä¾‹è¾“å…¥ï¼š\nerror: Expecting property name enclosed in double quotes: line\
            \ 3 column 5 (char 25)\nraw_text: {file: \"doc1\", section: \"main\",\
            \ chapters: [ ... ]}\n\n---\n\n# è¾“å‡ºæ ¼å¼ï¼ˆOutputï¼‰\nè¯·è¾“å‡ºä¸€ä¸ª **å®Œæ•´ JSON å¯¹è±¡**ï¼Œå¿…é¡»ä¸¥æ ¼ç¬¦åˆä»¥ä¸‹ç»“æ„ï¼ˆä¸è¦è¾“å‡ºå¤šä½™çš„è§£é‡Šæ€§æ–‡å­—ï¼‰ï¼š\
            \  \n```json\n{\n  \"file\": \"string\",\n  \"section\": \"string\",\n\
            \  \"experiment_root_ids\": [\"string\", \"...\"],\n  \"chapters\": [\n\
            \    {\n      \"chapter_id\": \"string\",\n      \"paramaters\": [\n \
            \       {\n          \"item\": \"string\",\n          \"constraint\":\
            \ \"<=|<|=|>=|>|range_closed|range_open|enum|boolean\",\n          \"\
            value\": \"string|array|null\",\n          \"unit\": \"string|null\",\n\
            \          \"source_text\": \"string\"\n        }\n      ],\n      \"\
            topic_keywords\": [\"string\", \"...\"],\n      \"context_keywords\":\
            \ [\"string\", \"...\"],\n      \"refs\": [\n        {\n          \"ref_type\"\
            : \"internal|external\",\n          \"doc_id\": \"string|null\",\n   \
            \       \"target_id\": \"string\",\n          \"anchor_text\": \"string\"\
            \n        }\n      ],\n      \"table_headers\": [\"string\", \"...\"]\n\
            \    }\n  ]\n}\n```\n\n# æ³¨æ„äº‹é¡¹\n\n1. å¹¶ä¸æ˜¯è¯´æ­¤jsonä»…åŒ…å«ä¸€ä¸ªé”™è¯¯ï¼Œåªæ˜¯ç”¨ä½œå‚è€ƒï¼Œä½ éœ€è¦ä¸¥æ ¼æŒ‰ç…§è¾“å‡ºæ ¼å¼è¦æ±‚è¿›è¡Œä¿®å¤ï¼Œå¯é€šè¿‡è¾“å…¥jsonè¿›è¡Œæ¨æ–­ï¼Œç¡®ä¿\
            \ JSON å­—ç¬¦ä¸²å¯è¢« `json.loads()` è§£æé€šè¿‡å³å¯\n2. ä¸¥æ ¼è¾“å‡ºä¸€ä¸ªåˆæ³• JSON å¯¹è±¡ï¼Œä¸å¾—è¾“å‡ºå¤šä½™æ–‡å­—ã€æ³¨é‡Šæˆ– Markdownã€‚\n\
            3. å¦‚æœæŸå­—æ®µæ— å†…å®¹ï¼Œè¯·ä½¿ç”¨ `null` æˆ–ç©ºæ•°ç»„ `[]`ã€‚\n4. å¿…é¡»è¡¥å…¨ç¼ºå¤±å­—æ®µï¼Œä¸å…è®¸çœç•¥ä»»ä½•å­—æ®µã€‚"
        - id: c68f56a1-5429-4a52-b480-a030d0c3f431
          role: user
          text: 'error_type:

            {{#1756557929661.error_type#}}


            error_message:

            {{#1756557929661.error_message#}}


            raw_text:

            {{#1756557804693.text#}}

            '
        selected: false
        title: LLM 3
        type: llm
        variables: []
        vision:
          enabled: false
      height: 89
      id: '1756698267184'
      parentId: '1756557801310'
      position:
        x: 807.7142857142858
        y: 118.78571428571433
      positionAbsolute:
        x: 1445.7142857142858
        y: 577.7857142857143
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
      zIndex: 1002
    - data:
        desc: ''
        isInIteration: true
        isInLoop: false
        iteration_id: '1756557801310'
        output_type: string
        selected: false
        title: å˜é‡èšåˆå™¨
        type: variable-aggregator
        variables:
        - - '1756557929661'
          - result
        - - '1756698848564'
          - result
      height: 129
      id: '1756698812908'
      parentId: '1756557801310'
      position:
        x: 1374.2857142857142
        y: 77.14285714285711
      positionAbsolute:
        x: 2012.2857142857142
        y: 536.1428571428571
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
      zIndex: 1002
    - data:
        code: "import json\nimport re\n\nimport json\nimport ast\nimport re\nfrom\
          \ typing import Dict, List, Any\n\ndef parse_input(src: str) -> List[Dict[str,\
          \ Any]]:\n    \"\"\"é¢„å¤„ç†å¹¶è§£æè¾“å…¥å­—ç¬¦ä¸²ä¸ºåˆ—è¡¨å­—å…¸ç»“æ„\"\"\"\n    try:\n        # å»é™¤ LLM\
          \ æ€ç»´é“¾æ ‡è®°\n        # 1. ç›´æ¥æŒ‰ </think> åˆ‡åˆ†å¹¶å–ååŠæ®µ\n        src = src.rsplit('</think>',\
          \ 1)[-1].strip()\n\n        try:\n            data = json.loads(src)\n \
          \       except json.JSONDecodeError:\n            try:\n               \
          \ if src.startswith('\\ufeff'):\n                    src = src[1:]\n   \
          \             data = ast.literal_eval(src)\n            except (SyntaxError,\
          \ ValueError):\n                try:\n                    processed = src.replace('\\\
          \\\"', '\"').replace('\\\\n', '\\n')\n                    data = json.loads(processed)\n\
          \                except Exception as e:\n                    raise ValueError(f\"\
          æ— æ³•è§£æï¼š{str(e)}\")\n\n        # ä¿è¯ä¸º list\n        if isinstance(data, dict):\n\
          \            return [data]\n        elif isinstance(data, list):\n     \
          \       return data\n        else:\n            return []\n    except Exception\
          \ as e:\n        raise ValueError(f\"é¢„å¤„ç†å¤±è´¥: {str(e)}\")\n        # return\
          \ []\n\n\ndef main(arg1: str) -> dict:\n    data = parse_input(arg1)\n\n\
          \    return {\n        \"result\":json.dumps(data, ensure_ascii=False),\n\
          \    }"
        code_language: python3
        desc: ''
        isInIteration: true
        isInLoop: false
        iteration_id: '1756557801310'
        outputs:
          result:
            children: null
            type: string
        selected: false
        title: ä»£ç æ‰§è¡Œ 9
        type: code
        variables:
        - value_selector:
          - '1756698267184'
          - text
          value_type: string
          variable: arg1
      height: 53
      id: '1756698848564'
      parentId: '1756557801310'
      position:
        x: 1087.4285714285713
        y: 119.64285714285722
      positionAbsolute:
        x: 1725.4285714285713
        y: 578.6428571428572
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
      zIndex: 1002
    viewport:
      x: -514.0000000000007
      y: -78
      zoom: 0.7
