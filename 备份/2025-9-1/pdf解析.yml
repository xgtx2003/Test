app:
  description: '‰øùÁïôÂºïÁî®„ÄÅÈù¢Âêë‰∏§‰∏™Âú∫ÊôØ

    '
  icon: ü§ñ
  icon_background: '#FFEAD5'
  mode: advanced-chat
  name: pdfËß£Êûê
  use_icon_as_answer_icon: false
dependencies:
- current_identifier: null
  type: marketplace
  value:
    marketplace_plugin_unique_identifier: langgenius/deepseek:0.0.6@dd589dc093c8084925858034ab5ec1fdf0d33819f43226c2f8c4a749a9acbbb2
kind: app
version: 0.3.0
workflow:
  conversation_variables: []
  environment_variables: []
  features:
    file_upload:
      allowed_file_extensions:
      - .JPG
      - .JPEG
      - .PNG
      - .GIF
      - .WEBP
      - .SVG
      allowed_file_types:
      - image
      allowed_file_upload_methods:
      - local_file
      - remote_url
      enabled: false
      fileUploadConfig:
        audio_file_size_limit: 50
        batch_count_limit: 5
        file_size_limit: 15
        image_file_size_limit: 10
        video_file_size_limit: 100
        workflow_file_upload_limit: 10
      image:
        enabled: false
        number_limits: 3
        transfer_methods:
        - local_file
        - remote_url
      number_limits: 3
    opening_statement: ''
    retriever_resource:
      enabled: true
    sensitive_word_avoidance:
      enabled: false
    speech_to_text:
      enabled: false
    suggested_questions: []
    suggested_questions_after_answer:
      enabled: false
    text_to_speech:
      enabled: false
      language: ''
      voice: ''
  graph:
    edges:
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '1756557801310'
        sourceType: iteration-start
        targetType: llm
      id: 1756557801310start-source-1756557804693-target
      source: 1756557801310start
      sourceHandle: source
      target: '1756557804693'
      targetHandle: target
      type: custom
      zIndex: 1002
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '1756557801310'
        sourceType: llm
        targetType: code
      id: 1756557804693-source-1756557929661-target
      source: '1756557804693'
      sourceHandle: source
      target: '1756557929661'
      targetHandle: target
      type: custom
      zIndex: 1002
    - data:
        isInLoop: false
        sourceType: iteration
        targetType: code
      id: 1756557801310-source-1756563307317-target
      source: '1756557801310'
      sourceHandle: source
      target: '1756563307317'
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInLoop: false
        sourceType: start
        targetType: code
      id: 1756550268945-source-1756550411122-target
      source: '1756550268945'
      sourceHandle: source
      target: '1756550411122'
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '1756613329065'
        sourceType: iteration-start
        targetType: llm
      id: 1756613329065start-source-1756613344845-target
      source: 1756613329065start
      sourceHandle: source
      target: '1756613344845'
      targetHandle: target
      type: custom
      zIndex: 1002
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '1756613329065'
        sourceType: llm
        targetType: code
      id: 1756613344845-source-1756613456815-target
      source: '1756613344845'
      sourceHandle: source
      target: '1756613456815'
      targetHandle: target
      type: custom
      zIndex: 1002
    - data:
        isInIteration: false
        isInLoop: false
        sourceType: iteration
        targetType: code
      id: 1756613329065-source-1756618044071-target
      source: '1756613329065'
      sourceHandle: source
      target: '1756618044071'
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInIteration: false
        isInLoop: false
        sourceType: code
        targetType: code
      id: 1756618044071-source-1756629493937-target
      source: '1756618044071'
      sourceHandle: source
      target: '1756629493937'
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInIteration: false
        isInLoop: false
        sourceType: code
        targetType: answer
      id: 1756629493937-source-1756558073574-target
      source: '1756629493937'
      sourceHandle: source
      target: '1756558073574'
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInLoop: false
        sourceType: code
        targetType: iteration
      id: 1756550411122-source-1756557801310-target
      source: '1756550411122'
      sourceHandle: source
      target: '1756557801310'
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInLoop: false
        sourceType: code
        targetType: iteration
      id: 1756563307317-source-1756613329065-target
      source: '1756563307317'
      sourceHandle: source
      target: '1756613329065'
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '1756557801310'
        sourceType: code
        targetType: llm
      id: 1756557929661-fail-branch-1756698267184-target
      source: '1756557929661'
      sourceHandle: fail-branch
      target: '1756698267184'
      targetHandle: target
      type: custom
      zIndex: 1002
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '1756557801310'
        sourceType: code
        targetType: variable-aggregator
      id: 1756557929661-source-1756698812908-target
      source: '1756557929661'
      sourceHandle: source
      target: '1756698812908'
      targetHandle: target
      type: custom
      zIndex: 1002
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '1756557801310'
        sourceType: llm
        targetType: code
      id: 1756698267184-source-1756698848564-target
      source: '1756698267184'
      sourceHandle: source
      target: '1756698848564'
      targetHandle: target
      type: custom
      zIndex: 1002
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '1756557801310'
        sourceType: code
        targetType: variable-aggregator
      id: 1756698848564-source-1756698812908-target
      source: '1756698848564'
      sourceHandle: source
      target: '1756698812908'
      targetHandle: target
      type: custom
      zIndex: 1002
    nodes:
    - data:
        desc: ''
        selected: false
        title: ÂºÄÂßã
        type: start
        variables:
        - allowed_file_extensions: []
          allowed_file_types:
          - document
          allowed_file_upload_methods:
          - local_file
          - remote_url
          label: file
          max_length: 48
          options: []
          required: false
          type: file
          variable: file
        - label: context
          max_length: 100000
          options: []
          required: false
          type: paragraph
          variable: context
        - label: array
          max_length: 1000000
          options: []
          required: false
          type: paragraph
          variable: array
        - label: regulation
          max_length: 1000000
          options: []
          required: false
          type: paragraph
          variable: regulation
      height: 167
      id: '1756550268945'
      position:
        x: 30
        y: 459
      positionAbsolute:
        x: 30
        y: 459
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        answer: '{{#1756550411122.tree#}}







          {{#1756550411122.context#}}





          {{#1756550411122.array#}}'
        desc: ''
        selected: false
        title: Áõ¥Êé•ÂõûÂ§ç
        type: answer
        variables: []
      height: 122
      id: answer
      position:
        x: 30
        y: 666
      positionAbsolute:
        x: 30
        y: 666
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        code: "import fitz  # PyMuPDF\nimport re\nimport json\nimport os\nfrom typing\
          \ import List, Dict, Tuple\nfrom collections import defaultdict\n\n# chapter_patterns\
          \ = [\n#     re.compile(r'^(ÈôÑ\\s*ÂΩï\\s*[A-Z])\\s+(.+)$'),\n#     re.compile(r'^([A-Z](?:\\\
          .\\d+)+)\\s+(.+)$'),\n#     re.compile(r'^(\\d+(?:\\.\\d+)*)(\\s+)(.+)$'),\n\
          # ]\n\n# chapter_patterns = [\n#     re.compile(r'^(APPENDIX\\s+[A-Z0-9]+)$',\
          \ re.I),          # APPENDIX A / APPENDIX 1\n#     re.compile(r'^([A-Z](?:\\\
          .\\d+)+)\\s+(.+)$'),\n#     re.compile(r'^(\\d+(?:\\.\\d+)*\\.?)\\s+(.+)$'),\
          \                     # 1.1. Title\n# ]\n\n# ÊóßÁöÑÁ´†ËäÇÊ®°ÂºèÔºàÂ∑≤Ê≥®ÈáäÔºâ\n# chapter_patterns\
          \ = [\n#     re.compile(r'^(ÈôÑ\\s*ÂΩï\\s*[A-Z0-9])$'), # ÈôÑ ÂΩï B\n#     re.compile(r'^((APPENDIX|ANNEX|ATTACHMENT)\\\
          s+(?:[A-Z0-9]+|\\([A-Z0-9]+\\)))$', re.I),  # ANNEX A / ANNEX 1\n#     re.compile(r'^([A-Z]\\\
          .)\\s+(.+)$'),                                # A. Title (ÂçïÁã¨Â≠óÊØçÁ´†ËäÇ)\n#   \
          \  re.compile(r'^([A-Z](?:\\.\\d+)+\\.?)\\s+(.+)$'),                   \
          \  # A.1. Title / A.1.1. Title\n#     re.compile(r'^(\\d+(?:\\.\\d+)*\\\
          .?)\\s+(.+)$'),                       # 1.1. Title\n#     re.compile(r'^(\\\
          d+(?:-\\d+)*-)\\s+(.+)$'),                          # 1- Title / 1-2- Title\n\
          # ]\n\n# Êñ∞ÁöÑÂêàÂπ∂ÂêéÁöÑÁ´†ËäÇÊ®°Âºè\nchapter_patterns = [\n    # 1. ‰∏≠ÊñáÈôÑÂΩïÔºöÈôÑÂΩïA, ÈôÑ ÂΩï B\n  \
          \  re.compile(r'^(ÈôÑ\\s*ÂΩï\\s*[A-Z0-9])$'),\n    \n    # 2. Ëã±ÊñáÈôÑÂΩïÔºöAPPENDIX\
          \ A, ANNEX A, ATTACHMENT A\n    re.compile(r'^((APPENDIX|ANNEX|ATTACHMENT)\\\
          s+(?:[A-Z0-9]+|\\([A-Z0-9]+\\)))$', re.I),\n    \n    # 3. Â≠óÊØçÁ´†ËäÇÔºàÊîØÊåÅÁÇπÂíåÊ®™Á∫øÂàÜÈöîÁ¨¶ÔºâÔºöA.\
          \ Title, A.1. Title, A-1- Title\n    re.compile(r'^([A-Z](?:[.\\-]\\d+)*[.\\\
          -]?)\\s+(.+)$'),\n    \n    # 4. Êï∞Â≠óÁ´†ËäÇÔºàÊîØÊåÅÁÇπÂíåÊ®™Á∫øÂàÜÈöîÁ¨¶ÔºâÔºö1. Title, 1.1. Title, 1-\
          \ Title, 1-2- Title\n    re.compile(r'^(\\d+(?:[.\\-]\\d+)*[.\\-]?)\\s+(.+)$'),\n\
          ]\n\ndef detect_document_language(lines: List[str]) -> str:\n    \"\"\"\n\
          \    Ê£ÄÊµãÊñáÊ°£ËØ≠Ë®ÄÔºö‰∏≠ÊñáÊàñËã±Êñá\n    :param lines: ÊñáÊ°£ÁöÑÊâÄÊúâË°å\n    :return: 'zh' Ë°®Á§∫‰∏≠ÊñáÔºå'en'\
          \ Ë°®Á§∫Ëã±Êñá\n    \"\"\"\n    chinese_char_count = 0\n    total_chars = 0\n  \
          \  \n    # ÈááÊ†∑Ââç1000Ë°åÊàñÂÖ®ÈÉ®Ë°å\n    sample_lines = lines[:1000] if len(lines) >\
          \ 1000 else lines\n    \n    for line in sample_lines:\n        for char\
          \ in line:\n            total_chars += 1\n            if '\\u4e00' <= char\
          \ <= '\\u9fff':  # ‰∏≠ÊñáÂ≠óÁ¨¶\n                chinese_char_count += 1\n    \n\
          \    # Âè™Ë¶ÅÊúâ‰∏≠ÊñáÂ≠óÁ¨¶Â∞±ËÆ§‰∏∫ÊòØ‰∏≠ÊñáÊñáÊ°£\n    if chinese_char_count > 0:\n        return 'zh'\n\
          \    else:\n        return 'en'\n\n# ‰∏≠ÊñáÁ´†ËäÇmax_chapter_num=50\n# ÂÖ®ÊñáÈ¶ñÂÖàÊ£ÄÊµãÊòØ‰∏≠ÊñáËøòÊòØËã±Êñá\n\
          def detect_chapter(line: str, max_chapter_num=1000, language='en', number_analysis=None):\n\
          \    clean_line = line.strip()\n    if not clean_line:\n        return None\n\
          \n    for pattern in chapter_patterns:\n        m = pattern.match(clean_line)\n\
          \        if m:\n            chapter_id = m.group(1).strip()\n          \
          \  chapter_title = m.group(len(m.groups())).strip() if m.group(len(m.groups()))\
          \ else \"\"\n            if re.match(r'^(ÈôÑ\\s*ÂΩï\\s*[A-Z0-9])$', chapter_id):\n\
          \                # ÂéªÊéâ‰∏≠Èó¥ÁöÑÁ©∫Ê†º\n                chapter_id = chapter_id.replace(\"\
          \ \", \"\")\n                # chapter_id = chapter_id[-1]\n           \
          \ # ---- Âü∫Á°ÄËøáÊª§ ----\n            first_num = None\n            if chapter_id.upper().startswith(\"\
          APPENDIX\"):\n                suffix = chapter_id[len(\"APPENDIX\"):].strip(\"\
          \ ()\")\n                if suffix.isdigit():\n                    first_num\
          \ = int(suffix)\n            else:\n                m_num = re.match(r'^(\\\
          d+)', chapter_id)\n                if m_num:\n                    first_num\
          \ = int(m_num.group(1))\n\n            if first_num is not None and number_analysis\
          \ is not None:\n                # ‰ΩøÁî®Êô∫ËÉΩÊï∞Â≠óËåÉÂõ¥Âà§Êñ≠\n                min_reasonable\
          \ = number_analysis.get(\"min_reasonable\", 1)\n                max_reasonable\
          \ = number_analysis.get(\"max_reasonable\", max_chapter_num)\n         \
          \       \n                # ÁâπÊÆäÂ§ÑÁêÜÊ≥ïËßÑÁºñÂè∑Ê®°Âºè\n                if number_analysis.get(\"\
          regulation_mode\", False):\n                    regulation_number = number_analysis.get(\"\
          regulation_number\")\n                    if first_num != regulation_number:\n\
          \                        return None  # ‰∏çÊòØÊ≥ïËßÑÁºñÂè∑ÔºåËøáÊª§Êéâ\n                else:\n\
          \                    # Ê≠£Â∏∏Á´†ËäÇÁºñÂè∑ËåÉÂõ¥Ê£ÄÊü•\n                    if first_num < min_reasonable\
          \ or first_num > max_reasonable:\n                        return None  #\
          \ Êï∞Â≠óËåÉÂõ¥‰∏çÂêàÁêÜ\n            elif first_num is not None:\n                # ÂÖúÂ∫ïÈÄªËæëÔºö‰ΩøÁî®‰º†ÁªüÁöÑmax_chapter_num\n\
          \                if first_num < 1 or first_num > max_chapter_num:\n    \
          \                return None  # Êï∞Â≠óËåÉÂõ¥‰∏çÂêàÁêÜ\n\n            # ---- ÂÜÖÂÆπÁâπÂæÅËøáÊª§ ----\n\
          \            # 1) Ê†áÈ¢òÂøÖÈ°ªÂåÖÂê´Â≠óÊØçÊàñ‰∏≠Êñá\n            if not re.search(r'[A-Za-z\\\
          u4e00-\\u9fff]', chapter_title):\n                return None\n\n      \
          \      # 2) ÂéªÊéâÁ∫ØÊï∞Â≠óË°®Ê†ºË°å\n            if re.fullmatch(r'[\\d\\s\\.\\-]+', chapter_title):\n\
          \                return None\n\n            # 3) Ë°®Ê†ºÂÜÖÂÆπËøáÊª§ - Ê£ÄÊµãÊòéÊòæÁöÑË°®Ê†ºÊï∞ÊçÆÊ®°Âºè\n\
          \            # Â¶ÇÊûúÊ†áÈ¢òÂåÖÂê´Â§ßÈáèÊï∞Â≠ó„ÄÅÁ©∫Ê†ºÂíåÂ∞ëÈáèÂ≠óÊØçÁöÑÁªÑÂêàÔºåÂèØËÉΩÊòØË°®Ê†ºÊï∞ÊçÆ\n            if re.search(r'^\\\
          d+\\s+\\d+.*[A-Z]\\s+\\d+\\s+\\d+', chapter_title):  # Â¶Ç \"10 0 E 0 16\"\
          \n                return None\n            \n            # Ê£ÄÊµãË°®Ê†ºË°åÊ®°ÂºèÔºöÂçï‰∏™Â≠óÊØç\
          \ + Êï∞Â≠óÁªÑÂêà\n            if re.fullmatch(r'[A-Z]\\s*\\d+.*', chapter_title)\
          \ and len(chapter_title.split()) >= 3:\n                # Â¶ÇÊûúÊ†áÈ¢òÊòØ \"A 10 0\"\
          \ ËøôÊ†∑ÁöÑÊ†ºÂºèÔºåÂæàÂèØËÉΩÊòØË°®Ê†ºÊï∞ÊçÆ\n                parts = chapter_title.split()\n      \
          \          if len(parts) >= 3 and all(part.isdigit() or part in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\
          \ for part in parts[:3]):\n                    return None\n\n         \
          \   # 4) Ê£ÄÊµãÂùêÊ†áÁÇπÊàñÂèÇÊï∞Ë°®Ê†ºÔºöÂ¶Ç \"A15 0 E 0 3\"\n            if re.search(r'^[A-Z]\\\
          d+\\s+\\d+\\s+[A-Z]\\s+\\d+\\s+\\d+', chapter_title):\n                return\
          \ None\n\n            # 5) Ë°åÂ§™Áü≠\n            if len(clean_line) < 4 and not\
          \ chapter_id.upper().startswith(\"APPENDIX\") and not chapter_id.startswith(\"\
          ÈôÑÂΩï\"):\n                return None\n\n            # 6) ËøáÊª§ÊòéÊòæÁöÑË°®Ê†ºÊ†áÈ¢òÁªÑÂêà\n  \
          \          if len(chapter_id) == 1 and chapter_id.isupper():\n         \
          \       # Âçï‰∏™Â§ßÂÜôÂ≠óÊØç‰Ωú‰∏∫Á´†ËäÇIDÔºåÊ£ÄÊü•Ê†áÈ¢òÊòØÂê¶ÂÉèË°®Ê†ºÊï∞ÊçÆ\n                if re.search(r'\\d+.*\\\
          d+', chapter_title) and len(chapter_title.split()) <= 6:\n             \
          \       return None\n\n            return {\n                \"chapter_id\"\
          : chapter_id,\n                \"chapter_title\": chapter_title\n      \
          \      }\n\n    return None\n\ndef build_tree(chapter_list: List[Dict])\
          \ -> List[Dict]:\n    id_map = {}\n    root = []\n\n    # ÂÖàÊ≥®ÂÜåÊâÄÊúâËäÇÁÇπ\n    for\
          \ chap in chapter_list:\n        chap[\"children\"] = []\n        # Áªü‰∏ÄÂéªÊéâÊú´Â∞æÁÇπÂíåÊ®™Á∫ø‰Ωú‰∏∫\
          \ key\n        key = chap[\"chapter_id\"].rstrip('.-')\n        id_map[key]\
          \ = chap\n\n    # ‰∏∫ÊØè‰∏™ËäÇÁÇπÂàõÂª∫Áº∫Â§±ÁöÑÁà∂ËäÇÁÇπÔºàÂè™ÈíàÂØπ‰∏âÁ∫ßÂèä‰ª•‰∏äÊ†áÈ¢òÔºâ\n    for chap in chapter_list:\n\
          \        cid = chap[\"chapter_id\"].rstrip('.')\n        parts = cid.split('.')\n\
          \        \n        # Âè™Êúâ‰∏âÁ∫ßÂèä‰ª•‰∏äÊ†áÈ¢òÊâçÂàõÂª∫‰∏≠Èó¥Áà∂ËäÇÁÇπ\n        if len(parts) >= 3:\n  \
          \          # ÂàõÂª∫ÊâÄÊúâÁº∫Â§±ÁöÑ‰∏≠Èó¥Áà∂Á∫ßËäÇÁÇπÔºà‰ΩÜ‰∏çÂåÖÊã¨È°∂Á∫ßÁà∂ËäÇÁÇπÔºâ\n            for i in range(2, len(parts)):\
          \  # ‰ªéÁ¨¨‰∫åÁ∫ßÂºÄÂßãÂàõÂª∫ÔºåË∑≥ËøáÈ°∂Á∫ß\n                parent_key = '.'.join(parts[:i])\n \
          \               if parent_key not in id_map:\n                    # ÂàõÂª∫Áº∫Â§±ÁöÑÁà∂ËäÇÁÇπ\n\
          \                    parent_node = {\n                        \"chapter_id\"\
          : parent_key + \".\",\n                        \"chapter_title\": \"\",\n\
          \                        \"raw_text\": \"\",\n                        \"\
          children\": []\n                    }\n                    id_map[parent_key]\
          \ = parent_node\n\n    # ÊûÑÂª∫Ê†ëÁªìÊûÑ\n    for chap in chapter_list:\n        cid\
          \ = chap[\"chapter_id\"].rstrip('.')\n        parts = cid.split('.')\n\n\
          \        # Ê†πËäÇÁÇπÂà§Êñ≠\n        if cid.startswith(\"APPENDIX\"):\n           \
          \ root.append(chap)\n        elif cid.startswith(\"ÈôÑÂΩï\") or len(parts) ==\
          \ 1:\n            root.append(chap)\n        else:\n            parent_key\
          \ = '.'.join(parts[:-1])\n            parent = id_map.get(parent_key)\n\
          \            if parent:\n                parent[\"children\"].append(chap)\n\
          \            else:\n                # Â¶ÇÊûúÁà∂ËäÇÁÇπ‰∏çÂ≠òÂú®ÔºåÂØπ‰∫é‰∫åÁ∫ßÊ†áÈ¢òÔºåÁõ¥Êé•‰Ωú‰∏∫Ê†πËäÇÁÇπ\n        \
          \        if len(parts) == 2:\n                    root.append(chap)\n  \
          \              # ‰∏âÁ∫ßÂèä‰ª•‰∏äÊ†áÈ¢òÊ≤°ÊúâÁà∂ËäÇÁÇπÊó∂Ôºå‰∏çÂÅöÂ§ÑÁêÜÔºàÂõ†‰∏∫ÂâçÈù¢Â∑≤ÁªèÂàõÂª∫‰∫ÜÁà∂ËäÇÁÇπÔºâ\n\n\n    # Â∞ÜÂàõÂª∫ÁöÑ‰∏≠Èó¥ËäÇÁÇπ‰πüÊ∑ªÂä†Âà∞ÊúÄÁªàÁöÑÁ´†ËäÇÂàóË°®‰∏≠Ôºå‰ΩÜÂè™ÊúâÈÇ£‰∫õÊúâÂ≠êËäÇÁÇπÁöÑ\n\
          \    created_parents = []\n    for key, node in id_map.items():\n      \
          \  if node not in chapter_list and len(node[\"children\"]) > 0:\n      \
          \      created_parents.append(node)\n    \n    # ÂØπÂàõÂª∫ÁöÑÁà∂ËäÇÁÇπ‰πüËøõË°åÊ†ëÁªìÊûÑÊûÑÂª∫\n    for\
          \ parent in created_parents:\n        cid = parent[\"chapter_id\"].rstrip('.')\n\
          \        parts = cid.split('.')\n        \n        if len(parts) == 1:\n\
          \            root.append(parent)\n        else:\n            parent_key\
          \ = '.'.join(parts[:-1])\n            grandparent = id_map.get(parent_key)\n\
          \            if grandparent and parent not in grandparent[\"children\"]:\n\
          \                grandparent[\"children\"].append(parent)\n            elif\
          \ len(parts) == 1:  # ËøôÊòØ‰∏ÄÁ∫ßÁ´†ËäÇ\n                if parent not in root:\n \
          \                   root.append(parent)\n    \n    return root\n\ndef build_full_path(chapters:\
          \ List[Dict], path_prefix=\"\"):\n    for chap in chapters:\n        if\
          \ path_prefix:\n            chap[\"full_path\"] = f\"{path_prefix}/{chap['chapter_id']}\
          \ {chap['chapter_title']}\"\n        else:\n            chap[\"full_path\"\
          ] = f\"{chap['chapter_id']} {chap['chapter_title']}\"\n        if chap.get(\"\
          children\"):\n            build_full_path(chap[\"children\"], chap[\"full_path\"\
          ])\n\ndef fullwidth_to_halfwidth(text: str) -> str:\n    result = []\n \
          \   for char in text:\n        code = ord(char)\n        if 0xFF01 <= code\
          \ <= 0xFF5E:\n            result.append(chr(code - 0xFEE0))\n        else:\n\
          \            result.append(char)\n    return ''.join(result)\n\ndef build_term_dict(raw_text:\
          \ str) -> Dict[str, str]:\n    text = re.sub(r'\\n+', '\\n', raw_text.strip())\n\
          \    pattern = re.compile(\n        r'^\\d+\\.\\d+\\n'\n        r'(?P<cn>[^\\\
          n]*?)\\s*'\n        r'(?P<en>[A-Za-z].*?)\\s*(?=\\n)',\n        re.MULTILINE\n\
          \    )\n\n    term_map = {}\n    for m in pattern.finditer(text):\n    \
          \    cn = m.group(\"cn\").strip()\n        en = m.group(\"en\").strip()\n\
          \        if cn and en:\n            term_map[cn] = en\n    return term_map\n\
          \ndef extract_terms_with_abbr_from_terms_section(raw_text: str) -> Dict[str,\
          \ Dict[str, str]]:\n    \"\"\"\n    ÊèêÂèñÊúØËØ≠Á´†ËäÇ‰∏≠ÁöÑ‰∏≠Ëã±ÊñáÊúØËØ≠ÂèäÁº©ÂÜô\n    ËøîÂõûÊ†ºÂºèÔºö\n    {\n\
          \      \"‰∏≠ÊñáÊúØËØ≠\": {\n         \"en\": \"Ëã±ÊñáÊúØËØ≠\",\n         \"abbr\": \"Áº©ÂÜôÔºàÂ¶ÇÊúâÔºâ\"\
          \n      }\n    }\n    \"\"\"\n    term_map = {}\n    text = re.sub(r'\\\
          n+', '\\n', raw_text.strip())\n\n    pattern = re.compile(\n        r'(?P<cn>[\\\
          u4e00-\\u9fffÔºàÔºâ()¬∑\\s]{2,})'        # ‰∏≠ÊñáÈÉ®ÂàÜ\n        r'\\s*'            \
          \                             # ÂèØÈÄâÁ©∫Ê†º\n        r'(?P<en>[A-Za-z][A-Za-z\\\
          s\\-/]*)'              # Ëã±ÊñáÊúØËØ≠\n        r'(?:[;Ôºõ:Ôºö]?\\s*(?P<abbr>[A-Z0-9¬∑]+))?',\
          \       # ÂèØÈÄâÁº©ÂÜô\n        re.MULTILINE\n    )\n\n\n    for m in pattern.finditer(text):\n\
          \        cn = m.group(\"cn\").strip()\n        en = m.group(\"en\").strip()\n\
          \        abbr = m.group(\"abbr\").strip() if m.group(\"abbr\") else \"\"\
          \n\n        term_map[cn] = {\"en\": en}\n        if abbr:\n            term_map[cn][\"\
          abbr\"] = abbr\n\n    return term_map\n\ndef extract_abbr_terms_from_symbols_section(raw_text:\
          \ str) -> Dict[str, Dict[str, str]]:\n    \"\"\"\n    ÊèêÂèñ‚ÄúÁ¨¶Âè∑ÂíåÁº©Áï•ËØ≠‚ÄùÁ´†ËäÇÁöÑ‰∏≠Ëã±Áº©ÂÜôÊò†Â∞ÑÔºåËøîÂõû‰ª•‰∏≠Êñá‰∏∫ÈîÆÁöÑÁªìÊûÑÔºö\n\
          \    {\n        \"‰∏≠Êñá\": {\n            \"abbr\": \"Áº©ÂÜô\",\n            \"\
          en\": \"Ëã±ÊñáÈáä‰πâ\"\n        }\n    }\n    \"\"\"\n    abbr_map = {}\n    # Ê∏ÖÁêÜÊñáÊú¨\n\
          \    text = re.sub(r'\\n+', '\\n', raw_text.strip())\n\n    # ÂåπÈÖçÊ®°ÂºèÔºöACLR:\
          \ ÈÇªÈÅìÊ≥ÑÊºèÂäüÁéáÊØî (Adjacent Channel Leakage Power Ratio)\n    pattern = re.compile(\n\
          \        r'(?P<abbr>[A-Za-z0-9¬∑\\-_]+)\\s*[:Ôºö]?\\s*'\n        r'(?P<cn>[\\\
          u4e00-\\u9fff¬∑]+)'\n        r'(?:[ÔºàÔºâ()]*\\s*(?P<en>[A-Za-z\\s/\\-]+)\\s*[ÔºàÔºâ()]*)?'\n\
          \    )\n\n    for m in pattern.finditer(text):\n        abbr = m.group(\"\
          abbr\").strip()\n        cn = m.group(\"cn\").strip(\"ÔºàÔºâ()\").strip()\n\
          \        en = m.group(\"en\").strip() if m.group(\"en\") else \"\"\n\n \
          \       if cn:\n            abbr_map[cn] = {}\n            if abbr:\n  \
          \              abbr_map[cn][\"abbr\"] = abbr\n            if en:\n     \
          \           abbr_map[cn][\"en\"] = en\n\n    return abbr_map\n\ndef should_merge_crossline(prev_text,\
          \ curr_text, prev_bbox, curr_bbox):\n    \"\"\"\n    Âà§Êñ≠ÊòØÂê¶ÈúÄË¶ÅÊääÂΩìÂâçË°åÂêàÂπ∂Âà∞‰∏ä‰∏ÄË°å\n\
          \    \"\"\"\n    text_stripped = curr_text.strip()\n\n    # Ê®°ÂºèÂåπÈÖçÔºöË°®Ê†ºÊ†áÈ¢ò„ÄÅÁºñÂè∑Ê†áÈ¢òÁ≠â\n\
          \    if re.match(r'^Ë°®\\s*\\d+', text_stripped):\n        return True\n\n\
          \    # ÂûÇÁõ¥Ë∑ùÁ¶ªÂæàÂ∞èÔºàËØ¥ÊòéÊòØËßÜËßâ‰∏äÁöÑÂêå‰∏ÄË°åÔºâ\n    prev_y = prev_bbox[1]\n    curr_y = curr_bbox[1]\n\
          \    line_height = prev_bbox[3] - prev_bbox[1]\n    if abs(curr_y - prev_y)\
          \ < 0.3 * line_height:\n        return True\n\n    return False\n\ndef fix_broken_chapters(lines:\
          \ list[str]) -> list[str]:\n    def normalize_chapter_spaces(s: str) ->\
          \ str:\n        line = s.strip()\n        \n        # 1. ‰øùÁïôÂéüÊù•ÁöÑÈÄªËæëÔºö‰øÆÂ§çÁÇπÂêéÈù¢ÁöÑÁ©∫Ê†ºÔºåÈÄÇÁî®‰∫éÊâÄÊúâÊÉÖÂÜµ\
          \ (A. 1, 7. 1)\n        line = re.sub(r'\\.\\s+(?=\\d)', '.', line)\n  \
          \      \n        # 2. ‰øÆÂ§çÊï∞Â≠ó/Â≠óÊØçÂíåÁÇπ‰πãÈó¥ÁöÑÁ©∫Ê†ºÔºö7 .1 -> 7.1, A .1 -> A.1\n        line\
          \ = re.sub(r'([A-Za-z0-9]+)\\s+(\\.\\d+)', r'\\1\\2', line)\n        \n\
          \        # 3. ‰øÆÂ§çÂ§çÊùÇÁöÑÂ§öÁ∫ßÁ©∫Ê†ºÔºö7 . 1 . 2 -> 7.1.2\n        # ÈúÄË¶ÅÂæ™ÁéØÂ§ÑÁêÜÔºåÁõ¥Âà∞Ê≤°ÊúâÊõ¥Â§öÂèòÂåñ\n\
          \        max_iterations = 10  # Èò≤Ê≠¢Êó†ÈôêÂæ™ÁéØ\n        iterations = 0\n       \
          \ prev_line = \"\"\n        while prev_line != line and iterations < max_iterations:\n\
          \            prev_line = line\n            # Â§ÑÁêÜÂêÑÁßçÁ©∫Ê†ºÁªÑÂêàÔºåÊîØÊåÅÂ≠óÊØçÂíåÊï∞Â≠óÂºÄÂ§¥\n      \
          \      line = re.sub(r'([A-Za-z0-9]+)\\s*\\.\\s*(\\d+)', r'\\1.\\2', line)\n\
          \            iterations += 1\n        \n        # 4. ‰øÆÂ§çOCRÂ∏∏ËßÅÈîôËØØÔºöÊï∞Â≠óÂºÄÂ§¥ÁöÑÁ´†ËäÇ\n\
          \        line = re.sub(r'(\\d+\\.\\d+)\\.\\s*l\\b', r'\\1.1', line)\n  \
          \      line = re.sub(r'([A-Za-z0-9]+)\\.l\\.(\\d+)', r'\\1.1.\\2', line)\n\
          \        line = re.sub(r'^l\\.(\\d+)', r'1.\\1', line)\n        \n     \
          \   # 5. ‰øÆÂ§çÂ≠óÊØçÂºÄÂ§¥Á´†ËäÇÁöÑOCRÈîôËØØÔºöB.l -> B.1, A.O -> A.0, C.I -> C.1\n        line\
          \ = re.sub(r'^([A-Z])\\.l\\b', r'\\1.1', line)\n        line = re.sub(r'^([A-Z])\\\
          .l\\.(\\d+)', r'\\1.1.\\2', line)\n        line = re.sub(r'^([A-Z])\\.O\\\
          .(\\d+)', r'\\1.0.\\2', line)\n        line = re.sub(r'^([A-Z])\\.I\\.(\\\
          d+)', r'\\1.1.\\2', line)\n        \n        # 6. ‰øÆÂ§çÂÖ∂‰ªñOCRÈîôËØØÔºöO -> 0, I ->\
          \ 1\n        line = re.sub(r'([A-Za-z0-9]+)\\.O\\.(\\d+)', r'\\1.0.\\2',\
          \ line)\n        line = re.sub(r'([A-Za-z0-9]+)\\.I\\.(\\d+)', r'\\1.1.\\\
          2', line)\n        \n        return line\n\n    lines = [normalize_chapter_spaces(line)\
          \ for line in lines]\n\n    return lines\n\ndef process_gb_terms_format(lines:\
          \ List[str]) -> List[str]:\n    \"\"\"\n    Â§ÑÁêÜÂõΩÊ†áÊúØËØ≠ÂÆö‰πâÊ†ºÂºèÔºö\n    Â∞Ü \"3.1\" (‰∏ã‰∏ÄË°å)\
          \ \"‰∏≠ÊñáÊúØËØ≠ Ëã±ÊñáÊúØËØ≠\" ÂêàÂπ∂‰∏∫ \"3.1 ‰∏≠ÊñáÊúØËØ≠ Ëã±ÊñáÊúØËØ≠\"\n    \"\"\"\n    result = []\n   \
          \ i = 0\n    \n    while i < len(lines):\n        current_line = lines[i].strip()\n\
          \        \n        # Ê£ÄÊµãÊòØÂê¶ÊòØÊúØËØ≠ÂÆö‰πâÁºñÂè∑ÔºöÁ∫ØÊï∞Â≠ó.Êï∞Â≠óÊ†ºÂºèÔºå‰∏î‰∏ã‰∏ÄË°åÂåÖÂê´‰∏≠Êñá+Ëã±ÊñáÔºåÊàñËÄÖÁ¨¨‰∫åË°åÊòØ‰∏≠ÊñáÔºåÁ¨¨‰∏âË°åÊòØËã±Êñá\n\
          \        if (i + 1 < len(lines) and \n            re.match(r'^\\d+\\.\\\
          d+$', current_line) and\n            current_line.startswith('3.')):  #\
          \ ÈÄöÂ∏∏ÊúØËØ≠Á´†ËäÇÊòØÁ¨¨3Á´†\n            \n            next_line = lines[i + 1].strip()\n\
          \            \n            # Ê£ÄÊü•‰∏ã‰∏ÄË°åÊòØÂê¶Á¨¶Âêà: ‰∏≠Êñá + Á©∫Ê†º + Ëã±Êñá ÁöÑÊ®°Âºè\n            if\
          \ re.search(r'[\\u4e00-\\u9fa5].*[A-Za-z]', next_line):\n              \
          \  # ÂêàÂπ∂ÊàêÊ†áÈ¢òÊ†ºÂºè\n                merged_line = f\"{current_line} {next_line}\"\
          \n                result.append(merged_line)\n                i += 2  #\
          \ Ë∑≥Ëøá‰∏ã‰∏ÄË°å\n                continue\n\n            # Ê£ÄÊü•Á¨¨‰∫åË°åÊòØÂê¶ÊòØ‰∏≠ÊñáÔºåÁ¨¨‰∏âË°åÊòØÂê¶ÊòØËã±Êñá\n\
          \            if (i + 2 < len(lines) and\n                re.search(r'[\\\
          u4e00-\\u9fa5]', lines[i + 1].strip()) and\n                re.search(r'[A-Za-z]',\
          \ lines[i + 2].strip())):\n                merged_line = f\"{current_line}\
          \ {lines[i + 1].strip()} {lines[i + 2].strip()}\"\n                result.append(merged_line)\n\
          \                i += 3  # Ë∑≥ËøáÂêé‰∏§Ë°å\n                continue\n\n        result.append(current_line)\n\
          \        i += 1\n    \n    return result\n\ndef extract_full_text_with_filter(pdf_path:\
          \ str, top_crop=0.08, bottom_crop=0.08):\n    doc = fitz.open(pdf_path)\n\
          \    all_lines = []\n\n    prev_line_text = None\n    prev_bbox = None\n\
          \n\n\n    for page in doc:\n\n        h = page.rect.height\n        clip_rect\
          \ = fitz.Rect(0, h * top_crop, page.rect.width, h * (1 - bottom_crop))\n\
          \        page_dict = page.get_text(\"dict\", clip=clip_rect)\n\n       \
          \ for block in page_dict[\"blocks\"]:\n            if block[\"type\"] !=\
          \ 0:  # Âè™Â§ÑÁêÜÊñáÊú¨\n                continue\n\n            for line in block[\"\
          lines\"]:\n                # 1. ÊåâxÂùêÊ†áÂêàÂπ∂Âêå‰∏ÄË°åÁöÑspan\n                spans =\
          \ sorted(line[\"spans\"], key=lambda s: s[\"bbox\"][0])\n              \
          \  merged = \"\"\n                last_x = None\n                for sp\
          \ in spans:\n                    x0, x1 = sp[\"bbox\"][0], sp[\"bbox\"][2]\n\
          \                    width = max(1.0, x1 - x0)\n                    avg_char_w\
          \ = width / max(len(sp[\"text\"]), 1)\n\n                    if last_x is\
          \ not None:\n                        gap = x0 - last_x\n               \
          \         if gap > max(avg_char_w * 0.5, 3.0):\n                       \
          \     merged += \" \"\n                    merged += sp[\"text\"]\n    \
          \                last_x = x1\n\n                merged = merged.strip()\n\
          \                curr_bbox = line[\"bbox\"]\n\n                # 2. Ë∑®Ë°åÊô∫ËÉΩÂêàÂπ∂Âà§ÂÆö\n\
          \                if prev_line_text is not None:\n                    if\
          \ should_merge_crossline(prev_line_text, merged, prev_bbox, curr_bbox):\n\
          \                        prev_line_text += \" \" + merged\n            \
          \            prev_bbox = (\n                            prev_bbox[0],\n\
          \                            prev_bbox[1],\n                           \
          \ max(prev_bbox[2], curr_bbox[2]),\n                            max(prev_bbox[3],\
          \ curr_bbox[3])\n                        )\n                        continue\n\
          \                    else:\n                        all_lines.append(prev_line_text)\n\
          \n                prev_line_text = merged\n                prev_bbox = curr_bbox\n\
          \n    # ÊúÄÂêé‰∏ÄË°å\n    if prev_line_text:\n        all_lines.append(prev_line_text)\n\
          \n    # ËøõË°åÂÖ®ËßíÂ≠óÁ¨¶ËΩ¨ÂçäËßíÂ≠óÁ¨¶\n    all_lines = [fullwidth_to_halfwidth(line.strip())\
          \ for line in all_lines]\n\n    # ËøõË°åÁ´†ËäÇÁºñÂè∑‰øÆÂ§ç\n    normalized = fix_broken_chapters(all_lines)\n\
          \    \n    # \U0001F195 ÂõΩÊ†áÊúØËØ≠ÂÆö‰πâÊ†ºÂºèÂ§ÑÁêÜ\n    normalized = process_gb_terms_format(normalized)\n\
          \n    # ÂÜôÂá∫Êñá‰ª∂‰∏éËøîÂõû\n    with open('extracted_full_text.txt', \"w\", encoding=\"\
          utf-8\") as f:\n        f.write(\"\\n\".join(normalized))\n\n    return\
          \ normalized\n\ndef detect_chapter_pattern(chapters: List[Dict]) -> str:\n\
          \    \"\"\"\n    Ê£ÄÊµãÊñáÊ°£ÁöÑÁ´†ËäÇÊ®°ÂºèÔºö\n    - 'alpha_first': Â≠óÊØçÁ´†ËäÇÂú®Ââç (A, A.1, A.2, B,\
          \ B.1, 1, 2, ...)\n    - 'numeric_first': Êï∞Â≠óÁ´†ËäÇÂú®Ââç (1, 2, ..., A, A.1, A.2,\
          \ B, B.1, ...)\n    \"\"\"\n    alpha_indices = []\n    numeric_indices\
          \ = []\n    \n    for i, ch in enumerate(chapters):\n        chapter_id\
          \ = ch[\"chapter_id\"].strip()\n        if re.match(r'^[A-Z](\\.\\d+)*\\\
          .?$', chapter_id):\n            alpha_indices.append(i)\n        elif re.match(r'^\\\
          d+(\\.\\d+)*\\.?$', chapter_id):\n            numeric_indices.append(i)\n\
          \    \n    if not alpha_indices or not numeric_indices:\n        return\
          \ 'numeric_first'  # ÈªòËÆ§Êï∞Â≠ó‰ºòÂÖà\n    \n    # ÊØîËæÉÁ¨¨‰∏Ä‰∏™Â≠óÊØçÁ´†ËäÇÂíåÁ¨¨‰∏Ä‰∏™Êï∞Â≠óÁ´†ËäÇÁöÑ‰ΩçÁΩÆ\n    first_alpha\
          \ = min(alpha_indices)\n    first_numeric = min(numeric_indices)\n    \n\
          \    if first_alpha < first_numeric:\n        return 'alpha_first'\n   \
          \ else:\n        return 'numeric_first'\n\ndef parse_chapter_id(chapter_id:\
          \ str, pattern: str = 'numeric_first') -> List[int]:\n    \"\"\"\n    Ê†πÊçÆÊñáÊ°£Ê®°ÂºèËß£ÊûêÁ´†ËäÇID\n\
          \    :param chapter_id: Á´†ËäÇIDÂ≠óÁ¨¶‰∏≤\n    :param pattern: ÊñáÊ°£Ê®°Âºè ('alpha_first'\
          \ Êàñ 'numeric_first')\n    \"\"\"\n    chapter_id = chapter_id.strip()\n\n\
          \    # Â≠óÊØçÁ´†ËäÇÊ†ºÂºè - ÊîØÊåÅÁÇπÂíåÊ®™Á∫øÂàÜÈöîÁ¨¶\n    if re.fullmatch(r'[A-Z](?:[.\\-]\\d+)*[.\\\
          -]?', chapter_id):\n        # Áªü‰∏ÄÂ§ÑÁêÜÁÇπÂíåÊ®™Á∫øÂàÜÈöîÁ¨¶\n        normalized = re.sub(r'[.\\\
          -]+', '.', chapter_id).rstrip('.')\n        parts = normalized.split('.')\n\
          \        letter = parts[0]\n        \n        if pattern == 'alpha_first':\n\
          \            # Â≠óÊØçÂú®ÂâçÊ®°ÂºèÔºöA=1, B=2, C=3, ...\n            letter_value = ord(letter)\
          \ - ord('A') + 1\n        else:\n            # Êï∞Â≠óÂú®ÂâçÊ®°ÂºèÔºöÂ≠óÊØçÁ´†ËäÇÊîæÂú®Êï∞Â≠óÁ´†ËäÇ‰πãÂêé\n   \
          \         # ÂÅáËÆæÊúÄÂ§öÊúâ100‰∏™Êï∞Â≠óÁ´†ËäÇÔºåÂ≠óÊØç‰ªé101ÂºÄÂßã\n            letter_value = ord(letter)\
          \ - ord('A') + 101\n        \n        try:\n            rest = [int(p) for\
          \ p in parts[1:]] if len(parts) > 1 else []\n            return [letter_value]\
          \ + rest\n        except ValueError:\n            return []\n\n    # Êï∞Â≠óÁ´†ËäÇÊ†ºÂºè\
          \ - ÊîØÊåÅÁÇπÂíåÊ®™Á∫øÂàÜÈöîÁ¨¶\n    elif re.fullmatch(r'\\d+(?:[.\\-]\\d+)*[.\\-]?', chapter_id):\n\
          \        try:\n            # Áªü‰∏ÄÂ§ÑÁêÜÁÇπÂíåÊ®™Á∫øÂàÜÈöîÁ¨¶\n            normalized = re.sub(r'[.\\\
          -]+', '.', chapter_id).rstrip('.')\n            parts = normalized.split('.')\n\
          \            numeric_parts = [int(p) for p in parts]\n            \n   \
          \         if pattern == 'alpha_first':\n                # Â≠óÊØçÂú®ÂâçÊ®°ÂºèÔºöÊï∞Â≠óÁ´†ËäÇÊîæÂú®Â≠óÊØçÁ´†ËäÇ‰πãÂêé\n\
          \                # ÂÅáËÆæÊúÄÂ§öÊúâ26‰∏™Â≠óÊØçÁ´†ËäÇÔºåÊï∞Â≠ó‰ªé27ÂºÄÂßã\n                numeric_parts[0]\
          \ += 26\n            # Êï∞Â≠óÂú®ÂâçÊ®°ÂºèÔºö‰øùÊåÅÂéüÊúâÊï∞Â≠ó\n            \n            return numeric_parts\n\
          \        except ValueError:\n            return []\n\n    return []\n\n\
          def is_chapter_a_before_b(a: list[int], b: list[int]) -> bool:\n    for\
          \ i in range(min(len(a), len(b))):\n        if a[i] < b[i]:\n          \
          \  return True\n        elif a[i] > b[i]:\n            return False\n  \
          \  return len(a) < len(b)\n\ndef is_reasonable_chapter_jump(prev_id: List[int],\
          \ curr_id: List[int]) -> bool:\n    \"\"\"\n    Âà§Êñ≠Á´†ËäÇË∑≥Ë∑ÉÊòØÂê¶ÂêàÁêÜÔºåÊõ¥ÂÆΩÊùæÁöÑÁ≠ñÁï•Ôºö\n   \
          \ ‰∏ªË¶ÅËøáÊª§ÊéâÊòéÊòæ‰∏çÂêàÁêÜÁöÑË∑≥Ë∑ÉÔºå‰ΩÜÂÖÅËÆ∏Ê≠£Â∏∏ÁöÑÁ´†ËäÇÁªìÊûÑ\n    ÂØπÂ≠óÊØçÈìæÂíåÊï∞Â≠óÈìæÈÉΩËøõË°åÂêàÁêÜÊÄßÂà§Êñ≠\n    \"\"\"\n    if not\
          \ prev_id or not curr_id:\n        return True  # Â¶ÇÊûúÊó†Ê≥ïËß£ÊûêÔºåÈªòËÆ§ÂÖÅËÆ∏\n    \n  \
          \  # Â¶ÇÊûúÊòØ‰∏çÂêåÂ±ÇÁ∫ßÔºå‰∏ÄËà¨ÈÉΩÊòØÂêàÁêÜÁöÑÔºàÂ¶Ç 1. -> 1.1 Êàñ 1.1 -> 2.Ôºâ\n    if len(prev_id) != len(curr_id):\n\
          \        return True\n    \n    # ÂêåÂ±ÇÁ∫ßÁöÑÊÉÖÂÜµ‰∏ãÔºåÊ£ÄÊü•Ë∑≥Ë∑ÉÂπÖÂ∫¶\n    if len(prev_id) ==\
          \ 1:  # ‰∏ÄÁ∫ßÁ´†ËäÇ\n        prev_num = prev_id[0]\n        curr_num = curr_id[0]\n\
          \        diff = curr_num - prev_num\n        \n        # Âà§Êñ≠ÊòØÂê¶‰∏∫Â≠óÊØçÁ´†ËäÇÔºàÁºñÁ†ÅËåÉÂõ¥101-126ÂØπÂ∫îA-ZÔºâ\n\
          \        if prev_num >= 101 and curr_num >= 101:  # Â≠óÊØçÁ´†ËäÇ\n            #\
          \ Â≠óÊØçË∑≥Ë∑ÉÊ£ÄÊü•Ôºö‰∏çÂÖÅËÆ∏Ë∑®Ë∂äË∂ÖËøá2‰∏™Â≠óÊØçÔºàÂ¶ÇBË∑≥Âà∞E‰ª•‰∏äÔºâ\n            return 1 <= diff <= 2\n     \
          \   else:  # Êï∞Â≠óÁ´†ËäÇ\n            return 1 <= diff <= 5  # ÂÖÅËÆ∏Ë∑≥Ë∑É1-5Á´†ÔºàËøáÊª§Êéâ‰ªé5Ë∑≥Âà∞100ËøôÁßçÊòéÊòæÈîôËØØÁöÑÔºâ\n\
          \    \n    elif len(prev_id) == 2:  # ‰∫åÁ∫ßÁ´†ËäÇ\n        # Â¶ÇÊûúÁ¨¨‰∏ÄÁ∫ßÁõ∏ÂêåÔºåÊ£ÄÊü•Á¨¨‰∫åÁ∫ßÁöÑË∑≥Ë∑É\n\
          \        if prev_id[0] == curr_id[0]:\n            prev_num = prev_id[1]\n\
          \            curr_num = curr_id[1]\n            diff = curr_num - prev_num\n\
          \            \n            # Âà§Êñ≠Á¨¨‰∏ÄÁ∫ßÊòØÂê¶‰∏∫Â≠óÊØçÁ´†ËäÇ\n            if prev_id[0] >=\
          \ 101:  # Â≠óÊØçÁ´†ËäÇÁöÑÂ≠êÁ∫ß\n                return 1 <= diff <= 5  # Â≠óÊØçÁ´†ËäÇÁöÑÂ≠êÁ∫ßË∑≥Ë∑ÉÁ®çÂæÆÂÆΩÊùæ‰∏Ä‰∫õ\n\
          \            else:  # Êï∞Â≠óÁ´†ËäÇÁöÑÂ≠êÁ∫ß\n                return 1 <= diff <= 10  #\
          \ ‰∫åÁ∫ßÁ´†ËäÇÂÖÅËÆ∏Êõ¥Â§ßË∑≥Ë∑É\n        else:\n            # ‰∏çÂêåÁöÑ‰∏ÄÁ∫ßÁ´†ËäÇÔºåÈÉΩÂêàÁêÜ\n            return\
          \ True\n    \n    else:  # ‰∏âÁ∫ßÂèä‰ª•‰∏äÁ´†ËäÇ\n        # ÂØπ‰∫éÊ∑±Â±ÇÊ¨°Á´†ËäÇÔºåÊõ¥ÂÆΩÊùæ‰∏Ä‰∫õ\n        return\
          \ True\n\ndef analyze_chapter_number_distribution(chapters: List[Dict])\
          \ -> Dict[str, int]:\n    \"\"\"\n    ÂàÜÊûêÁ´†ËäÇÁºñÂè∑ÁöÑÊï∞Â≠óÂàÜÂ∏ÉÔºåÁ°ÆÂÆöÂêàÁêÜÁöÑÊï∞Â≠óËåÉÂõ¥\n    ËøîÂõû: {\"\
          min_reasonable\": ÊúÄÂ∞èÂêàÁêÜÊï∞Â≠ó, \"max_reasonable\": ÊúÄÂ§ßÂêàÁêÜÊï∞Â≠ó, \"primary_range\"\
          : ‰∏ªË¶ÅÊï∞Â≠óËåÉÂõ¥}\n    \"\"\"\n    first_numbers = []\n    \n    for ch in chapters:\n\
          \        chapter_id = ch[\"chapter_id\"].strip()\n        # ÊèêÂèñÁ¨¨‰∏Ä‰∏™Êï∞Â≠ó\n  \
          \      m_num = re.match(r'^(\\d+)', chapter_id)\n        if m_num:\n   \
          \         first_numbers.append(int(m_num.group(1)))\n        # Â§ÑÁêÜAPPENDIXÂêéË∑üÊï∞Â≠óÁöÑÊÉÖÂÜµ\n\
          \        elif chapter_id.upper().startswith(\"APPENDIX\"):\n           \
          \ suffix = chapter_id[len(\"APPENDIX\"):].strip(\" ()\")\n            if\
          \ suffix.isdigit():\n                first_numbers.append(int(suffix))\n\
          \    \n    if not first_numbers:\n        return {\"min_reasonable\": 1,\
          \ \"max_reasonable\": 50, \"primary_range\": (1, 50)}\n    \n    first_numbers.sort()\n\
          \    \n    # ÂàÜÊûêÊï∞Â≠óÂàÜÂ∏ÉÊ®°Âºè\n    from collections import Counter\n    counter\
          \ = Counter(first_numbers)\n    \n    # Â¶ÇÊûúÂ§ßÂ§öÊï∞Á´†ËäÇÈÉΩÊòØÂêå‰∏Ä‰∏™Êï∞Â≠óÂºÄÂ§¥ÔºàÂ¶Ç60.1, 60.2, 60.3...ÔºâÔºåËøôÂèØËÉΩÊòØÊ≥ïËßÑÁºñÂè∑\n\
          \    most_common = counter.most_common(1)[0]\n    most_common_num, most_common_count\
          \ = most_common\n    \n    # Â¶ÇÊûúÊüê‰∏™Êï∞Â≠óÂá∫Áé∞Ê¨°Êï∞Ë∂ÖËøáÊÄªÊï∞ÁöÑ60%Ôºå‰∏îËøô‰∏™Êï∞Â≠óÂ§ß‰∫é30ÔºåÂèØËÉΩÊòØÊ≥ïËßÑÁºñÂè∑Ê®°Âºè\n  \
          \  if most_common_count > len(first_numbers) * 0.6 and most_common_num >\
          \ 30:\n        print(f\"Ê£ÄÊµãÂà∞ÂèØËÉΩÁöÑÊ≥ïËßÑÁºñÂè∑Ê®°Âºè: {most_common_num}.x (Âá∫Áé∞{most_common_count}Ê¨°)\"\
          )\n        # Âú®ËøôÁßçÊÉÖÂÜµ‰∏ãÔºåÂÖÅËÆ∏Ëøô‰∏™ÁâπÂÆöÁöÑÊ≥ïËßÑÁºñÂè∑\n        return {\n            \"min_reasonable\"\
          : most_common_num, \n            \"max_reasonable\": most_common_num, \n\
          \            \"primary_range\": (most_common_num, most_common_num),\n  \
          \          \"regulation_mode\": True,\n            \"regulation_number\"\
          : most_common_num\n        }\n    \n    # Ê≠£Â∏∏ÁöÑÁ´†ËäÇÁºñÂè∑Ê®°Âºè\n    min_num = min(first_numbers)\n\
          \    max_num = max(first_numbers)\n    \n    # Â¶ÇÊûúÊï∞Â≠óËåÉÂõ¥ÂæàÂ∞èÔºà<= 50ÔºâÔºåËÆ§‰∏∫ÊòØÊ≠£Â∏∏Á´†ËäÇ\n\
          \    if max_num <= 50:\n        return {\n            \"min_reasonable\"\
          : max(1, min_num), \n            \"max_reasonable\": min(50, max_num + 5),\
          \  # ÂÖÅËÆ∏Â∞ëÈáèË∂ÖÂá∫\n            \"primary_range\": (min_num, max_num)\n       \
          \ }\n    \n    # Â¶ÇÊûúÊï∞Â≠óËåÉÂõ¥ÂæàÂ§ßÔºåÂèØËÉΩÂåÖÂê´È°µÁ†ÅÁ≠âÂπ≤Êâ∞ÔºåÈááÁî®Êõ¥‰øùÂÆàÁ≠ñÁï•\n    # ÊâæÂà∞ÊúÄÂØÜÈõÜÁöÑÊï∞Â≠óÂå∫Èó¥\n    gaps\
          \ = []\n    for i in range(len(first_numbers) - 1):\n        gaps.append(first_numbers[i\
          \ + 1] - first_numbers[i])\n    \n    # Â¶ÇÊûúÊúâÊòéÊòæÁöÑÂ§ßË∑≥Ë∑ÉÔºà>20ÔºâÔºåÂèØËÉΩÂâçÈù¢ÊòØÊ≠£Â∏∏Á´†ËäÇÔºåÂêéÈù¢ÊòØÈ°µÁ†ÅÁ≠â\n\
          \    large_gap_idx = -1\n    for i, gap in enumerate(gaps):\n        if\
          \ gap > 20:\n            large_gap_idx = i\n            break\n    \n  \
          \  if large_gap_idx != -1:\n        # ÂèñË∑≥Ë∑ÉÂâçÁöÑÊï∞Â≠ó‰Ωú‰∏∫ÂêàÁêÜËåÉÂõ¥\n        reasonable_max\
          \ = first_numbers[large_gap_idx]\n        return {\n            \"min_reasonable\"\
          : max(1, min_num), \n            \"max_reasonable\": reasonable_max,\n \
          \           \"primary_range\": (min_num, reasonable_max)\n        }\n  \
          \  \n    # ÈªòËÆ§‰øùÂÆàÁ≠ñÁï•\n    return {\"min_reasonable\": 1, \"max_reasonable\"\
          : 50, \"primary_range\": (1, 50)}\n\ndef find_longest_chapter_chain_with_append(chapters:\
          \ List[Dict], language: str = 'en') -> Tuple[List[Dict], str]:\n    # ÂÖàÊ£ÄÊµãÁ´†ËäÇÊ®°Âºè\n\
          \    pattern = detect_chapter_pattern(chapters)\n    print(f\"Ê£ÄÊµãÂà∞Á´†ËäÇÊ®°Âºè: {pattern}\"\
          )\n    \n    # \U0001F195 ÂàÜÊûêÁ´†ËäÇÊï∞Â≠óÂàÜÂ∏É\n    number_analysis = analyze_chapter_number_distribution(chapters)\n\
          \    print(f\"Á´†ËäÇÊï∞Â≠óÂàÜÊûêÁªìÊûú: {number_analysis}\")\n    \n    # Áî®Ê£ÄÊµãÂà∞ÁöÑÊ®°ÂºèÈáçÊñ∞Ëß£ÊûêÁ´†ËäÇID\n\
          \    parsed_ids = [parse_chapter_id(ch[\"chapter_id\"], pattern) for ch\
          \ in chapters]\n    # print(f'Á¨¨‰∏Ä‰∏™Á´†ËäÇ: {chapters[0]}')\n    n = len(chapters)\n\
          \n    # Á¨¨‰∏ÄÊ≠•ÔºöËøáÊª§ÊéâÊòéÊòæ‰∏çÂêàÁêÜÁöÑÁ´†ËäÇÔºàÂ¶ÇËØØËØÜÂà´ÁöÑÊï∞Â≠óÔºâ\n    valid_indices = []\n    for i in range(n):\n\
          \        if not parsed_ids[i]:\n            continue\n            \n   \
          \     # Ê£ÄÊü•ÊòØÂê¶ÊòØÊòéÊòæÁöÑËØØËØÜÂà´\n        chapter_text = chapters[i][\"chapter_id\"]\
          \ + \" \" + chapters[i][\"chapter_title\"]\n        \n        # ËøáÊª§ÊòéÊòæÁöÑÊµãÈáèÂçï‰Ωç„ÄÅÈ¢ëÁéáËåÉÂõ¥„ÄÅÁ∫ØÊï∞Â≠óÁ≠â\n\
          \        if re.search(r'\\b\\d+\\s*(MHz|GHz|Hz|kHz|dB|V|mV|¬µV|A|mA|¬µA|W|mW|Œ©|%|¬∞C|¬∞F|mm|cm|m|km|kg|g|mg|ms|s|min|h|rpm|bar|Pa|kPa|MPa)\\\
          b', chapter_text, re.I):\n            continue\n        if re.search(r'\\\
          d+\\s*MHz\\s*[~-]\\s*\\d+\\s*MHz', chapter_text, re.I):\n            continue\n\
          \        if re.match(r'^\\d+\\s*$', chapters[i][\"chapter_title\"].strip()):\
          \  # Ê†áÈ¢òÊòØÁ∫ØÊï∞Â≠ó\n            continue\n        if len(chapters[i][\"chapter_title\"\
          ].strip()) < 2:  # Ê†áÈ¢òÂ§™Áü≠\n            continue\n            \n        # \U0001F195\
          \ Ë°®Ê†ºÊï∞ÊçÆÁâπÂæÅËøáÊª§\n        chapter_title = chapters[i][\"chapter_title\"].strip()\n\
          \        chapter_id = chapters[i][\"chapter_id\"].strip()\n        \n  \
          \      # Ê£ÄÊµãË°®Ê†ºË°åÊ®°ÂºèÔºöÂçï‰∏™Â≠óÊØç + ‰∏ªË¶ÅÊòØÊï∞Â≠óÁöÑÊ†áÈ¢ò\n        if (len(chapter_id) == 1 and chapter_id.isupper()\
          \ and \n            re.search(r'^\\d+.*\\d+', chapter_title) and \n    \
          \        len([x for x in chapter_title.split() if x.isdigit()]) >= 2):\n\
          \            continue\n            \n        # Ê£ÄÊµãÂùêÊ†áÁÇπÊ†ºÂºèÔºöÂ¶Ç \"10 0 E 0 16\"\
          \n        if re.match(r'^\\d+\\s+\\d+\\s+[A-Z]\\s+\\d+\\s+\\d+', chapter_title):\n\
          \            continue\n            \n        # Ê£ÄÊµãÂèÇÊï∞Ë°®Ê†ºÊ†ºÂºèÔºöÂ¶Ç \"34 65 F 25 77\"\
          \n        title_parts = chapter_title.split()\n        if (len(title_parts)\
          \ >= 4 and \n            sum(1 for part in title_parts if part.isdigit())\
          \ >= 3 and\n            sum(1 for part in title_parts if len(part) == 1\
          \ and part.isupper()) >= 1):\n            continue\n            \n     \
          \   # Ê£ÄÊµãÂõæË°®Ê†áÊ≥®ËØ¥ÊòéÔºöÂçï‰∏™Â≠óÊØç + ‰ª•Á†¥ÊäòÂè∑ÂºÄÂ§¥ÁöÑÊ†áÈ¢ò\n        if (len(chapter_id) == 1 and chapter_id.isupper()\
          \ and \n            chapter_title.startswith('‚Äî‚Äî‚Äî')):\n            continue\n\
          \            \n        valid_indices.append(i)\n    \n    # Á¨¨‰∫åÊ≠•ÔºöÈ™åËØÅÂ≠óÊØçÁ´†ËäÇÁöÑÂêàÁêÜÊÄßÔºàÈíàÂØπ‰∏≠Ëã±ÊñáÂ∑ÆÂºÇÂåñÂ§ÑÁêÜÔºâ\n\
          \    if valid_indices:\n        # Ê£ÄÊü•ÊòØÂê¶ÂåÖÂê´Â≠óÊØçÁ´†ËäÇ\n        alpha_chapters = []\n\
          \        for i, idx in enumerate(valid_indices):\n            chapter_id\
          \ = chapters[idx][\"chapter_id\"].strip()\n            if re.match(r'^[A-Z](?:\\\
          .\\d+)*\\.?$', chapter_id):\n                alpha_chapters.append((i, idx,\
          \ chapter_id[0]))  # (Âú®valid_indices‰∏≠ÁöÑ‰ΩçÁΩÆ, ÂéüÂßãÁ¥¢Âºï, È¶ñÂ≠óÊØç)\n        \n       \
          \ # Â¶ÇÊûúÊúâÂ≠óÊØçÁ´†ËäÇÔºåËøõË°åÂêàÁêÜÊÄßÈ™åËØÅ\n        if alpha_chapters:\n            if language\
          \ == 'en':\n                # Ëã±ÊñáÊñáÊ°£ÔºöË¶ÅÊ±ÇÂ≠óÊØçÁ´†ËäÇÂøÖÈ°ª‰ªéAÂºÄÂ§¥\n                first_alpha_letter\
          \ = alpha_chapters[0][2]\n                if first_alpha_letter != 'A':\n\
          \                    print(f\"Ëã±ÊñáÊñáÊ°£Â≠óÊØçÁ´†ËäÇ‰∏ç‰ª•AÂºÄÂ§¥ÔºåË∑≥Ëøá: Á¨¨‰∏Ä‰∏™Â≠óÊØçÁ´†ËäÇÊòØ {alpha_chapters[0][2]}\"\
          )\n                    # ÁßªÈô§ÊâÄÊúâÂ≠óÊØçÁ´†ËäÇ\n                    alpha_indices_set\
          \ = {item[1] for item in alpha_chapters}\n                    valid_indices\
          \ = [idx for idx in valid_indices if idx not in alpha_indices_set]\n   \
          \         else:\n                # ‰∏≠ÊñáÊñáÊ°£ÔºöÊ£ÄÊü•Â≠óÊØçÁ´†ËäÇÁöÑ‰∏ÄËá¥ÊÄßÔºàÂêå‰∏ÄÈôÑÂΩïÂ∫îËØ•‰ª•Âêå‰∏ÄÂ≠óÊØçÂºÄÂ§¥Ôºâ\n    \
          \            from collections import Counter\n                alpha_letters\
          \ = [item[2] for item in alpha_chapters]\n                letter_counter\
          \ = Counter(alpha_letters)\n                most_common_letter, most_common_count\
          \ = letter_counter.most_common(1)[0]\n                \n               \
          \ # Â¶ÇÊûúÊüê‰∏™Â≠óÊØçÂá∫Áé∞Ê¨°Êï∞Ë∂ÖËøá60%ÔºåËÆ§‰∏∫ËøôÊòØ‰∏ªË¶ÅÁöÑÈôÑÂΩïÂ≠óÊØç\n                if most_common_count >\
          \ len(alpha_chapters) * 0.6:\n                    print(f\"‰∏≠ÊñáÊñáÊ°£Ê£ÄÊµãÂà∞‰∏ªË¶ÅÈôÑÂΩïÂ≠óÊØç:\
          \ {most_common_letter} (Âá∫Áé∞{most_common_count}Ê¨°)\")\n                   \
          \ # ‰øùÁïô‰∏é‰∏ªË¶ÅÂ≠óÊØç‰∏ÄËá¥ÁöÑÁ´†ËäÇÔºåÁßªÈô§ÂÖ∂‰ªñÂ≠óÊØçÁ´†ËäÇ\n                    keep_alpha_indices = {item[1]\
          \ for item in alpha_chapters if item[2] == most_common_letter}\n       \
          \             remove_alpha_indices = {item[1] for item in alpha_chapters\
          \ if item[2] != most_common_letter}\n                    valid_indices =\
          \ [idx for idx in valid_indices if idx not in remove_alpha_indices]\n  \
          \                  if remove_alpha_indices:\n                        removed_letters\
          \ = {chapters[idx][\"chapter_id\"].strip()[0] for idx in remove_alpha_indices}\n\
          \                        print(f\"ÁßªÈô§‰∏ç‰∏ÄËá¥ÁöÑÂ≠óÊØçÁ´†ËäÇ: {removed_letters}\")\n   \
          \             else:\n                    # Â¶ÇÊûúÊ≤°ÊúâÊòéÊòæÁöÑ‰∏ªË¶ÅÂ≠óÊØçÔºå‰øùÊåÅÂéüÊúâÈÄªËæëÔºàÂèØËÉΩÊòØÊ∑∑ÂêàÊÉÖÂÜµÔºâ\n\
          \                    print(f\"‰∏≠ÊñáÊñáÊ°£Â≠óÊØçÁ´†ËäÇÂàÜÂ∏ÉËæÉÂùáÂåÄ: {dict(letter_counter)}\")\n\
          \                    # ‰∏çÂÅöÁâπÊÆäÂ§ÑÁêÜÔºå‰øùÁïôÊâÄÊúâÂ≠óÊØçÁ´†ËäÇ\n\n    # Á¨¨‰∏âÊ≠•Ôºö‰ªéÂêéÂæÄÂâçÊûÑÂª∫ÊúÄÈïøÈìæ\n    dp =\
          \ [1] * len(valid_indices)\n    next_link = [-1] * len(valid_indices)  #\
          \ Êîπ‰∏∫ËÆ∞ÂΩï‰∏ã‰∏Ä‰∏™ËäÇÁÇπ\n    max_len = 0\n    max_idx = -1\n\n    # ‰ªéÂêéÂæÄÂâçÈÅçÂéÜ\n    for\
          \ i in range(len(valid_indices) - 1, -1, -1):\n        curr_idx = valid_indices[i]\n\
          \        curr_parsed = parsed_ids[curr_idx]\n        \n        # ÊâæÂú®ÂΩìÂâçËäÇÁÇπ‰πãÂêéÁöÑÊâÄÊúâËäÇÁÇπ\n\
          \        for j in range(i + 1, len(valid_indices)):\n            next_idx\
          \ = valid_indices[j]\n            next_parsed = parsed_ids[next_idx]\n \
          \           \n            # Ê£ÄÊü•ÂΩìÂâçËäÇÁÇπÊòØÂê¶ÂèØ‰ª•ËøûÂà∞‰∏ã‰∏Ä‰∏™ËäÇÁÇπ\n            if (is_chapter_a_before_b(curr_parsed,\
          \ next_parsed) and \n                is_reasonable_chapter_jump(curr_parsed,\
          \ next_parsed)):\n                if dp[j] + 1 > dp[i]:\n              \
          \      dp[i] = dp[j] + 1\n                    next_link[i] = j\n       \
          \ \n        if dp[i] > max_len:\n            max_len = dp[i]\n         \
          \   max_idx = i\n\n    # Á¨¨ÂõõÊ≠•ÔºöÂ¶ÇÊûúÊ≤°ÊúâÊâæÂà∞ÂêàÁêÜÁöÑÈìæÔºåÈÄÄÂõûÂà∞ÁÆÄÂçïÁöÑÈ°∫Â∫èËøáÊª§\n    if max_len < 2:\n\
          \        # ÁÆÄÂçïÊåâÁ´†ËäÇÁºñÂè∑È°∫Â∫èËøáÊª§\n        filtered_chapters = simple_chapter_filter(chapters)\n\
          \        \n        # Â¶ÇÊûúËøáÊª§ÂêéËøòÊòØÊ≤°ÊúâÁ´†ËäÇÔºåÂ∞ÜÊâÄÊúâÂÜÖÂÆπÊîæÂÖ•Ë∑≥ËøáÁöÑÂÜÖÂÆπ‰∏≠\n        if not filtered_chapters:\n\
          \            all_content = []\n            for ch in chapters:\n       \
          \         content = f\"{ch['chapter_id']} {ch['chapter_title']}\"\n    \
          \            if ch.get('raw_text'):\n                    content += \" \"\
          \ + ch['raw_text']\n                all_content.append(content)\n      \
          \      skipped_text = \"\\n\".join(all_content)\n            return [],\
          \ skipped_text\n        \n        return filtered_chapters, \"\"\n\n   \
          \ # ÂõûÊ∫ØÂá∫‰∏ªÈìæÁ¥¢ÂºïÔºà‰ªéÂâçÂæÄÂêéÁöÑÊ≠£Á°ÆÈ°∫Â∫èÔºâ\n    chain_indices = []\n    idx = max_idx\n    while\
          \ idx != -1:\n        chain_indices.append(valid_indices[idx])\n       \
          \ idx = next_link[idx]\n    \n    print(f\"‰ªéÂêéÂæÄÂâçÁîüÊàêÁöÑÊúÄÈïøÈìæ: ÈïøÂ∫¶={len(chain_indices)},\
          \ ‰ΩçÁΩÆ={chain_indices[:5]}{'...' if len(chain_indices)>5 else ''}\")\n   \
          \ \n    # ÊúÄÈïøÈìæÁöÑÁ¨¨‰∏Ä‰∏™Á´†ËäÇÁ¥¢Âºï\n    first_chain_idx = chain_indices[0]\n    \n  \
          \  # ÁîüÊàêË∑≥ËøáÁöÑÂÜÖÂÆπÔºàÊúÄÈïøÈìæÁ¨¨‰∏Ä‰∏™Á´†ËäÇ‰πãÂâçÁöÑÊâÄÊúâÂÜÖÂÆπÔºâ\n    skipped_chapters = chapters[:first_chain_idx]\n\
          \    skipped_text = \"\\n\".join([f\"{ch['chapter_id']} {ch['chapter_title']}\
          \ {ch.get('raw_text','')}\" for ch in skipped_chapters])\n    \n    chain_set\
          \ = set(chain_indices)\n\n    # ÊúÄÁªàÁªìÊûúÊûÑÂª∫\n    result = []\n    last_valid\
          \ = None\n    for i, chap in enumerate(chapters):\n        if i in chain_set:\n\
          \            result.append(chap)\n            last_valid = chap\n      \
          \  elif i >= first_chain_idx:  # Âè™Â§ÑÁêÜÊúÄÈïøÈìæÂºÄÂßã‰πãÂêéÁöÑÁ´†ËäÇ\n            if last_valid:\n\
          \                content_to_add = \"\\n\" + chap[\"chapter_id\"] + chap[\"\
          chapter_title\"]\n                if chap.get(\"raw_text\"):\n         \
          \           content_to_add += \" \" + chap[\"raw_text\"]\n             \
          \   last_valid[\"raw_text\"] += content_to_add\n\n    # Âà§Êñ≠Á´†ËäÇÊ†áÈ¢òÊòØÂê¶Â∫îËØ•ÂêàÂπ∂Âà∞Ê≠£Êñá‰∏≠\n\
          \    for chap in result:\n        should_merge = False\n        \n     \
          \   # ‰∏≠ÊñáÂ§ÑÁêÜÔºöÂåÖÂê´‰∏≠Êñá‰∏îÊúâ‰∏≠ÊñáÈÄóÂè∑ÔºåÂè•Âè∑ÔºåÂÜíÂè∑ÔºåÊàñËÄÖÈïøÂ∫¶Â§ß‰∫é30Â≠óÁ¨¶\n        if re.search(r'[\\u4e00-\\\
          u9fa5]', chap[\"chapter_title\"]):\n            # Ëé∑ÂèñÁ´†ËäÇÁºñÂè∑ÁöÑÁ¨¨‰∏Ä‰∏™Êï∞Â≠óÔºåÂâç‰∏âÁ´†Ë∑≥ËøáÂêàÂπ∂Âà§Êñ≠\n\
          \            first_num = None\n            chapter_id = chap[\"chapter_id\"\
          ].strip('.-')\n            if re.match(r'^\\d+', chapter_id):\n        \
          \        first_num = int(re.match(r'^\\d+', chapter_id).group())\n     \
          \       # Ââç‰∏âÁ´†Ë∑≥ËøáÂêàÂπ∂Âà§Êñ≠\n            if first_num is not None and first_num\
          \ <= 3:\n                continue          \n            if re.search(r'[Ôºå„ÄÇÔºö,:]',\
          \ chap[\"chapter_title\"]) or len(chap[\"chapter_title\"]) > 30:\n     \
          \           should_merge = True\n        \n        # Ëã±ÊñáÂ§ÑÁêÜÔºöÊõ¥Êô∫ËÉΩÁöÑÂà§Êñ≠ÈÄªËæë\n   \
          \     else:\n            # Â¶ÇÊûúÂÖ®Â§ßÂÜôÔºåÂàôËÇØÂÆöÊòØÊ†áÈ¢ò\n            if chap[\"chapter_title\"\
          ].isupper():\n                continue\n            # 1. Â¶ÇÊûúraw_text‰ª•Â∞èÂÜôÂ≠óÊØçÂºÄÂ§¥ÔºåÂèØËÉΩÊòØÊ†áÈ¢òÁöÑÂª∂Áª≠\n\
          \            if len(chap[\"raw_text\"]) and chap[\"raw_text\"][0].islower():\n\
          \                should_merge = True\n            # 2. Â¶ÇÊûúchapter_titleÂåÖÂê´ÂÆåÊï¥Âè•Â≠êÁöÑÁâπÂæÅ\n\
          \            elif re.search(r'[,;!?]', chap[\"chapter_title\"]):\n     \
          \           should_merge = True\n            # 3. Â¶ÇÊûúchapter_titleÂæàÈïøÔºàË∂ÖËøá50‰∏™Â≠óÁ¨¶ÔºâÔºåÂèØËÉΩÊòØÊÆµËêΩÊñáÊú¨\n\
          \            elif len(chap[\"chapter_title\"]) > 50:\n                should_merge\
          \ = True\n        \n        if should_merge:\n            chap[\"raw_text\"\
          ] = chap[\"chapter_title\"] + ' ' + chap[\"raw_text\"]\n            chap[\"\
          chapter_title\"] = \"\"\n\n    return result, skipped_text\n\ndef simple_chapter_filter(chapters:\
          \ List[Dict]) -> List[Dict]:\n    \"\"\"\n    ÁÆÄÂçïÁöÑÁ´†ËäÇËøáÊª§Á≠ñÁï•ÔºöÂΩìÊúÄÈïøÈìæÁÆóÊ≥ïÂ§±ÊïàÊó∂ÁöÑÂ§áÁî®ÊñπÊ°à\n\
          \    \"\"\"\n    # Ê£ÄÊµãÁ´†ËäÇÊ®°Âºè\n    pattern = detect_chapter_pattern(chapters)\n\
          \    \n    result = []\n    parsed_ids = [parse_chapter_id(ch[\"chapter_id\"\
          ], pattern) for ch in chapters]\n    \n    for i, chap in enumerate(chapters):\n\
          \        parsed_id = parsed_ids[i]\n        \n        # Âü∫Êú¨ÂêàÁêÜÊÄßÊ£ÄÊü•\n      \
          \  if not parsed_id:\n            # Êó†Ê≥ïËß£ÊûêÁöÑÁ´†ËäÇÔºåËøΩÂä†Âà∞‰∏ä‰∏Ä‰∏™ÊúâÊïàÁ´†ËäÇ\n            if result:\n\
          \                content_to_add = \"\\n\" + chap[\"chapter_id\"] + chap[\"\
          chapter_title\"]\n                if chap.get(\"raw_text\"):\n         \
          \           content_to_add += \" \" + chap[\"raw_text\"]\n             \
          \   result[-1][\"raw_text\"] += content_to_add\n            continue\n \
          \       \n        # Ê£ÄÊü•Á´†ËäÇÁºñÂè∑ÊòØÂê¶Âú®ÂêàÁêÜËåÉÂõ¥ÂÜÖ\n        first_num = parsed_id[0]\n \
          \       \n        # Ê†πÊçÆÊ®°ÂºèË∞ÉÊï¥ÂêàÁêÜÊÄßÊ£ÄÊü•\n        if pattern == 'alpha_first':\n\
          \            # Â≠óÊØçÂú®ÂâçÔºöA=1, B=2, ..., 1=27, 2=28, ...\n            if 1 <=\
          \ first_num <= 50:  # ÂêàÁêÜËåÉÂõ¥Ôºö26‰∏™Â≠óÊØç + 20‰∏™Êï∞Â≠óÁ´†ËäÇ\n                result.append(chap)\n\
          \            else:\n                # ‰∏çÂêàÁêÜÁöÑÁ´†ËäÇÔºåËøΩÂä†Âà∞‰∏ä‰∏Ä‰∏™ÊúâÊïàÁ´†ËäÇ\n              \
          \  if result:\n                    content_to_add = \"\\n\" + chap[\"chapter_id\"\
          ] + chap[\"chapter_title\"]\n                    if chap.get(\"raw_text\"\
          ):\n                        content_to_add += \" \" + chap[\"raw_text\"\
          ]\n                    result[-1][\"raw_text\"] += content_to_add\n    \
          \    else:\n            # Êï∞Â≠óÂú®ÂâçÔºö1, 2, ..., A=101, B=102, ...\n          \
          \  if (1 <= first_num <= 20) or (101 <= first_num <= 126):  # Êï∞Â≠óÁ´†ËäÇÊàñÂ≠óÊØçÁ´†ËäÇ\n\
          \                result.append(chap)\n            else:\n              \
          \  # ‰∏çÂêàÁêÜÁöÑÁ´†ËäÇÔºåËøΩÂä†Âà∞‰∏ä‰∏Ä‰∏™ÊúâÊïàÁ´†ËäÇ\n                if result:\n                   \
          \ content_to_add = \"\\n\" + chap[\"chapter_id\"] + chap[\"chapter_title\"\
          ]\n                    if chap.get(\"raw_text\"):\n                    \
          \    content_to_add += \" \" + chap[\"raw_text\"]\n                    result[-1][\"\
          raw_text\"] += content_to_add\n    \n    return result\n    \n    return\
          \ result\n\ndef split_sections_by_attachment(chapters: List[Dict]) -> List[Dict]:\n\
          \    \"\"\"\n    Â∞ÜÊï¥‰∏™ÊñáÊ°£ÊåâÈôÑ‰ª∂ÔºàANNEXÔºâÂàáÂàÜ„ÄÇ\n    È°∂Â±Ç file: regulation / ANNEX n\n\
          \    ÊîπËøõÔºöÂêàÂπ∂ËøûÁª≠ÁöÑÁõ∏ÂêåÈôÑ‰ª∂Ê†áÈ¢ò\n    \"\"\"\n    sections = []\n    current_section\
          \ = {\n        \"section\": \"regulation\",  # ÈªòËÆ§‰∏ªÊñáÊ°£\n        \"chapters\"\
          : []\n    }\n\n    annex_pattern = re.compile(r'^(ANNEX|ATTACHMENT)\\s+([A-Z0-9]+)',\
          \ re.I)\n\n    for chap in chapters:\n        match = annex_pattern.match(chap['chapter_id'])\n\
          \        if match:\n            annex_name = match.group(1).upper() + \"\
          \ \" + match.group(2)  # Ê†áÂáÜÂåñÂêçÁß∞ÔºåÂ¶Ç \"ANNEX 1\"\n            \n           \
          \ # Ê£ÄÊü•ÊòØÂê¶‰∏éÂΩìÂâç section ÁöÑÂêçÁß∞Áõ∏Âêå\n            if current_section[\"section\"] !=\
          \ \"regulation\" and current_section[\"section\"].upper() == annex_name:\n\
          \                # Áõ∏ÂêåÁöÑÈôÑ‰ª∂ÔºåÁõ¥Êé•Ê∑ªÂä†Âà∞ÂΩìÂâç sectionÔºåË∑≥ËøáÈáçÂ§çÁöÑÊ†áÈ¢òÁ´†ËäÇ\n                if chap.get('chapter_title')\
          \ or chap.get('raw_text', '').strip():\n                    current_section[\"\
          chapters\"].append(chap)\n                # Â¶ÇÊûúÊòØÁ©∫ÁöÑÈáçÂ§çÊ†áÈ¢òÁ´†ËäÇÔºàÂè™Êúâchapter_idÊ≤°ÊúâÂÜÖÂÆπÔºâÔºåÂàôË∑≥Ëøá\n\
          \            else:\n                # ‰∏çÂêåÁöÑÈôÑ‰ª∂Ôºå‰øùÂ≠òÂΩìÂâçÂùóÂπ∂Êñ∞Âª∫\n                if\
          \ current_section[\"chapters\"]:\n                    sections.append(current_section)\n\
          \                # Êñ∞Âª∫ÈôÑ‰ª∂Âùó\n                current_section = {\n        \
          \            \"section\": annex_name,\n                    \"chapters\"\
          : [chap] if (chap.get('chapter_title') or chap.get('raw_text', '').strip())\
          \ else []\n                }\n        else:\n            current_section[\"\
          chapters\"].append(chap)\n\n    if current_section[\"chapters\"]:\n    \
          \    sections.append(current_section)\n\n    return sections\n\ndef split_sections_by_appendix(chapters):\n\
          \    sections = []\n    current_section = {\"section\": \"MAIN\", \"chapters\"\
          : []}\n\n    for ch in chapters:\n        # Ê£ÄÊµã APPENDIX ÂºÄÂ§¥ÁöÑÈ°∂Â±ÇÊ†áÈ¢òÔºåÊàñËÄÖÈôÑÂΩï\n \
          \       appendix_match = re.match(r'^(APPENDIX\\s+(?:[A-Z0-9]+|\\([A-Z0-9]+\\\
          )))$', ch['chapter_id'], re.IGNORECASE)\n        annex_match = ch[\"chapter_id\"\
          ].startswith(\"ÈôÑÂΩï\")\n        \n        if appendix_match or annex_match:\n\
          \            # Ê†áÂáÜÂåñÈôÑÂΩïÂêçÁß∞\n            if appendix_match:\n               \
          \ appendix_name = appendix_match.group(1).upper()\n            else:\n \
          \               appendix_name = ch['chapter_id'].strip()\n            \n\
          \            # Ê£ÄÊü•ÊòØÂê¶‰∏éÂΩìÂâç section ÁöÑÂêçÁß∞Áõ∏Âêå\n            if current_section[\"\
          section\"] != \"MAIN\" and current_section[\"section\"].upper() == appendix_name:\n\
          \                # Áõ∏ÂêåÁöÑÈôÑÂΩïÔºåÁõ¥Êé•Ê∑ªÂä†Âà∞ÂΩìÂâç sectionÔºàÂ¶ÇÊûúÊúâÂÆûÈôÖÂÜÖÂÆπÔºâ\n                if ch.get('chapter_title')\
          \ or ch.get('raw_text', '').strip():\n                    current_section[\"\
          chapters\"].append(ch)\n                # Â¶ÇÊûúÊòØÁ©∫ÁöÑÈáçÂ§çÊ†áÈ¢òÁ´†ËäÇÔºåÂàôË∑≥Ëøá\n            else:\n\
          \                # ‰∏çÂêåÁöÑÈôÑÂΩïÔºåÂÖà‰øùÂ≠òÂΩìÂâçÂùó\n                if current_section[\"chapters\"\
          ]:\n                    sections.append(current_section)\n             \
          \   # Êñ∞Âª∫ÈôÑÂΩïÂùó\n                current_section = {\n                    \"\
          section\": appendix_name,\n                    \"chapters\": [ch] if (ch.get('chapter_title')\
          \ or ch.get('raw_text', '').strip()) else []\n                }\n      \
          \  else:\n            current_section[\"chapters\"].append(ch)\n\n    #\
          \ Êú´Â∞æÂùóÂä†ÂÖ•\n    if current_section[\"chapters\"]:\n        sections.append(current_section)\n\
          \n    # # ÊâìÂç∞ÊèêÂèñÁöÑÊâÄÊúâÁ´†ËäÇÊ†áÈ¢ò\n    # for sec in sections:\n    #     print(f\"Section:\
          \ {sec['section']}\")\n    #     for chap in sec[\"chapters\"]:\n    # \
          \        print(f\"  Chapter ID: {chap['chapter_id']}, Title: {chap['chapter_title']}\"\
          )\n\n    return sections\n\ndef process_sections_with_lis(chapters, language='en'):\n\
          \    # ÂÖàÊãÜÂàÜÊàêÊ≠£ÊñáÂíåÂ§ö‰∏™ÈôÑÂΩï\n    sections = split_sections_by_appendix(chapters)\n\
          \n    # ÊØè‰∏™ÈÉ®ÂàÜÂÜÖÈÉ®ÂçïÁã¨Ë∑ëÊúÄÈïøÈìæ\n    processed_sections = []\n    for sec in sections:\n\
          \        valid_chaps, skipped_content = find_longest_chapter_chain_with_append(sec[\"\
          chapters\"], language)\n        processed_sections.append({\n          \
          \  \"section\": sec[\"section\"],\n            \"context\": skipped_content,\
          \  # Ê∑ªÂä†Ë¢´Ë∑≥ËøáÁöÑÂÜÖÂÆπ\n            \"chapters\": valid_chaps\n        })\n\n   \
          \ return processed_sections\n\ndef filter_start_of_main(chapters: List[Dict])\
          \ -> Tuple[List[Dict], str]:\n    \"\"\"\n    ÊâæÂà∞Á¨¨‰∏Ä‰∏™Ê≠£ÊñáÁ´†ËäÇ‰Ωú‰∏∫Ëµ∑ÁÇπÔºåË∑≥ËøáÁõÆÂΩï\n    \"\
          \"\"\n    start_index = 0\n    for i, chap in enumerate(chapters):\n   \
          \     chapter_id = chap.get(\"chapter_id\", \"\").strip()\n        # Ê≠£Êñá‰∏ªÈìæÊàñÈôÑ‰ª∂ÂÜÖÈÉ®Á´†ËäÇÔºöÊï∞Â≠óÂºÄÂ§¥ÊàñÂ≠óÊØçÂºÄÂ§¥\n\
          \        if chapter_id in {\"1\", \"1-\", \"1.\", \"A\", \"A.\", \"A.1\"\
          }:\n            # SCOPE / GENERAL / INTRO Á≠âÈÉΩÁÆóÊ≠£ÊñáËµ∑ÁÇπ\n            title_upper\
          \ = chap.get(\"chapter_title\", \"\").upper()\n            if any(k in title_upper\
          \ for k in [\"SCOPE\", \"GENERAL\", \"INTRO\", \"ÊÄªÂàô\", \"ËåÉÂõ¥\", \"LEGISLATIVE\"\
          , \"FUNCTION\"]):\n                start_index = i\n                break\n\
          \                \n        # ‰πüÊ£ÄÊü•Ê†áÂáÜÁöÑÁ´†ËäÇÂºÄÂ§¥Ê®°Âºè\n        if re.match(r'^[A-Z](\\\
          .\\d+)*\\.?$', chapter_id) or re.match(r'^\\d+(\\.\\d+)*\\.?$', chapter_id):\n\
          \            title_upper = chap.get(\"chapter_title\", \"\").upper()\n \
          \           if any(k in title_upper for k in [\"SCOPE\", \"GENERAL\", \"\
          INTRO\", \"ÊÄªÂàô\", \"ËåÉÂõ¥\", \"LEGISLATIVE\", \"FUNCTION\"]):\n            \
          \    start_index = i\n                break\n                \n    # print(f'chapters[str]:\
          \ {chapters[start_index]}')\n    filtered_chapters = chapters[start_index:]\n\
          \    skipped_content = chapters[:start_index]\n    skipped_text = \"\\n\"\
          .join([f\"{ch['chapter_id']} {ch['chapter_title']} {ch.get('raw_text','')}\"\
          \ for ch in skipped_content])\n\n    return filtered_chapters, skipped_text\n\
          \n\ndef smart_paragraph_join(lines: List[str]) -> str:\n    \"\"\"\n   \
          \ Êô∫ËÉΩÊÆµËêΩÂêàÂπ∂ÔºöÂè™Âú®ÊÆµËêΩÁªìÊùüÊó∂Êç¢Ë°å\n    \"\"\"\n    if not lines:\n        return \"\"\n\
          \    \n    result = []\n    current_paragraph = []\n    \n    for i, line\
          \ in enumerate(lines):\n        line = line.strip()\n        if not line:\
          \  # Á©∫Ë°åÁõ¥Êé•Ë∑≥Ëøá\n            continue\n            \n        # Ê£ÄÊü•ÊòØÂê¶ÊòØÊÆµËêΩÁªìÊùüÁöÑÊ†áÂøó\n\
          \        is_paragraph_end = False\n        \n        # 1. ‰ª•Ê†áÁÇπÁ¨¶Âè∑ÁªìÂ∞æÔºà‰∏≠Ëã±ÊñáÔºâ\n\
          \        if re.search(r'[„ÄÇÔºÅÔºüÔºõÔºö.!?;:]$', line):\n            is_paragraph_end\
          \ = True\n            \n        # 2. Ê£ÄÊü•‰∏ã‰∏ÄË°åÊòØÂê¶ÊòØÊñ∞ÊÆµËêΩÁöÑÂºÄÂßã\n        if i + 1 <\
          \ len(lines):\n            next_line = lines[i + 1].strip()\n          \
          \  # ‰∏ã‰∏ÄË°åÊòØÁ´†ËäÇÊ†áÈ¢ò„ÄÅÂàóË°®È°π„ÄÅÊàñÊòéÊòæÁöÑÊÆµËêΩÂºÄÂßã\n            if (detect_chapter(next_line) or\n\
          \                re.match(r'^[‰∏Ä‰∫å‰∏âÂõõ‰∫îÂÖ≠‰∏ÉÂÖ´‰πùÂçÅ\\d]+[„ÄÅ\\.\\)]', next_line) or \
          \ # ÂàóË°®È°π\n                re.match(r'^[Ôºà(]\\d+[Ôºâ)]', next_line) or  # ÁºñÂè∑È°π\n\
          \                re.match(r'^[‚Äî‚Äî\\-‚Äî]+', next_line)):  # Á†¥ÊäòÂè∑ÂºÄÂ§¥\n       \
          \         is_paragraph_end = True\n        \n        # 3. Ë°®Ê†ºÁõ∏ÂÖ≥ÂÜÖÂÆπ‰øùÊåÅÂéüÊúâÊç¢Ë°å\n\
          \        if ('Ë°®' in line and re.search(r'Ë°®\\s*[A-Z0-9]', line)) or \\\n\
          \           re.match(r'^[|\\s]*[A-Za-z0-9\\u4e00-\\u9fa5]+[|\\s]*$', line):\
          \  # ÁÆÄÂçïË°®Ê†ºË°åÊ£ÄÊµã\n            current_paragraph.append(line)\n            is_paragraph_end\
          \ = True\n        else:\n            current_paragraph.append(line)\n  \
          \      \n        # Â¶ÇÊûúÊòØÊÆµËêΩÁªìÊùüÔºåÂ∞ÜÂΩìÂâçÊÆµËêΩÂêàÂπ∂Âπ∂Âä†ÂÖ•ÁªìÊûú\n        if is_paragraph_end:\n\
          \            if current_paragraph:\n                paragraph_text = ''.join(current_paragraph).strip()\n\
          \                if paragraph_text:\n                    result.append(paragraph_text)\n\
          \                current_paragraph = []\n    \n    # Â§ÑÁêÜÊúÄÂêéÂâ©‰ΩôÁöÑÊÆµËêΩ\n    if current_paragraph:\n\
          \        paragraph_text = ' '.join(current_paragraph).strip()\n        if\
          \ paragraph_text:\n            result.append(paragraph_text)\n    \n   \
          \ return '\\n'.join(result)\n\ndef parse_pdf_to_chapter_tree(pdf_path: str)\
          \ -> Tuple[List[Dict], Dict[str, str]]:\n    \"\"\"\n    ‰ªé PDF ‰∏≠ÊèêÂèñÁ´†ËäÇÊ†ëÂíåÊúØËØ≠Êò†Â∞Ñ\n\
          \    :param pdf_path: PDF Êñá‰ª∂Ë∑ØÂæÑ\n    :return: (Á´†ËäÇÊ†ë, ÊúØËØ≠Êò†Â∞Ñ)\n    \"\"\"\n \
          \   cleaned_lines = extract_full_text_with_filter(pdf_path)\n\n    # \U0001F195\
          \ Ê£ÄÊµãÊñáÊ°£ËØ≠Ë®Ä\n    language = detect_document_language(cleaned_lines)\n    max_chapter_num\
          \ = 50 if language == 'zh' else 1000\n    print(f\"Ê£ÄÊµãÂà∞ÊñáÊ°£ËØ≠Ë®Ä: {'‰∏≠Êñá' if language\
          \ == 'zh' else 'Ëã±Êñá'}, max_chapter_num={max_chapter_num}\")\n\n    # \U0001F195\
          \ Á¨¨‰∏ÄËΩÆÔºöÁ≤óÁï•ÊèêÂèñÊâÄÊúâÂèØËÉΩÁöÑÁ´†ËäÇÔºåÁî®‰∫éÂàÜÊûêÊï∞Â≠óÂàÜÂ∏É\n    preliminary_chapters = []\n    current =\
          \ {\n        \"chapter_id\": \"\",\n        \"chapter_title\": \"\",\n \
          \       \"raw_text\": \"\"\n    }\n    buffer = []\n\n    for line in cleaned_lines:\n\
          \        # Á¨¨‰∏ÄËΩÆ‰ΩøÁî®ÂÆΩÊùæÁöÑÊï∞Â≠óËåÉÂõ¥ËøõË°åÁ≤óÊèêÂèñ\n        chapter_info = detect_chapter(line,\
          \ max_chapter_num=1000, language=language, number_analysis=None)\n\n   \
          \     if chapter_info:\n            if current:\n                current[\"\
          raw_text\"] = smart_paragraph_join(buffer)\n                preliminary_chapters.append(current)\n\
          \                buffer = []\n            current = {\n                \"\
          chapter_id\": chapter_info[\"chapter_id\"],\n                \"chapter_title\"\
          : chapter_info[\"chapter_title\"],\n                \"raw_text\": \"\"\n\
          \            }\n        else:\n            buffer.append(line)\n\n    if\
          \ current:\n        current[\"raw_text\"] = smart_paragraph_join(buffer)\n\
          \        preliminary_chapters.append(current)\n\n    # \U0001F195 ÂàÜÊûêÁ´†ËäÇÊï∞Â≠óÂàÜÂ∏É\n\
          \    number_analysis = analyze_chapter_number_distribution(preliminary_chapters)\n\
          \    print(f\"Êï∞Â≠óÂàÜÂ∏ÉÂàÜÊûê: {number_analysis}\")\n\n    # \U0001F195 Á¨¨‰∫åËΩÆÔºö‰ΩøÁî®ÂàÜÊûêÁªìÊûúÈáçÊñ∞Á≤æÁ°ÆÊèêÂèñÁ´†ËäÇ\n\
          \    chapters = []\n    current = {\n        \"chapter_id\": \"\",\n   \
          \     \"chapter_title\": \"\",\n        \"raw_text\": \"\"\n    }\n    buffer\
          \ = []\n\n    for line in cleaned_lines:\n        chapter_info = detect_chapter(line,\
          \ max_chapter_num=max_chapter_num, language=language, number_analysis=number_analysis)\n\
          \n        if chapter_info:\n            if current:\n                # ‰ΩøÁî®Êô∫ËÉΩÊÆµËêΩÂêàÂπ∂ËÄå‰∏çÊòØÁÆÄÂçïÁöÑ\
          \ \\n ËøûÊé•\n                current[\"raw_text\"] = smart_paragraph_join(buffer)\n\
          \                chapters.append(current)\n                buffer = []\n\
          \            current = {\n                \"chapter_id\": chapter_info[\"\
          chapter_id\"],\n                \"chapter_title\": chapter_info[\"chapter_title\"\
          ],\n                \"raw_text\": \"\"\n            }\n        else:\n \
          \           buffer.append(line)\n\n    if current:\n        current[\"raw_text\"\
          ] = smart_paragraph_join(buffer)\n        chapters.append(current)\n\n \
          \   # 1Ô∏è‚É£ ÂÖàÊåâÈôÑ‰ª∂ÂàáÂàÜÈ°∂Â±Ç\n    attachment_sections = split_sections_by_attachment(chapters)\n\
          \n    tree = []\n\n    for top_sec in attachment_sections:\n        \n \
          \       # 2Ô∏è‚É£ ÊØè‰∏™È°∂Â±ÇÂùóÂÜçÊåâÈôÑÂΩïÂàáÂàÜ\n        sections = split_sections_by_appendix(top_sec[\"\
          chapters\"])\n        section_tree_list = []\n\n        for sec in sections:\n\
          \            # filtered_chapters, skipped_text = filter_start_of_main(sec[\"\
          chapters\"])\n            # 3Ô∏è‚É£ ÂØπÊØè‰∏™ÈÉ®ÂàÜÂÜÖÈÉ®‰øùÁïôÊúÄÈïøÈìæ\n            valid_chaps_in_sec,\
          \ skipped_text = find_longest_chapter_chain_with_append(sec[\"chapters\"\
          ], language)\n            tree_in_sec = build_tree(valid_chaps_in_sec)\n\
          \            build_full_path(tree_in_sec)\n            # ÊèíÂÖ•ÈîÆÂÄºÂØπ section\n\
          \            section_tree_list.append({\n                \"section\": sec[\"\
          section\"],\n                \"context\": skipped_text,\n              \
          \  \"chapters\": tree_in_sec,\n            })\n\n        # 4Ô∏è‚É£ ÊûÑÂª∫È°∂Â±ÇÊ†ë\n \
          \       tree.append({\n            \"file\": top_sec[\"section\"],  # regulation\
          \ Êàñ ANNEX n\n            \"sections\": section_tree_list,\n        })\n\n\
          \    term_map = {}\n\n    for chap in chapters:\n        title = chap.get(\"\
          chapter_title\", \"\")\n        if \"ÊúØËØ≠\" in title:\n            # ÊèêÂèñÊúØËØ≠\n\
          \            terms = extract_terms_with_abbr_from_terms_section(chap[\"\
          chapter_title\"])\n            term_map.update(terms)\n            for child\
          \ in chap.get(\"children\", []):\n                terms = extract_terms_with_abbr_from_terms_section(child[\"\
          chapter_title\"])\n                term_map.update(terms)\n        elif\
          \ \"Áº©Áï•\" in title:\n            # ÊèêÂèñÁº©Áï•ËØ≠\n            abbr_terms = extract_abbr_terms_from_symbols_section(chap[\"\
          chapter_title\"] + chap[\"raw_text\"])\n            term_map.update(abbr_terms)\n\
          \            for child in chap.get(\"children\", []):\n                abbr_terms\
          \ = extract_abbr_terms_from_symbols_section(child[\"chapter_title\"] + child[\"\
          raw_text\"])\n                term_map.update(abbr_terms)\n\n    return\
          \ tree, term_map\n\nimport tempfile\nimport requests\nfrom urllib.parse\
          \ import urlparse\n\ndef resolve_pdf_path(pdf_path: str) -> str:\n    #\
          \ Â¶ÇÊûúÊòØ URLÔºåÂ∞±‰∏ãËΩΩÂà∞‰∏¥Êó∂Êñá‰ª∂Â§π\n    if pdf_path.startswith(\"http://\") or pdf_path.startswith(\"\
          https://\"):\n        response = requests.get(pdf_path)\n        response.raise_for_status()\n\
          \        suffix = os.path.splitext(urlparse(pdf_path).path)[-1]\n      \
          \  with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp_file:\n\
          \            tmp_file.write(response.content)\n            return tmp_file.name\n\
          \    else:\n        return pdf_path\n\ndef count_leaf_nodes(chapters):\n\
          \    \"\"\"ÈÄíÂΩíËÆ°ÁÆóÁ´†ËäÇÊ†ë‰∏≠ÁöÑÂè∂Â≠êËäÇÁÇπÊï∞Èáè\"\"\"\n    total = 0\n    for chapter in chapters:\n\
          \        if chapter.get(\"children\") and len(chapter[\"children\"]) > 0:\n\
          \            # ÊúâÂ≠êËäÇÁÇπÔºåÈÄíÂΩíËÆ°ÁÆó\n            total += count_leaf_nodes(chapter[\"\
          children\"])\n        else:\n            # Âè∂Â≠êËäÇÁÇπ\n            total += 1\n\
          \    return total\n\ndef extract_chapters_by_id(tree, chapter_ids):\n  \
          \  \"\"\"‰ªéÊ†ëÁªìÊûÑ‰∏≠ÊèêÂèñÊåáÂÆöIDÁöÑÁ´†ËäÇÔºå‰øùÊåÅÂéüÊúâÂ±ÇÁ∫ßÁªìÊûÑ\"\"\"\n    result = []\n    \n    for file_item\
          \ in tree:\n        # Âè™Â§ÑÁêÜ file ‰∏∫ regulation ÁöÑÂÜÖÂÆπ\n        if file_item[\"\
          file\"] != \"regulation\":\n            continue\n            \n       \
          \ new_file = {\n            \"file\": file_item[\"file\"],\n           \
          \ \"sections\": []\n        }\n        \n        for section in file_item[\"\
          sections\"]:\n            new_section = {\n                \"section\":\
          \ section[\"section\"],\n                \"context\": section[\"context\"\
          ],\n                \"chapters\": []\n            }\n            \n    \
          \        # ÊèêÂèñÂåπÈÖçÁöÑÁ´†ËäÇ\n            for chapter in section[\"chapters\"]:\n\
          \                if chapter[\"chapter_id\"] in chapter_ids:\n          \
          \          new_section[\"chapters\"].append(chapter)\n            \n   \
          \         # Âè™ÊúâÂΩìsectionÊúâÁ´†ËäÇÊó∂ÊâçÊ∑ªÂä†\n            if new_section[\"chapters\"]:\n\
          \                new_file[\"sections\"].append(new_section)\n        \n\
          \        # Âè™ÊúâÂΩìfileÊúâsectionsÊó∂ÊâçÊ∑ªÂä†\n        if new_file[\"sections\"]:\n  \
          \          result.append(new_file)\n    \n    return result\n\ndef get_first_three_chapters_from_main(tree):\n\
          \    \"\"\"‰ªé MAIN section ‰∏≠Ëé∑ÂèñÂâç‰∏âÁ´†ÁöÑ chapter_id\"\"\"\n    for file_item in\
          \ tree:\n        if file_item[\"file\"] != \"regulation\":\n           \
          \ continue\n            \n        for section in file_item[\"sections\"\
          ]:\n            if section[\"section\"] == \"MAIN\":\n                #\
          \ Ëé∑ÂèñÂâç‰∏âÁ´†ÁöÑ chapter_id\n                chapter_ids = []\n                for\
          \ i, chapter in enumerate(section[\"chapters\"]):\n                    if\
          \ i < 3:  # Ââç‰∏âÁ´†\n                        chapter_ids.append(chapter[\"chapter_id\"\
          ])\n                    else:\n                        break\n         \
          \       return set(chapter_ids)\n    return set()\n\ndef group_main_chapters_by_leaf_count(tree,\
          \ max_leaf_nodes=30):\n    \"\"\"Â∞Ü MAIN section ÁöÑÁ´†ËäÇÊåâÂè∂Â≠êËäÇÁÇπÊï∞ÈáèÂàÜÁªÑ\"\"\"\n   \
          \ groups = []\n    current_group_chapters = []\n    current_leaf_count =\
          \ 0\n    \n    # ÊâæÂà∞ regulation file ÁöÑ MAIN section\n    main_chapters =\
          \ []\n    for file_item in tree:\n        if file_item[\"file\"] != \"regulation\"\
          :\n            continue\n            \n        for section in file_item[\"\
          sections\"]:\n            if section[\"section\"] == \"MAIN\":\n       \
          \         # Ë∑≥ËøáÂâç‰∏âÁ´†ÔºàcontextÈÉ®ÂàÜÔºâ\n                for i, chapter in enumerate(section[\"\
          chapters\"]):\n                    # if i >= 3:  # ‰ªéÁ¨¨ÂõõÁ´†ÂºÄÂßã\n            \
          \        main_chapters.append(chapter)\n                break\n        break\n\
          \    \n    for chapter in main_chapters:\n        chapter_leaf_count = count_leaf_nodes([chapter])\n\
          \        \n        # Â¶ÇÊûúÂçï‰∏™Á´†ËäÇÂ∞±Ë∂ÖËøámax_leaf_nodesÔºåÂçïÁã¨ÊàêÁªÑ\n        if chapter_leaf_count\
          \ > max_leaf_nodes:\n            # ÂÖàÂ§ÑÁêÜÂΩìÂâçÁßØÁ¥ØÁöÑÁ´†ËäÇÁªÑ\n            if current_group_chapters:\n\
          \                groups.append(current_group_chapters)\n               \
          \ current_group_chapters = []\n                current_leaf_count = 0\n\
          \            \n            # ÂçïÁã¨ÊàêÁªÑ\n            groups.append([chapter[\"\
          chapter_id\"]])\n        else:\n            # Ê£ÄÊü•Âä†ÂÖ•ÂêéÊòØÂê¶Ë∂ÖËøáÈôêÂà∂\n            if\
          \ current_leaf_count + chapter_leaf_count <= max_leaf_nodes:\n         \
          \       current_group_chapters.append(chapter[\"chapter_id\"])\n       \
          \         current_leaf_count += chapter_leaf_count\n            else:\n\
          \                # Ë∂ÖËøáÈôêÂà∂ÔºåÂÖà‰øùÂ≠òÂΩìÂâçÁªÑÔºåÂºÄÂßãÊñ∞ÁªÑ\n                if current_group_chapters:\n\
          \                    groups.append(current_group_chapters)\n           \
          \     current_group_chapters = [chapter[\"chapter_id\"]]\n             \
          \   current_leaf_count = chapter_leaf_count\n    \n    # Â§ÑÁêÜÊúÄÂêé‰∏ÄÁªÑ\n    if\
          \ current_group_chapters:\n        groups.append(current_group_chapters)\n\
          \    \n    return groups\n\ndef get_non_main_sections(tree):\n    \"\"\"\
          Ëé∑ÂèñÊâÄÊúâÈùû MAIN ÁöÑ sections\"\"\"\n    non_main_sections = []\n    \n    for file_item\
          \ in tree:\n        if file_item[\"file\"] != \"regulation\":\n        \
          \    continue\n            \n        for section in file_item[\"sections\"\
          ]:\n            if section[\"section\"] != \"MAIN\":\n                non_main_sections.append(section[\"\
          section\"])\n    \n    return non_main_sections\n\ndef create_section_tree(tree,\
          \ section_name):\n    \"\"\"ÂàõÂª∫ÂåÖÂê´ÊåáÂÆö section ÁöÑÂÆåÊï¥Ê†ëÁªìÊûÑ\"\"\"\n    result = []\n\
          \    \n    for file_item in tree:\n        if file_item[\"file\"] != \"\
          regulation\":\n            continue\n            \n        new_file = {\n\
          \            \"file\": file_item[\"file\"],\n            \"sections\": []\n\
          \        }\n        \n        for section in file_item[\"sections\"]:\n\
          \            if section[\"section\"] == section_name:\n                new_file[\"\
          sections\"].append(section)\n                break\n        \n        if\
          \ new_file[\"sections\"]:\n            result.append(new_file)\n       \
          \     break\n    \n    return result\n\ndef main(url: str) -> dict:\n  \
          \  pdf_path = resolve_pdf_path(url)  # ÂÖºÂÆπ URL ÂíåÊú¨Âú∞Ë∑ØÂæÑ\n\n    # Á´†ËäÇÊ†ë\n    tree,\
          \ term_map = parse_pdf_to_chapter_tree(pdf_path)\n    \n    # ÊèêÂèñÂâç‰∏âÁ´†ÁöÑ chapter_idÔºà‰ªé\
          \ MAIN sectionÔºâ\n    first_three_chapter_ids = get_first_three_chapters_from_main(tree)\n\
          \    \n    # ÊèêÂèñÂâç‰∏âÁ´†‰Ωú‰∏∫context\n    context_tree = extract_chapters_by_id(tree,\
          \ first_three_chapter_ids)\n    \n    # ÊåâÂè∂Â≠êËäÇÁÇπÊï∞ÈáèÂàÜÁªÑ MAIN section ÁöÑÂÖ∂‰ΩôÁ´†ËäÇ\n \
          \   main_chapter_groups = group_main_chapters_by_leaf_count(tree, max_leaf_nodes=20)\n\
          \    \n    # Ëé∑ÂèñÊâÄÊúâÈùû MAIN ÁöÑ sections\n    non_main_sections = get_non_main_sections(tree)\n\
          \    \n    # ‰∏∫ÊØè‰∏™ MAIN Á´†ËäÇÁªÑÂíåÈùû MAIN section ÂàõÂª∫ÂÆåÊï¥ÁöÑÊ†ëÁªìÊûÑ\n    array_items = []\n\
          \    \n    # Â§ÑÁêÜ MAIN section ÁöÑÁ´†ËäÇÁªÑ\n    for group_chapter_ids in main_chapter_groups:\n\
          \        group_tree = extract_chapters_by_id(tree, set(group_chapter_ids))\n\
          \        if group_tree:  # Á°Æ‰øùÁªÑ‰∏ç‰∏∫Á©∫\n            array_items.append(json.dumps(group_tree,\
          \ ensure_ascii=False))\n    \n    # Â§ÑÁêÜÈùû MAIN sectionsÔºàÊØè‰∏™ÈôÑÂΩï‰Ωú‰∏∫‰∏Ä‰∏™Áã¨Á´ãÁöÑ itemÔºâ\n\
          \    for section_name in non_main_sections:\n        section_tree = create_section_tree(tree,\
          \ section_name)\n        if section_tree:  # Á°Æ‰øùsection‰∏ç‰∏∫Á©∫\n            array_items.append(json.dumps(section_tree,\
          \ ensure_ascii=False))\n\n    return {\n        \"tree\":json.dumps(tree,\
          \ ensure_ascii=False),\n        \"context\": json.dumps(context_tree, ensure_ascii=False),\n\
          \        \"array\": array_items\n    }"
        code_language: python3
        desc: ''
        outputs:
          array:
            children: null
            type: array[string]
          context:
            children: null
            type: string
          tree:
            children: null
            type: string
        selected: false
        title: ‰ª£Á†ÅÊâßË°å
        type: code
        variables:
        - value_selector:
          - '1756550268945'
          - file
          - url
          value_type: file
          variable: url
      height: 53
      id: '1756550411122'
      position:
        x: 334
        y: 459
      positionAbsolute:
        x: 334
        y: 459
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        desc: ''
        error_handle_mode: continue-on-error
        height: 236
        is_parallel: true
        iterator_input_type: array[string]
        iterator_selector:
        - '1756550411122'
        - array
        output_selector:
        - '1756698812908'
        - output
        output_type: array[string]
        parallel_nums: 10
        selected: false
        start_node_id: 1756557801310start
        title: Ëø≠‰ª£
        type: iteration
        width: 1664
      height: 236
      id: '1756557801310'
      position:
        x: 638
        y: 459
      positionAbsolute:
        x: 638
        y: 459
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 1664
      zIndex: 1
    - data:
        desc: ''
        isInIteration: true
        selected: false
        title: ''
        type: iteration-start
      draggable: false
      height: 48
      id: 1756557801310start
      parentId: '1756557801310'
      position:
        x: 60
        y: 100.5
      positionAbsolute:
        x: 698
        y: 559.5
      selectable: false
      sourcePosition: right
      targetPosition: left
      type: custom-iteration-start
      width: 44
      zIndex: 1002
    - data:
        context:
          enabled: true
          variable_selector:
          - '1756550411122'
          - context
        desc: ''
        isInIteration: true
        isInLoop: false
        iteration_id: '1756557801310'
        model:
          completion_params: {}
          mode: chat
          name: deepseek-reasoner
          provider: langgenius/deepseek/deepseek
        prompt_template:
        - id: b8df94d2-1037-41d9-8649-be6e2790a7d5
          role: system
          text: "# ËßíËâ≤\n\n‰Ω†ÊòØ‰∏ÄÂêçÊäÄÊúØÊ†áÂáÜÁü•ËØÜÂ∑•Á®ã‰∏ìÂÆ∂Ôºå‰∏ìÊ≥®‰∫éÂ∞ÜÊ±ΩËΩ¶ÂèäÁõ∏ÂÖ≥È¢ÜÂüüÁöÑÊ†áÂáÜÂíåÊ≥ïËßÑÊñáÊ°£ËΩ¨Âåñ‰∏∫ÂèØÁªìÊûÑÂåñËß£ÊûêÁöÑÊï∞ÊçÆËµÑ‰∫ß„ÄÇ\n\n‰Ω†ÁöÑ‰ªªÂä°ÊòØÂü∫‰∫éËæìÂÖ•ÁöÑÊ†áÂáÜÊñáÊ°£\
            \ JSON Ê†ëÁªìÊûÑÔºåÁ≤æÂáÜÊäΩÂèñÊäÄÊúØÊù°Ê¨æ„ÄÅÂÆûÈ™åË¶ÅÊ±Ç„ÄÅÂèÇÊï∞Á∫¶ÊùüÂíåÂºïÁî®‰ø°ÊÅØÔºåÁîüÊàêÁªü‰∏ÄÁöÑÊú∫Âô®ÂèØËØªÊï∞ÊçÆÔºå‰ª•ÊîØÊåÅË∑®Ê†áÂáÜÊù°Ê¨æÊØîÂØπ„ÄÅÊ≥ïËßÑ‰∏ÄËá¥ÊÄßÈ™åËØÅ„ÄÅËÆæÂ§áËÉΩÂäõËØÑ‰º∞Á≠â‰∏ãÊ∏∏Â∫îÁî®„ÄÇ\n\
            \n---\n\n# ËæìÂÖ•ÂÜÖÂÆπ\n\n‰Ω†Â∞ÜÊî∂Âà∞‰∏Ä‰∏™ JSON Êï∞ÁªÑÔºåÂåÖÂê´Â§ö‰∏™ÂØπË±°ÔºåÊØè‰∏™ÂØπË±°ÁöÑÁªìÊûÑÂ¶Ç‰∏ãÔºö\n\n- `file`: Êñá‰ª∂Ê†áËØÜÔºàÂ¶Ç\
            \ \"regulation\"„ÄÅ\"ANNEX 1\"Ôºâ\n- `sections`: ÂùóÊï∞ÁªÑÔºåÊØè‰∏™ÂÖÉÁ¥†ÂåÖÂê´Ôºö\n  - `section`:\
            \ Ê†áËØÜÂΩìÂâçÂùóÔºàÂ¶Ç \"MAIN\", \"APPENDIX 1\"Ôºâ\n  - `context`: ÂΩìÂâçÂùóÊ†áÈ¢ò‰∏éÁ¨¨‰∏Ä‰∏™Á´†ËäÇÈó¥ÁöÑÊñáÊú¨ÔºàÊÄªËø∞Ôºâ\n\
            \  - `chapters`: Á´†ËäÇÊï∞ÁªÑÔºåÊØè‰∏™ÂÖÉÁ¥†ÂåÖÂê´Ôºö\n    - `chapter_id`: Á´†ËäÇÁºñÂè∑Ôºà‰øùÊåÅÂéüÊ†∑Ôºâ\n    - `chapter_title`:\
            \ Á´†ËäÇÊ†áÈ¢ò\n    - `raw_text`: ËØ•ËäÇÁöÑÁ∫ØÊñáÊú¨ÂÜÖÂÆπ\n    - `children`: Â≠êÊù°Ê¨æÊï∞ÁªÑÔºàÁªìÊûÑ‰∏éÁà∂Á∫ßÁõ∏ÂêåÔºâ\n\
            \    - `full_path`: ÂÆåÊï¥Ë∑ØÂæÑÔºàÂèØÈÄâÔºâ\n\n> ÂêåÊó∂Êèê‰æõËÉåÊôØ‰∏ä‰∏ãÊñáÔºà{{#context#}}ÔºâÔºåÂåÖÂê´ËåÉÂõ¥„ÄÅÊúØËØ≠ÂÆö‰πâÁ≠âÁ´†ËäÇÔºå‰æø‰∫éÁêÜËß£„ÄÇ\n\
            \n---\n\n# ËæìÂá∫Ê†ºÂºè\n\nËØ∑ËæìÂá∫‰∏Ä‰∏™ JSON ÂØπË±°Ôºà‰∏çÂæóÂåÖÂê´‰ªª‰ΩïËß£ÈáäÊÄßÊñáÂ≠óÔºâÔºåÂøÖÈ°ªÂÆåÂÖ®Á¨¶Âêà‰ª•‰∏ãÁªìÊûÑÔºö\n\n```json\n\
            \    {\n      \"file\": \"string\",\n      \"section\": \"string\",\n\
            \      \"experiment_root_ids\": [\"string\", \"...\"],\n       \"chapters\"\
            :[\n        {      \n          \"chapter_id\": \"string\",\n\n       \
            \   \"paramaters\": [\n            {\n              \"item\": \"string\"\
            ,\n              \"constraint\": \"<=|<|=|>=|>|range_closed|range_open|enum|boolean\"\
            ,\n              \"value\": \"string|array|null\",\n              \"unit\"\
            : \"string|null\",\n              \"source_text\": \"string\"\n      \
            \      }\n          ],\n\n          \"topic_keywords\": [\"string\", \"\
            ...\"],\n          \"context_keywords\": [\"string\", \"...\"],\n\n  \
            \        \"refs\": [\n            {\n              \"ref_type\": \"internal|external\"\
            ,\n              \"doc_id\": \"string|null\",\n              \"target_id\"\
            : \"string\",\n              \"anchor_text\": \"string\"\n           \
            \ }\n          ],\n\n          \"table_headers\": [\"string\", \"...\"\
            ]}\n        ]\n\n    }\n```\n\n# Â§ÑÁêÜÈÄªËæëÔºàÈìæÂºèÊÄùËÄÉÔºâ\n\n1. **ÈÅçÂéÜÁ´†ËäÇÊ†ë**\n    ÈÅçÂéÜÊØè‰∏™\
            \ `chapter_id` ÂèäÂÖ∂Â≠êÁ´†ËäÇÔºåÂàÜÂà´Â§ÑÁêÜÔºåÁ°Æ‰øùËæìÂá∫‰∏≠ÊØè‰∏™Á´†ËäÇÈÉΩÁã¨Á´ãÊàêÊù°„ÄÇ\n2. **ÂèÇÊï∞ÊèêÂèñ (`paramaters`)**\n\
            \   - ÊèêÂèñÊ†áÂáÜ‰∏≠ÁöÑÂÆöÈáèÊåáÊ†áÊàñÁ∫¶ÊùüÊù°‰ª∂ÔºåÊØèÊù°ÂèÇÊï∞Áã¨Á´ãÊàêÂØπË±°„ÄÇ\n   - `constraint`Ôºö\n     - ÂçïÂÄºÁ∫¶ÊùüÔºö`<`,\
            \ `<=`, `>`, `>=`, `=`\n     - Âå∫Èó¥Èó≠ÂêàÔºö`range_closed`\n     - Âå∫Èó¥ÂºÄÂå∫Èó¥Ôºö`range_open`\n\
            \     - Êûö‰∏æÔºö`enum`\n     - Â∏ÉÂ∞îÂÄºÔºö`boolean`\n   - `value`Ôºö‰øùÊåÅÂ≠óÁ¨¶‰∏≤ÊàñÂ≠óÁ¨¶‰∏≤Êï∞ÁªÑÔºå‰∏≠ÊñáÊï∞Â≠óÈúÄËΩ¨ÈòøÊãâ‰ºØÊï∞Â≠ó„ÄÇ\n\
            \   - `source_text`Ôºö‰øùÁïôÂéüÊñáÂÆåÊï¥ÁâáÊÆµÔºåÁ¨¶Âè∑‰∏çÂÅöÊõøÊç¢„ÄÇ\n3. **ÂÖ≥ÈîÆËØçÊèêÂèñ**\n   - `topic_keywords`ÔºöÊ†∏ÂøÉ‰∏ªÈ¢òÂÖ≥ÈîÆËØçÔºåÁ™ÅÂá∫ÂΩìÂâçÁ´†ËäÇÂõ¥Áªï‰ªÄ‰πà‰∏ªÈ¢òÂ±ïÂºÄÔºåÂØπ‰ªÄ‰πàÂÜÖÂÆπËøõË°åËßÑÂÆöÔºåÂêçËØçÁü≠ËØ≠Ôºå1-6‰∏™„ÄÇ\n\
            \   - `context_keywords`ÔºöËæÖÂä©‰∏ä‰∏ãÊñáÂÖ≥ÈîÆËØçÔºåÊèèËø∞ÂÆûÈ™åÂØπË±°„ÄÅÁéØÂ¢É„ÄÅÊù°‰ª∂Á≠âÔºå0-6‰∏™\n4. **ÂºïÁî®ÊèêÂèñ (`refs`)**\n\
            \   - ÊèêÂèñÂÜÖÈÉ®ÂºïÁî®ÔºàÊú¨Ê†áÂáÜÂÜÖÁ´†ËäÇ/Ë°®Ê†º/ÂõæÁâáÔºâ‰∏éÂ§ñÈÉ®ÂºïÁî®ÔºàÂ§ñÈÉ®Ê†áÂáÜÁºñÂè∑ÔºâÔºåÂπ∂Âå∫ÂàÜ `ref_type`„ÄÇ\n   - `doc_id`\
            \ Â°´ÂÜôÊ†áÂáÜÁºñÂè∑ÔºàÂ¶ÇÊúâÔºâÔºåÂÜÖÈÉ®ÂºïÁî®Â°´ `null`„ÄÇ\n   - `target_id` ‰øùÁïôÂºïÁî®ÁõÆÊ†áÁºñÂè∑ÔºàÂ¶Ç‚ÄúB.2.1.1‚Äù„ÄÅ‚ÄúË°®B.1‚ÄùÔºâ„ÄÇ\n\
            \   - `anchor_text` Âú®‰∏ä‰∏ãÊñáÊèêÂèñÁÆÄË¶ÅÊñáÊú¨ÔºåËØ¥ÊòéÂºïÁî®ÁöÑÂÜÖÂÆπÔºåÁ°Æ‰øùÂèØÁ≤æÁ°ÆÂÆö‰Ωç„ÄÇ\n   - ÂéüÊñáÁõ∏Âêå‰ΩçÁΩÆÂá∫Áé∞ÁöÑÂºïÁî®ÔºåÂèØÂè™ÁîüÊàê‰∏ÄÊù°ÂÜÖÂÆπÔºåÂêåÊ†∑‰øùÊåÅÂπ∂ÂàóÂç≥ÂèØ„ÄÇ‰æãÂ¶ÇÔºöÊåâA„ÄÅBËøõË°åÂÆûÈ™åÔºåÊèêÂèñÊó∂ÂèØÂ∞ÜA„ÄÅBÂπ∂ÂàóÔºåËÄå‰∏çÁî®ÁîüÊàê‰∏§Êù°ÂÜÖÂÆπ„ÄÇ\n\
            5. **Ë°®Ê†ºË°®Â§¥ (`table_headers`)**\n   - Â¶ÇÊûúÂΩìÂâçÁ´†ËäÇÂåÖÂê´Ë°®Ê†ºÂºïÁî®ÔºåÊèêÂèñË°®Ê†ºÁöÑË°®Â§¥Â≠óÊÆµÔºåÊåâÈ°∫Â∫èËæìÂá∫„ÄÇ\n6.\
            \ **ÂÆûÈ™åÁ´†ËäÇËØÜÂà´ (`experiment_root_ids`)**\n   - Âà§Êñ≠Á´†ËäÇÂÜÖÂÆπÊòØÂê¶‰∏∫ÂÆûÈ™åÁ´†ËäÇÔºåÊ†áËÆ∞ÂÖ∂ `chapter_id`\
            \ ‰∏∫ÂÆûÈ™åÊ†πËäÇÁÇπ„ÄÇÊúâ‰ª•‰∏ã‰∏âÁßçÊÉÖÂÜµÔºö\n     - Ê†áÈ¢ò‰∏≠Áõ¥Êé•ÂåÖÂê´‰∫ÜÂÖ≥ÈîÆÂ≠ó‰æãÂ¶ÇÂÆûÈ™å„ÄÅtestÁ≠âÔºå‰ΩÜÈúÄÊ≥®ÊÑèÂå∫ÂàÜÂÆûÈ™åÂíåÂÆûÈ™åÁöÑÈÉ®ÂàÜÔºåÊØîÂ¶Ç‚ÄúD.1\
            \ ÂÆûÈ™åÊù°‰ª∂‚Äù„ÄÅ‚ÄúD.2 ÂÆûÈ™åÊñπÊ≥ï‚Äù„ÄÅ‚ÄúÈôÑÂΩïD Ëá™Ê£ÄËØïÈ™åÊñπÊ≥ï‚ÄùÔºåÂàô‚ÄúÈôÑÂΩïD‚ÄùÂ∫îËØ•‰Ωú‰∏∫ÂÆûÈ™åÊ†πËäÇÁÇπÔºåÂ∞ΩÁÆ°D.1/D.2ÈÉΩÂåÖÂê´‰∫ÜÂÖ≥ÈîÆËØçÂÆûÈ™å\n \
            \    - Ê≠£Êñá‰∏≠ÂåÖÂê´‰∫Ü‚ÄúÊåâXXÂÆûÈ™å‚ÄùÁ≠âÂÜÖÂÆπ\n     - Ê†πÊçÆÊé®Êñ≠ÔºåËØ•Á´†ËäÇÂèäÂÖ∂Â≠êÁ´†ËäÇÂùáÂõ¥ÁªïÊüê‰∏™ÂÆûÈ™åÂ±ïÂºÄ\n   - Ëã•‰∏Ä‰∏™Á´†ËäÇÊòØÂÆûÈ™åÁ´†ËäÇÔºåÂàôÊâÄÊúâÂ≠êÁ´†ËäÇÈÉΩÁÆóÂÆûÈ™åÂÜÖÂÆπÔºåÊó†ÈúÄÂçïÁã¨ÈáçÂ§çÊ†áÊ≥®„ÄÇ‰ΩÜËã•ÊòØÂ≠êÁ´†ËäÇÂåÖÂê´‰∫Ü‰∏çÂêåÁöÑÂÆûÈ™åÔºåÂàôÈúÄË¶ÅÂØπÊØè‰∏™ÂÆûÈ™åËøõË°åÊ†áËÆ∞ÔºåËÄå‰∏çÊòØÂΩìÂâçÁ´†ËäÇ\n\
            \   - Ëã•ÁªºÂêàÂà§ÂÆö‰∏Ä‰∏™section‰ªÖÂõ¥Áªï‰∏Ä‰∏™ÂÆûÈ™åÂ±ïÂºÄÔºà‰æãÂ¶ÇÔºöÈôÑÂΩïD Ëá™Ê£ÄÂÆûÈ™åÊñπÊ≥ïÔºâÔºåÂàô‰ª•ÁâπÊÆäÊ†áËÆ∞ALL‰Ωú‰∏∫ÁªìÊûúÔºà‰æãÂ¶ÇÔºöexperiment_root_idsÔºö[\"\
            ALL\"]ÔºåÁ¶ÅÊ≠¢‰ªÖ‰øùÁïô‚ÄúD‚ÄùÂØºËá¥‰ª£Á†ÅÊó†Ê≥ïÂåπÈÖçÔºâ\n7. **ËæìÂá∫ËßÑËåÉ**\n   - ÂøÖÈ°ªËæìÂá∫ÊâÄÊúâÁ´†ËäÇÁöÑÁªìÊûÑÂåñ‰ø°ÊÅØ„ÄÇ\n   - `experiment_root_ids`\
            \ ÂàóË°®‰∏≠‰ªÖÂåÖÂê´ÊúÄÈ°∂Â±ÇÂÆûÈ™åËäÇÁÇπ„ÄÇ\n\n------\n\n# Ê≥®ÊÑè‰∫ãÈ°π\n\n- ËæìÂá∫ÂøÖÈ°ªÊòØ‰∏•Ê†º JSONÔºå‰∏çÂæóÊ∑ªÂä†Ëß£ÈáäËØ¥Êòé„ÄÅÊ≥®ÈáäÊàñ Markdown\
            \ Á¨¶Âè∑„ÄÇ\n- ‰∏çÂÖÅËÆ∏Êé®Êñ≠Á´†ËäÇÁºñÂè∑ÊàñËôöÊûÑÂÄºÔºåÂè™ËÉΩÊ†πÊçÆËæìÂÖ•ÂÜÖÂÆπÊèêÂèñ„ÄÇ\n- Áº∫Â§±‰ø°ÊÅØÊó∂ËøîÂõûÁ©∫Êï∞ÁªÑ`[]`Êàñ`null`Ôºå‰∏çÁúÅÁï•Â≠óÊÆµ„ÄÇ\n-\
            \ Êï∞ÂÄºÁªü‰∏ÄÁî®ÈòøÊãâ‰ºØÊï∞Â≠óÔºå‰∏çË¶ÅÊ∑ªÂä†Âçï‰ΩçÂà∞ `value` Â≠óÊÆµ„ÄÇ\n- ÂØπ‰∫éË°®Ê†ºÂÜÖÂÆπÊó†ÈúÄÊèêÂèñ‰∏∫table_headersÂ§ñÁöÑ‰ªª‰ΩïÂ≠óÊÆµ\n-\
            \ topic_keywordsÂπ∂‰∏çÊòØË∂äÂ§öË∂äÂ•ΩÔºåËÄåÊòØÊúçÂä°‰∫éÂêéÁª≠ÂêëÈáèÂåñ-Âè¨ÂõûÊ≠•È™§„ÄÇËåÉÂõ¥„ÄÅÂºïÁî®ÊÄßÊñá‰ª∂Á≠âÁ∫≤È¢ÜÊÄßÂÜÖÂÆπÔºå‰ªÖÈúÄÂØπÁ´†ËäÇÈ°∂Â±ÇÁºñÂè∑ÊèêÂèñ‰∏Ä‰∏™topic_keywordsÔºåÊúØËØ≠Á≠âÁ´†ËäÇÔºå‰ªÖÈúÄÂØπÊØè‰∏™ÈòêÈáäÊúØËØ≠ÁöÑÁ´†ËäÇÊèêÂèñ‰∏Ä‰∏™topic_keywords\n\
            - `topic_keywords` Âíå `context_keywords`‰øùÁïôÂéüÊñáÔºåÂèØ‰∏∫‰∏≠ÊñáÊàñËã±Êñá\n- ÊâÄÊúâÂºïÁî®ÂøÖÈ°ªËÉΩÂú®ÂéüÊñá‰∏≠Á≤æÂáÜÂÆö‰ΩçÔºå‰∏îÂøÖÈ°ªÊòØÂÆûÈôÖÂºïÁî®ÊâçËÉΩÊèêÂèñrefsÂ≠óÊÆµÔºåÂç≥‰∏ä‰∏ãÊñáÊúâÁ±ª‰ºº‰∫é‚ÄúÂèÇËßÅXX‚ÄùÊàñËÄÖ‚ÄúÊåâXXÂÆûÈ™å‚ÄùÁ≠âË°®Ëø∞ÔºåËã•ÊòØÂ≠§Á´ã„ÄÅÁ™ÅÂÖÄÂá∫Áé∞ÂàôÂèØ‰ª•ÁêÜËß£‰∏∫È°µÁúâË¢´ÈîôËØØËß£ÊûêÔºåÊàñËÄÖ‚ÄúËßÑËåÉÊÄßÂºïÁî®Êñá‰ª∂‚ÄùÁ´†ËäÇ‰∏≠ÂØπÂºïÁî®Êñá‰ª∂ÁöÑÁΩóÂàó\n\
            \n------\n\n# Few-shot Á§∫‰æã\n\n## ËæìÂÖ•Á§∫‰æã1\n\n```\n[\n  {\n    \"file\": \"\
            regulation\",\n    \"sections\": [\n      {\n        \"section\": \"ÈôÑÂΩïB\"\
            ,\n        \"context\": \"(ËßÑËåÉÊÄß)Ëá™Âä®Ëß¶ÂèëËØïÈ™åÊñπÊ≥ï\",\n        \"chapters\": [\n\
            \          {\n          {\n            \"chapter_id\": \"B.2\",\n    \
            \        \"chapter_title\": \"ËØïÈ™åÈ°πÁõÆ\",\n            \"raw_text\": \"\"\
            ,\n            \"children\": [\n              {\n                \"chapter_id\"\
            : \"B.2.1\",\n                \"chapter_title\": \"Ê≠£Èù¢Á¢∞Êíû\",\n         \
            \       \"raw_text\": \"\",\n                \"children\": [\n       \
            \           {\n                    \"chapter_id\": \"B.2.1.1\",\n    \
            \                \"chapter_title\": \"ÊªëÂè∞Ê≠£Èù¢Á¢∞ÊíûËØïÈ™å\",\n                  \
            \  \"raw_text\": \"\",\n                    \"children\": [\n        \
            \              {\n                        \"chapter_id\": \"B.2.1.1.1\"\
            ,\n                        \"chapter_title\": \"\",\n                \
            \        \"raw_text\": \"Â∞ÜÁôΩËΩ¶Ë∫´ÊàñÂ∑•Ë£ÖÂõ∫ÂÆöÂú®Á¢∞ÊíûËØïÈ™åÊªëÂè∞‰∏ä,ÂÆâË£ÖÊñπÂêëÊ®°ÊãüÊ≠£Èù¢Á¢∞Êíû„ÄÇ \",\n         \
            \               \"children\": [],\n                        \"full_path\"\
            : \"B.2 ËØïÈ™åÈ°πÁõÆ/B.2.1 Ê≠£Èù¢Á¢∞Êíû/B.2.1.1 ÊªëÂè∞Ê≠£Èù¢Á¢∞ÊíûËØïÈ™å/B.2.1.1.1 \"\n              \
            \        },\n                      {\n                        \"chapter_id\"\
            : \"B.2.1.1.2\",\n                        \"chapter_title\": \"\",\n \
            \                       \"raw_text\": \"ÊªëÂè∞ÊåâÁÖß‰ª•‰∏ãÂä†ÈÄüÂ∫¶Ê≥¢ÂΩ¢‰πã‰∏ÄËøõË°åÁ¢∞ÊíûËØïÈ™å„ÄÇ a) ‰ΩøÁî®Âà∂ÈÄ†ÂïÜÊåáÂÆöÁöÑÂä†ÈÄüÂ∫¶Ê≥¢ÂΩ¢ËøõË°åËØïÈ™å,ÊåáÂÆöÁöÑÂä†ÈÄüÂ∫¶Ê≥¢ÂΩ¢Â∫î‰∏∫Âú®B.2.1.2‰∏≠ÊèèËø∞ÁöÑÂÆûËΩ¶Á¢∞ÊíûËØïÈ™åÊù°‰ª∂‰∏≠,ËΩ¶Ë∫´ÈùûÂèòÂΩ¢Âå∫ÂüüÈááÈõÜÁöÑÂä†ÈÄüÂ∫¶-Êó∂Èó¥Êõ≤Á∫ø,Âπ∂ÁªèËøáÊª§Ê≥¢Á≠âÁ∫ßCFC60\
            \ Êª§Ê≥¢Êàñ100Hz‰ΩéÈÄöÊª§Ê≥¢„ÄÇÂÆûÈôÖËØïÈ™åÁªìÊûúÊ≥¢ÂΩ¢ÁöÑÁßØÂàÜÈÄüÂ∫¶ÂèòÂåñÈáèŒîvs( t)Â∫îÂú®‰ªªÊÑèÊó∂Âàª,‰∏çË∂ÖËøáÊåáÂÆöÊ≥¢ÂΩ¢ÁöÑÁßØÂàÜÈÄüÂ∫¶ÂèòÂåñÈáè[Œîvt( t)¬±1]km/hÁöÑËåÉÂõ¥„ÄÇ\\\
            nb) ÊåâÂõæB.1 ÁöÑÊ†áÂáÜÂä†ÈÄüÂ∫¶ÈÄöÈÅìËåÉÂõ¥ÂíåË°®B.1 ÁöÑÂèÇÊï∞ËøõË°åÂä†ÈÄüÊàñÂáèÈÄü,ÂÖ∂ÈÄüÂ∫¶ÂèòÂåñÈáèŒîv ‰∏∫\\n(25¬±1)km/h„ÄÇ\\nGB45672‚Äî2025ÂõæB.1\
            \ Ê≠£Èù¢Á¢∞ÊíûËá™Âä®Ëß¶ÂèëÂä†ÈÄüÂ∫¶ÈÄöÈÅìË°®B.1 Ê≠£Èù¢Á¢∞ÊíûËá™Âä®Ëß¶ÂèëÂä†ÈÄüÂ∫¶ÂèÇÊï∞\\nÁÇπ\\nÊó∂Èó¥t\\nms\\nÂä†ÈÄüÂ∫¶‰∏ãÈôê(√óg) ÁÇπ Êó∂Èó¥tms\\\
            nÂä†ÈÄüÂ∫¶‰∏äÈôê(√óg) A 15 0 E 0 3 B 45 10 F 40 17 C 60 10 G 63 17 D 85 0 H 105 0\"\
            ,\n                        \"children\": [],\n                       \
            \ \"full_path\": \"B.2 ËØïÈ™åÈ°πÁõÆ/B.2.1 Ê≠£Èù¢Á¢∞Êíû/B.2.1.1 ÊªëÂè∞Ê≠£Èù¢Á¢∞ÊíûËØïÈ™å/B.2.1.1.2 \"\n\
            \                      }\n                    ],\n                   \
            \ \"full_path\": \"B.2 ËØïÈ™åÈ°πÁõÆ/B.2.1 Ê≠£Èù¢Á¢∞Êíû/B.2.1.1 ÊªëÂè∞Ê≠£Èù¢Á¢∞ÊíûËØïÈ™å\"\n          \
            \        },\n                ],\n              },\n            ],\n  \
            \        }\n        ]\n      }\n    ]\n  }\n]\n```\n\n## ËæìÂá∫Á§∫‰æã1\n\n```\n\
            \  {\n      \"file\": \"regulation\",\n      \"section\": \"ÈôÑÂΩïB\",\n \
            \     \"experiment_root_ids\": [\"B.2.1.1\"]\n      \"chapters\":[\n \
            \         {\n      \"chapter_id\": \"B.2\",\n      \"topic_keywords\"\
            : [\"ËØïÈ™åÈ°πÁõÆ\", \"Ëá™Âä®Ëß¶ÂèëËØïÈ™åÊñπÊ≥ï\"],\n    },\n        {\n      \"chapter_id\":\
            \ \"B.2.1\",\n      \"topic_keywords\": [\"Ê≠£Èù¢Á¢∞Êíû\"],\n    },\n        {\n\
            \      \"chapter_id\": \"B.2.1.1\",\n      \"topic_keywords\": [\"ÊªëÂè∞Ê≠£Èù¢Á¢∞ÊíûËØïÈ™å\"\
            ],\n    },\n        {\n      \"chapter_id\": \"B.2.1.1.1\",\n      \"\
            topic_keywords\": [\"ÂÆâË£ÖÊñπÂêë\", \"Ê≠£Èù¢Á¢∞Êíû\"],\n      \"context_keywords\": [\"\
            ÁôΩËΩ¶Ë∫´\", \"Â∑•Ë£Ö\", \"Á¢∞ÊíûËØïÈ™åÊªëÂè∞\"],\n    },\n     {\n      \"chapter_id\": \"\
            B.2.1.1.2\",\n      \"paramaters\": [\n        {\n          \"item\":\
            \ \"ÈÄüÂ∫¶ÂèòÂåñÈáèŒîv\",\n          \"constraint\": \"=\",\n          \"value\"\
            : \"25\",\n          \"unit\": \"km/h\",\n          \"source_text\": \"\
            ÂÖ∂ÈÄüÂ∫¶ÂèòÂåñÈáèŒîv‰∏∫(25¬±1)km/h\"\n        }\n      ],\n      \"topic_keywords\":\
            \ [\"ÊªëÂè∞Ê≠£Èù¢Á¢∞ÊíûËØïÈ™å\",\"ÊªëÂè∞\",\"Âä†ÈÄüÂ∫¶Ê≥¢ÂΩ¢\", \"Á¢∞ÊíûËØïÈ™å\", \"Âä†ÈÄüÂ∫¶-Êó∂Èó¥Êõ≤Á∫ø\"],\n      \"context_keywords\"\
            : [\"CFC60Êª§Ê≥¢\", \"100Hz‰ΩéÈÄöÊª§Ê≥¢\"],\n      \"refs\": [\n        {\n      \
            \    \"ref_type\": \"internal\",\n          \"doc_id\": null,\n      \
            \    \"target_id\": \"B.2.1.2\",\n          \"anchor_text\": \"ÂÆûËΩ¶Á¢∞ÊíûËØïÈ™åÊù°‰ª∂-Âä†ÈÄüÂ∫¶Ê≥¢ÂΩ¢\"\
            \n        },\n        {\n          \"ref_type\": \"internal\",\n     \
            \     \"doc_id\": null,\n          \"target_id\": \"ÂõæB.1\",\n        \
            \  \"anchor_text\": \"\"\n        },\n        {\n          \"ref_type\"\
            : \"internal\",\n          \"doc_id\": null,\n          \"target_id\"\
            : \"Ë°®B.1\",\n          \"anchor_text\": \"\"\n        },\n      ],\n \
            \     \"table_headers\": [\"ÁÇπ\", \"Êó∂Èó¥t(ms)\", \"Âä†ÈÄüÂ∫¶‰∏ãÈôê(√óg)\", \"Âä†ÈÄüÂ∫¶‰∏äÈôê(√óg)\"\
            ]\n    }\n      ]\n  }\n\n```\n\n## ËæìÂÖ•Á§∫‰æã2\n\n```\n[\n  {\n    \"file\"\
            : \"regulation\",\n    \"sections\": [\n      {\n        \"section\":\
            \ \"MAIN\",\n        \"context\": \"\",\n        \"chapters\": [\n   \
            \       {\n            \"chapter_id\": \"1\",\n            \"chapter_title\"\
            : \"ËåÉÂõ¥\",\n            \"raw_text\": \"Êú¨Êñá‰ª∂ËßÑÂÆö‰∫ÜËΩ¶ËΩΩ‰∫ãÊïÖÁ¥ßÊÄ•ÂëºÂè´Á≥ªÁªüÁöÑÊäÄÊúØË¶ÅÊ±Ç„ÄÅÂêå‰∏ÄÂûãÂºèÂà§ÂÆöË¶ÅÊ±Ç,ÊèèËø∞‰∫ÜÁõ∏Â∫îÁöÑËØïÈ™åÊñπÊ≥ï„ÄÇ\\\
            nÊú¨Êñá‰ª∂ÈÄÇÁî®‰∫éM1 Á±ªÂèäN1 Á±ªËΩ¶ËæÜÁöÑËΩ¶ËΩΩ‰∫ãÊïÖÁ¥ßÊÄ•ÂëºÂè´Á≥ªÁªü„ÄÇ\",\n            \"children\": [],\n   \
            \         \"full_path\": \"1 ËåÉÂõ¥\"\n          },\n          {\n       \
            \     \"chapter_id\": \"2\",\n            \"chapter_title\": \"ËßÑËåÉÊÄßÂºïÁî®Êñá‰ª∂\"\
            ,\n            \"raw_text\": \"‰∏ãÂàóÊñá‰ª∂‰∏≠ÁöÑÂÜÖÂÆπÈÄöËøáÊñá‰∏≠ÁöÑËßÑËåÉÊÄßÂºïÁî®ËÄåÊûÑÊàêÊú¨Êñá‰ª∂ÂøÖ‰∏çÂèØÂ∞ëÁöÑÊù°Ê¨æ„ÄÇÂÖ∂‰∏≠,Ê≥®Êó•ÊúüÁöÑÂºïÁî®Êñá‰ª∂,‰ªÖËØ•Êó•ÊúüÂØπÂ∫îÁöÑÁâàÊú¨ÈÄÇÁî®‰∫éÊú¨Êñá‰ª∂;‰∏çÊ≥®Êó•ÊúüÁöÑÂºïÁî®Êñá‰ª∂,ÂÖ∂ÊúÄÊñ∞ÁâàÊú¨(ÂåÖÊã¨ÊâÄÊúâÁöÑ‰øÆÊîπÂçï)ÈÄÇÁî®‰∫éÊú¨Êñá‰ª∂„ÄÇ\\\
            nGB11551‚Äî2014 Ê±ΩËΩ¶Ê≠£Èù¢Á¢∞ÊíûÁöÑ‰πòÂëò‰øùÊä§ GB/T15089 Êú∫Âä®ËΩ¶ËæÜÂèäÊåÇËΩ¶ÂàÜÁ±ª GB16735 ÈÅìË∑ØËΩ¶ËæÜ ËΩ¶ËæÜËØÜÂà´‰ª£Âè∑(VIN)\
            \ GB20071‚Äî2025 Ê±ΩËΩ¶‰æßÈù¢Á¢∞ÊíûÁöÑ‰πòÂëò‰øùÊä§ GB20072‚Äî2024 ‰πòÁî®ËΩ¶ÂêéÁ¢∞ÊíûÂÆâÂÖ®Ë¶ÅÊ±Ç GB/T20913‚Äî2007 ‰πòÁî®ËΩ¶Ê≠£Èù¢ÂÅèÁΩÆÁ¢∞ÊíûÁöÑ‰πòÂëò‰øùÊä§\
            \ GB34660‚Äî2017 ÈÅìË∑ØËΩ¶ËæÜ ÁîµÁ£ÅÂÖºÂÆπÊÄßË¶ÅÊ±ÇÂíåËØïÈ™åÊñπÊ≥ï GB39732‚Äî2020 Ê±ΩËΩ¶‰∫ã‰ª∂Êï∞ÊçÆËÆ∞ÂΩïÁ≥ªÁªü GB/T43187‚Äî2023\
            \ ËΩ¶ËΩΩÊó†Á∫øÈÄö‰ø°ÁªàÁ´Ø GB/T45086.1‚Äî2024 ËΩ¶ËΩΩÂÆö‰ΩçÁ≥ªÁªüÊäÄÊúØË¶ÅÊ±ÇÂèäËØïÈ™åÊñπÊ≥ï Á¨¨1ÈÉ®ÂàÜ:Âç´ÊòüÂÆö‰Ωç GB/T45314‚Äî2025 ÈÅìË∑ØËΩ¶ËæÜÂÖçÊèêÈÄöËØùÂíåËØ≠Èü≥‰∫§‰∫íÊÄßËÉΩË¶ÅÊ±ÇÂèäËØïÈ™åÊñπÊ≥ï\"\
            ,\n            \"children\": [],\n            \"full_path\": \"2 ËßÑËåÉÊÄßÂºïÁî®Êñá‰ª∂\"\
            \n          },\n          {\n            \"chapter_id\": \"3\",\n    \
            \        \"chapter_title\": \"ÊúØËØ≠ÂíåÂÆö‰πâ\",\n            \"raw_text\": \"‰∏ãÂàóÊúØËØ≠ÂíåÂÆö‰πâÈÄÇÁî®‰∫éÊú¨Êñá‰ª∂„ÄÇ\"\
            ,\n            \"children\": [\n              {\n                \"chapter_id\"\
            : \"3.1\",\n                \"chapter_title\": \"ËΩ¶ËΩΩ‰∫ãÊïÖÁ¥ßÊÄ•ÂëºÂè´Á≥ªÁªü on-boardaccidentemergencycallsystem;AECS\"\
            ,\n                \"raw_text\": \"ÈÄöËøáËΩ¶ËæÜÂÜÖÈÉ®Á≠ñÁï•Âú®ÂèëÁîü‰∫ãÊïÖÊó∂Ëá™Âä®ÊøÄÊ¥ª,ÊàñÁî±ËΩ¶ÂÜÖ‰∫∫ÂëòËøõË°åÊâãÂä®Ëß¶ÂèëÂêé,Â∞ÜËΩ¶ËæÜÁöÑ‰ΩçÁΩÆÂèäËΩ¶ËæÜÁõ∏ÂÖ≥Áä∂ÊÄÅ‰ø°ÊÅØÂêåÊ≠•ÂèëÈÄÅÁªôÁ¥ßÊÄ•ÂëºÂè´ÊúçÂä°Âπ≥Âè∞Âπ∂Âª∫Á´ãËØ≠Èü≥ÈÄöËØùÁöÑÁ≥ªÁªü„ÄÇ\"\
            ,\n                \"children\": [],\n                \"full_path\": \"\
            3 ÊúØËØ≠ÂíåÂÆö‰πâ/3.1 ËΩ¶ËΩΩ‰∫ãÊïÖÁ¥ßÊÄ•ÂëºÂè´Á≥ªÁªü on-boardaccidentemergencycallsystem;AECS\"\n  \
            \            },\n              {\n                \"chapter_id\": \"3.2\"\
            ,\n                \"chapter_title\": \"ËΩ¶ËΩΩÊó†Á∫øÈÄö‰ø°ÁªàÁ´Ø on-boardwirelesscommunicationterminal\"\
            ,\n                \"raw_text\": \"ÂÆâË£ÖÂú®ËΩ¶ËæÜ‰∏ä,ËÉΩÂ§üÈÄöËøáËúÇÁ™ùÁßªÂä®ÈÄö‰ø°Á≠âÊäÄÊúØÂª∫Á´ãËøûÊé•Âπ∂ËøõË°å‰ø°ÊÅØ‰∫§‰∫íÁöÑÁîµÂ≠êËÆæÂ§á„ÄÇ\\\
            nÊ≥®:ËΩ¶ËΩΩÊó†Á∫øÈÄö‰ø°ÁªàÁ´ØÈÄöÂ∏∏Áî±ÂèëÂ∞ÑÊú∫„ÄÅÊé•Êî∂Êú∫„ÄÅÂ§©Á∫ø„ÄÅÊéßÂà∂Âô®ÂíåÁ∫øÁºÜÁ≠âÈÉ®‰ª∂ÊûÑÊàê„ÄÇ\\n[Êù•Ê∫ê:GB/T43187‚Äî2023,3.1]\",\n \
            \               \"children\": [],\n                \"full_path\": \"3\
            \ ÊúØËØ≠ÂíåÂÆö‰πâ/3.2 ËΩ¶ËΩΩÊó†Á∫øÈÄö‰ø°ÁªàÁ´Ø on-boardwirelesscommunicationterminal\"\n       \
            \       }\n            ],\n            \"full_path\": \"3 ÊúØËØ≠ÂíåÂÆö‰πâ\"\n  \
            \        }\n        ]\n      }\n    ]\n  }\n]\n```\n\n## ËæìÂá∫Á§∫‰æã2\n\n```\n\
            \  {\n      \"file\": \"regulation\",\n      \"section\": \"MAIN\",\n\
            \      \"experiment_root_ids\": []\n      \"chapters\":[\n          {\n\
            \            \"chapter_id\": \"1\",\n            \"topic_keywords\": [\n\
            \              \"ËåÉÂõ¥\"\n            ],\n          },\n          {\n   \
            \         \"chapter_id\": \"2\",\n            \"parameters\": [],\n  \
            \          \"topic_keywords\": [\n              \"ËßÑËåÉÊÄßÂºïÁî®Êñá‰ª∂\"\n        \
            \    ],\n          },\n          {\n            \"chapter_id\": \"3\"\
            ,\n            \"topic_keywords\": [\n              \"ÊúØËØ≠ÂíåÂÆö‰πâ\"\n      \
            \      ],\n          },\n          {\n            \"chapter_id\": \"3.1\"\
            ,\n            \"topic_keywords\": [\n              \"ËΩ¶ËΩΩ‰∫ãÊïÖÁ¥ßÊÄ•ÂëºÂè´Á≥ªÁªü\"\n \
            \           ],\n          },\n          {\n            \"chapter_id\"\
            : \"3.2\",\n            \"topic_keywords\": [\n              \"ËΩ¶ËΩΩÊó†Á∫øÈÄö‰ø°ÁªàÁ´Ø\"\
            \n            ],\n            \"refs\": [\n              {\n         \
            \       \"ref_type\": \"external\",\n                \"doc_id\": \"GB/T43187‚Äî2023\"\
            ,\n                \"target_id\": \"3.1\",\n                \"anchor_text\"\
            : \"ËΩ¶ËΩΩÊó†Á∫øÈÄö‰ø°ÁªàÁ´ØÂÆö‰πâ\"\n              }\n            ],\n          },\n    \
            \  ]\n  }\n\n```\n\n"
        - id: 2e89206c-ded1-462d-9db7-2832e587c8ad
          role: user
          text: '{{#1756557801310.item#}}'
        selected: false
        title: LLM 2
        type: llm
        variables: []
        vision:
          enabled: false
      height: 89
      id: '1756557804693'
      parentId: '1756557801310'
      position:
        x: 204
        y: 80
      positionAbsolute:
        x: 842
        y: 539
      selected: true
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
      zIndex: 1002
    - data:
        code: "import json\nimport re\n\nimport json\nimport ast\nimport re\nfrom\
          \ typing import Dict, List, Any\n\ndef parse_input(src: str) -> List[Dict[str,\
          \ Any]]:\n    \"\"\"È¢ÑÂ§ÑÁêÜÂπ∂Ëß£ÊûêËæìÂÖ•Â≠óÁ¨¶‰∏≤‰∏∫ÂàóË°®Â≠óÂÖ∏ÁªìÊûÑ\"\"\"\n    try:\n        # 1. Áõ¥Êé•Êåâ\
          \ </think> ÂàáÂàÜÂπ∂ÂèñÂêéÂçäÊÆµ\n        src = src.rsplit('</think>', 1)[-1].strip()\n\
          \n        try:\n            data = json.loads(src)\n        except json.JSONDecodeError:\n\
          \            try:\n                if src.startswith('\\ufeff'):\n     \
          \               src = src[1:]\n                data = ast.literal_eval(src)\n\
          \            except (SyntaxError, ValueError):\n                try:\n \
          \                   processed = src.replace('\\\\\"', '\"').replace('\\\\\
          n', '\\n')\n                    data = json.loads(processed)\n         \
          \       except Exception as e:\n                    raise ValueError(f\"\
          Êó†Ê≥ïËß£ÊûêÔºö{str(e)}\")\n\n        # ‰øùËØÅ‰∏∫ list\n        if isinstance(data, dict):\n\
          \            return [data]\n        elif isinstance(data, list):\n     \
          \       return data\n        else:\n            return []\n    except Exception\
          \ as e:\n        raise ValueError(f\"È¢ÑÂ§ÑÁêÜÂ§±Ë¥•: {str(e)}\")\n        # return\
          \ []\n\n\ndef main(arg1: str) -> dict:\n    data = parse_input(arg1)\n\n\
          \    return {\n        \"result\":json.dumps(data, ensure_ascii=False),\n\
          \    }"
        code_language: python3
        desc: ''
        error_strategy: fail-branch
        isInIteration: true
        isInLoop: false
        iteration_id: '1756557801310'
        outputs:
          result:
            children: null
            type: string
        selected: false
        title: ‰ª£Á†ÅÊâßË°å 2
        type: code
        variables:
        - value_selector:
          - '1756557804693'
          - text
          value_type: string
          variable: arg1
      height: 89
      id: '1756557929661'
      parentId: '1756557801310'
      position:
        x: 508
        y: 80
      positionAbsolute:
        x: 1146
        y: 539
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
      zIndex: 1002
    - data:
        answer: '{{#1756613329065.output#}}'
        desc: ''
        selected: false
        title: Áõ¥Êé•ÂõûÂ§ç 2
        type: answer
        variables: []
      height: 104
      id: '1756558073574'
      position:
        x: 4206
        y: 459
      positionAbsolute:
        x: 4206
        y: 459
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        code: "import json\nimport re\nimport os\n\nimport json\nimport ast\nimport\
          \ re\nfrom typing import Dict, List, Any\n\ndef parse_input(src: str) ->\
          \ List[Dict[str, Any]]:\n    \"\"\"È¢ÑÂ§ÑÁêÜÂπ∂Ëß£ÊûêËæìÂÖ•Â≠óÁ¨¶‰∏≤‰∏∫ÂàóË°®Â≠óÂÖ∏ÁªìÊûÑ\"\"\"\n    try:\n\
          \        # ÂéªÈô§ LLM ÊÄùÁª¥ÈìæÊ†áËÆ∞\n        try: \n            src = re.sub(r'<think>.*?</think>',\
          \ '', src, flags=re.DOTALL).strip()\n        except:\n            pass\n\
          \n        try:\n            data = json.loads(src)\n        except json.JSONDecodeError:\n\
          \            try:\n                if src.startswith('\\ufeff'):\n     \
          \               src = src[1:]\n                data = ast.literal_eval(src)\n\
          \            except (SyntaxError, ValueError):\n                try:\n \
          \                   processed = src.replace('\\\\\"', '\"').replace('\\\\\
          n', '\\n')\n                    data = json.loads(processed)\n         \
          \       except Exception as e:\n                    raise ValueError(f\"\
          Êó†Ê≥ïËß£ÊûêÔºö{str(e)}\")\n\n        # ‰øùËØÅ‰∏∫ list\n        if isinstance(data, dict):\n\
          \            return [data]\n        elif isinstance(data, list):\n     \
          \       return data\n        else:\n            return []\n    except Exception\
          \ as e:\n        raise ValueError(f\"È¢ÑÂ§ÑÁêÜÂ§±Ë¥•: {str(e)}\")\n        # return\
          \ []\n\ndef find_chapters_and_children_by_ids(tree, target_file, target_section,\
          \ experiment_root_ids):\n    \"\"\"\n    Ê†πÊçÆ experiment_root_ids ÊâæÂà∞ÂØπÂ∫îÁöÑÁ´†ËäÇÂèäÂÖ∂ÊâÄÊúâÂ≠êÁ´†ËäÇ\n\
          \    \n    Args:\n        tree: ÂÆåÊï¥ÁöÑÁ´†ËäÇÊ†ëÁªìÊûÑ\n        target_file: ÁõÆÊ†áÊñá‰ª∂Âêç (Â¶Ç\
          \ \"regulation\")\n        target_section: ÁõÆÊ†ásectionÂêç (Â¶Ç \"ÈôÑÂΩïB\")\n    \
          \    experiment_root_ids: Ë¶ÅÊü•ÊâæÁöÑÁ´†ËäÇIDÂàóË°® (Â¶Ç [\"B.2.1.1\",\"B.2.4.1\"])\n   \
          \ \n    Returns:\n        List[Dict]: ÊØè‰∏™itemÂåÖÂê´ÊâæÂà∞ÁöÑÁ´†ËäÇÂèäÂÖ∂ÊâÄÊúâÂ≠êÁ´†ËäÇ\n    \"\"\"\n\
          \    \n    def get_chapter_with_all_children(chapter):\n        \"\"\"ÈÄíÂΩíËé∑ÂèñÁ´†ËäÇÂèäÂÖ∂ÊâÄÊúâÂ≠êÁ´†ËäÇ\"\
          \"\"\n        result = {\n            \"chapter_id\": chapter[\"chapter_id\"\
          ],\n            \"chapter_title\": chapter[\"chapter_title\"],\n       \
          \     \"raw_text\": chapter.get(\"raw_text\", \"\"),\n            \"full_path\"\
          : chapter.get(\"full_path\", \"\"),\n            \"children\": []\n    \
          \    }\n        \n        # Â¶ÇÊûúÊúâÂ≠êÁ´†ËäÇÔºåÈÄíÂΩíËé∑Âèñ\n        if \"children\" in chapter\
          \ and chapter[\"children\"]:\n            for child in chapter[\"children\"\
          ]:\n                result[\"children\"].append(get_chapter_with_all_children(child))\n\
          \        \n        return result\n    \n    def find_chapter_by_id(chapters,\
          \ target_id):\n        \"\"\"Âú®Á´†ËäÇÂàóË°®‰∏≠ÈÄíÂΩíÊü•ÊâæÊåáÂÆöIDÁöÑÁ´†ËäÇ\"\"\"\n        for chapter\
          \ in chapters:\n            if chapter[\"chapter_id\"] == target_id:\n \
          \               return chapter\n\n            # Âú®Â≠êÁ´†ËäÇ‰∏≠ÈÄíÂΩíÊü•Êâæ\n            if\
          \ \"children\" in chapter and chapter[\"children\"]:\n                found\
          \ = find_chapter_by_id(chapter[\"children\"], target_id)\n             \
          \   if found:\n                    return found\n        \n        return\
          \ None\n    \n    result = []\n    \n    # ÈÅçÂéÜÊ†ëÁªìÊûÑÔºåÊâæÂà∞ÁõÆÊ†áÊñá‰ª∂Âíåsection\n    for\
          \ file_item in tree:\n        if file_item[\"file\"] != target_file:\n \
          \           continue\n            \n        for section in file_item[\"\
          sections\"]:\n            if section[\"section\"] != target_section:\n \
          \               continue\n            \n            # Âú® find_chapters_and_children_by_ids\
          \ ÈáåÔºåÊâæÂà∞ÁõÆÊ†á section ÂêéÂä†Ôºö\n            if experiment_root_ids == [\"ALL\"]:\n\
          \                # ÊääÁ¨¨‰∏ÄÂ±ÇÊâÄÊúâ chapter_id ÂèñÂá∫Êù•\n                experiment_root_ids\
          \ = [ch[\"chapter_id\"] for ch in section.get(\"chapters\", [])]\n\n   \
          \         # Âú®ËØ•section‰∏≠Êü•ÊâæÊØè‰∏™experiment_root_id\n            for root_id in\
          \ experiment_root_ids:\n                found_chapter = find_chapter_by_id(section[\"\
          chapters\"], root_id)\n                if found_chapter:\n             \
          \       # Ëé∑ÂèñËØ•Á´†ËäÇÂèäÂÖ∂ÊâÄÊúâÂ≠êÁ´†ËäÇ\n                    chapter_with_children = get_chapter_with_all_children(found_chapter)\n\
          \                    result.append(chapter_with_children)\n            \
          \    else:\n                    print(f\"Ë≠¶Âëä: Êú™ÊâæÂà∞Á´†ËäÇID '{root_id}' Âú® {target_file}/{target_section}\
          \ ‰∏≠\")\n    \n    return result\n\ndef extract_experiment_chapters(tree,\
          \ experiment_info):\n    \"\"\"\n    Ê†πÊçÆÂÆûÈ™å‰ø°ÊÅØÊèêÂèñÂØπÂ∫îÁöÑÁ´†ËäÇ\n    \n    Args:\n  \
          \      tree: ÂÆåÊï¥ÁöÑÁ´†ËäÇÊ†ë\n        experiment_info: ÂåÖÂê´ file, section, experiment_root_ids\
          \ ÁöÑÂ≠óÂÖ∏\n    \n    Returns:\n        List[Dict]: ÂÆûÈ™åÁõ∏ÂÖ≥ÁöÑÁ´†ËäÇÂàóË°®\n    \"\"\"\n \
          \   return find_chapters_and_children_by_ids(\n        tree=tree,\n    \
          \    target_file=experiment_info[\"file\"],\n        target_section=experiment_info[\"\
          section\"],\n        experiment_root_ids=experiment_info[\"experiment_root_ids\"\
          ]\n    )\n\ndef main(arg1: list[str], arg2: str) -> dict:\n    \"\"\"\n\
          \    Â§ÑÁêÜÂÆûÈ™åÊï∞ÊçÆÁöÑ‰∏ªÂáΩÊï∞\n    \n    Args:\n        arg1: JSONÂ≠óÁ¨¶‰∏≤ÂàóË°®ÔºåÊØè‰∏™ÂåÖÂê´ file, section,\
          \ experiment_root_ids\n        arg2: Á´†ËäÇÊ†ëÁöÑJSONÂ≠óÁ¨¶‰∏≤\n    \n    Returns:\n \
          \       dict: Â§ÑÁêÜÂêéÁöÑÂÆûÈ™åÁ´†ËäÇÊï∞ÊçÆ\n    \"\"\"\n    import json\n    from collections\
          \ import defaultdict\n    \n    # Ëß£ÊûêËæìÂÖ•Êï∞ÊçÆ\n    experiment_data = []\n   \
          \ solved_tree = []\n    for t in arg1:\n        items = json.loads(t)\n\
          \        for item in items:\n            experiment_data.append({\n    \
          \            'file': item['file'],\n                'section': item['section'],\n\
          \                'experiment_root_ids': item['experiment_root_ids']\n  \
          \          })\n            solved_tree.append({\n                'file':\
          \ item['file'],\n                'section': item['section'],\n         \
          \       'chapters':item['chapters']\n            })\n    \n    # Êåâ file\
          \ Âíå section ÂêàÂπ∂Áõ∏ÂêåÁöÑÈ°πÁõÆ\n    merged_experiments = defaultdict(list)\n    for\
          \ item in experiment_data:\n        key = (item['file'], item['section'])\n\
          \        merged_experiments[key].extend(item['experiment_root_ids'])\n \
          \   \n    # ÂéªÈáçÂπ∂ÁîüÊàêÊúÄÁªàÁöÑ experiment_info ÂàóË°®\n    experiment_infos = []\n   \
          \ for (file, section), root_ids in merged_experiments.items():\n       \
          \ # ÂéªÈáç‰ΩÜ‰øùÊåÅÈ°∫Â∫è\n        unique_root_ids = []\n        seen = set()\n      \
          \  for root_id in root_ids:\n            if root_id not in seen:\n     \
          \           unique_root_ids.append(root_id)\n                seen.add(root_id)\n\
          \        if len(unique_root_ids):\n            experiment_infos.append({\n\
          \                'file': file,\n                'section': section,\n  \
          \              'experiment_root_ids': unique_root_ids\n            })\n\
          \    \n    # 1. ÂÖàÊää solved_tree Êåâ (file, section) ÂÅöÂàÜÁªÑ\n    merged_chapters\
          \ = defaultdict(list)\n    for node in solved_tree:\n        key = (node['file'],\
          \ node['section'])\n        merged_chapters[key].extend(node['chapters'])\n\
          \n    # 2. Âú®ÊØè‰∏™ÂàÜÁªÑÂÜÖÂØπ chapters ÂéªÈáç‰∏î‰øùÊåÅÂéüÈ°∫Â∫è\n    final_tree = []\n    for (file,\
          \ section), chapters in merged_chapters.items():\n        seen_id = set()\n\
          \        unique_chapters = []\n        for c in chapters:\n            cid\
          \ = c.get(\"chapter_id\")\n            if cid not in seen_id:\n        \
          \        unique_chapters.append(c)\n                seen_id.add(cid)\n \
          \       if unique_chapters:\n            final_tree.append({\n         \
          \       \"file\": file,\n                \"section\": section,\n       \
          \         \"chapters\": unique_chapters\n            })\n\n    # Ëß£ÊûêÁ´†ËäÇÊ†ë\n\
          \    regulation_tree = parse_input(arg2)\n    \n    # print(experiment_infos)\n\
          \n\n    # Â§ÑÁêÜÊØè‰∏™ÂÆûÈ™å‰ø°ÊÅØÔºåÊèêÂèñÂØπÂ∫îÁöÑÁ´†ËäÇ\n    result = []\n    for i, experiment_info\
          \ in enumerate(experiment_infos):\n        chapters_with_hierarchy = extract_experiment_chapters(regulation_tree,\
          \ experiment_info)\n        \n        result.append(json.dumps(chapters_with_hierarchy,\
          \ ensure_ascii=False))\n    \n    # 2. ÁõÆÊ†áË∑ØÂæÑ\n    path = \"/tmp/mydata/final_tree.json\"\
          \n\n    os.makedirs(os.path.dirname(path), exist_ok=True)\n    with open(path,\
          \ \"w\", encoding=\"utf-8\") as f:\n        json.dump(final_tree, f, ensure_ascii=False,\
          \ indent=2)\n\n    return {\n        \"chapters\":result,\n        \"experiment_infos\"\
          :json.dumps(experiment_infos, ensure_ascii=False),\n        # \"regulation_tree\"\
          :str(regulation_tree),\n        \"final_tree\":json.dumps(final_tree, ensure_ascii=False),\n\
          \    }"
        code_language: python3
        desc: ''
        outputs:
          chapters:
            children: null
            type: array[string]
          experiment_infos:
            children: null
            type: string
          final_tree:
            children: null
            type: string
        selected: false
        title: ‰ª£Á†ÅÊâßË°å 3
        type: code
        variables:
        - value_selector:
          - '1756557801310'
          - output
          value_type: array[string]
          variable: arg1
        - value_selector:
          - '1756550411122'
          - tree
          value_type: string
          variable: arg2
      height: 53
      id: '1756563307317'
      position:
        x: 2422
        y: 459
      positionAbsolute:
        x: 2422
        y: 459
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        code: "\ndef main(arg1: str) -> dict:\n    return {\n        \"array\": [arg1],\n\
          \    }\n"
        code_language: python3
        desc: ''
        outputs:
          array:
            children: null
            type: array[string]
        selected: false
        title: ‰ª£Á†ÅÊâßË°å 4
        type: code
        variables:
        - value_selector:
          - '1756550268945'
          - array
          value_type: string
          variable: arg1
      height: 53
      id: '1756567749405'
      position:
        x: 30
        y: 828
      positionAbsolute:
        x: 30
        y: 828
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        code: "\ndef main(arg1: list[str]) -> dict:\n    return {\n        \"result\"\
          :json.dumps(arg1, ensure_ascii=False)\n    }\n"
        code_language: python3
        desc: ''
        outputs:
          result:
            children: null
            type: string
        selected: false
        title: ‰ª£Á†ÅÊâßË°å 5
        type: code
        variables:
        - value_selector:
          - '1756557801310'
          - output
          value_type: array[string]
          variable: arg1
      height: 53
      id: '1756568067080'
      position:
        x: 30
        y: 921
      positionAbsolute:
        x: 30
        y: 921
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        desc: ''
        error_handle_mode: continue-on-error
        height: 209
        is_parallel: true
        iterator_input_type: array[string]
        iterator_selector:
        - '1756563307317'
        - chapters
        output_selector:
        - '1756613456815'
        - result
        output_type: array[string]
        parallel_nums: 10
        selected: false
        start_node_id: 1756613329065start
        title: Ëø≠‰ª£ 2
        type: iteration
        width: 812
      height: 209
      id: '1756613329065'
      position:
        x: 2726
        y: 459
      positionAbsolute:
        x: 2726
        y: 459
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 812
      zIndex: 1
    - data:
        desc: ''
        isInIteration: true
        selected: false
        title: ''
        type: iteration-start
      draggable: false
      height: 48
      id: 1756613329065start
      parentId: '1756613329065'
      position:
        x: 60
        y: 80.5
      positionAbsolute:
        x: 2786
        y: 539.5
      selectable: false
      sourcePosition: right
      targetPosition: left
      type: custom-iteration-start
      width: 44
      zIndex: 1002
    - data:
        context:
          enabled: true
          variable_selector:
          - '1756550411122'
          - context
        desc: ''
        isInIteration: true
        isInLoop: false
        iteration_id: '1756613329065'
        model:
          completion_params: {}
          mode: chat
          name: deepseek-reasoner
          provider: langgenius/deepseek/deepseek
        prompt_template:
        - id: 8e7bb797-e961-48ec-9029-47c638adcc67
          role: system
          text: "# ËßíËâ≤ÔºàRoleÔºâ\n‰Ω†ÊòØ‰∏ÄÂêç **Ê†áÂáÜÊµãËØïÁî®‰æãËÆæËÆ°‰∏ìÂÆ∂**ÔºåÁ≤æÈÄöÊ±ΩËΩ¶ÂèäÁõ∏ÂÖ≥È¢ÜÂüüÁöÑÊ†áÂáÜÂíåÊ≥ïËßÑÊñáÊ°£„ÄÇ  \n‰Ω†ÁöÑËÅåË¥£ÊòØÂ∞ÜÊ†áÂáÜÊàñÊ≥ïËßÑÊñá‰ª∂‰∏≠ÁöÑÊù°Ê¨æÂíåËØïÈ™åÊñπÊ≥ï\
            \ **‰∏•Ê†ºÊãÜËß£‰∏∫ÁªìÊûÑÂåñÊµãËØïÁî®‰æã**ÔºåËæìÂá∫ÁªìÊûúÁî®‰∫éÔºö  \n- Ê†áÂáÜÂíåÊ≥ïËßÑÁöÑÊØîÂØπÂàÜÊûê  \n- ÊµãËØïÊú∫ÊûÑËÆæÂ§áËÉΩÂäõÈ™åËØÅÂíåË¶ÜÁõñÁéáËØÑ‰º∞  \n-\
            \ Ëá™Âä®ÂåñÊµãËØïÁî®‰æãÁÆ°ÁêÜÁ≥ªÁªü  \nË¶ÅÊ±Ç‰ø°ÊÅØÂÆåÊï¥„ÄÅÊù°ÁêÜÊ∏ÖÊô∞„ÄÅÂ≠óÊÆµËßÑËåÉÂåñÔºåÁ°Æ‰øùÁªìÊûÑÂåñÁªìÊûúÂÖ∑Â§áÂèØËøΩÊ∫ØÊÄßÂíåÂèØÊâßË°åÊÄß„ÄÇ\n\n---\n\n# ËæìÂÖ•Ê†ºÂºèÔºàInput\
            \ FormatÔºâ\nËæìÂÖ•‰∏∫‰∏Ä‰∏™ JSON Êï∞ÁªÑÔºåÊØè‰∏™ÂÖÉÁ¥†Ë°®Á§∫‰∏Ä‰∏™Á´†ËäÇÔºö\n```json\n[\n  {\n    \"chapter_id\"\
            : \"string\",      // Á´†ËäÇÁºñÂè∑\n    \"chapter_title\": \"string\",   // Á´†ËäÇÊ†áÈ¢ò\n\
            \    \"raw_text\": \"string\",        // Êú¨Á´†ËäÇÊ≠£Êñá\n    \"children\": [  \
            \              // Â≠êÁ´†ËäÇÊï∞ÁªÑÔºàÈÄíÂΩíÁªìÊûÑÔºâ\n      { ...ÂêåÁªìÊûÑ... }\n    ],\n    \"full_path\"\
            : \"string\"        // ‰ªéÊ†πÂà∞ËØ•Á´†ËäÇÁöÑË∑ØÂæÑ\n  }\n]\n```\nËØ¥ÊòéÔºö\n- ‰Ω†‰πüÂèØ‰ª•‰ªé {{#context#}}\
            \ ‰∏≠ÁêÜËß£Ê≠§Ê†áÂáÜÁöÑÁõ∏ÂÖ≥‰ø°ÊÅØ\n- `raw_text` ÂèØËÉΩ‰∏∫Á©∫ÔºåÊ≠£ÊñáÂèØËÉΩÂú® `children` ÂÜÖ„ÄÇ\n- Âêå‰∏ÄÁ´†ËäÇÂèØÂåÖÂê´Â§ö‰∏™ÂÆûÈ™åÊñπÊ≥ïÔºõÊØè‰∏™ÂÆûÈ™åÊñπÊ≥ïÈúÄÂçïÁã¨ÊãÜËß£ÊàêÂØπË±°„ÄÇ‰∏çÂêåÁ´†ËäÇÈó¥ÂêÑËá™Áã¨Á´ãÔºå‰∏çË¶ÅÁõ∏‰∫íÊé®Êñ≠\n\
            - ‰∏Ä‰∏™ÂÆûÈ™åÊñπÊ≥ï‰∏≠Â≠òÂú®Â§öÁßçËØïÈ™åÊñπÊ°àÊó∂Ôºå‰πüË¶ÅÂàÜÂà´ÊãÜËß£‰∏∫Â§ö‰∏™ÂØπË±°ÔºàÂëΩÂêçÊó∂Âú® `test_name` ÂêéÊ∑ªÂä†ÊñπÊ°àÊ†áËØÜÔºåÂ¶Ç‚Äú(XXÊñπÊ°à)‚ÄùÔºâ„ÄÇ\n\n\
            ---\n\n# ËæìÂá∫Ê†ºÂºèÔºàOutput FormatÔºâ\nËæìÂá∫‰∏∫‰∏Ä‰∏™ JSON Êï∞ÁªÑÔºåÂØπÂ∫îËæìÂÖ•ÁöÑÁ´†ËäÇÊï∞ÁªÑÔºåÊØè‰∏™Á´†ËäÇÂØπÂ∫î‰∏Ä‰∏™Êï∞ÁªÑÔºåÊï∞ÁªÑ‰∏≠ÁöÑÊØè‰∏™ÂØπË±°‰ª£Ë°®‰∏Ä‰∏™Áã¨Á´ãÁöÑÂÆûÈ™åÊàñÂÆûÈ™åÊñπÊ°àÔºö\n\
            \n```json\n[\n  [\n    {\n      \"chapter_id\": \"string\",    // Êù•Ê∫êÁ´†ËäÇÁöÑchapter_idÔºå‰øùËØÅÂèØËøΩÊ∫ØÊÄß\n\
            \      \"test_name\": \"string\",     // ÊµãËØïÂêçÁß∞Ôºå‰ªéÁ´†ËäÇÊ†áÈ¢òÊàñË∑ØÂæÑÊèêÁÇºÔºåÁÆÄÊ¥Å‰∏îÂîØ‰∏Ä\n     \
            \ \"conditions\": [            // ÊµãËØïÊù°‰ª∂ÔºöËØï‰ª∂Áä∂ÊÄÅ„ÄÅÂÆâË£Ö„ÄÅÁéØÂ¢ÉÁ≠âÔºåÂÆåÊï¥ÂàÜÁÇπÂàóÂá∫\n        \"\
            string\"\n      ],\n      \"criteria\": [              // Âà§ÂÆöÊ†áÂáÜÔºöÊòéÁ°ÆÊ£ÄÊµãÈ°π+ËææÊ†áÊù°‰ª∂ÔºåÂàÜÁÇπÂàóÂá∫\n\
            \        \"string\"\n      ],\n      \"equipment\": [             // ÊâÄÈúÄËÆæÂ§áÂèäËßÑÊ†ºÔºå‰æø‰∫éËÆæÂ§áËÉΩÂäõÈ™åËØÅ\n\
            \        {\n          \"name\": \"string\",          // ËÆæÂ§áÂêçÁß∞\n       \
            \   \"specification\": \"string\"  // ËÆæÂ§áËßÑÊ†º/ÊÄßËÉΩ\n        }\n      ],\n \
            \     \"parameters\": [            // ÊèêÂèñÁöÑÂÖ≥ÈîÆÂèÇÊï∞\n        {\n          \"\
            item\": \"string\",          // ÂèÇÊï∞È°π\n          \"constraint\": \"string\"\
            ,    // Á∫¶ÊùüÔºö<=|<|=|>=|>|range_closed|range_open|enum|boolean\n        \
            \  \"value\": \"string|array|null\", // ÂèÇÊï∞ÂÄºÔºåÂèØ‰∏∫Êï∞Â≠ó„ÄÅËåÉÂõ¥ÊàñÊûö‰∏æ\n          \"unit\"\
            : \"string|null\",     // Âçï‰ΩçÔºåËã•Êó†Âàônull\n          \"source_text\": \"string\"\
            \    // ÂéüÊñáÂÆåÊï¥ÁâáÊÆµÔºå‰øùÁïôÁ¨¶Âè∑Ôºå‰æø‰∫éËøΩÊ∫Ø\n        }\n      ],\n      \"refs\": [     \
            \             // ÂºïÁî®‰ø°ÊÅØ\n        {\n          \"ref_type\": \"internal|external\"\
            , // ÂÜÖÈÉ®/Â§ñÈÉ®ÂºïÁî®\n          \"doc_id\": \"string|null\",         // Â§ñÈÉ®Ê†áÂáÜÁºñÂè∑ÔºõÂÜÖÈÉ®ÂºïÁî®Â°´null\n\
            \          \"target_id\": \"string\",           // ÂºïÁî®ÁõÆÊ†áÁºñÂè∑ÔºàÂ¶Ç‚ÄúB.2.1.1‚Äù„ÄÅ‚ÄúË°®B.1‚ÄùÔºâ\n\
            \          \"anchor_text\": \"string\"          // ÂºïÁî®ÁöÑÁÆÄË¶Å‰∏ä‰∏ãÊñáÊèèËø∞\n      \
            \  }\n      ]\n    }\n  ]\n]\n```\n\n---\n\n# Â≠óÊÆµËØ¥Êòé\n- **chapter_id**ÔºöÊ†áËÆ∞Êù•Ê∫êÁ´†ËäÇÔºå‰øùËØÅÂèØËøΩÊ∫ØÊÄß„ÄÇ\
            \  \n- **test_name**ÔºöÂîØ‰∏ÄÊ†áËØÜËØ•ÂÆûÈ™åÊñπÊ≥ïÔºåËã•Âêå‰∏ÄÂÆûÈ™åÊúâÂ§öÊñπÊ°àÔºåÂú®ÂêéÂä†Êã¨Âè∑Ê†áÊòéÊñπÊ°àÔºåÂ¶Ç‚Äú(ÊñπÊ°àA)‚Äù„ÄÇ  \n- **conditions**ÔºöÂÆåÊï¥ÂàÜÁÇπÂàóÂá∫ÊµãËØïÂâçÁΩÆÊù°‰ª∂„ÄÅÂÆâË£ÖÁä∂ÊÄÅ„ÄÅÁéØÂ¢ÉË¶ÅÊ±ÇÁ≠â„ÄÇ\
            \  \n- **criteria**ÔºöÊØèÊù°ÂøÖÈ°ªÂåÖÂê´Ê£ÄÊµãÂØπË±°‰∏éÂà§ÂÆöÊù°‰ª∂ÔºåÈÅøÂÖçÁ¨ºÁªüÊèèËø∞„ÄÇ  \n- **equipment**ÔºöÂÆåÊï¥ÂàóÂá∫ÊâÄÊúâËÆæÂ§áÂèäÂÖ≥ÈîÆËßÑÊ†º„ÄÇ\
            \  \n- **parameters**ÔºöÊèêÂèñÊâÄÊúâÂÆöÈáèÊåáÊ†áÔºå‰∏≠ÊñáÊï∞Â≠óËΩ¨ÈòøÊãâ‰ºØÊï∞Â≠ó„ÄÇ  \n- **refs**ÔºöÊèêÂèñÁ´†ËäÇÂÜÖÁöÑÊ†áÂáÜÂºïÁî®Ôºö\n\
            \  - ÂÜÖÈÉ®ÂºïÁî®ÔºöÊåáÂêëÊú¨Ê†áÂáÜÁöÑÁ´†ËäÇ„ÄÅË°®„ÄÅÂõæÔºå`ref_type=internal`Ôºå`doc_id=null`„ÄÇ\n  - Â§ñÈÉ®ÂºïÁî®ÔºöÊåáÂêëÂ§ñÈÉ®Ê†áÂáÜÔºå`ref_type=external`ÔºåÂ°´ÂÜôÊ†áÂáÜÁºñÂè∑„ÄÇ\n\
            \  - Â§ö‰∏™ÂºïÁî®ÂèØÂêàÂπ∂Êàê‰∏ÄÊù°ËÆ∞ÂΩïÔºàÂ¶Ç‚ÄúË°®B.1„ÄÅÂõæB.1‚ÄùÂèØ‰∏∫‰∏Ä‰∏™Êù°ÁõÆÔºâ„ÄÇ\n\n---\n\n# Â§ÑÁêÜÈÄªËæëÔºàChain-of-ThoughtÔºâ\n\
            1. **Ëß£ÊûêÁ´†ËäÇÂÜÖÂÆπ**ÔºöÈÄíÂΩíÈÅçÂéÜ`children`ÔºåÂêàÂπ∂ÊâÄÊúâÊ≠£Êñá„ÄÇ  \n2. **ÊãÜÂàÜÂÆûÈ™åÊñπÊ≥ï‰∏éÊñπÊ°à**ÔºöÊ†πÊçÆÊèèËø∞ÈÄªËæë„ÄÅÂ∫èÂè∑„ÄÅa)/b)ÂàÜÈ°πÁ≠âÊãÜÊàêÂ§ö‰∏™ÂØπË±°„ÄÇ\
            \  \n3. **ÊèêÂèñÁ´†ËäÇID**ÔºöÂÜôÂÖ•ÊØè‰∏™ÂØπË±°ÁöÑ`chapter_id`Â≠óÊÆµ„ÄÇ  \n4. **ÊèêÂèñÂÆûÈ™åÂêçÁß∞**Ôºö‰ºòÂÖàÁ´†ËäÇÊ†áÈ¢òÔºåÁªìÂêàË∑ØÂæÑÁÆÄÂåñÔºåÂøÖË¶ÅÊó∂Ê†áÊòéÊñπÊ°à„ÄÇ\
            \  \n5. **Ëß£ÊûêÂÆûÈ™åÊù°‰ª∂**Ôºö\n   - ÂàóÂá∫ÂÆâË£ÖÊñπÂºè„ÄÅËØï‰ª∂Áä∂ÊÄÅ„ÄÅÁéØÂ¢ÉÊù°‰ª∂„ÄÅÁîµÊ∫êÁä∂ÊÄÅÁ≠â„ÄÇ\n   - Âç≥‰Ωø‰∏éÂèÇÊï∞ÈáçÂ§çÔºå‰πüÈúÄÂÆåÊï¥‰øùÁïôÔºå‰øùËØÅ‰∏ä‰∏ãÊñáÂÆåÊï¥ÊÄß„ÄÇ\
            \  \n6. **ÊèêÂèñÂà§ÂÆöÊ†áÂáÜ**Ôºö\n   - ÂàÜÁÇπÊèèËø∞Ê£ÄÊµãÈ°π+Âà§ÂÆöÊù°‰ª∂ÔºåÈÅøÂÖç‚ÄúÁ¨¶ÂêàË¶ÅÊ±Ç‚ÄùËøôÁßçÊ®°Á≥äË°®Ëø∞„ÄÇ  \n7. **ÊèêÂèñËÆæÂ§á**Ôºö\n\
            \   - ÊçïÊçâÊâÄÊúâÊèêÂèäÁöÑËÆæÂ§á‰∏éÂÖ≥ÈîÆËßÑÊ†º„ÄÇ  \n8. **ÊèêÂèñÂèÇÊï∞**Ôºö\n   - ÊâÄÊúâÂÆöÈáèÂÄº„ÄÅËåÉÂõ¥„ÄÅÁ≤æÂ∫¶Ë¶ÅÊ±ÇÁ≠âÔºå‰∏•Ê†ºÊ†áÊ≥®Á∫¶ÊùüÁ±ªÂûãÂíåÂçï‰Ωç„ÄÇ\n\
            \   - ‰∏≠ÊñáÊï∞Â≠óÈúÄËΩ¨ÈòøÊãâ‰ºØÊï∞Â≠ó„ÄÇ  \n9. **ÊèêÂèñÂºïÁî®**Ôºö\n   - Â∞ÜÂÜÖÈÉ®ÂíåÂ§ñÈÉ®ÂºïÁî®ÂçïÁã¨ËÆ∞ÂΩïÂú®`refs`Êï∞ÁªÑ‰∏≠„ÄÇ  \n10.\
            \ **ËæìÂá∫Ê†áÂáÜÂåñ**Ôºö\n    - Áº∫Â§±È°π‰øùÊåÅÂ≠óÊÆµ‰ΩÜÂ°´Á©∫ÂÄºÔºàÂ¶ÇÁ©∫Êï∞ÁªÑÊàñnullÔºâ„ÄÇ  \n    - ‰∏•Ê†ºJSONÁªìÊûÑÔºåÈîÆÂêçÂÖ®ÈÉ®Â∞èÂÜôËã±Êñá„ÄÇ\
            \  \n\n---\n\n# Few-shotÁ§∫‰æã\n\n### ËæìÂÖ•\n```json\n[\n  {\n    \"chapter_id\"\
            : \"B.2.1.1\",\n    \"chapter_title\": \"ÊªëÂè∞Ê≠£Èù¢Á¢∞ÊíûËØïÈ™å\",\n    \"raw_text\"\
            : \"\",\n    \"children\": [\n      {\n        \"chapter_id\": \"B.2.1.1.1\"\
            ,\n        \"chapter_title\": \"\",\n        \"raw_text\": \"Â∞ÜÁôΩËΩ¶Ë∫´ÊàñÂ∑•Ë£ÖÂõ∫ÂÆöÂú®Á¢∞ÊíûËØïÈ™åÊªëÂè∞‰∏ä,ÂÆâË£ÖÊñπÂêëÊ®°ÊãüÊ≠£Èù¢Á¢∞Êíû„ÄÇ\"\
            ,\n        \"children\": [],\n        \"full_path\": \"B.2 ËØïÈ™åÈ°πÁõÆ/B.2.1\
            \ Ê≠£Èù¢Á¢∞Êíû/B.2.1.1 ÊªëÂè∞Ê≠£Èù¢Á¢∞ÊíûËØïÈ™å/B.2.1.1.1 \"\n      },\n      {\n        \"chapter_id\"\
            : \"B.2.1.1.2\",\n        \"chapter_title\": \"\",\n        \"raw_text\"\
            : \"ÊªëÂè∞ÊåâÁÖß‰ª•‰∏ãÂä†ÈÄüÂ∫¶Ê≥¢ÂΩ¢‰πã‰∏ÄËøõË°åÁ¢∞ÊíûËØïÈ™å„ÄÇ a) ‰ΩøÁî®Âà∂ÈÄ†ÂïÜÊåáÂÆöÁöÑÂä†ÈÄüÂ∫¶Ê≥¢ÂΩ¢ËøõË°åËØïÈ™å,ÊåáÂÆöÁöÑÂä†ÈÄüÂ∫¶Ê≥¢ÂΩ¢Â∫î‰∏∫Âú®B.2.1.2‰∏≠ÊèèËø∞ÁöÑÂÆûËΩ¶Á¢∞ÊíûËØïÈ™åÊù°‰ª∂‰∏≠,ËΩ¶Ë∫´ÈùûÂèòÂΩ¢Âå∫ÂüüÈááÈõÜÁöÑÂä†ÈÄüÂ∫¶-Êó∂Èó¥Êõ≤Á∫ø,Âπ∂ÁªèËøáÊª§Ê≥¢Á≠âÁ∫ßCFC60\
            \ Êª§Ê≥¢Êàñ100Hz‰ΩéÈÄöÊª§Ê≥¢„ÄÇÂÆûÈôÖËØïÈ™åÁªìÊûúÊ≥¢ÂΩ¢ÁöÑÁßØÂàÜÈÄüÂ∫¶ÂèòÂåñÈáèŒîvs(t)Â∫îÂú®‰ªªÊÑèÊó∂Âàª,‰∏çË∂ÖËøáÊåáÂÆöÊ≥¢ÂΩ¢ÁöÑÁßØÂàÜÈÄüÂ∫¶ÂèòÂåñÈáè[Œîvt(t)¬±1]km/hÁöÑËåÉÂõ¥„ÄÇ\\\
            nb) ÊåâÂõæB.1 ÁöÑÊ†áÂáÜÂä†ÈÄüÂ∫¶ÈÄöÈÅìËåÉÂõ¥ÂíåË°®B.1 ÁöÑÂèÇÊï∞ËøõË°åÂä†ÈÄüÊàñÂáèÈÄü,ÂÖ∂ÈÄüÂ∫¶ÂèòÂåñÈáèŒîv ‰∏∫(25¬±1)km/h„ÄÇ\",\n      \
            \  \"children\": [],\n        \"full_path\": \"B.2 ËØïÈ™åÈ°πÁõÆ/B.2.1 Ê≠£Èù¢Á¢∞Êíû/B.2.1.1\
            \ ÊªëÂè∞Ê≠£Èù¢Á¢∞ÊíûËØïÈ™å/B.2.1.1.2 \"\n      }\n    ],\n    \"full_path\": \"B.2 ËØïÈ™åÈ°πÁõÆ/B.2.1\
            \ Ê≠£Èù¢Á¢∞Êíû/B.2.1.1 ÊªëÂè∞Ê≠£Èù¢Á¢∞ÊíûËØïÈ™å\"\n  }\n]\n```\n\n### ËæìÂá∫\n```json\n[\n  [\n  \
            \  {\n      \"chapter_id\": \"B.2.1.1\",\n      \"test_name\": \"ÊªëÂè∞Ê≠£Èù¢Á¢∞ÊíûËØïÈ™å(ÊñπÊ°àA)\"\
            ,\n      \"conditions\": [\n        \"Â∞ÜÁôΩËΩ¶Ë∫´ÊàñÂ∑•Ë£ÖÂõ∫ÂÆöÂú®Á¢∞ÊíûËØïÈ™åÊªëÂè∞‰∏äÔºåÂÆâË£ÖÊñπÂêëÊ®°ÊãüÊ≠£Èù¢Á¢∞Êíû„ÄÇ\"\
            ,\n        \"ËØïÈ™åÊªëÂè∞ÊåâÂà∂ÈÄ†ÂïÜÊåáÂÆöÁöÑÂä†ÈÄüÂ∫¶Ê≥¢ÂΩ¢ËøõË°åËØïÈ™å„ÄÇ\",\n        \"Âä†ÈÄüÂ∫¶Ê≥¢ÂΩ¢Êù•Ê∫êÔºöÂÆûËΩ¶Á¢∞ÊíûËØïÈ™åÊù°‰ª∂‰∏ãÔºåËΩ¶Ë∫´ÈùûÂèòÂΩ¢Âå∫ÂüüÈááÈõÜÁöÑÂä†ÈÄüÂ∫¶-Êó∂Èó¥Êõ≤Á∫ø„ÄÇ\"\
            \n      ],\n      \"criteria\": [\n        \"Ê£ÄÊü•Á¢∞ÊíûËß¶Âèë‰ø°Âè∑ÔºåÁ°Æ‰øù‰∏éMSD‰∏≠ËÆ∞ÂΩïÁöÑËß¶ÂèëÁ±ªÂûã‰∏ÄËá¥„ÄÇ\"\
            ,\n        \"ÁßØÂàÜÈÄüÂ∫¶ÂèòÂåñÈáèŒîvs(t)Âú®‰ªªÊÑèÊó∂Âàª‰∏çÂæóË∂ÖÂá∫ÊåáÂÆöÊ≥¢ÂΩ¢Œîvt(t)¬±1 km/h„ÄÇ\"\n      ],\n  \
            \    \"equipment\": [\n        {\n          \"name\": \"Á¢∞ÊíûËØïÈ™åÊªëÂè∞\",\n  \
            \        \"specification\": \"ÂèØÊ®°ÊãüÊ≠£Èù¢Á¢∞ÊíûÔºõÊîØÊåÅËá™ÂÆö‰πâÂä†ÈÄüÂ∫¶Ê≥¢ÂΩ¢ËæìÂÖ•\"\n        }\n    \
            \  ],\n      \"parameters\": [\n        {\n          \"item\": \"ÁßØÂàÜÈÄüÂ∫¶ÂèòÂåñÈáèŒîvs(t)\"\
            ,\n          \"constraint\": \"range_closed\",\n          \"value\": [\"\
            Œîvt(t)-1\", \"Œîvt(t)+1\"],\n          \"unit\": \"km/h\",\n          \"\
            source_text\": \"ÁßØÂàÜÈÄüÂ∫¶ÂèòÂåñÈáèŒîvs(t)Â∫îÂú®‰ªªÊÑèÊó∂Âàª,‰∏çË∂ÖËøáÊåáÂÆöÊ≥¢ÂΩ¢ÁöÑÁßØÂàÜÈÄüÂ∫¶ÂèòÂåñÈáè[Œîvt(t)¬±1]km/h\"\n\
            \        },\n        {\n          \"item\": \"Êª§Ê≥¢Á≠âÁ∫ß\",\n          \"constraint\"\
            : \"enum\",\n          \"value\": [\"CFC60\", \"100Hz‰ΩéÈÄö\"],\n        \
            \  \"unit\": null,\n          \"source_text\": \"Âπ∂ÁªèËøáÊª§Ê≥¢Á≠âÁ∫ßCFC60Êª§Ê≥¢Êàñ100Hz‰ΩéÈÄöÊª§Ê≥¢\"\
            \n        }\n      ],\n      \"refs\": [\n        {\n          \"ref_type\"\
            : \"internal\",\n          \"doc_id\": null,\n          \"target_id\"\
            : \"B.2.1.2\",\n          \"anchor_text\": \"ÂÆûËΩ¶Á¢∞ÊíûËØïÈ™åÊù°‰ª∂\"\n        }\n \
            \     ]\n    },\n    {\n      \"chapter_id\": \"B.2.1.1\",\n      \"test_name\"\
            : \"ÊªëÂè∞Ê≠£Èù¢Á¢∞ÊíûËØïÈ™å(ÊñπÊ°àB)\",\n      \"conditions\": [\n        \"Â∞ÜÁôΩËΩ¶Ë∫´ÊàñÂ∑•Ë£ÖÂõ∫ÂÆöÂú®Á¢∞ÊíûËØïÈ™åÊªëÂè∞‰∏äÔºåÂÆâË£ÖÊñπÂêëÊ®°ÊãüÊ≠£Èù¢Á¢∞Êíû„ÄÇ\"\
            ,\n        \"ÊåâÂõæB.1Ê†áÂáÜÂä†ÈÄüÂ∫¶ÈÄöÈÅìÂíåË°®B.1ÂèÇÊï∞ËøõË°åÊªëÂè∞Âä†ÈÄüÊàñÂáèÈÄü„ÄÇ\"\n      ],\n      \"criteria\"\
            : [\n        \"Ê£ÄÊü•Á¢∞ÊíûËß¶Âèë‰ø°Âè∑ÔºåÁ°Æ‰øù‰∏éMSD‰∏≠ËÆ∞ÂΩïÁöÑËß¶ÂèëÁ±ªÂûã‰∏ÄËá¥„ÄÇ\",\n        \"ÈÄüÂ∫¶ÂèòÂåñÈáèŒîvÂ∫î‰∏∫25¬±1\
            \ km/h„ÄÇ\"\n      ],\n      \"equipment\": [\n        {\n          \"name\"\
            : \"Á¢∞ÊíûËØïÈ™åÊªëÂè∞\",\n          \"specification\": \"Êª°Ë∂≥ÂõæB.1„ÄÅË°®B.1Âä†ÈÄüÂ∫¶Êõ≤Á∫øÂèÇÊï∞\"\n \
            \       }\n      ],\n      \"parameters\": [\n        {\n          \"\
            item\": \"ÈÄüÂ∫¶ÂèòÂåñÈáèŒîv\",\n          \"constraint\": \"range_closed\",\n  \
            \        \"value\": [\"24\", \"26\"],\n          \"unit\": \"km/h\",\n\
            \          \"source_text\": \"ÈÄüÂ∫¶ÂèòÂåñÈáèŒîv‰∏∫(25¬±1)km/h\"\n        },\n     \
            \ ],\n      \"refs\": [\n        {\n          \"ref_type\": \"internal\"\
            ,\n          \"doc_id\": null,\n          \"target_id\": \"ÂõæB.1\",\n \
            \         \"anchor_text\": \"Ê†áÂáÜÂä†ÈÄüÂ∫¶ÈÄöÈÅì\"\n        },\n        {\n      \
            \    \"ref_type\": \"internal\",\n          \"doc_id\": null,\n      \
            \    \"target_id\": \"Ë°®B.1\",\n          \"anchor_text\": \"Ê†áÂáÜÂä†ÈÄüÂ∫¶ÂèÇÊï∞\"\n\
            \        }\n      ]\n    }\n  ]\n]\n```"
        - id: 8ee6a7aa-9eb1-44fd-8cf5-1d32d1a65fea
          role: user
          text: '{{#1756613329065.item#}}'
        selected: false
        title: LLM 2
        type: llm
        variables: []
        vision:
          enabled: false
      height: 89
      id: '1756613344845'
      parentId: '1756613329065'
      position:
        x: 204
        y: 60
      positionAbsolute:
        x: 2930
        y: 519
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
      zIndex: 1002
    - data:
        code: "import json\nimport re\n\nimport json\nimport ast\nimport re\nfrom\
          \ typing import Dict, List, Any\n\ndef parse_input(src: str) -> List[Dict[str,\
          \ Any]]:\n    \"\"\"È¢ÑÂ§ÑÁêÜÂπ∂Ëß£ÊûêËæìÂÖ•Â≠óÁ¨¶‰∏≤‰∏∫ÂàóË°®Â≠óÂÖ∏ÁªìÊûÑ\"\"\"\n    try:\n        # 1. Áõ¥Êé•Êåâ\
          \ </think> ÂàáÂàÜÂπ∂ÂèñÂêéÂçäÊÆµ\n        src = src.rsplit('</think>', 1)[-1].strip()\n\
          \n        try:\n            data = json.loads(src)\n        except json.JSONDecodeError:\n\
          \            try:\n                if src.startswith('\\ufeff'):\n     \
          \               src = src[1:]\n                data = ast.literal_eval(src)\n\
          \            except (SyntaxError, ValueError):\n                try:\n \
          \                   processed = src.replace('\\\\\"', '\"').replace('\\\\\
          n', '\\n')\n                    data = json.loads(processed)\n         \
          \       except Exception as e:\n                    raise ValueError(f\"\
          Êó†Ê≥ïËß£ÊûêÔºö{str(e)}\")\n\n        # ‰øùËØÅ‰∏∫ list\n        if isinstance(data, dict):\n\
          \            return [data]\n        elif isinstance(data, list):\n     \
          \       return data\n        else:\n            return []\n    except Exception\
          \ as e:\n        # raise ValueError(f\"È¢ÑÂ§ÑÁêÜÂ§±Ë¥•: {str(e)}\")\n        return\
          \ []\n\n\ndef main(arg1: str) -> dict:\n    data = parse_input(arg1)\n\n\
          \    return {\n        \"result\":json.dumps(data, ensure_ascii=False),\n\
          \    }"
        code_language: python3
        desc: ''
        isInIteration: true
        isInLoop: false
        iteration_id: '1756613329065'
        outputs:
          result:
            children: null
            type: string
        selected: false
        title: ‰ª£Á†ÅÊâßË°å 6
        type: code
        variables:
        - value_selector:
          - '1756613344845'
          - text
          value_type: string
          variable: arg1
      height: 53
      id: '1756613456815'
      parentId: '1756613329065'
      position:
        x: 508
        y: 78
      positionAbsolute:
        x: 3234
        y: 537
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
      zIndex: 1002
    - data:
        code: "import os, json, traceback, subprocess, platform\n\ndef main(arg1:\
          \ list[str], arg2: str) -> dict:\n    \"\"\"\n    Â∞ÜËØïÈ™åÊï∞ÊçÆ‰∏éÁ´†ËäÇ‰ø°ÊÅØ‰∏Ä‰∏ÄÂØπÂ∫îÔºåÁîüÊàêÁõÆÊ†áÊ†ºÂºè\n\
          \    [\n        {\n            \"file\": \"regulation\",\n            \"\
          section\": \"ÈôÑÂΩïB\",\n            \"experiments\": [...]\n        },\n  \
          \      ...\n    ]\n    \"\"\"\n    # 1. ÂÖàÊää arg2 ÁöÑÂ≠óÁ¨¶‰∏≤ÂèçÂ∫èÂàóÂåñÊàêÂàóË°®\n    sec_list\
          \ = json.loads(arg2)\n\n    # 2. ‰æùÊ¨°Â§ÑÁêÜ arg1 ‰∏≠ÁöÑÊØè‰∏™ÂÖÉÁ¥†\n    result = []\n   \
          \ for sec, exp_str in zip(sec_list, arg1):\n        # ÊääÂΩìÂâçËØïÈ™åÂ≠óÁ¨¶‰∏≤ÂèçÂ∫èÂàóÂåñÊàêÂàóË°®\n\
          \        experiments = json.loads(exp_str)\n\n        # 3. ÊãºÊàêÁõÆÊ†áÂ≠óÂÖ∏\n    \
          \    result.append({\n            \"file\": sec[\"file\"],\n           \
          \ \"section\": sec[\"section\"],\n            \"experiments\": experiments\n\
          \        })\n        \n\n    # 2. ÁõÆÊ†áË∑ØÂæÑ\n    path = \"/tmp/mydata/result.json\"\
          \n\n    # 3. Ë∞ÉËØï‰ø°ÊÅØ\n    log = []\n    log.append(f\"[PWD] ÂΩìÂâçÂ∑•‰ΩúÁõÆÂΩï: {os.getcwd()}\"\
          )\n    log.append(f\"[PATH] ÁªùÂØπË∑ØÂæÑ: {os.path.abspath(path)}\")\n    log.append(f\"\
          [DIR] Áà∂ÁõÆÂΩïÊòØÂê¶Â≠òÂú®: {os.path.exists(os.path.dirname(path))}\")\n    log.append(f\"\
          [VOLUMES] ÊåÇËΩΩ‰ø°ÊÅØ: {subprocess.getoutput('mount | grep mydata')}\")\n\n   \
          \ # 4. ÂÜôÊñá‰ª∂\n    try:\n        os.makedirs(os.path.dirname(path), exist_ok=True)\n\
          \        with open(path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(result,\
          \ f, ensure_ascii=False, indent=2)\n        log.append(\"[SUCCESS] Êñá‰ª∂Â∑≤ÂÜôÂÖ•\"\
          )\n    except Exception as e:\n        log.append(f\"[ERROR] {type(e).__name__}:\
          \ {e}\")\n        log.append(traceback.format_exc())\n\n    # 5. ÂÜçÊ£ÄÊü•‰∏ÄÊ¨°\n\
          \    log.append(f\"[EXISTS] Êñá‰ª∂ÊòØÂê¶Â≠òÂú®: {os.path.exists(path)}\")\n    if os.path.exists(path):\n\
          \        log.append(f\"[SIZE] Êñá‰ª∂Â§ßÂ∞è: {os.path.getsize(path)} Â≠óËäÇ\")\n\n  \
          \  return {\"debug\": \"\\n\".join(log), \"result\": json.dumps(result,\
          \ ensure_ascii=False)}"
        code_language: python3
        desc: ''
        outputs:
          debug:
            children: null
            type: string
          result:
            children: null
            type: string
        selected: false
        title: ‰ª£Á†ÅÊâßË°å 7
        type: code
        variables:
        - value_selector:
          - '1756613329065'
          - output
          value_type: array[string]
          variable: arg1
        - value_selector:
          - '1756563307317'
          - experiment_infos
          value_type: string
          variable: arg2
      height: 53
      id: '1756618044071'
      position:
        x: 3598
        y: 459
      positionAbsolute:
        x: 3598
        y: 459
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        code: "import json\nimport os\nfrom collections import defaultdict\n\ndef\
          \ build_chapter_index(tree_data):\n    \"\"\"ÊûÑÂª∫chapterÁ¥¢ÂºïÔºåkey‰∏∫(file, section,\
          \ chapter_id)Ôºåvalue‰∏∫chapterÂØπË±°ÁöÑÂºïÁî®\"\"\"\n    chapter_index = {}\n    section_index\
          \ = {}\n    \n    def add_chapter_recursive(chapter, file_name, section_name):\n\
          \        \"\"\"ÈÄíÂΩíÊ∑ªÂä†Á´†ËäÇÂèäÂÖ∂Â≠êÁ´†ËäÇÂà∞Á¥¢Âºï‰∏≠\"\"\"\n        if 'chapter_id' in chapter:\n\
          \            chapter_id = chapter['chapter_id']\n            chapter_key\
          \ = (file_name, section_name, chapter_id)\n            chapter_index[chapter_key]\
          \ = chapter\n        \n        # ÈÄíÂΩíÂ§ÑÁêÜÂ≠êÁ´†ËäÇ\n        if 'children' in chapter:\n\
          \            for child_chapter in chapter['children']:\n               \
          \ add_chapter_recursive(child_chapter, file_name, section_name)\n    \n\
          \    for file_data in tree_data:\n        file_name = file_data['file']\n\
          \        for section in file_data['sections']:\n            section_name\
          \ = section['section']\n            section_key = (file_name, section_name)\n\
          \            section_index[section_key] = section\n            \n      \
          \      # Â§ÑÁêÜÊØè‰∏™È°∂Á∫ßÁ´†ËäÇ\n            for chapter in section['chapters']:\n   \
          \             add_chapter_recursive(chapter, file_name, section_name)\n\
          \    \n    return chapter_index, section_index\n\ndef merge_final_tree_data(chapter_index,\
          \ final_tree_data):\n    \"\"\"Â∞Üfinal_treeÁöÑÊï∞ÊçÆËûçÂêàÂà∞tree‰∏≠\"\"\"\n    merged_count\
          \ = 0\n    not_found_count = 0\n    \n    for final_section in final_tree_data:\n\
          \        file_name = final_section['file']\n        section_name = final_section['section']\n\
          \        \n        for final_chapter in final_section['chapters']:\n   \
          \         if 'chapter_id' not in final_chapter:\n                continue\n\
          \                \n            chapter_id = final_chapter['chapter_id']\n\
          \            chapter_key = (file_name, section_name, chapter_id)\n     \
          \       \n            # O(1)Êü•Êâæ\n            tree_chapter = chapter_index.get(chapter_key)\n\
          \            \n            if tree_chapter:\n                # Áõ¥Êé•‰øÆÊîπÂºïÁî®ÁöÑÂØπË±°\n\
          \                tree_chapter['parameters'] = final_chapter.get('paramaters',\
          \ [])\n                tree_chapter['topic_keywords'] = final_chapter.get('topic_keywords',\
          \ [])\n                tree_chapter['context_keywords'] = final_chapter.get('context_keywords',\
          \ [])\n                tree_chapter['refs'] = final_chapter.get('refs',\
          \ [])\n                tree_chapter['table_headers'] = final_chapter.get('table_headers',\
          \ [])\n                merged_count += 1\n            else:\n          \
          \      not_found_count += 1\n    \n    return merged_count, not_found_count\n\
          \ndef merge_result_data(chapter_index, result_data):\n    \"\"\"Â∞ÜresultÁöÑexperimentsÊï∞ÊçÆËûçÂêàÂà∞tree‰∏≠\"\
          \"\"\n    merged_count = 0\n    not_found_count = 0\n    \n    for result_section\
          \ in result_data:\n        file_name = result_section['file']\n        section_name\
          \ = result_section['section']\n        experiments_groups = result_section.get('experiments',\
          \ [])\n        \n        # ÈÅçÂéÜÊØè‰∏™ÂÆûÈ™åÁªÑ\n        for experiments_group in experiments_groups:\n\
          \            if not experiments_group:\n                continue\n     \
          \           \n            # Ëé∑ÂèñÁ¨¨‰∏Ä‰∏™ÂÆûÈ™åÁöÑchapter_id‰Ωú‰∏∫ËØ•ÁªÑÁöÑÂÆö‰ΩçÊ†áËØÜ\n            first_experiment\
          \ = experiments_group[0]\n            chapter_id = first_experiment.get('chapter_id')\n\
          \            \n            if not chapter_id:\n                not_found_count\
          \ += 1\n                continue\n            \n            chapter_key\
          \ = (file_name, section_name, chapter_id)\n            # O(1)Êü•ÊâæÂØπÂ∫îÁöÑÁ´†ËäÇ\n \
          \           chapter = chapter_index.get(chapter_key)\n            \n   \
          \         if chapter:\n                # Â∞ÜËØ•ÁªÑÂÆûÈ™åÊ∑ªÂä†Âà∞ÂØπÂ∫îÁ´†ËäÇÁöÑexperimentsÂ≠óÊÆµ\n  \
          \              if 'experiments' not in chapter:\n                    chapter['experiments']\
          \ = []\n                chapter['experiments'].extend(experiments_group)\n\
          \                merged_count += 1\n            else:\n                not_found_count\
          \ += 1\n    \n    return merged_count, not_found_count\n\ndef main(result:\
          \ str, tree: str, final_tree: str, file_name) -> dict:\n    # Âä†ËΩΩJSONÊï∞ÊçÆ\n\
          \    tree_data = json.loads(tree)\n    final_tree_data = json.loads(final_tree)\n\
          \    result_data = json.loads(result)\n    \n    # ÊûÑÂª∫Á¥¢Âºï\n    chapter_index,\
          \ section_index = build_chapter_index(tree_data)\n    \n    # ËûçÂêàÊï∞ÊçÆ\n   \
          \ chapter_merged, chapter_not_found = merge_final_tree_data(chapter_index,\
          \ final_tree_data)\n    experiment_merged, experiment_not_found = merge_result_data(chapter_index,\
          \ result_data)\n    \n    # ÁªüËÆ°‰ø°ÊÅØ\n    total_chapters = len(chapter_index)\n\
          \    chapters_with_params = 0\n    chapters_with_keywords = 0\n    chapters_with_experiments\
          \ = 0\n    \n    for chapter in chapter_index.values():\n        if chapter.get('parameters'):\n\
          \            chapters_with_params += 1\n        if chapter.get('topic_keywords'):\n\
          \            chapters_with_keywords += 1\n        if chapter.get('experiments'):\n\
          \            chapters_with_experiments += 1\n    \n    # ÂêàÂπ∂ÂêéÁöÑÊï∞ÊçÆÂ∞±ÊòØ‰øÆÊîπÂêéÁöÑtree_data\n\
          \    merged_tree = tree_data\n\n    # ÂéªÊéâÊâ©Â±ïÂêçÔºåÂÜçÊãº .json\n    base_name = os.path.splitext(file_name)[0]\
          \          # ÂæóÂà∞Êó†ÂêéÁºÄÁöÑÁ∫ØÊñá‰ª∂Âêç\n    path = f\"/tmp/mydata/{base_name}.json\"\n\
          \    os.makedirs(os.path.dirname(path), exist_ok=True)\n    with open(path,\
          \ \"w\", encoding=\"utf-8\") as f:\n        json.dump(merged_tree, f, ensure_ascii=False,\
          \ indent=2)\n    \n    return {\n        \"result\": f\"ËûçÂêàÂÆåÊàêÔºÅÁªüËÆ°‰ø°ÊÅØ: ÊÄªÁ´†ËäÇÊï∞:\
          \ {total_chapters}, ÂèÇÊï∞Á´†ËäÇ: {chapters_with_params}, ÂÖ≥ÈîÆËØçÁ´†ËäÇ: {chapters_with_keywords},\
          \ ËØïÈ™åÁ´†ËäÇ: {chapters_with_experiments}, ËûçÂêàÊàêÂäü: Á´†ËäÇÊï∞ÊçÆ{chapter_merged}‰∏™, ËØïÈ™åÊï∞ÊçÆ{experiment_merged}‰∏™\"\
          \n    }"
        code_language: python3
        desc: ''
        outputs:
          result:
            children: null
            type: string
        selected: false
        title: ‰ª£Á†ÅÊâßË°å 8
        type: code
        variables:
        - value_selector:
          - '1756618044071'
          - result
          value_type: string
          variable: result
        - value_selector:
          - '1756550411122'
          - tree
          value_type: string
          variable: tree
        - value_selector:
          - '1756563307317'
          - final_tree
          value_type: string
          variable: final_tree
        - value_selector:
          - '1756550268945'
          - file
          - name
          value_type: file
          variable: file_name
      height: 53
      id: '1756629493937'
      position:
        x: 3902
        y: 459
      positionAbsolute:
        x: 3902
        y: 459
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        context:
          enabled: false
          variable_selector: []
        desc: ''
        isInIteration: true
        isInLoop: false
        iteration_id: '1756557801310'
        model:
          completion_params: {}
          mode: chat
          name: deepseek-reasoner
          provider: langgenius/deepseek/deepseek
        prompt_template:
        - id: fb74cb72-3859-4cad-ac1e-cdbb9834f569
          role: system
          text: "# ËßíËâ≤ÔºàRoleÔºâ\n‰Ω†ÊòØ‰∏ÄÂêç‰∏•Ê†ºÁöÑ JSON ËæìÂá∫‰øÆÂ§çÂô®ÔºàJSON FixerÔºâ„ÄÇ  \n‰Ω†ÁöÑËÅåË¥£ÊòØÔºö\n- Êé•Êî∂‰∏Ä‰∏™ÂèØËÉΩÊ†ºÂºèÈîôËØØÁöÑ\
            \ JSON Â≠óÁ¨¶‰∏≤ÂíåÈîôËØØ‰ø°ÊÅØ„ÄÇ\n- ‰∏•Ê†º‰øÆÂ§çÂÖ∂‰∏≠ÁöÑËØ≠Ê≥ïÈóÆÈ¢òÊàñÁªìÊûÑÈóÆÈ¢ò„ÄÇ\n- ËæìÂá∫ÂÆåÂÖ®Á¨¶ÂêàÈ¢ÑÊúüÊ†ºÂºèÁöÑ JSON ÂØπË±°„ÄÇ\n\n---\n\
            \n# ËæìÂÖ•ÔºàInputÔºâ\n‰Ω†Â∞ÜÊî∂Âà∞Ôºö\n1. **error_type**ÔºöËß£ÊûêÂ§±Ë¥•ÁöÑÁ±ªÂûã\n2. **error_message**ÔºöÂÖ∑‰ΩìÈîôËØØÂéüÂõ†\n\
            2. **raw_output**ÔºöÊ®°ÂûãÂéüÂßãËæìÂá∫Ôºå‰Ω†ÈúÄË¶ÅÂøΩÁï•‰ªé‚Äù<think>‚ÄúÂà∞\"</think>\"ÁöÑÈÉ®ÂàÜÔºåÂè™‰øùÁïôÈîôËØØ JSON„ÄÇ\n\
            \nÁ§∫‰æãËæìÂÖ•Ôºö\nerror: Expecting property name enclosed in double quotes: line\
            \ 3 column 5 (char 25)\nraw_text: {file: \"doc1\", section: \"main\",\
            \ chapters: [ ... ]}\n\n---\n\n# ËæìÂá∫Ê†ºÂºèÔºàOutputÔºâ\nËØ∑ËæìÂá∫‰∏Ä‰∏™ **ÂÆåÊï¥ JSON ÂØπË±°**ÔºåÂøÖÈ°ª‰∏•Ê†ºÁ¨¶Âêà‰ª•‰∏ãÁªìÊûÑÔºà‰∏çË¶ÅËæìÂá∫Â§ö‰ΩôÁöÑËß£ÈáäÊÄßÊñáÂ≠óÔºâÔºö\
            \  \n```json\n{\n  \"file\": \"string\",\n  \"section\": \"string\",\n\
            \  \"experiment_root_ids\": [\"string\", \"...\"],\n  \"chapters\": [\n\
            \    {\n      \"chapter_id\": \"string\",\n      \"paramaters\": [\n \
            \       {\n          \"item\": \"string\",\n          \"constraint\":\
            \ \"<=|<|=|>=|>|range_closed|range_open|enum|boolean\",\n          \"\
            value\": \"string|array|null\",\n          \"unit\": \"string|null\",\n\
            \          \"source_text\": \"string\"\n        }\n      ],\n      \"\
            topic_keywords\": [\"string\", \"...\"],\n      \"context_keywords\":\
            \ [\"string\", \"...\"],\n      \"refs\": [\n        {\n          \"ref_type\"\
            : \"internal|external\",\n          \"doc_id\": \"string|null\",\n   \
            \       \"target_id\": \"string\",\n          \"anchor_text\": \"string\"\
            \n        }\n      ],\n      \"table_headers\": [\"string\", \"...\"]\n\
            \    }\n  ]\n}\n```\n\n# Ê≥®ÊÑè‰∫ãÈ°π\n\n1. Âπ∂‰∏çÊòØËØ¥Ê≠§json‰ªÖÂåÖÂê´‰∏Ä‰∏™ÈîôËØØÔºåÂè™ÊòØÁî®‰ΩúÂèÇËÄÉÔºå‰Ω†ÈúÄË¶Å‰∏•Ê†ºÊåâÁÖßËæìÂá∫Ê†ºÂºèË¶ÅÊ±ÇËøõË°å‰øÆÂ§çÔºåÂèØÈÄöËøáËæìÂÖ•jsonËøõË°åÊé®Êñ≠ÔºåÁ°Æ‰øù\
            \ JSON Â≠óÁ¨¶‰∏≤ÂèØË¢´ `json.loads()` Ëß£ÊûêÈÄöËøáÂç≥ÂèØ\n2. ‰∏•Ê†ºËæìÂá∫‰∏Ä‰∏™ÂêàÊ≥ï JSON ÂØπË±°Ôºå‰∏çÂæóËæìÂá∫Â§ö‰ΩôÊñáÂ≠ó„ÄÅÊ≥®ÈáäÊàñ Markdown„ÄÇ\n\
            3. Â¶ÇÊûúÊüêÂ≠óÊÆµÊó†ÂÜÖÂÆπÔºåËØ∑‰ΩøÁî® `null` ÊàñÁ©∫Êï∞ÁªÑ `[]`„ÄÇ\n4. ÂøÖÈ°ªË°•ÂÖ®Áº∫Â§±Â≠óÊÆµÔºå‰∏çÂÖÅËÆ∏ÁúÅÁï•‰ªª‰ΩïÂ≠óÊÆµ„ÄÇ"
        - id: c68f56a1-5429-4a52-b480-a030d0c3f431
          role: user
          text: 'error_type:

            {{#1756557929661.error_type#}}


            error_message:

            {{#1756557929661.error_message#}}


            raw_text:

            {{#1756557804693.text#}}

            '
        selected: false
        title: LLM 3
        type: llm
        variables: []
        vision:
          enabled: false
      height: 89
      id: '1756698267184'
      parentId: '1756557801310'
      position:
        x: 807.7142857142858
        y: 118.78571428571433
      positionAbsolute:
        x: 1445.7142857142858
        y: 577.7857142857143
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
      zIndex: 1002
    - data:
        desc: ''
        isInIteration: true
        isInLoop: false
        iteration_id: '1756557801310'
        output_type: string
        selected: false
        title: ÂèòÈáèËÅöÂêàÂô®
        type: variable-aggregator
        variables:
        - - '1756557929661'
          - result
        - - '1756698848564'
          - result
      height: 129
      id: '1756698812908'
      parentId: '1756557801310'
      position:
        x: 1374.2857142857142
        y: 77.14285714285711
      positionAbsolute:
        x: 2012.2857142857142
        y: 536.1428571428571
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
      zIndex: 1002
    - data:
        code: "import json\nimport re\n\nimport json\nimport ast\nimport re\nfrom\
          \ typing import Dict, List, Any\n\ndef parse_input(src: str) -> List[Dict[str,\
          \ Any]]:\n    \"\"\"È¢ÑÂ§ÑÁêÜÂπ∂Ëß£ÊûêËæìÂÖ•Â≠óÁ¨¶‰∏≤‰∏∫ÂàóË°®Â≠óÂÖ∏ÁªìÊûÑ\"\"\"\n    try:\n        # ÂéªÈô§ LLM\
          \ ÊÄùÁª¥ÈìæÊ†áËÆ∞\n        # 1. Áõ¥Êé•Êåâ </think> ÂàáÂàÜÂπ∂ÂèñÂêéÂçäÊÆµ\n        src = src.rsplit('</think>',\
          \ 1)[-1].strip()\n\n        try:\n            data = json.loads(src)\n \
          \       except json.JSONDecodeError:\n            try:\n               \
          \ if src.startswith('\\ufeff'):\n                    src = src[1:]\n   \
          \             data = ast.literal_eval(src)\n            except (SyntaxError,\
          \ ValueError):\n                try:\n                    processed = src.replace('\\\
          \\\"', '\"').replace('\\\\n', '\\n')\n                    data = json.loads(processed)\n\
          \                except Exception as e:\n                    raise ValueError(f\"\
          Êó†Ê≥ïËß£ÊûêÔºö{str(e)}\")\n\n        # ‰øùËØÅ‰∏∫ list\n        if isinstance(data, dict):\n\
          \            return [data]\n        elif isinstance(data, list):\n     \
          \       return data\n        else:\n            return []\n    except Exception\
          \ as e:\n        raise ValueError(f\"È¢ÑÂ§ÑÁêÜÂ§±Ë¥•: {str(e)}\")\n        # return\
          \ []\n\n\ndef main(arg1: str) -> dict:\n    data = parse_input(arg1)\n\n\
          \    return {\n        \"result\":json.dumps(data, ensure_ascii=False),\n\
          \    }"
        code_language: python3
        desc: ''
        isInIteration: true
        isInLoop: false
        iteration_id: '1756557801310'
        outputs:
          result:
            children: null
            type: string
        selected: false
        title: ‰ª£Á†ÅÊâßË°å 9
        type: code
        variables:
        - value_selector:
          - '1756698267184'
          - text
          value_type: string
          variable: arg1
      height: 53
      id: '1756698848564'
      parentId: '1756557801310'
      position:
        x: 1087.4285714285713
        y: 119.64285714285722
      positionAbsolute:
        x: 1725.4285714285713
        y: 578.6428571428572
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
      zIndex: 1002
    viewport:
      x: -514.0000000000007
      y: -78
      zoom: 0.7
