app:
  description: '‰øùÁïôÂºïÁî®„ÄÅÈù¢Âêë‰∏§‰∏™Âú∫ÊôØ

    '
  icon: ü§ñ
  icon_background: '#FFEAD5'
  mode: advanced-chat
  name: pdfËß£Êûê
  use_icon_as_answer_icon: false
dependencies:
- current_identifier: null
  type: marketplace
  value:
    marketplace_plugin_unique_identifier: langgenius/deepseek:0.0.6@dd589dc093c8084925858034ab5ec1fdf0d33819f43226c2f8c4a749a9acbbb2
kind: app
version: 0.3.0
workflow:
  conversation_variables: []
  environment_variables: []
  features:
    file_upload:
      allowed_file_extensions:
      - .JPG
      - .JPEG
      - .PNG
      - .GIF
      - .WEBP
      - .SVG
      allowed_file_types:
      - image
      allowed_file_upload_methods:
      - local_file
      - remote_url
      enabled: false
      fileUploadConfig:
        audio_file_size_limit: 50
        batch_count_limit: 5
        file_size_limit: 15
        image_file_size_limit: 10
        video_file_size_limit: 100
        workflow_file_upload_limit: 10
      image:
        enabled: false
        number_limits: 3
        transfer_methods:
        - local_file
        - remote_url
      number_limits: 3
    opening_statement: ''
    retriever_resource:
      enabled: true
    sensitive_word_avoidance:
      enabled: false
    speech_to_text:
      enabled: false
    suggested_questions: []
    suggested_questions_after_answer:
      enabled: false
    text_to_speech:
      enabled: false
      language: ''
      voice: ''
  graph:
    edges:
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '1756557801310'
        sourceType: iteration-start
        targetType: llm
      id: 1756557801310start-source-1756557804693-target
      selected: false
      source: 1756557801310start
      sourceHandle: source
      target: '1756557804693'
      targetHandle: target
      type: custom
      zIndex: 1002
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '1756557801310'
        sourceType: llm
        targetType: code
      id: 1756557804693-source-1756557929661-target
      selected: false
      source: '1756557804693'
      sourceHandle: source
      target: '1756557929661'
      targetHandle: target
      type: custom
      zIndex: 1002
    - data:
        isInLoop: false
        sourceType: start
        targetType: code
      id: 1756550268945-source-1756550411122-target
      selected: false
      source: '1756550268945'
      sourceHandle: source
      target: '1756550411122'
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '1756613329065'
        sourceType: iteration-start
        targetType: llm
      id: 1756613329065start-source-1756613344845-target
      selected: false
      source: 1756613329065start
      sourceHandle: source
      target: '1756613344845'
      targetHandle: target
      type: custom
      zIndex: 1002
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '1756613329065'
        sourceType: llm
        targetType: code
      id: 1756613344845-source-1756613456815-target
      selected: false
      source: '1756613344845'
      sourceHandle: source
      target: '1756613456815'
      targetHandle: target
      type: custom
      zIndex: 1002
    - data:
        isInIteration: false
        isInLoop: false
        sourceType: iteration
        targetType: code
      id: 1756613329065-source-1756618044071-target
      selected: false
      source: '1756613329065'
      sourceHandle: source
      target: '1756618044071'
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '1756557801310'
        sourceType: llm
        targetType: code
      id: 1756698267184-source-1756698848564-target
      selected: false
      source: '1756698267184'
      sourceHandle: source
      target: '1756698848564'
      targetHandle: target
      type: custom
      zIndex: 1002
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '1756613329065'
        sourceType: code
        targetType: llm
      id: 1756613456815-fail-branch-1757762170810-target
      selected: false
      source: '1756613456815'
      sourceHandle: fail-branch
      target: '1757762170810'
      targetHandle: target
      type: custom
      zIndex: 1002
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '1756613329065'
        sourceType: llm
        targetType: code
      id: 1757762170810-source-1757762176397-target
      selected: false
      source: '1757762170810'
      sourceHandle: source
      target: '1757762176397'
      targetHandle: target
      type: custom
      zIndex: 1002
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '1756613329065'
        sourceType: code
        targetType: variable-aggregator
      id: 1757762176397-source-1757762184699-target
      selected: false
      source: '1757762176397'
      sourceHandle: source
      target: '1757762184699'
      targetHandle: target
      type: custom
      zIndex: 1002
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '1756613329065'
        sourceType: code
        targetType: variable-aggregator
      id: 1756613456815-source-1757762184699-target
      selected: false
      source: '1756613456815'
      sourceHandle: source
      target: '1757762184699'
      targetHandle: target
      type: custom
      zIndex: 1002
    - data:
        isInLoop: false
        sourceType: code
        targetType: code
      id: 1756618044071-source-1756629493937-target
      selected: false
      source: '1756618044071'
      sourceHandle: source
      target: '1756629493937'
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInLoop: false
        sourceType: code
        targetType: answer
      id: 17580960600210-source-1756558073574-target
      selected: false
      source: '17580960600210'
      sourceHandle: source
      target: '1756558073574'
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInLoop: false
        sourceType: code
        targetType: code
      id: 1756563307317-source-17580960600210-target
      selected: false
      source: '1756563307317'
      sourceHandle: source
      target: '17580960600210'
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '1756557801310'
        sourceType: iteration-start
        targetType: llm
      id: 1758096258752start-source-1758096258752017580962587520-target
      selected: false
      source: 1758096258752start
      sourceHandle: source
      target: '1758096258752017580962587520'
      targetHandle: target
      type: custom
      zIndex: 1002
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '1756557801310'
        sourceType: llm
        targetType: code
      id: 1758096258752017580962587520-source-1758096258752017580962587521-target
      selected: false
      source: '1758096258752017580962587520'
      sourceHandle: source
      target: '1758096258752017580962587521'
      targetHandle: target
      type: custom
      zIndex: 1002
    - data:
        isInIteration: false
        isInLoop: false
        sourceType: code
        targetType: code
      id: 1758096294198-source-1756563307317-target
      source: '1758096294198'
      sourceHandle: source
      target: '1756563307317'
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '1756557801310'
        sourceType: code
        targetType: llm
      id: 1756557929661-fail-branch-1756698267184-target
      source: '1756557929661'
      sourceHandle: fail-branch
      target: '1756698267184'
      targetHandle: target
      type: custom
      zIndex: 1002
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '1756557801310'
        sourceType: code
        targetType: variable-aggregator
      id: 1756698848564-source-1756698812908-target
      source: '1756698848564'
      sourceHandle: source
      target: '1756698812908'
      targetHandle: target
      type: custom
      zIndex: 1002
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '1756557801310'
        sourceType: code
        targetType: variable-aggregator
      id: 1756557929661-source-1756698812908-target
      source: '1756557929661'
      sourceHandle: source
      target: '1756698812908'
      targetHandle: target
      type: custom
      zIndex: 1002
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '17580962587520'
        sourceType: llm
        targetType: code
      id: 1758105741313-source-1758105753732-target
      source: '1758105741313'
      sourceHandle: source
      target: '1758105753732'
      targetHandle: target
      type: custom
      zIndex: 1002
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '17580962587520'
        sourceType: code
        targetType: variable-aggregator
      id: 1758105753732-source-1758105758637-target
      source: '1758105753732'
      sourceHandle: source
      target: '1758105758637'
      targetHandle: target
      type: custom
      zIndex: 1002
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '17580962587520'
        sourceType: code
        targetType: llm
      id: 1758096258752017580962587521-fail-branch-1758105741313-target
      source: '1758096258752017580962587521'
      sourceHandle: fail-branch
      target: '1758105741313'
      targetHandle: target
      type: custom
      zIndex: 1002
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '17580962587520'
        sourceType: code
        targetType: variable-aggregator
      id: 1758096258752017580962587521-source-1758105758637-target
      source: '1758096258752017580962587521'
      sourceHandle: source
      target: '1758105758637'
      targetHandle: target
      type: custom
      zIndex: 1002
    - data:
        isInLoop: false
        sourceType: iteration
        targetType: code
      id: 1756557801310-source-1758096294198-target
      source: '1756557801310'
      sourceHandle: source
      target: '1758096294198'
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInLoop: false
        sourceType: iteration
        targetType: code
      id: 17580962587520-source-1758096294198-target
      source: '17580962587520'
      sourceHandle: source
      target: '1758096294198'
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInIteration: false
        isInLoop: false
        sourceType: llm
        targetType: code
      id: 1758595213024-source-1758622742335-target
      source: '1758595213024'
      sourceHandle: source
      target: '1758622742335'
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInIteration: false
        isInLoop: false
        sourceType: code
        targetType: code
      id: 1758622742335-source-1758096294198-target
      source: '1758622742335'
      sourceHandle: source
      target: '1758096294198'
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInLoop: false
        sourceType: code
        targetType: llm
      id: 1756550411122-source-1758595213024-target
      source: '1756550411122'
      sourceHandle: source
      target: '1758595213024'
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInLoop: false
        sourceType: code
        targetType: iteration
      id: 1756550411122-source-17580962587520-target
      source: '1756550411122'
      sourceHandle: source
      target: '17580962587520'
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInLoop: false
        sourceType: code
        targetType: iteration
      id: 1756550411122-source-1756557801310-target
      source: '1756550411122'
      sourceHandle: source
      target: '1756557801310'
      targetHandle: target
      type: custom
      zIndex: 0
    nodes:
    - data:
        desc: ''
        selected: false
        title: ÂºÄÂßã
        type: start
        variables:
        - allowed_file_extensions: []
          allowed_file_types:
          - document
          allowed_file_upload_methods:
          - local_file
          - remote_url
          label: file
          max_length: 48
          options: []
          required: false
          type: file
          variable: file
        - label: context
          max_length: 100000
          options: []
          required: false
          type: paragraph
          variable: context
        - label: array
          max_length: 1000000
          options: []
          required: false
          type: paragraph
          variable: array
        - label: regulation
          max_length: 1000000
          options: []
          required: false
          type: paragraph
          variable: regulation
        - label: result
          max_length: 1000000
          options: []
          required: false
          type: paragraph
          variable: result
      height: 193
      id: '1756550268945'
      position:
        x: 30
        y: 352
      positionAbsolute:
        x: 30
        y: 352
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        answer: '{{#1758595213024.text#}}'
        desc: ''
        selected: false
        title: Áõ¥Êé•ÂõûÂ§ç
        type: answer
        variables: []
      height: 104
      id: answer
      position:
        x: 945.7859968508628
        y: 1443.3008747276417
      positionAbsolute:
        x: 945.7859968508628
        y: 1443.3008747276417
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        code: "import fitz  # PyMuPDF\nimport re\nimport json\nimport os\nfrom typing\
          \ import List, Dict, Tuple\nfrom collections import defaultdict\n\n# chapter_patterns\
          \ = [\n#     re.compile(r'^(ÈôÑ\\s*ÂΩï\\s*[A-Z])\\s+(.+)$'),\n#     re.compile(r'^([A-Z](?:\\\
          .\\d+)+)\\s+(.+)$'),\n#     re.compile(r'^(\\d+(?:\\.\\d+)*)(\\s+)(.+)$'),\n\
          # ]\n\n# chapter_patterns = [\n#     re.compile(r'^(APPENDIX\\s+[A-Z0-9]+)$',\
          \ re.I),          # APPENDIX A / APPENDIX 1\n#     re.compile(r'^([A-Z](?:\\\
          .\\d+)+)\\s+(.+)$'),\n#     re.compile(r'^(\\d+(?:\\.\\d+)*\\.?)\\s+(.+)$'),\
          \                     # 1.1. Title\n# ]\n\n# ÊóßÁöÑÁ´†ËäÇÊ®°ÂºèÔºàÂ∑≤Ê≥®ÈáäÔºâ\n# chapter_patterns\
          \ = [\n#     re.compile(r'^(ÈôÑ\\s*ÂΩï\\s*[A-Z0-9])$'), # ÈôÑ ÂΩï B\n#     re.compile(r'^((APPENDIX|ANNEX|ATTACHMENT)\\\
          s+(?:[A-Z0-9]+|\\([A-Z0-9]+\\)))$', re.I),  # ANNEX A / ANNEX 1\n#     re.compile(r'^([A-Z]\\\
          .)\\s+(.+)$'),                                # A. Title (ÂçïÁã¨Â≠óÊØçÁ´†ËäÇ)\n#   \
          \  re.compile(r'^([A-Z](?:\\.\\d+)+\\.?)\\s+(.+)$'),                   \
          \  # A.1. Title / A.1.1. Title\n#     re.compile(r'^(\\d+(?:\\.\\d+)*\\\
          .?)\\s+(.+)$'),                       # 1.1. Title\n#     re.compile(r'^(\\\
          d+(?:-\\d+)*-)\\s+(.+)$'),                          # 1- Title / 1-2- Title\n\
          # ]\n\n# Êñ∞ÁöÑÂêàÂπ∂ÂêéÁöÑÁ´†ËäÇÊ®°Âºè\nchapter_patterns = [\n    # 1. ‰∏≠ÊñáÈôÑÂΩïÔºöÈôÑÂΩïA, ÈôÑ ÂΩï B\n  \
          \  re.compile(r'^(ÈôÑ\\s*ÂΩï\\s*[A-Z0-9])$'),\n    \n    # 2. Ëã±ÊñáÈôÑÂΩïÔºöAPPENDIX\
          \ A, ANNEX A, ATTACHMENT A\n    re.compile(r'^((APPENDIX|ANNEX|ATTACHMENT)\\\
          s+(?:[A-Z0-9]+|\\([A-Z0-9]+\\)))$', re.I),\n    \n    # 3. Â≠óÊØçÁ´†ËäÇÔºàÊîØÊåÅÁÇπÂíåÊ®™Á∫øÂàÜÈöîÁ¨¶ÔºâÔºöA.\
          \ Title, A.1. Title, A-1- Title\n    re.compile(r'^([A-Z](?:[.\\-]\\d+)*[.\\\
          -]?)\\s+(.+)$'),\n    \n    # 4. Êï∞Â≠óÁ´†ËäÇÔºàÊîØÊåÅÁÇπÂíåÊ®™Á∫øÂàÜÈöîÁ¨¶ÔºâÔºö1. Title, 1.1. Title, 1-\
          \ Title, 1-2- Title\n    re.compile(r'^(\\d+(?:[.\\-]\\d+)*[.\\-]?)\\s+(.+)$'),\n\
          ]\n\ndef detect_document_language(lines: List[str]) -> str:\n    \"\"\"\n\
          \    Ê£ÄÊµãÊñáÊ°£ËØ≠Ë®ÄÔºö‰∏≠ÊñáÊàñËã±Êñá\n    :param lines: ÊñáÊ°£ÁöÑÊâÄÊúâË°å\n    :return: 'zh' Ë°®Á§∫‰∏≠ÊñáÔºå'en'\
          \ Ë°®Á§∫Ëã±Êñá\n    \"\"\"\n    chinese_char_count = 0\n    total_chars = 0\n  \
          \  \n    # ÈááÊ†∑Ââç1000Ë°åÊàñÂÖ®ÈÉ®Ë°å\n    sample_lines = lines[:1000] if len(lines) >\
          \ 1000 else lines\n    \n    for line in sample_lines:\n        for char\
          \ in line:\n            total_chars += 1\n            if '\\u4e00' <= char\
          \ <= '\\u9fff':  # ‰∏≠ÊñáÂ≠óÁ¨¶\n                chinese_char_count += 1\n    \n\
          \    # Âè™Ë¶ÅÊúâ‰∏≠ÊñáÂ≠óÁ¨¶Â∞±ËÆ§‰∏∫ÊòØ‰∏≠ÊñáÊñáÊ°£\n    if chinese_char_count > 0:\n        return 'zh'\n\
          \    else:\n        return 'en'\n\n# ‰∏≠ÊñáÁ´†ËäÇmax_chapter_num=50\n# ÂÖ®ÊñáÈ¶ñÂÖàÊ£ÄÊµãÊòØ‰∏≠ÊñáËøòÊòØËã±Êñá\n\
          def detect_chapter(line: str, max_chapter_num=1000, language='en', number_analysis=None):\n\
          \    clean_line = line.strip()\n    if not clean_line:\n        return None\n\
          \n    for pattern in chapter_patterns:\n        m = pattern.match(clean_line)\n\
          \        if m:\n            chapter_id = m.group(1).strip()\n          \
          \  chapter_title = m.group(len(m.groups())).strip() if m.group(len(m.groups()))\
          \ else \"\"\n            if re.match(r'^(ÈôÑ\\s*ÂΩï\\s*[A-Z0-9])$', chapter_id):\n\
          \                # ÂéªÊéâ‰∏≠Èó¥ÁöÑÁ©∫Ê†º\n                chapter_id = chapter_id.replace(\"\
          \ \", \"\")\n                # chapter_id = chapter_id[-1]\n           \
          \ # ---- Âü∫Á°ÄËøáÊª§ ----\n            first_num = None\n            if chapter_id.upper().startswith(\"\
          APPENDIX\"):\n                suffix = chapter_id[len(\"APPENDIX\"):].strip(\"\
          \ ()\")\n                if suffix.isdigit():\n                    first_num\
          \ = int(suffix)\n            else:\n                m_num = re.match(r'^(\\\
          d+)', chapter_id)\n                if m_num:\n                    first_num\
          \ = int(m_num.group(1))\n\n            if first_num is not None and number_analysis\
          \ is not None:\n                # ‰ΩøÁî®Êô∫ËÉΩÊï∞Â≠óËåÉÂõ¥Âà§Êñ≠\n                min_reasonable\
          \ = number_analysis.get(\"min_reasonable\", 1)\n                max_reasonable\
          \ = number_analysis.get(\"max_reasonable\", max_chapter_num)\n         \
          \       \n                # ÁâπÊÆäÂ§ÑÁêÜÊ≥ïËßÑÁºñÂè∑Ê®°Âºè\n                if number_analysis.get(\"\
          regulation_mode\", False):\n                    regulation_number = number_analysis.get(\"\
          regulation_number\")\n                    if first_num != regulation_number:\n\
          \                        return None  # ‰∏çÊòØÊ≥ïËßÑÁºñÂè∑ÔºåËøáÊª§Êéâ\n                else:\n\
          \                    # Ê≠£Â∏∏Á´†ËäÇÁºñÂè∑ËåÉÂõ¥Ê£ÄÊü•\n                    if first_num < min_reasonable\
          \ or first_num > max_reasonable:\n                        return None  #\
          \ Êï∞Â≠óËåÉÂõ¥‰∏çÂêàÁêÜ\n            elif first_num is not None:\n                # ÂÖúÂ∫ïÈÄªËæëÔºö‰ΩøÁî®‰º†ÁªüÁöÑmax_chapter_num\n\
          \                if first_num < 1 or first_num > max_chapter_num:\n    \
          \                return None  # Êï∞Â≠óËåÉÂõ¥‰∏çÂêàÁêÜ\n\n            # ---- ÂÜÖÂÆπÁâπÂæÅËøáÊª§ ----\n\
          \            # 1) Ê†áÈ¢òÂøÖÈ°ªÂåÖÂê´Â≠óÊØçÊàñ‰∏≠Êñá\n            if not re.search(r'[A-Za-z\\\
          u4e00-\\u9fff]', chapter_title):\n                return None\n\n      \
          \      # 2) ÂéªÊéâÁ∫ØÊï∞Â≠óË°®Ê†ºË°å\n            if re.fullmatch(r'[\\d\\s\\.\\-]+', chapter_title):\n\
          \                return None\n\n            # 3) Ë°®Ê†ºÂÜÖÂÆπËøáÊª§ - Ê£ÄÊµãÊòéÊòæÁöÑË°®Ê†ºÊï∞ÊçÆÊ®°Âºè\n\
          \            # Â¶ÇÊûúÊ†áÈ¢òÂåÖÂê´Â§ßÈáèÊï∞Â≠ó„ÄÅÁ©∫Ê†ºÂíåÂ∞ëÈáèÂ≠óÊØçÁöÑÁªÑÂêàÔºåÂèØËÉΩÊòØË°®Ê†ºÊï∞ÊçÆ\n            if re.search(r'^\\\
          d+\\s+\\d+.*[A-Z]\\s+\\d+\\s+\\d+', chapter_title):  # Â¶Ç \"10 0 E 0 16\"\
          \n                return None\n            \n            # Ê£ÄÊµãË°®Ê†ºË°åÊ®°ÂºèÔºöÂçï‰∏™Â≠óÊØç\
          \ + Êï∞Â≠óÁªÑÂêà\n            if re.fullmatch(r'[A-Z]\\s*\\d+.*', chapter_title)\
          \ and len(chapter_title.split()) >= 3:\n                # Â¶ÇÊûúÊ†áÈ¢òÊòØ \"A 10 0\"\
          \ ËøôÊ†∑ÁöÑÊ†ºÂºèÔºåÂæàÂèØËÉΩÊòØË°®Ê†ºÊï∞ÊçÆ\n                parts = chapter_title.split()\n      \
          \          if len(parts) >= 3 and all(part.isdigit() or part in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\
          \ for part in parts[:3]):\n                    return None\n\n         \
          \   # 4) Ê£ÄÊµãÂùêÊ†áÁÇπÊàñÂèÇÊï∞Ë°®Ê†ºÔºöÂ¶Ç \"A15 0 E 0 3\"\n            if re.search(r'^[A-Z]\\\
          d+\\s+\\d+\\s+[A-Z]\\s+\\d+\\s+\\d+', chapter_title):\n                return\
          \ None\n\n            # 5) Ë°åÂ§™Áü≠\n            if len(clean_line) < 4 and not\
          \ chapter_id.upper().startswith(\"APPENDIX\") and not chapter_id.startswith(\"\
          ÈôÑÂΩï\"):\n                return None\n\n            # 6) ËøáÊª§ÊòéÊòæÁöÑË°®Ê†ºÊ†áÈ¢òÁªÑÂêà\n  \
          \          if len(chapter_id) == 1 and chapter_id.isupper():\n         \
          \       # Âçï‰∏™Â§ßÂÜôÂ≠óÊØç‰Ωú‰∏∫Á´†ËäÇIDÔºåÊ£ÄÊü•Ê†áÈ¢òÊòØÂê¶ÂÉèË°®Ê†ºÊï∞ÊçÆ\n                if re.search(r'\\d+.*\\\
          d+', chapter_title) and len(chapter_title.split()) <= 6:\n             \
          \       return None\n\n            return {\n                \"chapter_id\"\
          : chapter_id,\n                \"chapter_title\": chapter_title\n      \
          \      }\n\n    return None\n\ndef build_tree(chapter_list: List[Dict])\
          \ -> List[Dict]:\n    id_map = {}\n    root = []\n\n    # ÂÖàÊ≥®ÂÜåÊâÄÊúâËäÇÁÇπ\n    for\
          \ chap in chapter_list:\n        chap[\"children\"] = []\n        # Áªü‰∏ÄÂéªÊéâÊú´Â∞æÁÇπÂíåÊ®™Á∫ø‰Ωú‰∏∫\
          \ key\n        key = chap[\"chapter_id\"].rstrip('.-')\n        id_map[key]\
          \ = chap\n\n    # ‰∏∫ÊØè‰∏™ËäÇÁÇπÂàõÂª∫Áº∫Â§±ÁöÑÁà∂ËäÇÁÇπÔºàÂè™ÈíàÂØπ‰∏âÁ∫ßÂèä‰ª•‰∏äÊ†áÈ¢òÔºâ\n    for chap in chapter_list:\n\
          \        cid = chap[\"chapter_id\"].rstrip('.')\n        parts = cid.split('.')\n\
          \        \n        # Âè™Êúâ‰∏âÁ∫ßÂèä‰ª•‰∏äÊ†áÈ¢òÊâçÂàõÂª∫‰∏≠Èó¥Áà∂ËäÇÁÇπ\n        if len(parts) >= 3:\n  \
          \          # ÂàõÂª∫ÊâÄÊúâÁº∫Â§±ÁöÑ‰∏≠Èó¥Áà∂Á∫ßËäÇÁÇπÔºà‰ΩÜ‰∏çÂåÖÊã¨È°∂Á∫ßÁà∂ËäÇÁÇπÔºâ\n            for i in range(2, len(parts)):\
          \  # ‰ªéÁ¨¨‰∫åÁ∫ßÂºÄÂßãÂàõÂª∫ÔºåË∑≥ËøáÈ°∂Á∫ß\n                parent_key = '.'.join(parts[:i])\n \
          \               if parent_key not in id_map:\n                    # ÂàõÂª∫Áº∫Â§±ÁöÑÁà∂ËäÇÁÇπ\n\
          \                    parent_node = {\n                        \"chapter_id\"\
          : parent_key + \".\",\n                        \"chapter_title\": \"\",\n\
          \                        \"raw_text\": \"\",\n                        \"\
          children\": []\n                    }\n                    id_map[parent_key]\
          \ = parent_node\n\n    # ÊûÑÂª∫Ê†ëÁªìÊûÑ\n    for chap in chapter_list:\n        cid\
          \ = chap[\"chapter_id\"].rstrip('.')\n        parts = cid.split('.')\n\n\
          \        # Ê†πËäÇÁÇπÂà§Êñ≠\n        if cid.startswith(\"APPENDIX\"):\n           \
          \ root.append(chap)\n        elif cid.startswith(\"ÈôÑÂΩï\") or len(parts) ==\
          \ 1:\n            root.append(chap)\n        else:\n            parent_key\
          \ = '.'.join(parts[:-1])\n            parent = id_map.get(parent_key)\n\
          \            if parent:\n                parent[\"children\"].append(chap)\n\
          \            else:\n                # Â¶ÇÊûúÁà∂ËäÇÁÇπ‰∏çÂ≠òÂú®ÔºåÂØπ‰∫é‰∫åÁ∫ßÊ†áÈ¢òÔºåÁõ¥Êé•‰Ωú‰∏∫Ê†πËäÇÁÇπ\n        \
          \        if len(parts) == 2:\n                    root.append(chap)\n  \
          \              # ‰∏âÁ∫ßÂèä‰ª•‰∏äÊ†áÈ¢òÊ≤°ÊúâÁà∂ËäÇÁÇπÊó∂Ôºå‰∏çÂÅöÂ§ÑÁêÜÔºàÂõ†‰∏∫ÂâçÈù¢Â∑≤ÁªèÂàõÂª∫‰∫ÜÁà∂ËäÇÁÇπÔºâ\n\n\n    # Â∞ÜÂàõÂª∫ÁöÑ‰∏≠Èó¥ËäÇÁÇπ‰πüÊ∑ªÂä†Âà∞ÊúÄÁªàÁöÑÁ´†ËäÇÂàóË°®‰∏≠Ôºå‰ΩÜÂè™ÊúâÈÇ£‰∫õÊúâÂ≠êËäÇÁÇπÁöÑ\n\
          \    created_parents = []\n    for key, node in id_map.items():\n      \
          \  if node not in chapter_list and len(node[\"children\"]) > 0:\n      \
          \      created_parents.append(node)\n    \n    # ÂØπÂàõÂª∫ÁöÑÁà∂ËäÇÁÇπ‰πüËøõË°åÊ†ëÁªìÊûÑÊûÑÂª∫\n    for\
          \ parent in created_parents:\n        cid = parent[\"chapter_id\"].rstrip('.')\n\
          \        parts = cid.split('.')\n        \n        if len(parts) == 1:\n\
          \            root.append(parent)\n        else:\n            parent_key\
          \ = '.'.join(parts[:-1])\n            grandparent = id_map.get(parent_key)\n\
          \            if grandparent and parent not in grandparent[\"children\"]:\n\
          \                grandparent[\"children\"].append(parent)\n            elif\
          \ len(parts) == 1:  # ËøôÊòØ‰∏ÄÁ∫ßÁ´†ËäÇ\n                if parent not in root:\n \
          \                   root.append(parent)\n    \n    return root\n\ndef build_full_path(chapters:\
          \ List[Dict], path_prefix=\"\"):\n    for chap in chapters:\n        if\
          \ path_prefix:\n            chap[\"full_path\"] = f\"{path_prefix}/{chap['chapter_id']}\
          \ {chap['chapter_title']}\"\n        else:\n            chap[\"full_path\"\
          ] = f\"{chap['chapter_id']} {chap['chapter_title']}\"\n        if chap.get(\"\
          children\"):\n            build_full_path(chap[\"children\"], chap[\"full_path\"\
          ])\n\ndef fullwidth_to_halfwidth(text: str) -> str:\n    result = []\n \
          \   for char in text:\n        code = ord(char)\n        if 0xFF01 <= code\
          \ <= 0xFF5E:\n            result.append(chr(code - 0xFEE0))\n        else:\n\
          \            result.append(char)\n    return ''.join(result)\n\ndef build_term_dict(raw_text:\
          \ str) -> Dict[str, str]:\n    text = re.sub(r'\\n+', '\\n', raw_text.strip())\n\
          \    pattern = re.compile(\n        r'^\\d+\\.\\d+\\n'\n        r'(?P<cn>[^\\\
          n]*?)\\s*'\n        r'(?P<en>[A-Za-z].*?)\\s*(?=\\n)',\n        re.MULTILINE\n\
          \    )\n\n    term_map = {}\n    for m in pattern.finditer(text):\n    \
          \    cn = m.group(\"cn\").strip()\n        en = m.group(\"en\").strip()\n\
          \        if cn and en:\n            term_map[cn] = en\n    return term_map\n\
          \ndef extract_terms_with_abbr_from_terms_section(raw_text: str) -> Dict[str,\
          \ Dict[str, str]]:\n    \"\"\"\n    ÊèêÂèñÊúØËØ≠Á´†ËäÇ‰∏≠ÁöÑ‰∏≠Ëã±ÊñáÊúØËØ≠ÂèäÁº©ÂÜô\n    ËøîÂõûÊ†ºÂºèÔºö\n    {\n\
          \      \"‰∏≠ÊñáÊúØËØ≠\": {\n         \"en\": \"Ëã±ÊñáÊúØËØ≠\",\n         \"abbr\": \"Áº©ÂÜôÔºàÂ¶ÇÊúâÔºâ\"\
          \n      }\n    }\n    \"\"\"\n    term_map = {}\n    text = re.sub(r'\\\
          n+', '\\n', raw_text.strip())\n\n    pattern = re.compile(\n        r'(?P<cn>[\\\
          u4e00-\\u9fffÔºàÔºâ()¬∑\\s]{2,})'        # ‰∏≠ÊñáÈÉ®ÂàÜ\n        r'\\s*'            \
          \                             # ÂèØÈÄâÁ©∫Ê†º\n        r'(?P<en>[A-Za-z][A-Za-z\\\
          s\\-/]*)'              # Ëã±ÊñáÊúØËØ≠\n        r'(?:[;Ôºõ:Ôºö]?\\s*(?P<abbr>[A-Z0-9¬∑]+))?',\
          \       # ÂèØÈÄâÁº©ÂÜô\n        re.MULTILINE\n    )\n\n\n    for m in pattern.finditer(text):\n\
          \        cn = m.group(\"cn\").strip()\n        en = m.group(\"en\").strip()\n\
          \        abbr = m.group(\"abbr\").strip() if m.group(\"abbr\") else \"\"\
          \n\n        term_map[cn] = {\"en\": en}\n        if abbr:\n            term_map[cn][\"\
          abbr\"] = abbr\n\n    return term_map\n\ndef extract_abbr_terms_from_symbols_section(raw_text:\
          \ str) -> Dict[str, Dict[str, str]]:\n    \"\"\"\n    ÊèêÂèñ‚ÄúÁ¨¶Âè∑ÂíåÁº©Áï•ËØ≠‚ÄùÁ´†ËäÇÁöÑ‰∏≠Ëã±Áº©ÂÜôÊò†Â∞ÑÔºåËøîÂõû‰ª•‰∏≠Êñá‰∏∫ÈîÆÁöÑÁªìÊûÑÔºö\n\
          \    {\n        \"‰∏≠Êñá\": {\n            \"abbr\": \"Áº©ÂÜô\",\n            \"\
          en\": \"Ëã±ÊñáÈáä‰πâ\"\n        }\n    }\n    \"\"\"\n    abbr_map = {}\n    # Ê∏ÖÁêÜÊñáÊú¨\n\
          \    text = re.sub(r'\\n+', '\\n', raw_text.strip())\n\n    # ÂåπÈÖçÊ®°ÂºèÔºöACLR:\
          \ ÈÇªÈÅìÊ≥ÑÊºèÂäüÁéáÊØî (Adjacent Channel Leakage Power Ratio)\n    pattern = re.compile(\n\
          \        r'(?P<abbr>[A-Za-z0-9¬∑\\-_]+)\\s*[:Ôºö]?\\s*'\n        r'(?P<cn>[\\\
          u4e00-\\u9fff¬∑]+)'\n        r'(?:[ÔºàÔºâ()]*\\s*(?P<en>[A-Za-z\\s/\\-]+)\\s*[ÔºàÔºâ()]*)?'\n\
          \    )\n\n    for m in pattern.finditer(text):\n        abbr = m.group(\"\
          abbr\").strip()\n        cn = m.group(\"cn\").strip(\"ÔºàÔºâ()\").strip()\n\
          \        en = m.group(\"en\").strip() if m.group(\"en\") else \"\"\n\n \
          \       if cn:\n            abbr_map[cn] = {}\n            if abbr:\n  \
          \              abbr_map[cn][\"abbr\"] = abbr\n            if en:\n     \
          \           abbr_map[cn][\"en\"] = en\n\n    return abbr_map\n\ndef should_merge_crossline(prev_text,\
          \ curr_text, prev_bbox, curr_bbox):\n    \"\"\"\n    Âà§Êñ≠ÊòØÂê¶ÈúÄË¶ÅÊääÂΩìÂâçË°åÂêàÂπ∂Âà∞‰∏ä‰∏ÄË°å\n\
          \    \"\"\"\n    text_stripped = curr_text.strip()\n\n    # Ê®°ÂºèÂåπÈÖçÔºöË°®Ê†ºÊ†áÈ¢ò„ÄÅÁºñÂè∑Ê†áÈ¢òÁ≠â\n\
          \    if re.match(r'^Ë°®\\s*\\d+', text_stripped):\n        return True\n\n\
          \    # ÂûÇÁõ¥Ë∑ùÁ¶ªÂæàÂ∞èÔºàËØ¥ÊòéÊòØËßÜËßâ‰∏äÁöÑÂêå‰∏ÄË°åÔºâ\n    prev_y = prev_bbox[1]\n    curr_y = curr_bbox[1]\n\
          \    line_height = prev_bbox[3] - prev_bbox[1]\n    if abs(curr_y - prev_y)\
          \ < 0.3 * line_height:\n        return True\n\n    return False\n\ndef fix_broken_chapters(lines:\
          \ list[str]) -> list[str]:\n    def normalize_chapter_spaces(s: str) ->\
          \ str:\n        line = s.strip()\n        \n        # 1. ‰øùÁïôÂéüÊù•ÁöÑÈÄªËæëÔºö‰øÆÂ§çÁÇπÂêéÈù¢ÁöÑÁ©∫Ê†ºÔºåÈÄÇÁî®‰∫éÊâÄÊúâÊÉÖÂÜµ\
          \ (A. 1, 7. 1)\n        line = re.sub(r'\\.\\s+(?=\\d)', '.', line)\n  \
          \      \n        # 2. ‰øÆÂ§çÊï∞Â≠ó/Â≠óÊØçÂíåÁÇπ‰πãÈó¥ÁöÑÁ©∫Ê†ºÔºö7 .1 -> 7.1, A .1 -> A.1\n        line\
          \ = re.sub(r'([A-Za-z0-9]+)\\s+(\\.\\d+)', r'\\1\\2', line)\n        \n\
          \        # 3. ‰øÆÂ§çÂ§çÊùÇÁöÑÂ§öÁ∫ßÁ©∫Ê†ºÔºö7 . 1 . 2 -> 7.1.2\n        # ÈúÄË¶ÅÂæ™ÁéØÂ§ÑÁêÜÔºåÁõ¥Âà∞Ê≤°ÊúâÊõ¥Â§öÂèòÂåñ\n\
          \        max_iterations = 10  # Èò≤Ê≠¢Êó†ÈôêÂæ™ÁéØ\n        iterations = 0\n       \
          \ prev_line = \"\"\n        while prev_line != line and iterations < max_iterations:\n\
          \            prev_line = line\n            # Â§ÑÁêÜÂêÑÁßçÁ©∫Ê†ºÁªÑÂêàÔºåÊîØÊåÅÂ≠óÊØçÂíåÊï∞Â≠óÂºÄÂ§¥\n      \
          \      line = re.sub(r'([A-Za-z0-9]+)\\s*\\.\\s*(\\d+)', r'\\1.\\2', line)\n\
          \            iterations += 1\n        \n        # 4. ‰øÆÂ§çOCRÂ∏∏ËßÅÈîôËØØÔºöÊï∞Â≠óÂºÄÂ§¥ÁöÑÁ´†ËäÇ\n\
          \        line = re.sub(r'(\\d+\\.\\d+)\\.\\s*l\\b', r'\\1.1', line)\n  \
          \      line = re.sub(r'([A-Za-z0-9]+)\\.l\\.(\\d+)', r'\\1.1.\\2', line)\n\
          \        line = re.sub(r'^l\\.(\\d+)', r'1.\\1', line)\n        \n     \
          \   # 5. ‰øÆÂ§çÂ≠óÊØçÂºÄÂ§¥Á´†ËäÇÁöÑOCRÈîôËØØÔºöB.l -> B.1, A.O -> A.0, C.I -> C.1\n        line\
          \ = re.sub(r'^([A-Z])\\.l\\b', r'\\1.1', line)\n        line = re.sub(r'^([A-Z])\\\
          .l\\.(\\d+)', r'\\1.1.\\2', line)\n        line = re.sub(r'^([A-Z])\\.O\\\
          .(\\d+)', r'\\1.0.\\2', line)\n        line = re.sub(r'^([A-Z])\\.I\\.(\\\
          d+)', r'\\1.1.\\2', line)\n        \n        # 6. ‰øÆÂ§çÂÖ∂‰ªñOCRÈîôËØØÔºöO -> 0, I ->\
          \ 1\n        line = re.sub(r'([A-Za-z0-9]+)\\.O\\.(\\d+)', r'\\1.0.\\2',\
          \ line)\n        line = re.sub(r'([A-Za-z0-9]+)\\.I\\.(\\d+)', r'\\1.1.\\\
          2', line)\n        \n        return line\n\n    lines = [normalize_chapter_spaces(line)\
          \ for line in lines]\n\n    return lines\n\ndef process_gb_terms_format(lines:\
          \ List[str]) -> List[str]:\n    \"\"\"\n    Â§ÑÁêÜÂõΩÊ†áÊúØËØ≠ÂÆö‰πâÊ†ºÂºèÔºö\n    Â∞Ü \"3.1\" (‰∏ã‰∏ÄË°å)\
          \ \"‰∏≠ÊñáÊúØËØ≠ Ëã±ÊñáÊúØËØ≠\" ÂêàÂπ∂‰∏∫ \"3.1 ‰∏≠ÊñáÊúØËØ≠ Ëã±ÊñáÊúØËØ≠\"\n    \"\"\"\n    result = []\n   \
          \ i = 0\n    \n    while i < len(lines):\n        current_line = lines[i].strip()\n\
          \        \n        # Ê£ÄÊµãÊòØÂê¶ÊòØÊúØËØ≠ÂÆö‰πâÁºñÂè∑ÔºöÁ∫ØÊï∞Â≠ó.Êï∞Â≠óÊ†ºÂºèÔºå‰∏î‰∏ã‰∏ÄË°åÂåÖÂê´‰∏≠Êñá+Ëã±ÊñáÔºåÊàñËÄÖÁ¨¨‰∫åË°åÊòØ‰∏≠ÊñáÔºåÁ¨¨‰∏âË°åÊòØËã±Êñá\n\
          \        if (i + 1 < len(lines) and \n            re.match(r'^\\d+\\.\\\
          d+$', current_line) and\n            current_line.startswith('3.')):  #\
          \ ÈÄöÂ∏∏ÊúØËØ≠Á´†ËäÇÊòØÁ¨¨3Á´†\n            \n            next_line = lines[i + 1].strip()\n\
          \            \n            # Ê£ÄÊü•‰∏ã‰∏ÄË°åÊòØÂê¶Á¨¶Âêà: ‰∏≠Êñá + Á©∫Ê†º + Ëã±Êñá ÁöÑÊ®°Âºè\n            if\
          \ re.search(r'[\\u4e00-\\u9fa5].*[A-Za-z]', next_line):\n              \
          \  # ÂêàÂπ∂ÊàêÊ†áÈ¢òÊ†ºÂºè\n                merged_line = f\"{current_line} {next_line}\"\
          \n                result.append(merged_line)\n                i += 2  #\
          \ Ë∑≥Ëøá‰∏ã‰∏ÄË°å\n                continue\n\n            # Ê£ÄÊü•Á¨¨‰∫åË°åÊòØÂê¶ÊòØ‰∏≠ÊñáÔºåÁ¨¨‰∏âË°åÊòØÂê¶ÊòØËã±Êñá\n\
          \            if (i + 2 < len(lines) and\n                re.search(r'[\\\
          u4e00-\\u9fa5]', lines[i + 1].strip()) and\n                re.search(r'[A-Za-z]',\
          \ lines[i + 2].strip())):\n                merged_line = f\"{current_line}\
          \ {lines[i + 1].strip()} {lines[i + 2].strip()}\"\n                result.append(merged_line)\n\
          \                i += 3  # Ë∑≥ËøáÂêé‰∏§Ë°å\n                continue\n\n        result.append(current_line)\n\
          \        i += 1\n    \n    return result\n\ndef extract_full_text_with_filter(pdf_path:\
          \ str, top_crop=0.08, bottom_crop=0.08):\n    doc = fitz.open(pdf_path)\n\
          \    all_lines = []\n\n    prev_line_text = None\n    prev_bbox = None\n\
          \n\n\n    for page in doc:\n\n        h = page.rect.height\n        clip_rect\
          \ = fitz.Rect(0, h * top_crop, page.rect.width, h * (1 - bottom_crop))\n\
          \        page_dict = page.get_text(\"dict\", clip=clip_rect)\n\n       \
          \ for block in page_dict[\"blocks\"]:\n            if block[\"type\"] !=\
          \ 0:  # Âè™Â§ÑÁêÜÊñáÊú¨\n                continue\n\n            for line in block[\"\
          lines\"]:\n                # 1. ÊåâxÂùêÊ†áÂêàÂπ∂Âêå‰∏ÄË°åÁöÑspan\n                spans =\
          \ sorted(line[\"spans\"], key=lambda s: s[\"bbox\"][0])\n              \
          \  merged = \"\"\n                last_x = None\n                for sp\
          \ in spans:\n                    x0, x1 = sp[\"bbox\"][0], sp[\"bbox\"][2]\n\
          \                    width = max(1.0, x1 - x0)\n                    avg_char_w\
          \ = width / max(len(sp[\"text\"]), 1)\n\n                    if last_x is\
          \ not None:\n                        gap = x0 - last_x\n               \
          \         if gap > max(avg_char_w * 0.5, 3.0):\n                       \
          \     merged += \" \"\n                    merged += sp[\"text\"]\n    \
          \                last_x = x1\n\n                merged = merged.strip()\n\
          \                curr_bbox = line[\"bbox\"]\n\n                # 2. Ë∑®Ë°åÊô∫ËÉΩÂêàÂπ∂Âà§ÂÆö\n\
          \                if prev_line_text is not None:\n                    if\
          \ should_merge_crossline(prev_line_text, merged, prev_bbox, curr_bbox):\n\
          \                        prev_line_text += \" \" + merged\n            \
          \            prev_bbox = (\n                            prev_bbox[0],\n\
          \                            prev_bbox[1],\n                           \
          \ max(prev_bbox[2], curr_bbox[2]),\n                            max(prev_bbox[3],\
          \ curr_bbox[3])\n                        )\n                        continue\n\
          \                    else:\n                        all_lines.append(prev_line_text)\n\
          \n                prev_line_text = merged\n                prev_bbox = curr_bbox\n\
          \n    # ÊúÄÂêé‰∏ÄË°å\n    if prev_line_text:\n        all_lines.append(prev_line_text)\n\
          \n    # ËøõË°åÂÖ®ËßíÂ≠óÁ¨¶ËΩ¨ÂçäËßíÂ≠óÁ¨¶\n    all_lines = [fullwidth_to_halfwidth(line.strip())\
          \ for line in all_lines]\n\n    # ËøõË°åÁ´†ËäÇÁºñÂè∑‰øÆÂ§ç\n    normalized = fix_broken_chapters(all_lines)\n\
          \    \n    # \U0001F195 ÂõΩÊ†áÊúØËØ≠ÂÆö‰πâÊ†ºÂºèÂ§ÑÁêÜ\n    normalized = process_gb_terms_format(normalized)\n\
          \n    # ÂÜôÂá∫Êñá‰ª∂‰∏éËøîÂõû\n    with open('extracted_full_text.txt', \"w\", encoding=\"\
          utf-8\") as f:\n        f.write(\"\\n\".join(normalized))\n\n    return\
          \ normalized\n\ndef detect_chapter_pattern(chapters: List[Dict]) -> str:\n\
          \    \"\"\"\n    Ê£ÄÊµãÊñáÊ°£ÁöÑÁ´†ËäÇÊ®°ÂºèÔºö\n    - 'alpha_first': Â≠óÊØçÁ´†ËäÇÂú®Ââç (A, A.1, A.2, B,\
          \ B.1, 1, 2, ...)\n    - 'numeric_first': Êï∞Â≠óÁ´†ËäÇÂú®Ââç (1, 2, ..., A, A.1, A.2,\
          \ B, B.1, ...)\n    \"\"\"\n    alpha_indices = []\n    numeric_indices\
          \ = []\n    \n    for i, ch in enumerate(chapters):\n        chapter_id\
          \ = ch[\"chapter_id\"].strip()\n        if re.match(r'^[A-Z](\\.\\d+)*\\\
          .?$', chapter_id):\n            alpha_indices.append(i)\n        elif re.match(r'^\\\
          d+(\\.\\d+)*\\.?$', chapter_id):\n            numeric_indices.append(i)\n\
          \    \n    if not alpha_indices or not numeric_indices:\n        return\
          \ 'numeric_first'  # ÈªòËÆ§Êï∞Â≠ó‰ºòÂÖà\n    \n    # ÊØîËæÉÁ¨¨‰∏Ä‰∏™Â≠óÊØçÁ´†ËäÇÂíåÁ¨¨‰∏Ä‰∏™Êï∞Â≠óÁ´†ËäÇÁöÑ‰ΩçÁΩÆ\n    first_alpha\
          \ = min(alpha_indices)\n    first_numeric = min(numeric_indices)\n    \n\
          \    if first_alpha < first_numeric:\n        return 'alpha_first'\n   \
          \ else:\n        return 'numeric_first'\n\ndef parse_chapter_id(chapter_id:\
          \ str, pattern: str = 'numeric_first') -> List[int]:\n    \"\"\"\n    Ê†πÊçÆÊñáÊ°£Ê®°ÂºèËß£ÊûêÁ´†ËäÇID\n\
          \    :param chapter_id: Á´†ËäÇIDÂ≠óÁ¨¶‰∏≤\n    :param pattern: ÊñáÊ°£Ê®°Âºè ('alpha_first'\
          \ Êàñ 'numeric_first')\n    \"\"\"\n    chapter_id = chapter_id.strip()\n\n\
          \    # Â≠óÊØçÁ´†ËäÇÊ†ºÂºè - ÊîØÊåÅÁÇπÂíåÊ®™Á∫øÂàÜÈöîÁ¨¶\n    if re.fullmatch(r'[A-Z](?:[.\\-]\\d+)*[.\\\
          -]?', chapter_id):\n        # Áªü‰∏ÄÂ§ÑÁêÜÁÇπÂíåÊ®™Á∫øÂàÜÈöîÁ¨¶\n        normalized = re.sub(r'[.\\\
          -]+', '.', chapter_id).rstrip('.')\n        parts = normalized.split('.')\n\
          \        letter = parts[0]\n        \n        if pattern == 'alpha_first':\n\
          \            # Â≠óÊØçÂú®ÂâçÊ®°ÂºèÔºöA=1, B=2, C=3, ...\n            letter_value = ord(letter)\
          \ - ord('A') + 1\n        else:\n            # Êï∞Â≠óÂú®ÂâçÊ®°ÂºèÔºöÂ≠óÊØçÁ´†ËäÇÊîæÂú®Êï∞Â≠óÁ´†ËäÇ‰πãÂêé\n   \
          \         # ÂÅáËÆæÊúÄÂ§öÊúâ100‰∏™Êï∞Â≠óÁ´†ËäÇÔºåÂ≠óÊØç‰ªé101ÂºÄÂßã\n            letter_value = ord(letter)\
          \ - ord('A') + 101\n        \n        try:\n            rest = [int(p) for\
          \ p in parts[1:]] if len(parts) > 1 else []\n            return [letter_value]\
          \ + rest\n        except ValueError:\n            return []\n\n    # Êï∞Â≠óÁ´†ËäÇÊ†ºÂºè\
          \ - ÊîØÊåÅÁÇπÂíåÊ®™Á∫øÂàÜÈöîÁ¨¶\n    elif re.fullmatch(r'\\d+(?:[.\\-]\\d+)*[.\\-]?', chapter_id):\n\
          \        try:\n            # Áªü‰∏ÄÂ§ÑÁêÜÁÇπÂíåÊ®™Á∫øÂàÜÈöîÁ¨¶\n            normalized = re.sub(r'[.\\\
          -]+', '.', chapter_id).rstrip('.')\n            parts = normalized.split('.')\n\
          \            numeric_parts = [int(p) for p in parts]\n            \n   \
          \         if pattern == 'alpha_first':\n                # Â≠óÊØçÂú®ÂâçÊ®°ÂºèÔºöÊï∞Â≠óÁ´†ËäÇÊîæÂú®Â≠óÊØçÁ´†ËäÇ‰πãÂêé\n\
          \                # ÂÅáËÆæÊúÄÂ§öÊúâ26‰∏™Â≠óÊØçÁ´†ËäÇÔºåÊï∞Â≠ó‰ªé27ÂºÄÂßã\n                numeric_parts[0]\
          \ += 26\n            # Êï∞Â≠óÂú®ÂâçÊ®°ÂºèÔºö‰øùÊåÅÂéüÊúâÊï∞Â≠ó\n            \n            return numeric_parts\n\
          \        except ValueError:\n            return []\n\n    return []\n\n\
          def is_chapter_a_before_b(a: list[int], b: list[int]) -> bool:\n    for\
          \ i in range(min(len(a), len(b))):\n        if a[i] < b[i]:\n          \
          \  return True\n        elif a[i] > b[i]:\n            return False\n  \
          \  return len(a) < len(b)\n\ndef is_reasonable_chapter_jump(prev_id: List[int],\
          \ curr_id: List[int]) -> bool:\n    \"\"\"\n    Âà§Êñ≠Á´†ËäÇË∑≥Ë∑ÉÊòØÂê¶ÂêàÁêÜÔºåÊõ¥ÂÆΩÊùæÁöÑÁ≠ñÁï•Ôºö\n   \
          \ ‰∏ªË¶ÅËøáÊª§ÊéâÊòéÊòæ‰∏çÂêàÁêÜÁöÑË∑≥Ë∑ÉÔºå‰ΩÜÂÖÅËÆ∏Ê≠£Â∏∏ÁöÑÁ´†ËäÇÁªìÊûÑ\n    ÂØπÂ≠óÊØçÈìæÂíåÊï∞Â≠óÈìæÈÉΩËøõË°åÂêàÁêÜÊÄßÂà§Êñ≠\n    \"\"\"\n    if not\
          \ prev_id or not curr_id:\n        return True  # Â¶ÇÊûúÊó†Ê≥ïËß£ÊûêÔºåÈªòËÆ§ÂÖÅËÆ∏\n    \n  \
          \  # Â¶ÇÊûúÊòØ‰∏çÂêåÂ±ÇÁ∫ßÔºå‰∏ÄËà¨ÈÉΩÊòØÂêàÁêÜÁöÑÔºàÂ¶Ç 1. -> 1.1 Êàñ 1.1 -> 2.Ôºâ\n    if len(prev_id) != len(curr_id):\n\
          \        return True\n    \n    # ÂêåÂ±ÇÁ∫ßÁöÑÊÉÖÂÜµ‰∏ãÔºåÊ£ÄÊü•Ë∑≥Ë∑ÉÂπÖÂ∫¶\n    if len(prev_id) ==\
          \ 1:  # ‰∏ÄÁ∫ßÁ´†ËäÇ\n        prev_num = prev_id[0]\n        curr_num = curr_id[0]\n\
          \        diff = curr_num - prev_num\n        \n        # Âà§Êñ≠ÊòØÂê¶‰∏∫Â≠óÊØçÁ´†ËäÇÔºàÁºñÁ†ÅËåÉÂõ¥101-126ÂØπÂ∫îA-ZÔºâ\n\
          \        if prev_num >= 101 and curr_num >= 101:  # Â≠óÊØçÁ´†ËäÇ\n            #\
          \ Â≠óÊØçË∑≥Ë∑ÉÊ£ÄÊü•Ôºö‰∏çÂÖÅËÆ∏Ë∑®Ë∂äË∂ÖËøá2‰∏™Â≠óÊØçÔºàÂ¶ÇBË∑≥Âà∞E‰ª•‰∏äÔºâ\n            return 1 <= diff <= 2\n     \
          \   else:  # Êï∞Â≠óÁ´†ËäÇ\n            return 1 <= diff <= 5  # ÂÖÅËÆ∏Ë∑≥Ë∑É1-5Á´†ÔºàËøáÊª§Êéâ‰ªé5Ë∑≥Âà∞100ËøôÁßçÊòéÊòæÈîôËØØÁöÑÔºâ\n\
          \    \n    elif len(prev_id) == 2:  # ‰∫åÁ∫ßÁ´†ËäÇ\n        # Â¶ÇÊûúÁ¨¨‰∏ÄÁ∫ßÁõ∏ÂêåÔºåÊ£ÄÊü•Á¨¨‰∫åÁ∫ßÁöÑË∑≥Ë∑É\n\
          \        if prev_id[0] == curr_id[0]:\n            prev_num = prev_id[1]\n\
          \            curr_num = curr_id[1]\n            diff = curr_num - prev_num\n\
          \            \n            # Âà§Êñ≠Á¨¨‰∏ÄÁ∫ßÊòØÂê¶‰∏∫Â≠óÊØçÁ´†ËäÇ\n            if prev_id[0] >=\
          \ 101:  # Â≠óÊØçÁ´†ËäÇÁöÑÂ≠êÁ∫ß\n                return 1 <= diff <= 5  # Â≠óÊØçÁ´†ËäÇÁöÑÂ≠êÁ∫ßË∑≥Ë∑ÉÁ®çÂæÆÂÆΩÊùæ‰∏Ä‰∫õ\n\
          \            else:  # Êï∞Â≠óÁ´†ËäÇÁöÑÂ≠êÁ∫ß\n                return 1 <= diff <= 10  #\
          \ ‰∫åÁ∫ßÁ´†ËäÇÂÖÅËÆ∏Êõ¥Â§ßË∑≥Ë∑É\n        else:\n            # ‰∏çÂêåÁöÑ‰∏ÄÁ∫ßÁ´†ËäÇÔºåÈÉΩÂêàÁêÜ\n            return\
          \ True\n    \n    else:  # ‰∏âÁ∫ßÂèä‰ª•‰∏äÁ´†ËäÇ\n        # ÂØπ‰∫éÊ∑±Â±ÇÊ¨°Á´†ËäÇÔºåÊõ¥ÂÆΩÊùæ‰∏Ä‰∫õ\n        return\
          \ True\n\ndef analyze_chapter_number_distribution(chapters: List[Dict])\
          \ -> Dict[str, int]:\n    \"\"\"\n    ÂàÜÊûêÁ´†ËäÇÁºñÂè∑ÁöÑÊï∞Â≠óÂàÜÂ∏ÉÔºåÁ°ÆÂÆöÂêàÁêÜÁöÑÊï∞Â≠óËåÉÂõ¥\n    ËøîÂõû: {\"\
          min_reasonable\": ÊúÄÂ∞èÂêàÁêÜÊï∞Â≠ó, \"max_reasonable\": ÊúÄÂ§ßÂêàÁêÜÊï∞Â≠ó, \"primary_range\"\
          : ‰∏ªË¶ÅÊï∞Â≠óËåÉÂõ¥}\n    \"\"\"\n    first_numbers = []\n    \n    for ch in chapters:\n\
          \        chapter_id = ch[\"chapter_id\"].strip()\n        # ÊèêÂèñÁ¨¨‰∏Ä‰∏™Êï∞Â≠ó\n  \
          \      m_num = re.match(r'^(\\d+)', chapter_id)\n        if m_num:\n   \
          \         first_numbers.append(int(m_num.group(1)))\n        # Â§ÑÁêÜAPPENDIXÂêéË∑üÊï∞Â≠óÁöÑÊÉÖÂÜµ\n\
          \        elif chapter_id.upper().startswith(\"APPENDIX\"):\n           \
          \ suffix = chapter_id[len(\"APPENDIX\"):].strip(\" ()\")\n            if\
          \ suffix.isdigit():\n                first_numbers.append(int(suffix))\n\
          \    \n    if not first_numbers:\n        return {\"min_reasonable\": 1,\
          \ \"max_reasonable\": 50, \"primary_range\": (1, 50)}\n    \n    first_numbers.sort()\n\
          \    \n    # ÂàÜÊûêÊï∞Â≠óÂàÜÂ∏ÉÊ®°Âºè\n    from collections import Counter\n    counter\
          \ = Counter(first_numbers)\n    \n    # Â¶ÇÊûúÂ§ßÂ§öÊï∞Á´†ËäÇÈÉΩÊòØÂêå‰∏Ä‰∏™Êï∞Â≠óÂºÄÂ§¥ÔºàÂ¶Ç60.1, 60.2, 60.3...ÔºâÔºåËøôÂèØËÉΩÊòØÊ≥ïËßÑÁºñÂè∑\n\
          \    most_common = counter.most_common(1)[0]\n    most_common_num, most_common_count\
          \ = most_common\n    \n    # Â¶ÇÊûúÊüê‰∏™Êï∞Â≠óÂá∫Áé∞Ê¨°Êï∞Ë∂ÖËøáÊÄªÊï∞ÁöÑ60%Ôºå‰∏îËøô‰∏™Êï∞Â≠óÂ§ß‰∫é30ÔºåÂèØËÉΩÊòØÊ≥ïËßÑÁºñÂè∑Ê®°Âºè\n  \
          \  if most_common_count > len(first_numbers) * 0.6 and most_common_num >\
          \ 30:\n        print(f\"Ê£ÄÊµãÂà∞ÂèØËÉΩÁöÑÊ≥ïËßÑÁºñÂè∑Ê®°Âºè: {most_common_num}.x (Âá∫Áé∞{most_common_count}Ê¨°)\"\
          )\n        # Âú®ËøôÁßçÊÉÖÂÜµ‰∏ãÔºåÂÖÅËÆ∏Ëøô‰∏™ÁâπÂÆöÁöÑÊ≥ïËßÑÁºñÂè∑\n        return {\n            \"min_reasonable\"\
          : most_common_num, \n            \"max_reasonable\": most_common_num, \n\
          \            \"primary_range\": (most_common_num, most_common_num),\n  \
          \          \"regulation_mode\": True,\n            \"regulation_number\"\
          : most_common_num\n        }\n    \n    # Ê≠£Â∏∏ÁöÑÁ´†ËäÇÁºñÂè∑Ê®°Âºè\n    min_num = min(first_numbers)\n\
          \    max_num = max(first_numbers)\n    \n    # Â¶ÇÊûúÊï∞Â≠óËåÉÂõ¥ÂæàÂ∞èÔºà<= 50ÔºâÔºåËÆ§‰∏∫ÊòØÊ≠£Â∏∏Á´†ËäÇ\n\
          \    if max_num <= 50:\n        return {\n            \"min_reasonable\"\
          : max(1, min_num), \n            \"max_reasonable\": min(50, max_num + 5),\
          \  # ÂÖÅËÆ∏Â∞ëÈáèË∂ÖÂá∫\n            \"primary_range\": (min_num, max_num)\n       \
          \ }\n    \n    # Â¶ÇÊûúÊï∞Â≠óËåÉÂõ¥ÂæàÂ§ßÔºåÂèØËÉΩÂåÖÂê´È°µÁ†ÅÁ≠âÂπ≤Êâ∞ÔºåÈááÁî®Êõ¥‰øùÂÆàÁ≠ñÁï•\n    # ÊâæÂà∞ÊúÄÂØÜÈõÜÁöÑÊï∞Â≠óÂå∫Èó¥\n    gaps\
          \ = []\n    for i in range(len(first_numbers) - 1):\n        gaps.append(first_numbers[i\
          \ + 1] - first_numbers[i])\n    \n    # Â¶ÇÊûúÊúâÊòéÊòæÁöÑÂ§ßË∑≥Ë∑ÉÔºà>20ÔºâÔºåÂèØËÉΩÂâçÈù¢ÊòØÊ≠£Â∏∏Á´†ËäÇÔºåÂêéÈù¢ÊòØÈ°µÁ†ÅÁ≠â\n\
          \    large_gap_idx = -1\n    for i, gap in enumerate(gaps):\n        if\
          \ gap > 20:\n            large_gap_idx = i\n            break\n    \n  \
          \  if large_gap_idx != -1:\n        # ÂèñË∑≥Ë∑ÉÂâçÁöÑÊï∞Â≠ó‰Ωú‰∏∫ÂêàÁêÜËåÉÂõ¥\n        reasonable_max\
          \ = first_numbers[large_gap_idx]\n        return {\n            \"min_reasonable\"\
          : max(1, min_num), \n            \"max_reasonable\": reasonable_max,\n \
          \           \"primary_range\": (min_num, reasonable_max)\n        }\n  \
          \  \n    # ÈªòËÆ§‰øùÂÆàÁ≠ñÁï•\n    return {\"min_reasonable\": 1, \"max_reasonable\"\
          : 50, \"primary_range\": (1, 50)}\n\ndef evaluate_content_richness_and_decide_abandonment(chapters:\
          \ List[Dict], chain_indices: List[int]) -> bool:\n    \"\"\"ËØÑ‰º∞Á´†ËäÇÈìæÁöÑÂÜÖÂÆπ‰∏∞Â∫¶ÔºåÂπ∂ÂÜ≥ÂÆöÊòØÂê¶ÊîæÂºÉÂΩìÂâçÈìæ\"\
          \"\"\n    ############## ÂÜÖÂÆπ‰∏∞Â∫¶ËØÑ‰º∞ÈÄªËæë #################\n    def evaluate_content_richness(chain_chapters:\
          \ List[Dict]) -> Dict:\n        \"\"\"ËØÑ‰º∞Á´†ËäÇÈìæÁöÑÂÜÖÂÆπ‰∏∞Â∫¶\"\"\"\n        total_chars\
          \ = 0\n        chapters_with_content = 0\n        total_chapters = len(chain_chapters)\n\
          \        \n        # ËÆ°ÁÆóÂπ≥ÂùáÊ†áÈ¢òÈïøÂ∫¶\n        title_lengths = [len(ch.get('chapter_title',\
          \ '').strip()) for ch in chain_chapters]\n        avg_title_length = sum(title_lengths)\
          \ / len(title_lengths) if title_lengths else 0\n        \n        # ÁªüËÆ°ÊúâÂÆûË¥®ÂÜÖÂÆπÁöÑÁ´†ËäÇ\n\
          \        for ch in chain_chapters:\n            raw_text = ch.get('raw_text',\
          \ '').strip()\n            title = ch.get('chapter_title', '').strip()\n\
          \            \n            # ËÆ°ÁÆóÊÄªÂ≠óÁ¨¶Êï∞ÔºàÊ†áÈ¢ò+Ê≠£ÊñáÔºâ\n            total_chars += len(title)\
          \ + len(raw_text)\n            \n            # Âà§Êñ≠ÊòØÂê¶ÊúâÂÆûË¥®ÂÜÖÂÆπ\n            if\
          \ (len(raw_text) > 20 or  # Ê≠£ÊñáË∂ÖËøá20Â≠óÁ¨¶\n                len(title) > 30 or\
          \     # Ê†áÈ¢òË∂ÖËøá30Â≠óÁ¨¶ÔºàÂèØËÉΩÊòØÊÆµËêΩÔºâ\n                '.' in title or ',' in title or\
          \ 'Ôºõ' in title or '„ÄÇ' in title):  # ÂåÖÂê´Ê†áÁÇπÁ¨¶Âè∑\n                chapters_with_content\
          \ += 1\n        \n        # ËÆ°ÁÆóÂêÑÁßçÊåáÊ†á\n        avg_chars_per_chapter = total_chars\
          \ / total_chapters if total_chapters > 0 else 0\n        content_ratio =\
          \ chapters_with_content / total_chapters if total_chapters > 0 else 0\n\
          \        \n        # Ê£ÄÊµãÊòØÂê¶‰∏∫Ë°®Ê†º/Ë°®ÂçïÁªìÊûÑÔºàÂ§ßÈáèÁü≠Ê†áÈ¢òÔºåÊó†ÂÆûË¥®ÂÜÖÂÆπÔºâ\n        short_titles = sum(1\
          \ for length in title_lengths if length <= 20)\n        short_title_ratio\
          \ = short_titles / total_chapters if total_chapters > 0 else 0\n       \
          \ \n        return {\n            'total_chars': total_chars,\n        \
          \    'avg_chars_per_chapter': avg_chars_per_chapter,\n            'content_ratio':\
          \ content_ratio,\n            'avg_title_length': avg_title_length,\n  \
          \          'short_title_ratio': short_title_ratio,\n            'total_chapters':\
          \ total_chapters,\n            'chapters_with_content': chapters_with_content\n\
          \        }\n    \n    # ËØÑ‰º∞ÂΩìÂâçÊúÄÈïøÈìæÁöÑÂÜÖÂÆπ‰∏∞Â∫¶\n    chain_chapters = [chapters[i]\
          \ for i in chain_indices]\n    richness_metrics = evaluate_content_richness(chain_chapters)\n\
          \    \n    print(f\"ÂÜÖÂÆπ‰∏∞Â∫¶ËØÑ‰º∞: {richness_metrics}\")\n    # ÂÜ≥Á≠ñÈÄªËæëÔºöÂ¶ÇÊûúÁ´†ËäÇÊï∞ÈáèÂ§ö‰ΩÜÂÜÖÂÆπË¥´‰πèÔºåÊîæÂºÉÂΩìÂâçÈìæ\n\
          \    should_abandon_chain = (\n        richness_metrics['total_chapters']\
          \ > 10 and (  # Á´†ËäÇÊï∞ÈáèÂ§öÁöÑÊÉÖÂÜµ‰∏ã\n            (richness_metrics['avg_chars_per_chapter']\
          \ < 30 and  # Âπ≥ÂùáÂ≠óÁ¨¶Êï∞ÂæàÂ∞ë\n                richness_metrics['content_ratio']\
          \ < 0.4) or         # ‰∏îÊúâÂÆûË¥®ÂÜÖÂÆπÊØî‰æã‰Ωé\n                \n            (richness_metrics['short_title_ratio']\
          \ > 0.6 and     # ÊàñËÄÖÁü≠Ê†áÈ¢òÊØî‰æãÂæàÈ´ò\n                richness_metrics['avg_chars_per_chapter']\
          \ < 50)     # ‰∏îÂπ≥ÂùáÂ≠óÁ¨¶Êï∞‰∏çÂ§ö\n        )\n    )\n    \n    return should_abandon_chain\n\
          \ndef find_longest_chapter_chain_with_append(chapters: List[Dict], language:\
          \ str = 'en') -> Tuple[List[Dict], str]:\n    # ÂÖàÊ£ÄÊµãÁ´†ËäÇÊ®°Âºè\n    pattern = detect_chapter_pattern(chapters)\n\
          \    print(f\"Ê£ÄÊµãÂà∞Á´†ËäÇÊ®°Âºè: {pattern}\")\n    \n    # \U0001F195 ÂàÜÊûêÁ´†ËäÇÊï∞Â≠óÂàÜÂ∏É\n \
          \   number_analysis = analyze_chapter_number_distribution(chapters)\n  \
          \  print(f\"Á´†ËäÇÊï∞Â≠óÂàÜÊûêÁªìÊûú: {number_analysis}\")\n    \n    # Áî®Ê£ÄÊµãÂà∞ÁöÑÊ®°ÂºèÈáçÊñ∞Ëß£ÊûêÁ´†ËäÇID\n\
          \    parsed_ids = [parse_chapter_id(ch[\"chapter_id\"], pattern) for ch\
          \ in chapters]\n    # print(f'Á¨¨‰∏Ä‰∏™Á´†ËäÇ: {chapters[0]}')\n    n = len(chapters)\n\
          \n    # Á¨¨‰∏ÄÊ≠•ÔºöËøáÊª§ÊéâÊòéÊòæ‰∏çÂêàÁêÜÁöÑÁ´†ËäÇÔºàÂ¶ÇËØØËØÜÂà´ÁöÑÊï∞Â≠óÔºâ\n    valid_indices = []\n    for i in range(n):\n\
          \        if not parsed_ids[i]:\n            continue\n            \n   \
          \     # Ê£ÄÊü•ÊòØÂê¶ÊòØÊòéÊòæÁöÑËØØËØÜÂà´\n        chapter_text = chapters[i][\"chapter_id\"]\
          \ + \" \" + chapters[i][\"chapter_title\"]\n        \n        # ËøáÊª§ÊòéÊòæÁöÑÊµãÈáèÂçï‰Ωç„ÄÅÈ¢ëÁéáËåÉÂõ¥„ÄÅÁ∫ØÊï∞Â≠óÁ≠â\n\
          \        if re.search(r'\\b\\d+\\s*(MHz|GHz|Hz|kHz|dB|V|mV|¬µV|A|mA|¬µA|W|mW|Œ©|%|¬∞C|¬∞F|mm|cm|m|km|kg|g|mg|ms|s|min|h|rpm|bar|Pa|kPa|MPa)\\\
          b', chapter_text, re.I):\n            continue\n        if re.search(r'\\\
          d+\\s*MHz\\s*[~-]\\s*\\d+\\s*MHz', chapter_text, re.I):\n            continue\n\
          \        if re.match(r'^\\d+\\s*$', chapters[i][\"chapter_title\"].strip()):\
          \  # Ê†áÈ¢òÊòØÁ∫ØÊï∞Â≠ó\n            continue\n        if len(chapters[i][\"chapter_title\"\
          ].strip()) < 2:  # Ê†áÈ¢òÂ§™Áü≠\n            continue\n            \n        # \U0001F195\
          \ Ë°®Ê†ºÊï∞ÊçÆÁâπÂæÅËøáÊª§\n        chapter_title = chapters[i][\"chapter_title\"].strip()\n\
          \        chapter_id = chapters[i][\"chapter_id\"].strip()\n        \n  \
          \      # Ê£ÄÊµãË°®Ê†ºË°åÊ®°ÂºèÔºöÂçï‰∏™Â≠óÊØç + ‰∏ªË¶ÅÊòØÊï∞Â≠óÁöÑÊ†áÈ¢ò\n        if (len(chapter_id) == 1 and chapter_id.isupper()\
          \ and \n            re.search(r'^\\d+.*\\d+', chapter_title) and \n    \
          \        len([x for x in chapter_title.split() if x.isdigit()]) >= 2):\n\
          \            continue\n            \n        # Ê£ÄÊµãÂùêÊ†áÁÇπÊ†ºÂºèÔºöÂ¶Ç \"10 0 E 0 16\"\
          \n        if re.match(r'^\\d+\\s+\\d+\\s+[A-Z]\\s+\\d+\\s+\\d+', chapter_title):\n\
          \            continue\n            \n        # Ê£ÄÊµãÂèÇÊï∞Ë°®Ê†ºÊ†ºÂºèÔºöÂ¶Ç \"34 65 F 25 77\"\
          \n        title_parts = chapter_title.split()\n        if (len(title_parts)\
          \ >= 4 and \n            sum(1 for part in title_parts if part.isdigit())\
          \ >= 3 and\n            sum(1 for part in title_parts if len(part) == 1\
          \ and part.isupper()) >= 1):\n            continue\n            \n     \
          \   # Ê£ÄÊµãÂõæË°®Ê†áÊ≥®ËØ¥ÊòéÔºöÂçï‰∏™Â≠óÊØç + ‰ª•Á†¥ÊäòÂè∑ÂºÄÂ§¥ÁöÑÊ†áÈ¢ò\n        if (len(chapter_id) == 1 and chapter_id.isupper()\
          \ and \n            chapter_title.startswith('‚Äî‚Äî‚Äî')):\n            continue\n\
          \            \n        valid_indices.append(i)\n    \n    # Á¨¨‰∫åÊ≠•ÔºöÈ™åËØÅÂ≠óÊØçÁ´†ËäÇÁöÑÂêàÁêÜÊÄßÔºàÈíàÂØπ‰∏≠Ëã±ÊñáÂ∑ÆÂºÇÂåñÂ§ÑÁêÜÔºâ\n\
          \    if valid_indices:\n        # Ê£ÄÊü•ÊòØÂê¶ÂåÖÂê´Â≠óÊØçÁ´†ËäÇ\n        alpha_chapters = []\n\
          \        for i, idx in enumerate(valid_indices):\n            chapter_id\
          \ = chapters[idx][\"chapter_id\"].strip()\n            if re.match(r'^[A-Z](?:\\\
          .\\d+)*\\.?$', chapter_id):\n                alpha_chapters.append((i, idx,\
          \ chapter_id[0]))  # (Âú®valid_indices‰∏≠ÁöÑ‰ΩçÁΩÆ, ÂéüÂßãÁ¥¢Âºï, È¶ñÂ≠óÊØç)\n        \n       \
          \ # Â¶ÇÊûúÊúâÂ≠óÊØçÁ´†ËäÇÔºåËøõË°åÂêàÁêÜÊÄßÈ™åËØÅ\n        if alpha_chapters:\n            if language\
          \ == 'en':\n                # Ëã±ÊñáÊñáÊ°£ÔºöË¶ÅÊ±ÇÂ≠óÊØçÁ´†ËäÇÂøÖÈ°ª‰ªéAÂºÄÂ§¥\n                first_alpha_letter\
          \ = alpha_chapters[0][2]\n                if first_alpha_letter != 'A':\n\
          \                    print(f\"Ëã±ÊñáÊñáÊ°£Â≠óÊØçÁ´†ËäÇ‰∏ç‰ª•AÂºÄÂ§¥ÔºåË∑≥Ëøá: Á¨¨‰∏Ä‰∏™Â≠óÊØçÁ´†ËäÇÊòØ {alpha_chapters[0][2]}\"\
          )\n                    # ÁßªÈô§ÊâÄÊúâÂ≠óÊØçÁ´†ËäÇ\n                    alpha_indices_set\
          \ = {item[1] for item in alpha_chapters}\n                    valid_indices\
          \ = [idx for idx in valid_indices if idx not in alpha_indices_set]\n   \
          \         else:\n                # ‰∏≠ÊñáÊñáÊ°£ÔºöÊ£ÄÊü•Â≠óÊØçÁ´†ËäÇÁöÑ‰∏ÄËá¥ÊÄßÔºàÂêå‰∏ÄÈôÑÂΩïÂ∫îËØ•‰ª•Âêå‰∏ÄÂ≠óÊØçÂºÄÂ§¥Ôºâ\n    \
          \            from collections import Counter\n                alpha_letters\
          \ = [item[2] for item in alpha_chapters]\n                letter_counter\
          \ = Counter(alpha_letters)\n                most_common_letter, most_common_count\
          \ = letter_counter.most_common(1)[0]\n                \n               \
          \ # Â¶ÇÊûúÊüê‰∏™Â≠óÊØçÂá∫Áé∞Ê¨°Êï∞Ë∂ÖËøá60%ÔºåËÆ§‰∏∫ËøôÊòØ‰∏ªË¶ÅÁöÑÈôÑÂΩïÂ≠óÊØç\n                if most_common_count >\
          \ len(alpha_chapters) * 0.6:\n                    print(f\"‰∏≠ÊñáÊñáÊ°£Ê£ÄÊµãÂà∞‰∏ªË¶ÅÈôÑÂΩïÂ≠óÊØç:\
          \ {most_common_letter} (Âá∫Áé∞{most_common_count}Ê¨°)\")\n                   \
          \ # ‰øùÁïô‰∏é‰∏ªË¶ÅÂ≠óÊØç‰∏ÄËá¥ÁöÑÁ´†ËäÇÔºåÁßªÈô§ÂÖ∂‰ªñÂ≠óÊØçÁ´†ËäÇ\n                    keep_alpha_indices = {item[1]\
          \ for item in alpha_chapters if item[2] == most_common_letter}\n       \
          \             remove_alpha_indices = {item[1] for item in alpha_chapters\
          \ if item[2] != most_common_letter}\n                    valid_indices =\
          \ [idx for idx in valid_indices if idx not in remove_alpha_indices]\n  \
          \                  if remove_alpha_indices:\n                        removed_letters\
          \ = {chapters[idx][\"chapter_id\"].strip()[0] for idx in remove_alpha_indices}\n\
          \                        print(f\"ÁßªÈô§‰∏ç‰∏ÄËá¥ÁöÑÂ≠óÊØçÁ´†ËäÇ: {removed_letters}\")\n   \
          \             else:\n                    # Â¶ÇÊûúÊ≤°ÊúâÊòéÊòæÁöÑ‰∏ªË¶ÅÂ≠óÊØçÔºå‰øùÊåÅÂéüÊúâÈÄªËæëÔºàÂèØËÉΩÊòØÊ∑∑ÂêàÊÉÖÂÜµÔºâ\n\
          \                    print(f\"‰∏≠ÊñáÊñáÊ°£Â≠óÊØçÁ´†ËäÇÂàÜÂ∏ÉËæÉÂùáÂåÄ: {dict(letter_counter)}\")\n\
          \                    # ‰∏çÂÅöÁâπÊÆäÂ§ÑÁêÜÔºå‰øùÁïôÊâÄÊúâÂ≠óÊØçÁ´†ËäÇ\n\n    # Á¨¨‰∏âÊ≠•Ôºö‰ªéÂêéÂæÄÂâçÊûÑÂª∫ÊúÄÈïøÈìæ\n    dp =\
          \ [1] * len(valid_indices)\n    next_link = [-1] * len(valid_indices)  #\
          \ Êîπ‰∏∫ËÆ∞ÂΩï‰∏ã‰∏Ä‰∏™ËäÇÁÇπ\n    max_len = 0\n    max_idx = -1\n\n    # ‰ªéÂêéÂæÄÂâçÈÅçÂéÜ\n    for\
          \ i in range(len(valid_indices) - 1, -1, -1):\n        curr_idx = valid_indices[i]\n\
          \        curr_parsed = parsed_ids[curr_idx]\n        \n        # ÊâæÂú®ÂΩìÂâçËäÇÁÇπ‰πãÂêéÁöÑÊâÄÊúâËäÇÁÇπ\n\
          \        for j in range(i + 1, len(valid_indices)):\n            next_idx\
          \ = valid_indices[j]\n            next_parsed = parsed_ids[next_idx]\n \
          \           \n            # Ê£ÄÊü•ÂΩìÂâçËäÇÁÇπÊòØÂê¶ÂèØ‰ª•ËøûÂà∞‰∏ã‰∏Ä‰∏™ËäÇÁÇπ\n            if (is_chapter_a_before_b(curr_parsed,\
          \ next_parsed) and \n                is_reasonable_chapter_jump(curr_parsed,\
          \ next_parsed)):\n                if dp[j] + 1 > dp[i]:\n              \
          \      dp[i] = dp[j] + 1\n                    next_link[i] = j\n       \
          \ \n        if dp[i] > max_len:\n            max_len = dp[i]\n         \
          \   max_idx = i\n\n    # Á¨¨ÂõõÊ≠•ÔºöÂ¶ÇÊûúÊ≤°ÊúâÊâæÂà∞ÂêàÁêÜÁöÑÈìæÔºåÈÄÄÂõûÂà∞ÁÆÄÂçïÁöÑÈ°∫Â∫èËøáÊª§\n    if max_len < 2:\n\
          \        # ÁÆÄÂçïÊåâÁ´†ËäÇÁºñÂè∑È°∫Â∫èËøáÊª§\n        filtered_chapters = simple_chapter_filter(chapters)\n\
          \        \n        # Â¶ÇÊûúËøáÊª§ÂêéËøòÊòØÊ≤°ÊúâÁ´†ËäÇÔºåÂ∞ÜÊâÄÊúâÂÜÖÂÆπÊîæÂÖ•Ë∑≥ËøáÁöÑÂÜÖÂÆπ‰∏≠\n        if not filtered_chapters:\n\
          \            all_content = []\n            for ch in chapters:\n       \
          \         content = f\"{ch['chapter_id']} {ch['chapter_title']}\"\n    \
          \            if ch.get('raw_text'):\n                    content += \" \"\
          \ + ch['raw_text']\n                all_content.append(content)\n      \
          \      skipped_text = \"\\n\".join(all_content)\n            return [],\
          \ skipped_text\n        \n        return filtered_chapters, \"\"\n\n   \
          \ # ÂõûÊ∫ØÂá∫‰∏ªÈìæÁ¥¢ÂºïÔºà‰ªéÂâçÂæÄÂêéÁöÑÊ≠£Á°ÆÈ°∫Â∫èÔºâ\n    chain_indices = []\n    idx = max_idx\n    while\
          \ idx != -1:\n        chain_indices.append(valid_indices[idx])\n       \
          \ idx = next_link[idx]\n    \n    print(f\"‰ªéÂêéÂæÄÂâçÁîüÊàêÁöÑÊúÄÈïøÈìæ: ÈïøÂ∫¶={len(chain_indices)},\
          \ ‰ΩçÁΩÆ={chain_indices[:5]}{'...' if len(chain_indices)>5 else ''}\")\n   \
          \ \n    # ÊúÄÈïøÈìæÁöÑÁ¨¨‰∏Ä‰∏™Á´†ËäÇÁ¥¢Âºï\n    first_chain_idx = chain_indices[0]\n    \n  \
          \  # ÁîüÊàêË∑≥ËøáÁöÑÂÜÖÂÆπÔºàÊúÄÈïøÈìæÁ¨¨‰∏Ä‰∏™Á´†ËäÇ‰πãÂâçÁöÑÊâÄÊúâÂÜÖÂÆπÔºâ\n    skipped_chapters = chapters[:first_chain_idx]\n\
          \    skipped_text = \"\\n\".join([f\"{ch['chapter_id']} {ch['chapter_title']}\
          \ {ch.get('raw_text','')}\" for ch in skipped_chapters])\n    \n    chain_set\
          \ = set(chain_indices)\n    \n    # \U0001F195 Ê∑ªÂä†ÂÜÖÂÆπ‰∏∞Â∫¶Ê£ÄÊü•\n    if language\
          \ == 'en' and evaluate_content_richness_and_decide_abandonment(chapters,\
          \ chain_indices):\n        # Â∞ÜÊâÄÊúâÁ´†ËäÇÂÜÖÂÆπÂêàÂπ∂‰∏∫Ë∑≥ËøáÁöÑÂÜÖÂÆπ\n        print(\"=====================================\"\
          )\n        print(\"ÂÜÖÂÆπ‰∏∞Â∫¶ËØÑ‰º∞ÂÜ≥ÂÆöÊîæÂºÉÂΩìÂâçÈìæÔºåËøîÂõûÁ©∫ÁªìÊûú\")\n        print(\"=====================================\"\
          )\n        all_content = []\n        for ch in chapters:\n            content\
          \ = f\"{ch['chapter_id']} {ch['chapter_title']}\"\n            if ch.get('raw_text'):\n\
          \                content += \" \" + ch['raw_text']\n            all_content.append(content)\n\
          \        skipped_text = \"\\n\".join(all_content)\n        \n        return\
          \ [], skipped_text\n    \n    # ÊúÄÁªàÁªìÊûúÊûÑÂª∫\n    result = []\n    last_valid\
          \ = None\n    for i, chap in enumerate(chapters):\n        if i in chain_set:\n\
          \            result.append(chap)\n            last_valid = chap\n      \
          \  elif i >= first_chain_idx:  # Âè™Â§ÑÁêÜÊúÄÈïøÈìæÂºÄÂßã‰πãÂêéÁöÑÁ´†ËäÇ\n            if last_valid:\n\
          \                content_to_add = \"\\n\" + chap[\"chapter_id\"] + chap[\"\
          chapter_title\"]\n                if chap.get(\"raw_text\"):\n         \
          \           content_to_add += \" \" + chap[\"raw_text\"]\n             \
          \   last_valid[\"raw_text\"] += content_to_add\n\n    # Âà§Êñ≠Á´†ËäÇÊ†áÈ¢òÊòØÂê¶Â∫îËØ•ÂêàÂπ∂Âà∞Ê≠£Êñá‰∏≠\n\
          \    for chap in result:\n        should_merge = False\n        \n     \
          \   # ‰∏≠ÊñáÂ§ÑÁêÜÔºöÂåÖÂê´‰∏≠Êñá‰∏îÊúâ‰∏≠ÊñáÈÄóÂè∑ÔºåÂè•Âè∑ÔºåÂÜíÂè∑ÔºåÊàñËÄÖÈïøÂ∫¶Â§ß‰∫é30Â≠óÁ¨¶\n        if re.search(r'[\\u4e00-\\\
          u9fa5]', chap[\"chapter_title\"]):\n            # Ëé∑ÂèñÁ´†ËäÇÁºñÂè∑ÁöÑÁ¨¨‰∏Ä‰∏™Êï∞Â≠óÔºåÂâç‰∏âÁ´†Ë∑≥ËøáÂêàÂπ∂Âà§Êñ≠\n\
          \            first_num = None\n            chapter_id = chap[\"chapter_id\"\
          ].strip('.-')\n            if re.match(r'^\\d+', chapter_id):\n        \
          \        first_num = int(re.match(r'^\\d+', chapter_id).group())\n     \
          \       # Ââç‰∏âÁ´†Ë∑≥ËøáÂêàÂπ∂Âà§Êñ≠\n            if first_num is not None and first_num\
          \ <= 3:\n                continue          \n            if re.search(r'[Ôºå„ÄÇÔºö,:]',\
          \ chap[\"chapter_title\"]) or len(chap[\"chapter_title\"]) > 30:\n     \
          \           should_merge = True\n        \n        # Ëã±ÊñáÂ§ÑÁêÜÔºöÊõ¥Êô∫ËÉΩÁöÑÂà§Êñ≠ÈÄªËæë\n   \
          \     else:\n            # Â¶ÇÊûúÂÖ®Â§ßÂÜôÔºåÂàôËÇØÂÆöÊòØÊ†áÈ¢ò\n            if chap[\"chapter_title\"\
          ].isupper():\n                continue\n            # 1. Â¶ÇÊûúraw_text‰ª•Â∞èÂÜôÂ≠óÊØçÂºÄÂ§¥ÔºåÂèØËÉΩÊòØÊ†áÈ¢òÁöÑÂª∂Áª≠\n\
          \            if len(chap[\"raw_text\"]) and chap[\"raw_text\"][0].islower():\n\
          \                should_merge = True\n            # 2. Â¶ÇÊûúchapter_titleÂåÖÂê´ÂÆåÊï¥Âè•Â≠êÁöÑÁâπÂæÅ\n\
          \            elif re.search(r'[,;!?]', chap[\"chapter_title\"]):\n     \
          \           should_merge = True\n            # 3. Â¶ÇÊûúchapter_titleÂæàÈïøÔºàË∂ÖËøá50‰∏™Â≠óÁ¨¶ÔºâÔºåÂèØËÉΩÊòØÊÆµËêΩÊñáÊú¨\n\
          \            elif len(chap[\"chapter_title\"]) > 50:\n                should_merge\
          \ = True\n        \n        if should_merge:\n            chap[\"raw_text\"\
          ] = chap[\"chapter_title\"] + ' ' + chap[\"raw_text\"]\n            chap[\"\
          chapter_title\"] = \"\"\n\n    return result, skipped_text\n\ndef simple_chapter_filter(chapters:\
          \ List[Dict]) -> List[Dict]:\n    \"\"\"\n    ÁÆÄÂçïÁöÑÁ´†ËäÇËøáÊª§Á≠ñÁï•ÔºöÂΩìÊúÄÈïøÈìæÁÆóÊ≥ïÂ§±ÊïàÊó∂ÁöÑÂ§áÁî®ÊñπÊ°à\n\
          \    \"\"\"\n    # Ê£ÄÊµãÁ´†ËäÇÊ®°Âºè\n    pattern = detect_chapter_pattern(chapters)\n\
          \    \n    result = []\n    parsed_ids = [parse_chapter_id(ch[\"chapter_id\"\
          ], pattern) for ch in chapters]\n    \n    for i, chap in enumerate(chapters):\n\
          \        parsed_id = parsed_ids[i]\n        \n        # Âü∫Êú¨ÂêàÁêÜÊÄßÊ£ÄÊü•\n      \
          \  if not parsed_id:\n            # Êó†Ê≥ïËß£ÊûêÁöÑÁ´†ËäÇÔºåËøΩÂä†Âà∞‰∏ä‰∏Ä‰∏™ÊúâÊïàÁ´†ËäÇ\n            if result:\n\
          \                content_to_add = \"\\n\" + chap[\"chapter_id\"] + chap[\"\
          chapter_title\"]\n                if chap.get(\"raw_text\"):\n         \
          \           content_to_add += \" \" + chap[\"raw_text\"]\n             \
          \   result[-1][\"raw_text\"] += content_to_add\n            continue\n \
          \       \n        # Ê£ÄÊü•Á´†ËäÇÁºñÂè∑ÊòØÂê¶Âú®ÂêàÁêÜËåÉÂõ¥ÂÜÖ\n        first_num = parsed_id[0]\n \
          \       \n        # Ê†πÊçÆÊ®°ÂºèË∞ÉÊï¥ÂêàÁêÜÊÄßÊ£ÄÊü•\n        if pattern == 'alpha_first':\n\
          \            # Â≠óÊØçÂú®ÂâçÔºöA=1, B=2, ..., 1=27, 2=28, ...\n            if 1 <=\
          \ first_num <= 50:  # ÂêàÁêÜËåÉÂõ¥Ôºö26‰∏™Â≠óÊØç + 20‰∏™Êï∞Â≠óÁ´†ËäÇ\n                result.append(chap)\n\
          \            else:\n                # ‰∏çÂêàÁêÜÁöÑÁ´†ËäÇÔºåËøΩÂä†Âà∞‰∏ä‰∏Ä‰∏™ÊúâÊïàÁ´†ËäÇ\n              \
          \  if result:\n                    content_to_add = \"\\n\" + chap[\"chapter_id\"\
          ] + chap[\"chapter_title\"]\n                    if chap.get(\"raw_text\"\
          ):\n                        content_to_add += \" \" + chap[\"raw_text\"\
          ]\n                    result[-1][\"raw_text\"] += content_to_add\n    \
          \    else:\n            # Êï∞Â≠óÂú®ÂâçÔºö1, 2, ..., A=101, B=102, ...\n          \
          \  if (1 <= first_num <= 20) or (101 <= first_num <= 126):  # Êï∞Â≠óÁ´†ËäÇÊàñÂ≠óÊØçÁ´†ËäÇ\n\
          \                result.append(chap)\n            else:\n              \
          \  # ‰∏çÂêàÁêÜÁöÑÁ´†ËäÇÔºåËøΩÂä†Âà∞‰∏ä‰∏Ä‰∏™ÊúâÊïàÁ´†ËäÇ\n                if result:\n                   \
          \ content_to_add = \"\\n\" + chap[\"chapter_id\"] + chap[\"chapter_title\"\
          ]\n                    if chap.get(\"raw_text\"):\n                    \
          \    content_to_add += \" \" + chap[\"raw_text\"]\n                    result[-1][\"\
          raw_text\"] += content_to_add\n    \n    return result\n    \n    return\
          \ result\n\ndef split_sections_by_attachment(chapters: List[Dict]) -> List[Dict]:\n\
          \    \"\"\"\n    Â∞ÜÊï¥‰∏™ÊñáÊ°£ÊåâÈôÑ‰ª∂ÔºàANNEXÔºâÂàáÂàÜ„ÄÇ\n    È°∂Â±Ç file: regulation / ANNEX n\n\
          \    ÊîπËøõÔºöÂêàÂπ∂ËøûÁª≠ÁöÑÁõ∏ÂêåÈôÑ‰ª∂Ê†áÈ¢ò\n    \"\"\"\n    sections = []\n    current_section\
          \ = {\n        \"section\": \"regulation\",  # ÈªòËÆ§‰∏ªÊñáÊ°£\n        \"chapters\"\
          : []\n    }\n\n    annex_pattern = re.compile(r'^(ANNEX|ATTACHMENT)\\s+([A-Z0-9]+)',\
          \ re.I)\n\n    for chap in chapters:\n        match = annex_pattern.match(chap['chapter_id'])\n\
          \        if match:\n            annex_name = match.group(1).upper() + \"\
          \ \" + match.group(2)  # Ê†áÂáÜÂåñÂêçÁß∞ÔºåÂ¶Ç \"ANNEX 1\"\n            \n           \
          \ # Ê£ÄÊü•ÊòØÂê¶‰∏éÂΩìÂâç section ÁöÑÂêçÁß∞Áõ∏Âêå\n            if current_section[\"section\"] !=\
          \ \"regulation\" and current_section[\"section\"].upper() == annex_name:\n\
          \                # Áõ∏ÂêåÁöÑÈôÑ‰ª∂ÔºåÁõ¥Êé•Ê∑ªÂä†Âà∞ÂΩìÂâç sectionÔºåË∑≥ËøáÈáçÂ§çÁöÑÊ†áÈ¢òÁ´†ËäÇ\n                if chap.get('chapter_title')\
          \ or chap.get('raw_text', '').strip():\n                    current_section[\"\
          chapters\"].append(chap)\n                # Â¶ÇÊûúÊòØÁ©∫ÁöÑÈáçÂ§çÊ†áÈ¢òÁ´†ËäÇÔºàÂè™Êúâchapter_idÊ≤°ÊúâÂÜÖÂÆπÔºâÔºåÂàôË∑≥Ëøá\n\
          \            else:\n                # ‰∏çÂêåÁöÑÈôÑ‰ª∂Ôºå‰øùÂ≠òÂΩìÂâçÂùóÂπ∂Êñ∞Âª∫\n                if\
          \ current_section[\"chapters\"]:\n                    sections.append(current_section)\n\
          \                # Êñ∞Âª∫ÈôÑ‰ª∂Âùó\n                current_section = {\n        \
          \            \"section\": annex_name,\n                    \"chapters\"\
          : [chap] if (chap.get('chapter_title') or chap.get('raw_text', '').strip())\
          \ else []\n                }\n        else:\n            current_section[\"\
          chapters\"].append(chap)\n\n    if current_section[\"chapters\"]:\n    \
          \    sections.append(current_section)\n\n    return sections\n\ndef split_sections_by_appendix(chapters):\n\
          \    sections = []\n    current_section = {\"section\": \"MAIN\", \"chapters\"\
          : []}\n\n    for ch in chapters:\n        # Ê£ÄÊµã APPENDIX ÂºÄÂ§¥ÁöÑÈ°∂Â±ÇÊ†áÈ¢òÔºåÊàñËÄÖÈôÑÂΩï\n \
          \       appendix_match = re.match(r'^(APPENDIX\\s+(?:[A-Z0-9]+|\\([A-Z0-9]+\\\
          )))$', ch['chapter_id'], re.IGNORECASE)\n        annex_match = ch[\"chapter_id\"\
          ].startswith(\"ÈôÑÂΩï\")\n        \n        if appendix_match or annex_match:\n\
          \            # Ê†áÂáÜÂåñÈôÑÂΩïÂêçÁß∞\n            if appendix_match:\n               \
          \ appendix_name = appendix_match.group(1).upper()\n            else:\n \
          \               appendix_name = ch['chapter_id'].strip()\n            \n\
          \            # Ê£ÄÊü•ÊòØÂê¶‰∏éÂΩìÂâç section ÁöÑÂêçÁß∞Áõ∏Âêå\n            if current_section[\"\
          section\"] != \"MAIN\" and current_section[\"section\"].upper() == appendix_name:\n\
          \                # Áõ∏ÂêåÁöÑÈôÑÂΩïÔºåÁõ¥Êé•Ê∑ªÂä†Âà∞ÂΩìÂâç sectionÔºàÂ¶ÇÊûúÊúâÂÆûÈôÖÂÜÖÂÆπÔºâ\n                if ch.get('chapter_title')\
          \ or ch.get('raw_text', '').strip():\n                    current_section[\"\
          chapters\"].append(ch)\n                # Â¶ÇÊûúÊòØÁ©∫ÁöÑÈáçÂ§çÊ†áÈ¢òÁ´†ËäÇÔºåÂàôË∑≥Ëøá\n            else:\n\
          \                # ‰∏çÂêåÁöÑÈôÑÂΩïÔºåÂÖà‰øùÂ≠òÂΩìÂâçÂùó\n                if current_section[\"chapters\"\
          ]:\n                    sections.append(current_section)\n             \
          \   # Êñ∞Âª∫ÈôÑÂΩïÂùó\n                current_section = {\n                    \"\
          section\": appendix_name,\n                    \"chapters\": [ch] if (ch.get('chapter_title')\
          \ or ch.get('raw_text', '').strip()) else []\n                }\n      \
          \  else:\n            current_section[\"chapters\"].append(ch)\n\n    #\
          \ Êú´Â∞æÂùóÂä†ÂÖ•\n    if current_section[\"chapters\"]:\n        sections.append(current_section)\n\
          \n    # # ÊâìÂç∞ÊèêÂèñÁöÑÊâÄÊúâÁ´†ËäÇÊ†áÈ¢ò\n    # for sec in sections:\n    #     print(f\"Section:\
          \ {sec['section']}\")\n    #     for chap in sec[\"chapters\"]:\n    # \
          \        print(f\"  Chapter ID: {chap['chapter_id']}, Title: {chap['chapter_title']}\"\
          )\n\n    return sections\n\ndef process_sections_with_lis(chapters, language='en'):\n\
          \    # ÂÖàÊãÜÂàÜÊàêÊ≠£ÊñáÂíåÂ§ö‰∏™ÈôÑÂΩï\n    sections = split_sections_by_appendix(chapters)\n\
          \n    # ÊØè‰∏™ÈÉ®ÂàÜÂÜÖÈÉ®ÂçïÁã¨Ë∑ëÊúÄÈïøÈìæ\n    processed_sections = []\n    for sec in sections:\n\
          \        valid_chaps, skipped_content = find_longest_chapter_chain_with_append(sec[\"\
          chapters\"], language)\n        processed_sections.append({\n          \
          \  \"section\": sec[\"section\"],\n            \"context\": skipped_content,\
          \  # Ê∑ªÂä†Ë¢´Ë∑≥ËøáÁöÑÂÜÖÂÆπ\n            \"chapters\": valid_chaps\n        })\n\n   \
          \ return processed_sections\n\ndef filter_start_of_main(chapters: List[Dict])\
          \ -> Tuple[List[Dict], str]:\n    \"\"\"\n    ÊâæÂà∞Á¨¨‰∏Ä‰∏™Ê≠£ÊñáÁ´†ËäÇ‰Ωú‰∏∫Ëµ∑ÁÇπÔºåË∑≥ËøáÁõÆÂΩï\n    \"\
          \"\"\n    start_index = 0\n    for i, chap in enumerate(chapters):\n   \
          \     chapter_id = chap.get(\"chapter_id\", \"\").strip()\n        # Ê≠£Êñá‰∏ªÈìæÊàñÈôÑ‰ª∂ÂÜÖÈÉ®Á´†ËäÇÔºöÊï∞Â≠óÂºÄÂ§¥ÊàñÂ≠óÊØçÂºÄÂ§¥\n\
          \        if chapter_id in {\"1\", \"1-\", \"1.\", \"A\", \"A.\", \"A.1\"\
          }:\n            # SCOPE / GENERAL / INTRO Á≠âÈÉΩÁÆóÊ≠£ÊñáËµ∑ÁÇπ\n            title_upper\
          \ = chap.get(\"chapter_title\", \"\").upper()\n            if any(k in title_upper\
          \ for k in [\"SCOPE\", \"GENERAL\", \"INTRO\", \"ÊÄªÂàô\", \"ËåÉÂõ¥\", \"LEGISLATIVE\"\
          , \"FUNCTION\"]):\n                start_index = i\n                break\n\
          \                \n        # ‰πüÊ£ÄÊü•Ê†áÂáÜÁöÑÁ´†ËäÇÂºÄÂ§¥Ê®°Âºè\n        if re.match(r'^[A-Z](\\\
          .\\d+)*\\.?$', chapter_id) or re.match(r'^\\d+(\\.\\d+)*\\.?$', chapter_id):\n\
          \            title_upper = chap.get(\"chapter_title\", \"\").upper()\n \
          \           if any(k in title_upper for k in [\"SCOPE\", \"GENERAL\", \"\
          INTRO\", \"ÊÄªÂàô\", \"ËåÉÂõ¥\", \"LEGISLATIVE\", \"FUNCTION\"]):\n            \
          \    start_index = i\n                break\n                \n    # print(f'chapters[str]:\
          \ {chapters[start_index]}')\n    filtered_chapters = chapters[start_index:]\n\
          \    skipped_content = chapters[:start_index]\n    skipped_text = \"\\n\"\
          .join([f\"{ch['chapter_id']} {ch['chapter_title']} {ch.get('raw_text','')}\"\
          \ for ch in skipped_content])\n\n    return filtered_chapters, skipped_text\n\
          \n\ndef smart_paragraph_join(lines: List[str], language: str = 'en') ->\
          \ str:\n    \"\"\"\n    Êô∫ËÉΩÊÆµËêΩÂêàÂπ∂ÔºöÂè™Âú®ÊÆµËêΩÁªìÊùüÊó∂Êç¢Ë°å\n    \"\"\"\n    if not lines:\n\
          \        return \"\"\n    \n    result = []\n    current_paragraph = []\n\
          \    \n    for i, line in enumerate(lines):\n        line = line.strip()\n\
          \        if not line:  # Á©∫Ë°åÁõ¥Êé•Ë∑≥Ëøá\n            continue\n            \n  \
          \      # Ê£ÄÊü•ÊòØÂê¶ÊòØÊÆµËêΩÁªìÊùüÁöÑÊ†áÂøó\n        is_paragraph_end = False\n        \n    \
          \    # 1. ‰ª•Ê†áÁÇπÁ¨¶Âè∑ÁªìÂ∞æÔºà‰∏≠Ëã±ÊñáÔºâ\n        if re.search(r'[„ÄÇÔºÅÔºüÔºõÔºö.!?;:]$', line):\n\
          \            is_paragraph_end = True\n            \n        # 2. Ê£ÄÊü•‰∏ã‰∏ÄË°åÊòØÂê¶ÊòØÊñ∞ÊÆµËêΩÁöÑÂºÄÂßã\n\
          \        if i + 1 < len(lines):\n            next_line = lines[i + 1].strip()\n\
          \            # ‰∏ã‰∏ÄË°åÊòØÁ´†ËäÇÊ†áÈ¢ò„ÄÅÂàóË°®È°π„ÄÅÊàñÊòéÊòæÁöÑÊÆµËêΩÂºÄÂßã\n            if (detect_chapter(next_line)\
          \ or\n                re.match(r'^[‰∏Ä‰∫å‰∏âÂõõ‰∫îÂÖ≠‰∏ÉÂÖ´‰πùÂçÅ\\d]+[„ÄÅ\\.\\)]', next_line)\
          \ or  # ÂàóË°®È°π\n                re.match(r'^[Ôºà(]\\d+[Ôºâ)]', next_line) or  #\
          \ ÁºñÂè∑È°π\n                re.match(r'^[‚Äî‚Äî\\-‚Äî]+', next_line)):  # Á†¥ÊäòÂè∑ÂºÄÂ§¥\n \
          \               is_paragraph_end = True\n        \n        # 3. Ë°®Ê†ºÁõ∏ÂÖ≥ÂÜÖÂÆπ‰øùÊåÅÂéüÊúâÊç¢Ë°å\n\
          \        if ('Ë°®' in line and re.search(r'Ë°®\\s*[A-Z0-9]', line)) or \\\n\
          \           re.match(r'^[|\\s]*[A-Za-z0-9\\u4e00-\\u9fa5]+[|\\s]*$', line):\
          \  # ÁÆÄÂçïË°®Ê†ºË°åÊ£ÄÊµã\n            current_paragraph.append(line)\n            is_paragraph_end\
          \ = True\n        else:\n            current_paragraph.append(line)\n  \
          \      \n        # Â¶ÇÊûúÊòØÊÆµËêΩÁªìÊùüÔºåÂ∞ÜÂΩìÂâçÊÆµËêΩÂêàÂπ∂Âπ∂Âä†ÂÖ•ÁªìÊûú\n        if is_paragraph_end:\n\
          \            if current_paragraph:\n                # ‰∏≠ÊñáÁöÑÊç¢Ë°å‰∏çÁî®Á©∫Ê†ºËøûÊé•ÔºåËã±ÊñáÁöÑÁî®Á©∫Ê†ºËøûÊé•\n\
          \                if language == 'zh':\n                    paragraph_text\
          \ = ''.join(current_paragraph).strip()\n                else:\n        \
          \            paragraph_text = ' '.join(current_paragraph).strip()\n    \
          \            if paragraph_text:\n                    result.append(paragraph_text)\n\
          \                current_paragraph = []\n    \n    # Â§ÑÁêÜÊúÄÂêéÂâ©‰ΩôÁöÑÊÆµËêΩ\n    if current_paragraph:\n\
          \        paragraph_text = ' '.join(current_paragraph).strip()\n        if\
          \ paragraph_text:\n            result.append(paragraph_text)\n    \n   \
          \ return '\\n'.join(result)\n\ndef parse_pdf_to_chapter_tree(pdf_path: str)\
          \ -> Tuple[List[Dict], Dict[str, str]]:\n    \"\"\"\n    ‰ªé PDF ‰∏≠ÊèêÂèñÁ´†ËäÇÊ†ëÂíåÊúØËØ≠Êò†Â∞Ñ\n\
          \    :param pdf_path: PDF Êñá‰ª∂Ë∑ØÂæÑ\n    :return: (Á´†ËäÇÊ†ë, ÊúØËØ≠Êò†Â∞Ñ)\n    \"\"\"\n \
          \   cleaned_lines = extract_full_text_with_filter(pdf_path)\n\n    # \U0001F195\
          \ Ê£ÄÊµãÊñáÊ°£ËØ≠Ë®Ä\n    language = detect_document_language(cleaned_lines)\n    max_chapter_num\
          \ = 50 if language == 'zh' else 1000\n    print(f\"Ê£ÄÊµãÂà∞ÊñáÊ°£ËØ≠Ë®Ä: {'‰∏≠Êñá' if language\
          \ == 'zh' else 'Ëã±Êñá'}, max_chapter_num={max_chapter_num}\")\n\n    # \U0001F195\
          \ Á¨¨‰∏ÄËΩÆÔºöÁ≤óÁï•ÊèêÂèñÊâÄÊúâÂèØËÉΩÁöÑÁ´†ËäÇÔºåÁî®‰∫éÂàÜÊûêÊï∞Â≠óÂàÜÂ∏É\n    preliminary_chapters = []\n    current =\
          \ {\n        \"chapter_id\": \"\",\n        \"chapter_title\": \"\",\n \
          \       \"raw_text\": \"\"\n    }\n    buffer = []\n\n    for line in cleaned_lines:\n\
          \        # Á¨¨‰∏ÄËΩÆ‰ΩøÁî®ÂÆΩÊùæÁöÑÊï∞Â≠óËåÉÂõ¥ËøõË°åÁ≤óÊèêÂèñ\n        chapter_info = detect_chapter(line,\
          \ max_chapter_num=1000, language=language, number_analysis=None)\n\n   \
          \     if chapter_info:\n            if current:\n                current[\"\
          raw_text\"] = smart_paragraph_join(buffer, language)\n                preliminary_chapters.append(current)\n\
          \                buffer = []\n            current = {\n                \"\
          chapter_id\": chapter_info[\"chapter_id\"],\n                \"chapter_title\"\
          : chapter_info[\"chapter_title\"],\n                \"raw_text\": \"\"\n\
          \            }\n        else:\n            buffer.append(line)\n\n    if\
          \ current:\n        current[\"raw_text\"] = smart_paragraph_join(buffer,\
          \ language)\n        preliminary_chapters.append(current)\n\n    # \U0001F195\
          \ ÂàÜÊûêÁ´†ËäÇÊï∞Â≠óÂàÜÂ∏É\n    number_analysis = analyze_chapter_number_distribution(preliminary_chapters)\n\
          \    print(f\"Êï∞Â≠óÂàÜÂ∏ÉÂàÜÊûê: {number_analysis}\")\n\n    # \U0001F195 Á¨¨‰∫åËΩÆÔºö‰ΩøÁî®ÂàÜÊûêÁªìÊûúÈáçÊñ∞Á≤æÁ°ÆÊèêÂèñÁ´†ËäÇ\n\
          \    chapters = []\n    current = {\n        \"chapter_id\": \"\",\n   \
          \     \"chapter_title\": \"\",\n        \"raw_text\": \"\"\n    }\n    buffer\
          \ = []\n\n    for line in cleaned_lines:\n        chapter_info = detect_chapter(line,\
          \ max_chapter_num=max_chapter_num, language=language, number_analysis=number_analysis)\n\
          \n        if chapter_info:\n            if current:\n                # ‰ΩøÁî®Êô∫ËÉΩÊÆµËêΩÂêàÂπ∂ËÄå‰∏çÊòØÁÆÄÂçïÁöÑ\
          \ \\n ËøûÊé•\n                current[\"raw_text\"] = smart_paragraph_join(buffer,\
          \ language)\n                chapters.append(current)\n                buffer\
          \ = []\n            current = {\n                \"chapter_id\": chapter_info[\"\
          chapter_id\"],\n                \"chapter_title\": chapter_info[\"chapter_title\"\
          ],\n                \"raw_text\": \"\"\n            }\n        else:\n \
          \           buffer.append(line)\n\n    if current:\n        current[\"raw_text\"\
          ] = smart_paragraph_join(buffer, language)\n        chapters.append(current)\n\
          \n    # 1Ô∏è‚É£ ÂÖàÊåâÈôÑ‰ª∂ÂàáÂàÜÈ°∂Â±Ç\n    attachment_sections = split_sections_by_attachment(chapters)\n\
          \n    tree = []\n\n    for top_sec in attachment_sections:\n        \n \
          \       # 2Ô∏è‚É£ ÊØè‰∏™È°∂Â±ÇÂùóÂÜçÊåâÈôÑÂΩïÂàáÂàÜ\n        sections = split_sections_by_appendix(top_sec[\"\
          chapters\"])\n        section_tree_list = []\n\n        for sec in sections:\n\
          \            # filtered_chapters, skipped_text = filter_start_of_main(sec[\"\
          chapters\"])\n            # 3Ô∏è‚É£ ÂØπÊØè‰∏™ÈÉ®ÂàÜÂÜÖÈÉ®‰øùÁïôÊúÄÈïøÈìæ\n            valid_chaps_in_sec,\
          \ skipped_text = find_longest_chapter_chain_with_append(sec[\"chapters\"\
          ], language)\n            \n            # \U0001F195 Â¶ÇÊûúÊúÄÈïøÈìæÊèêÂèñÁªìÊûú‰∏∫Á©∫ÔºåÂàõÂª∫ËôöÊãüÁöÑALLÁ´†ËäÇ\n\
          \            if not valid_chaps_in_sec:\n                # Â∞ÜcontextÂÜÖÂÆπ‰Ωú‰∏∫ËôöÊãüÁ´†ËäÇÁöÑrawtext\n\
          \                virtual_chapter = {\n                    \"chapter_id\"\
          : \"ALL\",\n                    \"chapter_title\": \"\",\n             \
          \       \"raw_text\": skipped_text,\n                    \"full_path\":\
          \ \"ALL\",\n                    \"children\": []\n                }\n  \
          \              tree_in_sec = [virtual_chapter]\n                # Ê∏ÖÁ©∫contextÔºåÂõ†‰∏∫ÂÜÖÂÆπÂ∑≤ÁªèÊîæÂÖ•ËôöÊãüÁ´†ËäÇ\n\
          \                skipped_text = \"\"\n            else:\n              \
          \  tree_in_sec = build_tree(valid_chaps_in_sec)\n                build_full_path(tree_in_sec)\n\
          \            \n            # ÊèíÂÖ•ÈîÆÂÄºÂØπ section\n            section_tree_list.append({\n\
          \                \"section\": sec[\"section\"],\n                \"context\"\
          : skipped_text,\n                \"chapters\": tree_in_sec,\n          \
          \  })\n\n        # 4Ô∏è‚É£ ÊûÑÂª∫È°∂Â±ÇÊ†ë\n        tree.append({\n            \"file\"\
          : top_sec[\"section\"],  # regulation Êàñ ANNEX n\n            \"sections\"\
          : section_tree_list,\n        })\n\n    term_map = {}\n\n    for chap in\
          \ chapters:\n        title = chap.get(\"chapter_title\", \"\")\n       \
          \ if \"ÊúØËØ≠\" in title:\n            # ÊèêÂèñÊúØËØ≠\n            terms = extract_terms_with_abbr_from_terms_section(chap[\"\
          chapter_title\"])\n            term_map.update(terms)\n            for child\
          \ in chap.get(\"children\", []):\n                terms = extract_terms_with_abbr_from_terms_section(child[\"\
          chapter_title\"])\n                term_map.update(terms)\n        elif\
          \ \"Áº©Áï•\" in title:\n            # ÊèêÂèñÁº©Áï•ËØ≠\n            abbr_terms = extract_abbr_terms_from_symbols_section(chap[\"\
          chapter_title\"] + chap[\"raw_text\"])\n            term_map.update(abbr_terms)\n\
          \            for child in chap.get(\"children\", []):\n                abbr_terms\
          \ = extract_abbr_terms_from_symbols_section(child[\"chapter_title\"] + child[\"\
          raw_text\"])\n                term_map.update(abbr_terms)\n\n    return\
          \ tree, term_map\n\nimport re\nfrom collections import defaultdict\nfrom\
          \ typing import List, Dict\nimport pdfplumber\n\ndef _build_page_lines_from_words(page,\
          \ y_tol=3):\n    \"\"\"\n    Áî® page.extract_words() ÊûÑÂª∫Ë°åÔºöÊåâ top ÂàÜÊ°∂Ôºày_tol ÂÆπÂ∑ÆÔºâÔºåÊØèË°åÊåâ\
          \ x0 ÊéíÂ∫èÂπ∂ÂêàÂπ∂ÊñáÊú¨Ôºå\n    ËøîÂõûÂàóË°®Ôºö{'text', 'y', 'x0', 'x1'}\n    \"\"\"\n    words\
          \ = page.extract_words()  # ËøîÂõûÊØè‰∏™ word Â∏¶ x0,x1,top,bottom,text\n    if not\
          \ words:\n        return []\n\n    # Êåâ top, x0 ÊéíÂ∫è\n    words = sorted(words,\
          \ key=lambda w: (w['top'], w['x0']))\n\n    lines = []\n    cur = None\n\
          \    for w in words:\n        top = w['top']\n        x0 = float(w['x0'])\n\
          \        x1 = float(w['x1'])\n        text = w['text']\n\n        if cur\
          \ is None:\n            cur = {'text': text, 'y': top, 'x0': x0, 'x1': x1}\n\
          \            continue\n\n        if abs(top - cur['y']) <= y_tol:\n    \
          \        # Âêå‰∏ÄË°åÔºåÊåâ x È°∫Â∫èËøûÊé•Ôºàextract_words Â∑≤Êåâ x ÊéíÂ∫èÔºå‰ΩÜ‰ªçÂÅö‰øùÈô©Ôºâ\n            if x0\
          \ < cur['x1'] + 1:\n                # ÈáçÂè†ÊàñÁ¥ßÈÇªÔºåÁõ¥Êé•Áî®Á©∫Ê†ºÈöîÂºÄÔºàÈÅøÂÖçÊääËØçÁ≤òÂú®‰∏ÄËµ∑Ôºâ\n        \
          \        cur['text'] = cur['text'] + ' ' + text\n            else:\n   \
          \             cur['text'] = cur['text'] + ' ' + text\n            cur['x1']\
          \ = max(cur['x1'], x1)\n            cur['x0'] = min(cur['x0'], x0)\n   \
          \         # keep y as average to be robust\n            cur['y'] = (cur['y']\
          \ + top) / 2.0\n        else:\n            lines.append(cur)\n         \
          \   cur = {'text': text, 'y': top, 'x0': x0, 'x1': x1}\n    if cur:\n  \
          \      lines.append(cur)\n    return lines\n\ndef _find_table_title_near_bbox(page,\
          \ table_bbox, max_above=60, y_tol=4):\n    \"\"\"\n    Âú®Ë°®Ê†º‰∏äÊñπ max_above pt\
          \ ÁöÑËåÉÂõ¥ÂÜÖÊâæÂèØËÉΩÁöÑÊ†áÈ¢òÔºö\n    - ÂÖà‰ªé page.extract_words() ‰∏≠Á≠õÈÄâÂá∫ËØ•ÂûÇÁõ¥Â∏¶ÂÜÖÂπ∂‰∏î‰∏éË°®Ê†ºÊ∞¥Âπ≥ÊúâÈáçÂè†ÁöÑ words\n\
          \    - Êåâ top/x0 ÂàÜÁªÑÊàêË°åÂπ∂ÊãºÊé•ÔºåËøîÂõûÊãºÂÆåÁöÑÂ≠óÁ¨¶‰∏≤ÔºàÂèØËÉΩÂåÖÂê´ÁºñÂè∑Ôºâ\n    \"\"\"\n    table_top = float(table_bbox[1])\n\
          \    table_x0, table_x1 = float(table_bbox[0]), float(table_bbox[2])\n \
          \   words = page.extract_words()\n    if not words:\n        return None\n\
          \n    # Á≠õÈÄâÔºöÂûÇÁõ¥Âú® (table_top - max_above, table_top + 10) ËåÉÂõ¥ÂÜÖÔºå\n    # ÂêåÊó∂Ê∞¥Âπ≥‰∏äËá≥Â∞ë‰∏éË°®Ê†ºÂ∑¶Âè≥Êâ©Â±ï\
          \ 50pt ÊúâÈáçÂè†ÔºàÈò≤Ê≠¢ÂÆåÂÖ®Èù†Â∑¶ÁöÑÊ†áÈ¢òË¢´ÂøΩÁï•Ôºâ\n    relevant = []\n    margin_x = 60\n    for\
          \ w in words:\n        w_top = float(w['top'])\n        if not (table_top\
          \ - max_above <= w_top <= table_top + 10):\n            continue\n     \
          \   w_x0 = float(w['x0']); w_x1 = float(w['x1'])\n        # ‰∏éË°®Ê†ºÊ∞¥Âπ≥ÊäïÂΩ±ÊúâÈáçÂè† Êàñ\
          \ Âú®Ë°®Ê†ºÂ∑¶‰æßÊé•Ëøë‰ΩçÁΩÆ\n        if (w_x1 >= table_x0 - margin_x and w_x0 <= table_x1\
          \ + margin_x) or w_x0 < table_x0:\n            relevant.append(w)\n\n  \
          \  if not relevant:\n        return None\n\n    # Â∞ÜËøô‰∫õ words Êåâ top/x0 ÊéíÂ∫èÔºåÂàÜË°åÂπ∂ÂêàÂπ∂\n\
          \    relevant = sorted(relevant, key=lambda w: (w['top'], w['x0']))\n  \
          \  lines = []\n    cur = None\n    for w in relevant:\n        top = w['top'];\
          \ x0 = float(w['x0']); x1 = float(w['x1']); txt = w['text']\n        if\
          \ cur is None:\n            cur = {'text': txt, 'y': top, 'x0': x0, 'x1':\
          \ x1}\n        else:\n            if abs(top - cur['y']) <= y_tol:\n   \
          \             cur['text'] = cur['text'] + ' ' + txt\n                cur['x1']\
          \ = max(cur['x1'], x1)\n                cur['x0'] = min(cur['x0'], x0)\n\
          \                cur['y'] = (cur['y'] + top) / 2.0\n            else:\n\
          \                lines.append(cur)\n                cur = {'text': txt,\
          \ 'y': top, 'x0': x0, 'x1': x1}\n    if cur:\n        lines.append(cur)\n\
          \n    # Áé∞Âú®ÊääËøô‰∫õË°åÊãºÊàêÊúÄÁªàÊ†áÈ¢òÔºöÊåâ y ‰ªé‰∏äÂà∞‰∏ã„ÄÅÊåâ x0 ‰ªéÂ∑¶Âà∞Âè≥ËøûÊé•\n    # ‰ΩÜ‰ºòÂÖàÈÄâÊã©ÂåÖÂê´ \"Ë°®\" ÁöÑË°åÊàñ‰ª• \"Ë°®\"\
          \ ÂºÄÂ§¥ÁöÑË°åÂèäÂÖ∂Áõ∏ÈÇªË°å\n    full = \" \".join([ln['text'] for ln in lines]).strip()\n\
          \n    # ‰ºòÂÖàÁ≠ñÁï•ÔºöÊâæÂà∞ÂåÖÂê´\"Ë°®\"ÁöÑË°åÔºàÊúÄÈù†ËøëË°®Ê†ºÁöÑÈÇ£‰∏ÄË°å‰ºòÂÖàÔºâ\n    cand = None\n    candidates =\
          \ []\n    for ln in lines:\n        if 'Ë°®' in ln['text'] or 'Table' in ln['text']:\n\
          \            candidates.append((abs(table_top - ln['y']), ln))\n    if candidates:\n\
          \        # ÂèñË∑ùÁ¶ªÊúÄÂ∞èÁöÑÈÇ£‰∏ÄË°å‰Ωú‰∏∫Ê†∏ÂøÉÔºåÁÑ∂ÂêéÊääÂêå‰∏Ä y Â∏¶ÂÜÖÁöÑÂÖ∂‰ªñË°åÂêàÂπ∂ÔºàÂ∑¶Âè≥Êâ©Â±ïÔºâ\n        candidates.sort(key=lambda\
          \ x: x[0])\n        core_y = candidates[0][1]['y']\n        # ÂêàÂπ∂‰∏é core_y\
          \ Êé•ËøëÁöÑÊâÄÊúâË°å\n        merge_parts = [ln['text'] for ln in lines if abs(ln['y']\
          \ - core_y) <= max(y_tol, 10)]\n        cand = \" \".join(merge_parts).strip()\n\
          \    else:\n        # Êú™ÊâæÂà∞ÂåÖÂê´\"Ë°®\"ÁöÑÊòéÁ°ÆË°åÔºåÂ∞±ÈÄÄÂõûÁî® fullÔºàÂèØËÉΩÂ∞±ÊòØÊï¥ÊÆµÊ†áÈ¢òÔºâ\n        cand =\
          \ full if full else None\n\n    # print(f\"ÊâæÂà∞Ê†áÈ¢òÂÄôÈÄâ: {cand}\")\n    # Ëøõ‰∏ÄÊ≠•Ê∏ÖÁêÜÔºöÊääÂ§ö‰ΩôÁ©∫Ê†º„ÄÅËøûÁª≠Â§ö‰ΩôÊ†áÁÇπÊï¥ÁêÜ‰∏Ä‰∏ã\n\
          \    if cand:\n        cand = re.sub(r'\\s+', ' ', cand).strip()\n     \
          \   cand = re.sub(r'\\s*([Ôºå,Ôºö:Ôºõ;])\\s*', r'\\1 ', cand)\n    return cand\n\
          \ndef extract_tables_from_pdf(pdf_path: str) -> List[Dict]:\n    \"\"\"\n\
          \    ÊîπËøõÁâàÔºö‰ªéPDF‰∏≠ÊèêÂèñÊâÄÊúâË°®Ê†ºÂèäÂÖ∂Ê†áËØÜÔºåÂ∞ΩÈáèÊÅ¢Â§ç 'Ë°®F.1' ËøôÁ±ªÁºñÂè∑\n    ËøîÂõûÊ†ºÂºè: [{\"table_id\": \"\
          Ë°®X.x Ê†áÈ¢ò\", \"table_content\": ‰∫åÁª¥Êï∞ÁªÑ}, ...]\n    \"\"\"\n    all_tables =\
          \ []\n\n    with pdfplumber.open(pdf_path) as pdf:\n        for page_num,\
          \ page in enumerate(pdf.pages):\n            # ‰ΩøÁî® extract_words ÊûÑÂª∫È°µÈù¢Ë°åÔºà‰πü‰øùÁïôÂéüÂßã\
          \ words Áî®‰∫éÊõ¥Á≤æÁªÜÂà§Êñ≠Ôºâ\n            page_lines = _build_page_lines_from_words(page,\
          \ y_tol=3)\n\n            # ÊèêÂèñÂπ∂ÊéíÂ∫èË°®Ê†º\n            tables = page.find_tables()\n\
          \            if not tables:\n                continue\n            tables\
          \ = sorted(tables, key=lambda t: t.bbox[1])\n\n            for table_idx,\
          \ table in enumerate(tables):\n                # ËøáÊª§ÔºöÂè™Êúâ 1 ÂàóÁöÑÁõ¥Êé•‰∏¢ÂºÉ\n      \
          \          table_data = table.extract()\n                if not table_data:\n\
          \                    continue\n                sample_row = table_data[0]\n\
          \                if len(sample_row) <= 1:\n                    continue\n\
          \n                cleaned_data = [\n                    [cell.replace('\\\
          n', ' ').strip() if cell else \"\" for cell in row]\n                  \
          \  for row in table_data\n                ]\n\n                # ‰ºòÂÖàÈÄöËøá page_lines\
          \ ÊâæÊ†áÈ¢ò\n                table_top = table.bbox[1]\n                best_line\
          \ = None\n                min_gap = float('inf')\n\n                # ÂÖàÂ∞ùËØïÊõ¥Á®≥ÂÅ•ÁöÑÊñπÂºèÔºö‰ªé\
          \ words Âå∫ÂüüÊî∂ÈõÜÂπ∂ÊãºÊé•Ê†áÈ¢ò\n                cand = _find_table_title_near_bbox(page,\
          \ table.bbox, max_above=60, y_tol=4)\n                if cand:\n       \
          \             best_line = cand\n                else:\n                \
          \    # ÈÄÄÂõûÂà∞ÂéüÂÖàÈÄªËæëÔºöÂú® page_lines ‰∏≠ÊâæÂåÖÂê´ \"Ë°®\" ÁöÑË°åÔºàË∑ùÁ¶ªÊúÄËøëÁöÑÔºâ\n                    for\
          \ item in page_lines:\n                        if (\"Ë°®\" in item['text']\
          \ or 'Table' in item['text']) and 0 < (table_top - item['y']) < 60:\n  \
          \                          gap = table_top - item['y']\n               \
          \             if gap < min_gap:\n                                min_gap\
          \ = gap\n                                best_line = item['text']\n\n  \
          \              # ÂÖúÂ∫ïÔºöÂ¶ÇÊûúËøòÊòØÊ≤°ÊúâÁºñÂè∑ÔºåÊ£ÄÊü•Ë°®Ê†ºÁ¨¨‰∏ÄË°åÁöÑÂçïÂÖÉÊ†ºÈáåÊòØÂê¶Êúâ‚ÄúË°®X‚ÄùÊ†∑Âºè\n                if best_line\
          \ is None:\n                    first_row = cleaned_data[0]\n          \
          \          # ÊääÁ¨¨‰∏ÄË°åÊâÄÊúâÂçïÂÖÉÊ†ºÊãºËµ∑Êù•Êü•Êâæ‚ÄúË°®‚ÄùÂÖ≥ÈîÆËØç\n                    joined_first = \"\
          \ \".join(first_row).strip()\n                    if re.search(r'Ë°®\\s*[A-Z0-9]\\\
          .?\\d*', joined_first) or joined_first.startswith('Ë°®'):\n              \
          \          best_line = joined_first\n\n                # Â¶ÇÊûúÊâæÂà∞‰∫ÜÊ†áÈ¢òÂàôÊñ∞Â¢ûÔºåÂê¶ÂàôÁî®ÂÖúÂ∫ï\
          \ id\n                if best_line:\n                    # Ëøõ‰∏ÄÊ≠•ÂÅöÂ∞èÊ∏ÖÊ¥óÔºöÂ∞Ü \"\
          Ë°®  F.1\" Á≠â‰∏≠Èó¥Â§ö‰ΩôÁ©∫Ê†ºÂéªÊéâÔºà‰øùÁïôË°®Â≠óÂíåÁºñÂè∑Ôºâ\n                    best_line = re.sub(r'Ë°®\\\
          s+([A-Za-z0-9])', r'Ë°®\\1', best_line)\n                    all_tables.append({\n\
          \                        \"table_id\": best_line,\n                    \
          \    \"table_content\": cleaned_data\n                    })\n         \
          \       else:\n                    table_id = f\"Ë°®-È°µ{page_num + 1}-Ë°®{table_idx\
          \ + 1}\"\n                    all_tables.append({\n                    \
          \    \"table_id\": table_id,\n                        \"table_content\"\
          : cleaned_data\n                    })\n\n    return all_tables\n\n\nimport\
          \ tempfile\nimport requests\nfrom urllib.parse import urlparse\n\ndef resolve_pdf_path(pdf_path:\
          \ str) -> str:\n    # Â¶ÇÊûúÊòØ URLÔºåÂ∞±‰∏ãËΩΩÂà∞‰∏¥Êó∂Êñá‰ª∂Â§π\n    if pdf_path.startswith(\"http://\"\
          ) or pdf_path.startswith(\"https://\"):\n        response = requests.get(pdf_path)\n\
          \        response.raise_for_status()\n        suffix = os.path.splitext(urlparse(pdf_path).path)[-1]\n\
          \        with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as\
          \ tmp_file:\n            tmp_file.write(response.content)\n            return\
          \ tmp_file.name\n    else:\n        return pdf_path\n\ndef count_leaf_nodes(chapters):\n\
          \    \"\"\"ÈÄíÂΩíËÆ°ÁÆóÁ´†ËäÇÊ†ë‰∏≠ÁöÑÂè∂Â≠êËäÇÁÇπÊï∞Èáè\"\"\"\n    total = 0\n    for chapter in chapters:\n\
          \        if chapter.get(\"children\") and len(chapter[\"children\"]) > 0:\n\
          \            # ÊúâÂ≠êËäÇÁÇπÔºåÈÄíÂΩíËÆ°ÁÆó\n            total += count_leaf_nodes(chapter[\"\
          children\"])\n        else:\n            # Âè∂Â≠êËäÇÁÇπ\n            total += 1\n\
          \    return total\n\ndef extract_chapters_by_id(tree, chapter_ids):\n  \
          \  \"\"\"‰ªéÊ†ëÁªìÊûÑ‰∏≠ÊèêÂèñÊåáÂÆöIDÁöÑÁ´†ËäÇÔºå‰øùÊåÅÂéüÊúâÂ±ÇÁ∫ßÁªìÊûÑ\"\"\"\n    result = []\n    \n    for file_item\
          \ in tree:\n        # Âè™Â§ÑÁêÜ file ‰∏∫ regulation ÁöÑÂÜÖÂÆπ\n        if file_item[\"\
          file\"] != \"regulation\":\n            continue\n            \n       \
          \ new_file = {\n            \"file\": file_item[\"file\"],\n           \
          \ \"sections\": []\n        }\n        \n        for section in file_item[\"\
          sections\"]:\n            if section[\"section\"] != \"MAIN\":\n       \
          \         continue\n            new_section = {\n                \"section\"\
          : section[\"section\"],\n                # \"context\": section[\"context\"\
          ],\n                \"chapters\": []\n            }\n            \n    \
          \        # ÊèêÂèñÂåπÈÖçÁöÑÁ´†ËäÇ\n            for chapter in section[\"chapters\"]:\n\
          \                if chapter[\"chapter_id\"] in chapter_ids:\n          \
          \          new_section[\"chapters\"].append(chapter)\n            \n   \
          \         # Âè™ÊúâÂΩìsectionÊúâÁ´†ËäÇÊó∂ÊâçÊ∑ªÂä†\n            if new_section[\"chapters\"]:\n\
          \                new_file[\"sections\"].append(new_section)\n        \n\
          \        # Âè™ÊúâÂΩìfileÊúâsectionsÊó∂ÊâçÊ∑ªÂä†\n        if new_file[\"sections\"]:\n  \
          \          result.append(new_file)\n    \n    return result\n\ndef get_first_three_chapters_from_main(tree)\
          \ -> set:\n    \"\"\"\n    ÂèñÂâç 5 ‰∏™‰∏ÄÁ∫ßÁ´†ËäÇÈáåÔºå\n    Ê†áÈ¢òÂê´ scope/ËåÉÂõ¥ Êàñ definition/ÊúØËØ≠\
          \ ÁöÑ chapter_idÔºå\n    ÊúâÂá†‰∏™ÊãøÂá†‰∏™Ôºå‰∏çÈôêÂà∂ÂîØ‰∏Ä„ÄÇ\n    \"\"\"\n    main_chaps = []\n  \
          \  for file_item in tree:\n        if file_item[\"file\"] != \"regulation\"\
          :\n            continue\n        for sec in file_item[\"sections\"]:\n \
          \           if sec[\"section\"] == \"MAIN\":\n                # Âè™‰øùÁïô‰∏ÄÁ∫ß\n\
          \                main_chaps = [\n                    ch for ch in sec[\"\
          chapters\"]\n                    if re.fullmatch(r\"^\\d+$|^[A-Z]$|^(APPENDIX|ANNEX)\\\
          s+[A-Z0-9]+$\",\n                                    ch.get(\"chapter_id\"\
          , \"\").strip().rstrip(\".-\"), re.I)\n                ]\n             \
          \   break\n        break\n\n    wanted = set()\n    for ch in main_chaps[:5]:\
          \          # ÁúãÂâç 5 ‰∏™‰∏ÄÁ∫ß\n        t = ch.get(\"chapter_title\", \"\").lower()\n\
          \        if any(k in t for k in (\"scope\", \"ËåÉÂõ¥\", \"definition\", \"ÊúØËØ≠\"\
          )):\n            wanted.add(ch[\"chapter_id\"])\n    return wanted\n\ndef\
          \ group_main_chapters(tree):\n    \"\"\"\n    ÊØè‰∏™‰∏ÄÁ∫ßÁ´†ËäÇÂçïÁã¨‰Ωú‰∏∫‰∏ÄÁªÑ\n    \"\"\"\n\
          \    groups = []\n\n    # ÊâæÂà∞ regulation file ÁöÑ MAIN section\n    for file_item\
          \ in tree:\n        if file_item[\"file\"] != \"regulation\":\n        \
          \    continue\n\n        for section in file_item[\"sections\"]:\n     \
          \       if section[\"section\"] == \"MAIN\":\n                for chapter\
          \ in section[\"chapters\"]:\n                    groups.append([chapter[\"\
          chapter_id\"]])\n                break\n        break\n\n    return groups\n\
          \ndef get_non_main_sections(tree):\n    \"\"\"Ëé∑ÂèñÊâÄÊúâÈùû MAIN ÁöÑ sections\"\"\"\
          \n    non_main_sections = []\n    \n    for file_item in tree:\n       \
          \ if file_item[\"file\"] != \"regulation\":\n            continue\n    \
          \        \n        for section in file_item[\"sections\"]:\n           \
          \ if section[\"section\"] != \"MAIN\":\n                non_main_sections.append(section[\"\
          section\"])\n    \n    return non_main_sections\n\ndef create_section_tree(tree,\
          \ section_name):\n    \"\"\"ÂàõÂª∫ÂåÖÂê´ÊåáÂÆö section ÁöÑÂÆåÊï¥Ê†ëÁªìÊûÑ\"\"\"\n    result = []\n\
          \    \n    for file_item in tree:\n        if file_item[\"file\"] != \"\
          regulation\":\n            continue\n            \n        new_file = {\n\
          \            \"file\": file_item[\"file\"],\n            \"sections\": []\n\
          \        }\n        \n        for section in file_item[\"sections\"]:\n\
          \            if section[\"section\"] == section_name:\n                new_file[\"\
          sections\"].append(section)\n                break\n        \n        if\
          \ new_file[\"sections\"]:\n            result.append(new_file)\n       \
          \     break\n    \n    return result\n\ndef count_leaf_nodes(chapters):\n\
          \    \"\"\"ÈÄíÂΩíÁªüËÆ°Á´†ËäÇÊ†ëÁöÑÂè∂Â≠êËäÇÁÇπÊï∞Èáè\"\"\"\n    count = 0\n    for chapter in chapters:\n\
          \        if chapter.get(\"children\"):\n            count += count_leaf_nodes(chapter[\"\
          children\"])\n        else:\n            count += 1\n    return count\n\n\
          import copy\n\ndef split_array_items_if_needed(array_items, max_leaf_nodes=150):\n\
          \    \"\"\"\n    ÈÅçÂéÜÊúÄÁªàÁöÑ array_itemsÔºåÂ¶ÇÊûúÊüê‰∏™ item Ë∂ÖËøá max_leaf_nodesÔºå\n    ÂàôÊåâ‰∫åÁ∫ßÁ´†ËäÇÂàÜÁªÑÔºà‰øùËØÅ‰∫åÁ∫ßÁ´†ËäÇÂÆåÊï¥‰øùÁïôÔºå‰∏çËÉΩÊãÜÂºÄÔºâ\n\
          \    \"\"\"\n    new_array = []\n    for item in array_items:\n        tree\
          \ = json.loads(item)\n        chapters = tree[0][\"sections\"][0][\"chapters\"\
          ] if tree else []\n        leaf_count = count_leaf_nodes(chapters)\n\n \
          \       if leaf_count <= max_leaf_nodes:\n            new_array.append(item)\n\
          \        else:\n            big_chapter = chapters[0]\n            if big_chapter.get(\"\
          children\"):  # ‰∫åÁ∫ßÁ´†ËäÇÂ≠òÂú®\n                group = []\n                group_count\
          \ = 0\n                for child in big_chapter[\"children\"]:  # ÈÅçÂéÜ 6.1„ÄÅ6.2\
          \ ...\n                    child_count = count_leaf_nodes([child])\n\n \
          \                   # Â¶ÇÊûúÂä†‰∏äÂΩìÂâç child Ë∂ÖËøáÈôêÂà∂ÔºåÂàôÊää‰πãÂâçÁöÑ group ‰øùÂ≠ò\n               \
          \     if group_count + child_count > max_leaf_nodes and group:\n       \
          \                 new_tree = copy.deepcopy(tree)\n                     \
          \   new_tree[0][\"sections\"][0][\"chapters\"] = [\n                   \
          \         copy.deepcopy(big_chapter)\n                        ]\n      \
          \                  new_tree[0][\"sections\"][0][\"chapters\"][0][\"children\"\
          ] = group\n                        new_array.append(json.dumps(new_tree,\
          \ ensure_ascii=False))\n\n                        # ÂºÄÂêØÊñ∞ÁªÑ\n             \
          \           group = []\n                        group_count = 0\n\n    \
          \                # Êää child ÊîæËøõÂΩìÂâçÁªÑ\n                    group.append(child)\n\
          \                    group_count += child_count\n\n                # ‰øùÂ≠òÊúÄÂêé‰∏ÄÁªÑ\n\
          \                if group:\n                    new_tree = copy.deepcopy(tree)\n\
          \                    new_tree[0][\"sections\"][0][\"chapters\"] = [\n  \
          \                      copy.deepcopy(big_chapter)\n                    ]\n\
          \                    new_tree[0][\"sections\"][0][\"chapters\"][0][\"children\"\
          ] = group\n                    new_array.append(json.dumps(new_tree, ensure_ascii=False))\n\
          \            else:\n                # Ê≤°Êúâ‰∫åÁ∫ßÂ≠êÁ´†ËäÇÔºåÊó†Ê≥ïÂÜçÊãÜÔºå‰øùÊåÅÂéüÊ†∑\n              \
          \  new_array.append(item)\n\n    return new_array\n\n\ndef main(url: str)\
          \ -> dict:\n    pdf_path = resolve_pdf_path(url)  # ÂÖºÂÆπ URL ÂíåÊú¨Âú∞Ë∑ØÂæÑ\n\n   \
          \ # Á´†ËäÇÊ†ë\n    tree, term_map = parse_pdf_to_chapter_tree(pdf_path)\n    \n\
          \    # ÊèêÂèñÂâç‰∏âÁ´†ÁöÑ chapter_idÔºà‰ªé MAIN sectionÔºâ\n    first_three_chapter_ids =\
          \ get_first_three_chapters_from_main(tree)\n    \n    # ÊèêÂèñÂâç‰∏âÁ´†‰Ωú‰∏∫context\n\
          \    context_tree = extract_chapters_by_id(tree, first_three_chapter_ids)\n\
          \    \n    # ÊåâÂè∂Â≠êËäÇÁÇπÊï∞ÈáèÂàÜÁªÑ MAIN section ÁöÑÂÖ∂‰ΩôÁ´†ËäÇ\n    main_chapter_groups = group_main_chapters(tree)\n\
          \    \n    # # Ëé∑ÂèñÊâÄÊúâÈùû MAIN ÁöÑ sections\n    # non_main_sections = get_non_main_sections(tree)\n\
          \    \n    # ‰∏∫ÊØè‰∏™ MAIN Á´†ËäÇÁªÑÂíåÈùû MAIN section ÂàõÂª∫ÂÆåÊï¥ÁöÑÊ†ëÁªìÊûÑ\n    array_items = []\n\
          \    \n    # Â§ÑÁêÜ MAIN section ÁöÑÁ´†ËäÇÁªÑ\n    for group_chapter_ids in main_chapter_groups:\n\
          \        group_tree = extract_chapters_by_id(tree, set(group_chapter_ids))\n\
          \        if group_tree:  # Á°Æ‰øùÁªÑ‰∏ç‰∏∫Á©∫\n            array_items.append(json.dumps(group_tree,\
          \ ensure_ascii=False))\n    \n    # # Â§ÑÁêÜÈùû MAIN sectionsÔºàÊØè‰∏™ÈôÑÂΩï‰Ωú‰∏∫‰∏Ä‰∏™Áã¨Á´ãÁöÑ itemÔºâ\n\
          \    # for section_name in non_main_sections:\n    #     section_tree =\
          \ create_section_tree(tree, section_name)\n    #     if section_tree:  #\
          \ Á°Æ‰øùsection‰∏ç‰∏∫Á©∫\n    #         array_items.append(json.dumps(section_tree,\
          \ ensure_ascii=False))\n\n    array_items = split_array_items_if_needed(array_items,\
          \ max_leaf_nodes=150)\n\n\n    return {\n        \"tree\":json.dumps(tree,\
          \ ensure_ascii=False),\n        \"context\": json.dumps(context_tree, ensure_ascii=False),\n\
          \        \"array\": array_items\n    }"
        code_language: python3
        desc: ''
        outputs:
          array:
            children: null
            type: array[string]
          context:
            children: null
            type: string
          tree:
            children: null
            type: string
        selected: false
        title: ‰ª£Á†ÅÊâßË°å
        type: code
        variables:
        - value_selector:
          - '1756550268945'
          - file
          - url
          value_type: file
          variable: url
      height: 53
      id: '1756550411122'
      position:
        x: 334
        y: 352
      positionAbsolute:
        x: 334
        y: 352
      selected: true
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        desc: ''
        error_handle_mode: continue-on-error
        height: 304
        is_parallel: true
        iterator_input_type: array[string]
        iterator_selector:
        - '1756550411122'
        - array
        output_selector:
        - '1756698812908'
        - output
        output_type: array[string]
        parallel_nums: 10
        selected: false
        start_node_id: 1756557801310start
        title: Ëø≠‰ª£ 1A
        type: iteration
        width: 1724
      height: 304
      id: '1756557801310'
      position:
        x: 638
        y: 352
      positionAbsolute:
        x: 638
        y: 352
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 1724
      zIndex: 1
    - data:
        desc: ''
        isInIteration: true
        selected: false
        title: ''
        type: iteration-start
      draggable: false
      height: 48
      id: 1756557801310start
      parentId: '1756557801310'
      position:
        x: 60
        y: 100.5
      positionAbsolute:
        x: 698
        y: 452.5
      selectable: false
      sourcePosition: right
      targetPosition: left
      type: custom-iteration-start
      width: 44
      zIndex: 1002
    - data:
        context:
          enabled: true
          variable_selector:
          - '1756550411122'
          - context
        desc: ''
        isInIteration: true
        isInLoop: false
        iteration_id: '1756557801310'
        model:
          completion_params: {}
          mode: chat
          name: deepseek-reasoner
          provider: langgenius/deepseek/deepseek
        prompt_template:
        - id: b8df94d2-1037-41d9-8649-be6e2790a7d5
          role: system
          text: "# ËßíËâ≤\n\n‰Ω†ÊòØ‰∏ÄÂêçÊäÄÊúØÊ†áÂáÜÁü•ËØÜÂ∑•Á®ã‰∏ìÂÆ∂Ôºå‰∏ìÊ≥®‰∫éÂ∞ÜÊ±ΩËΩ¶ÂèäÁõ∏ÂÖ≥È¢ÜÂüüÁöÑÊ†áÂáÜÂíåÊ≥ïËßÑÊñáÊ°£ËΩ¨Âåñ‰∏∫ÂèØÁªìÊûÑÂåñËß£ÊûêÁöÑÊï∞ÊçÆËµÑ‰∫ßÔºåÁî®‰∫éÊûÑÂª∫Áü•ËØÜÂõæË∞±„ÄÇ\
            \  \n\n‰Ω†ÁöÑ‰ªªÂä°ÊòØÂü∫‰∫éËæìÂÖ•ÁöÑÊ†áÂáÜÊñáÊ°£ JSON Ê†ëÁªìÊûÑÔºåÁ≤æÂáÜÊäΩÂèñÁõ∏ÂÖ≥‰ø°ÊÅØÔºå‰ª•ÊîØÊåÅË∑®Ê†áÂáÜÊù°Ê¨æÊØîÂØπ„ÄÅÊ≥ïËßÑ‰∏ÄËá¥ÊÄßÈ™åËØÅ„ÄÅËÆæÂ§áËÉΩÂäõËØÑ‰º∞Á≠â‰∏ãÊ∏∏Â∫îÁî®\
            \ Ôºå‰øùËØÅË∑®Ê†áÂáÜÂèØÊØîÊÄßÔºà‰∏çÂêåÊñá‰ª∂Èó¥ÂêåÁ±ªÊù°Ê¨æËÉΩÂØπÈΩêÔºâ\n\n---\n\n# ËæìÂÖ•ÂÜÖÂÆπ\n\n‰Ω†Â∞ÜÊî∂Âà∞‰∏Ä‰∏™ JSON Êï∞ÁªÑÔºåÂåÖÂê´‰∏Ä‰∏™ÂØπË±°ÔºåÂÖ∂ÁªìÊûÑÂ¶Ç‰∏ãÔºö\n\
            \n- `file`: Êñá‰ª∂Ê†áËØÜÔºàÂ¶Ç \"regulation\"„ÄÅ\"ANNEX 1\"Ôºâ\n- `sections`: ÂùóÊï∞ÁªÑÔºåÊØè‰∏™ÂÖÉÁ¥†ÂåÖÂê´Ôºö\n\
            \  - `section`: Ê†áËØÜÂΩìÂâçÂùóÔºàÂ¶Ç \"MAIN\", \"APPENDIX 1\"Ôºâ\n  - `chapters`: Á´†ËäÇÊï∞ÁªÑÔºåÊØè‰∏™ÂÖÉÁ¥†ÂåÖÂê´Ôºö\n\
            \    - `chapter_id`: Á´†ËäÇÁºñÂè∑Ôºà‰øùÊåÅÂéüÊ†∑Ôºâ\n    - `chapter_title`: Á´†ËäÇÊ†áÈ¢ò\n    - `raw_text`:\
            \ ËØ•ËäÇÁöÑÁ∫ØÊñáÊú¨ÂÜÖÂÆπ\n    - `children`: Â≠êÊù°Ê¨æÊï∞ÁªÑÔºàÁªìÊûÑ‰∏éÁà∂Á∫ßÁõ∏ÂêåÔºâ\n    - `full_path`: ÂÆåÊï¥Ë∑ØÂæÑÔºàÂèØÈÄâÔºâ\n\
            \n> ÂêåÊó∂Êèê‰æõËÉåÊôØ‰∏ä‰∏ãÊñáÔºà{{#context#}}ÔºâÔºåÂåÖÂê´ËåÉÂõ¥„ÄÅÊúØËØ≠ÂÆö‰πâÁ≠âÁ´†ËäÇÔºå‰æø‰∫éÁêÜËß£„ÄÇ\n---\n\n# ËæìÂá∫Ê†ºÂºè\n\nËæìÂá∫ÂøÖÈ°ªÊòØÁ∫ØÂáÄ„ÄÅÂÆåÊï¥ÁöÑJSONÂØπË±°ÔºåÁ°Æ‰øùÂèØ‰ª•Ë¢´`json.loads()`Áõ¥Êé•Ëß£ÊûêÔºåÁªìÊûÑÂ¶Ç‰∏ãÔºö\n\
            \n```json\n{\n  \"file\": \"string\",\n  \"section\": \"string\",\n  \"\
            chapters\":[\n    {\n      \"chapter_id\": \"string\",\n      \"scope\"\
            : \"string\",\n      \"topic_keywords\": [\"string\", \"...\"],\n    \
            \  \"context_keywords\": [\"string\", \"...\"]\n    }\n  ]\n}\n```\n\n\
            # Â§ÑÁêÜÈÄªËæëÔºàÈìæÂºèÊÄùËÄÉÔºâ\n\nËÆ©Êàë‰ª¨‰∏ÄÊ≠•‰∏ÄÊ≠•ÊÄùËÄÉÔºö\n\n1. **ÈÅçÂéÜÁ´†ËäÇÊ†ë**\n\n   ÈÅçÂéÜÊØè‰∏™ `chapter_id` ÂèäÂÖ∂Â≠êÁ´†ËäÇÔºå‰æùÊ¨°ËøõË°åÂêéÁª≠Êìç‰ΩúÔºåÊòéÁ°ÆÊèêÂèñÁöÑÁõÆÁöÑÊòØ‰∏∫‰∫ÜË∑®Ê†áÂáÜÂåπÈÖç(bge-m3Âíåbge-v2-reranker-m3)„ÄÇ\n\
            \n2. **ËØÜÂà´Ê†∏ÂøÉ‰∏ª‰Ωì**\n   \n    -   **ËøôÊòØÊúÄÈáçË¶ÅÁöÑ‰∏ÄÊ≠•ÔºÅ** È¶ñÂÖàÈóÆËá™Â∑±Ôºö‚ÄúËøô‰∏™Êù°Ê¨æÂú®ËßÑÂÆöÊàñÊèèËø∞‰ªÄ‰πà‰∏úË•øÔºü‚ÄùÔºåÂèØ‰ª•ÁªìÂêàËÉåÊôØ‰∏ä‰∏ãÊñá‰∏≠ÁöÑËåÉÂõ¥Á´†ËäÇÔºå‰∫ÜËß£Ê†∏ÂøÉ‰∏ª‰ΩìÊòØ‰ªÄ‰πàÔºåÊØîÂ¶Ç‚ÄùËΩ¶ËΩΩÁ¥ßÊÄ•ÂëºÊïëÁ≥ªÁªü‚ÄúÔºå‚ÄùÁÅØÁöÑÂÆâË£Ö‚ÄúÁ≠âÔºåÁÑ∂Âêé‰ªéÁ¨¨‰∏ÄÂ±ÇÁ´†ËäÇÂà∞ÂΩìÂâçÁ´†ËäÇÔºåÂ±ÇÂ±ÇÁêÜÊ∏ÖËÑâÁªúÔºåÂÜçÁªìÂêà`rawtext`ÔºåÂøÖË¶ÅÊó∂ÂèÇËÄÉ`children`‰∏≠ÈÉΩËÆ≤‰∫Ü‰ªÄ‰πà\n\
            \    -   ‰æãÂ¶ÇÔºö‚ÄùËΩ¶ËΩΩÁ¥ßÊÄ•ÂëºÂè´Á≥ªÁªüÁöÑËá™Âä®Ëß¶ÂèëÂÆûÈ™å‰∏≠ÁöÑÊªëÂè∞Ê≠£Èù¢Á¢∞ÊíûËØïÈ™åÁöÑÂÆâË£ÖÊ≠•È™§‚ÄúÔºå‚ÄùÂàπËΩ¶ÁÅØÁöÑÂÆâË£Ö‰ΩçÁΩÆÁöÑÂÆΩÂ∫¶Ë¶ÅÊ±Ç‚ÄúÔºå‚ÄùÊ∞î‰ΩìÊ±°ÊüìÁâ©ÁöÑÊéíÊîæË¥®ÈáèÈôêÂà∂‚Äú\n\
            \    -   ÁÑ∂ÂêéÂ∏¶ÁùÄÂØπÊ†∏ÂøÉ‰∏ª‰ΩìÁöÑÁêÜËß£ÔºåÁªßÁª≠‰∏ãÈù¢ÁöÑÊ≠•È™§\n    \n3. **scopeÊèêÂèñ**\n\n   - Â∞ÜËØÜÂà´Âà∞ÁöÑÊ†∏ÂøÉ‰∏ª‰ΩìÔºåËßÑËåÉ‰∏∫Á±ª‰ºº‰∫é‚Äù[Âà∂Âä®ÁÅØ]\
            \ - [ÂÆâË£Ö] - [ÂÆΩÂ∫¶,È´òÂ∫¶Ë¶ÅÊ±Ç]‚ÄúÁöÑÊ†ºÂºèÔºå‰Ωú‰∏∫scope\n   \n4. **topic_keywordsÊèêÂèñ**\n\n   -\
            \ 1‚Äì6 ‰∏™ÂÖ≥ÈîÆËØçÔºåÁî®‰∫éÂÖ∑‰ΩìÊ¶ÇÊã¨ËØ•Â±Ç‰∏ªÈ¢ò\n   - Ê¶ÇÊã¨ÂΩìÂâçÊù°Ê¨æÁöÑÁõ¥Êé•ËßÑÂÆöÂØπË±°ÔºåÊØîÂ¶ÇÂΩìÂâçÁ´†ËäÇËÆ≤‰∫Ü‰ªÄ‰πàÊ±°ÊüìÁâ©ÊéíÊîæÁöÑË¥®ÈáèÂú®‰ªÄ‰πàÂå∫Èó¥Êó∂ÈúÄË¶ÅËøõË°åÁ¨¨‰∫åÊ¨°ÂÆûÈ™åÔºåÂèØ‰ª•ÊãÜÂàÜÂÖ≥ÈîÆËØç‰∏∫\"\
            test repetition condition\"„ÄÅ\"Gaseous pollutants‚Äù„ÄÅ‚Äúmass-based emission\
            \ limits\"Ôºå‰ΩÜ‰∏çÂÖÅËÆ∏‰ªÖÊúâÁ±ª‰ºº‰∫é‚ÄùÊñπÊ≥ï‚ÄúËøôÁßçÊèèËø∞ÔºàÊó†Ê≥ïÁêÜËß£ÂíåÂåπÈÖçÔºâÔºå‰πüÂ∞±ÊòØËØ¥‰∏ÄÂÆöË¶ÅËÉΩÁêÜËß£ÊòØÂØπ‰ªÄ‰πàÁöÑËßÑÂÆöÊàñÊèèËø∞Ôºå‰ΩÜ‰∏∫‰∫ÜË°®ËææÁöÑÁÆÄÊ¥ÅÔºåÂèØ‰ª•ÊãÜÂàÜÊàêÂ§ö‰∏™ÂÖ≥ÈîÆËØç„ÄÇ\n\
            \   - ÈùûÂè∂Â≠êËäÇÁÇπÔºöÂèØ‰ª•ÊòØ‰∏Ä‰∏™ÁÆÄÊ¥Å‰∏ªÈ¢òËØçÔºåÁîöËá≥‰∏∫Á©∫ÔºõÂè∂Â≠êËäÇÁÇπÔºöÂ∫îÁªÜÂåñÂà∞ÂÖ∑‰ΩìË¶ÅÊ±ÇÊàñÂÆûÈ™å„ÄÇ\n   - ÂØπÈòêÈáäÊúØËØ≠ÊàñÂÆö‰πâÁöÑÂ≠êÁ´†ËäÇÔºå‰ªÖÂ∞ÜËØ•ÊúØËØ≠‰Ωú‰∏∫topicÔºõËåÉÂõ¥„ÄÅÂºïÁî®ÊÄßÊñá‰ª∂ÂíåÊìç‰ΩúÊâãÂÜåÁ≠âÂÖ±ÊÄßÁ´†ËäÇ‰ªÖÊèêÂèñÊúâÊïàÊØîÂØπÂÜÖÂÆπÔºå‰æãÂ¶Ç‚ÄúËåÉÂõ¥‚Äù\n\
            \n5. **context_keywordsÊèêÂèñ**\n   \n   - 0‚Äì6 ‰∏™ÂÖ≥ÈîÆËØçÔºåËøôÈÉ®ÂàÜÊòØÂ∫îÁî®‰∫é‰∏ª‰ΩìÁöÑ**Ë°å‰∏∫„ÄÅÊù°‰ª∂„ÄÅÂ±ûÊÄßÊàñÊñπÊ≥ï**ÔºåÊèèËø∞ÂÆûÈ™åÂØπË±°„ÄÅÁéØÂ¢É„ÄÅÊù°‰ª∂„ÄÅÊúØËØ≠ÂÖ≥ËÅîÁ≠â„ÄÇ\n\
            \   - Âú®Á°ÆÂÆö‰∫Ü‰∏ª‰Ωì‰πãÂêéÔºåÂÜçÈóÆËá™Â∑±Ôºö‚ÄúËøô‰∏™Êù°Ê¨æÂØπ‰∏ª‰Ωì**ÂÅö‰∫Ü‰ªÄ‰πà**ÔºüÊàñËÄÖÊèèËø∞‰∫ÜÂÆÉÁöÑ**‰ªÄ‰πàÁâπÊÄß**Ôºü‚Äù\n    -   ÊØîÂ¶ÇÊ±°ÊüìÁâ©Ê∂âÂèäÂà∞‰∏ÄÊ∞ßÂåñÁ¢≥„ÄÅ‰∏ÄÊ∞ßÂåñÊ∞ÆÁ≠âÔºåÂ∞±ÂèØ‰ª•ÊèêÂèñÂá∫Êù•ÔºåËøòÊúâ‚ÄùÂÜ∑ÂêØÂä®‚ÄúËøôÁßçÂÆûÈ™åÊù°‰ª∂Ôºå‚ÄùË°∞ÂáèÂõ†Â≠ê‚ÄúÁ≠âÂÆûÈ™åÁªÜËäÇÁ≠âÁ≠âÔºå‰πüÂ∞±ÊòØÂèØ‰ª•Ëøõ‰∏ÄÊ≠•ÊèêÈ´òÂåπÈÖçÂáÜÁ°ÆÂ∫¶ÁöÑÂÜÖÂÆπ\n\
            \   - Ëã•‰∏ä‰∏ãÊñáÂÆö‰πâ‰∫ÜÊúØËØ≠ÔºåÂàôË¶Å‰øùÁïô‰∏éÊúØËØ≠ÂÖ≥ËÅîÁöÑÂÖ≥ÈîÆËØçÔºàÂ¶Ç ‚ÄúGaseous pollutants ‚Üí CO, HC, NOx‚ÄùÔºâ„ÄÇ\n\
            \   - ËåÉÂõ¥„ÄÅÂºïÁî®ÊÄßÊñá‰ª∂Á≠âÁ∫≤È¢ÜÊÄßÁ´†ËäÇ‰∏çÂÅöÊèêÂèñ\n   \n6. **Êó†Êïà‰ø°ÊÅØÂâîÈô§**\n\n   - ÂâîÈô§ÂÜÖÂÆπÔºö\n        -\
            \   ÂÖ∑‰ΩìÁöÑÊï∞ÂÄºÂíåÂçï‰Ωç (Â¶Ç \"10 per cent\", \"25 km/h\", \"CFC60\")„ÄÇ\n        - \
            \  ÂØπÂÖ∂‰ªñÊñá‰ª∂„ÄÅÁ´†ËäÇ„ÄÅÂõæË°®ÁöÑÂºïÁî® (Â¶Ç \"paragraph 5.3.1.4\", \"Table 1\", \"Âõæ B.1\",\"\
            Annex 1\",\"Appendix 1\",\"GSO standards\"Á≠â)„ÄÇ\n        -   ÂÆΩÊ≥õ„ÄÅÊó†ÊÑè‰πâÁöÑËØç (Â¶Ç\
            \ \"Ë¶ÅÊ±Ç\", \"ËßÑÂÆö\", \"ÊñπÊ≥ï\", \"ÊµãËØï\")„ÄÇ\n\n\n---\n\n# Few-shot Á§∫‰æã\n\n## ËæìÂÖ•Á§∫‰æã1\n\
            \n```\n[\n  {\n    \"file\": \"regulation\",\n    \"sections\": [\n  \
            \    {\n        \"section\": \"ÈôÑÂΩïB\",\n        \"context\": \"(ËßÑËåÉÊÄß)Ëá™Âä®Ëß¶ÂèëËØïÈ™åÊñπÊ≥ï\"\
            ,\n        \"chapters\": [\n          {\n          {\n            \"chapter_id\"\
            : \"B.2\",\n            \"chapter_title\": \"ËØïÈ™åÈ°πÁõÆ\",\n            \"raw_text\"\
            : \"\",\n            \"children\": [\n              {\n              \
            \  \"chapter_id\": \"B.2.1\",\n                \"chapter_title\": \"Ê≠£Èù¢Á¢∞Êíû\"\
            ,\n                \"raw_text\": \"\",\n                \"children\":\
            \ [\n                  {\n                    \"chapter_id\": \"B.2.1.1\"\
            ,\n                    \"chapter_title\": \"ÊªëÂè∞Ê≠£Èù¢Á¢∞ÊíûËØïÈ™å\",\n            \
            \        \"raw_text\": \"\",\n                    \"children\": [\n  \
            \                    {\n                        \"chapter_id\": \"B.2.1.1.1\"\
            ,\n                        \"chapter_title\": \"\",\n                \
            \        \"raw_text\": \"Â∞ÜÁôΩËΩ¶Ë∫´ÊàñÂ∑•Ë£ÖÂõ∫ÂÆöÂú®Á¢∞ÊíûËØïÈ™åÊªëÂè∞‰∏ä,ÂÆâË£ÖÊñπÂêëÊ®°ÊãüÊ≠£Èù¢Á¢∞Êíû„ÄÇ \",\n         \
            \               \"children\": [],\n                        \"full_path\"\
            : \"B.2 ËØïÈ™åÈ°πÁõÆ/B.2.1 Ê≠£Èù¢Á¢∞Êíû/B.2.1.1 ÊªëÂè∞Ê≠£Èù¢Á¢∞ÊíûËØïÈ™å/B.2.1.1.1 \"\n              \
            \        },\n                      {\n                        \"chapter_id\"\
            : \"B.2.1.1.2\",\n                        \"chapter_title\": \"\",\n \
            \                       \"raw_text\": \"ÊªëÂè∞ÊåâÁÖß‰ª•‰∏ãÂä†ÈÄüÂ∫¶Ê≥¢ÂΩ¢‰πã‰∏ÄËøõË°åÁ¢∞ÊíûËØïÈ™å„ÄÇ a) ‰ΩøÁî®Âà∂ÈÄ†ÂïÜÊåáÂÆöÁöÑÂä†ÈÄüÂ∫¶Ê≥¢ÂΩ¢ËøõË°åËØïÈ™å,ÊåáÂÆöÁöÑÂä†ÈÄüÂ∫¶Ê≥¢ÂΩ¢Â∫î‰∏∫Âú®B.2.1.2‰∏≠ÊèèËø∞ÁöÑÂÆûËΩ¶Á¢∞ÊíûËØïÈ™åÊù°‰ª∂‰∏≠,ËΩ¶Ë∫´ÈùûÂèòÂΩ¢Âå∫ÂüüÈááÈõÜÁöÑÂä†ÈÄüÂ∫¶-Êó∂Èó¥Êõ≤Á∫ø,Âπ∂ÁªèËøáÊª§Ê≥¢Á≠âÁ∫ßCFC60\
            \ Êª§Ê≥¢Êàñ100Hz‰ΩéÈÄöÊª§Ê≥¢„ÄÇÂÆûÈôÖËØïÈ™åÁªìÊûúÊ≥¢ÂΩ¢ÁöÑÁßØÂàÜÈÄüÂ∫¶ÂèòÂåñÈáèŒîvs( t)Â∫îÂú®‰ªªÊÑèÊó∂Âàª,‰∏çË∂ÖËøáÊåáÂÆöÊ≥¢ÂΩ¢ÁöÑÁßØÂàÜÈÄüÂ∫¶ÂèòÂåñÈáè[Œîvt( t)¬±1]km/hÁöÑËåÉÂõ¥„ÄÇ\\\
            nb) ÊåâÂõæB.1 ÁöÑÊ†áÂáÜÂä†ÈÄüÂ∫¶ÈÄöÈÅìËåÉÂõ¥ÂíåË°®B.1 ÁöÑÂèÇÊï∞ËøõË°åÂä†ÈÄüÊàñÂáèÈÄü,ÂÖ∂ÈÄüÂ∫¶ÂèòÂåñÈáèŒîv ‰∏∫\\n(25¬±1)km/h„ÄÇ\",\n   \
            \                     \"children\": [],\n                        \"full_path\"\
            : \"B.2 ËØïÈ™åÈ°πÁõÆ/B.2.1 Ê≠£Èù¢Á¢∞Êíû/B.2.1.1 ÊªëÂè∞Ê≠£Èù¢Á¢∞ÊíûËØïÈ™å/B.2.1.1.2 \"\n              \
            \        }\n                    ],\n                    \"full_path\"\
            : \"B.2 ËØïÈ™åÈ°πÁõÆ/B.2.1 Ê≠£Èù¢Á¢∞Êíû/B.2.1.1 ÊªëÂè∞Ê≠£Èù¢Á¢∞ÊíûËØïÈ™å\"\n                  },\n   \
            \             ],\n              },\n            ],\n          }\n    \
            \    ]\n      }\n    ]\n  }\n]\n```\n\n## ËæìÂá∫Á§∫‰æã1\n\n```\n[\n{\n\"file\"\
            : \"regulation\",\n\"section\": \"ÈôÑÂΩïB\",\n\"chapters\": [\n{\n\"chapter_id\"\
            : \"B.2\",\n\"scope\": \"[Ëá™Âä®Ëß¶ÂèëËØïÈ™åÊñπÊ≥ï]-[ËØïÈ™åÈ°πÁõÆ]\",\n\"topic_keywords\": [\n\
            \"Ëá™Âä®Ëß¶ÂèëËØïÈ™åÈ°πÁõÆ\"\n],\n\"context_keywords\": []\n},\n{\n\"chapter_id\": \"\
            B.2.1\",\n\"scope\": \"[Ëá™Âä®Ëß¶ÂèëËØïÈ™åÊñπÊ≥ï]-[Ê≠£Èù¢Á¢∞Êíû]\",\n\"topic_keywords\": [\n\"\
            Ëá™Âä®Ëß¶ÂèëËØïÈ™å\"\n\"Ê≠£Èù¢Á¢∞Êíû\"\n],\n\"context_keywords\": []\n},\n{\n\"chapter_id\"\
            : \"B.2.1.1\",\n\"scope\": \"[Ëá™Âä®Ëß¶ÂèëËØïÈ™åÊñπÊ≥ï]-[Ê≠£Èù¢Á¢∞Êíû]-[ÊªëÂè∞Ê≠£Èù¢Á¢∞ÊíûËØïÈ™å]\",\n\"topic_keywords\"\
            : [\n\"Ëá™Âä®Ëß¶ÂèëËØïÈ™å\"\n\"ÊªëÂè∞Ê≠£Èù¢Á¢∞ÊíûËØïÈ™å\"\n],\n\"context_keywords\": []\n},\n{\n\"\
            chapter_id\": \"B.2.1.1.1\",\n\"scope\": \"[Ëá™Âä®Ëß¶ÂèëËØïÈ™åÊñπÊ≥ï]-[Ê≠£Èù¢Á¢∞Êíû]-[ÊªëÂè∞Ê≠£Èù¢Á¢∞ÊíûËØïÈ™å]\"\
            ,\n\"topic_keywords\": [\n\"ÊªëÂè∞Ê≠£Èù¢Á¢∞ÊíûËØïÈ™å\",\n\"ÂÆâË£ÖÊ≠•È™§\",\n\"ÂÆâË£ÖÊñπÂêë\",\n],\n\"\
            context_keywords\": [\n\"ÁôΩËΩ¶Ë∫´\",\n\"Â∑•Ë£Ö\",\n\"Á¢∞ÊíûËØïÈ™åÊªëÂè∞\"\n]\n},\n{\n\"chapter_id\"\
            : \"B.2.1.1.2\",\n\"scope\": \"[Ëá™Âä®Ëß¶ÂèëËØïÈ™åÊñπÊ≥ï]-[Ê≠£Èù¢Á¢∞Êíû]-[ÊªëÂè∞Ê≠£Èù¢Á¢∞ÊíûËØïÈ™å]\",\n\"topic_keywords\"\
            : [\n\"Á¢∞ÊíûËØïÈ™å\",\n\"ÂÆûÈ™åË¶ÅÊ±Ç\"\n],\n\"context_keywords\": [\n\"Âä†ÈÄüÂ∫¶Ê≥¢ÂΩ¢\",\n\"\
            ÈÄüÂ∫¶ÂèòÂåñÈáè\",\n\"Êª§Ê≥¢Á≠âÁ∫ß\",\n\"‰ΩéÈÄöÊª§Ê≥¢\",\n\"Âä†ÈÄüÂ∫¶ÈÄöÈÅìËåÉÂõ¥\"\n]\n}\n]\n}\n]\n```\n\n##\
            \ ËæìÂÖ•Á§∫‰æã2\n\n```\n[\n  {\n    \"file\": \"regulation\",\n    \"sections\"\
            : [\n      {\n        \"section\": \"MAIN\",\n        \"context\": \"\"\
            ,\n        \"chapters\": [\n          {\n            \"chapter_id\": \"\
            1\",\n            \"chapter_title\": \"ËåÉÂõ¥\",\n            \"raw_text\"\
            : \"Êú¨Êñá‰ª∂ËßÑÂÆö‰∫ÜËΩ¶ËΩΩ‰∫ãÊïÖÁ¥ßÊÄ•ÂëºÂè´Á≥ªÁªüÁöÑÊäÄÊúØË¶ÅÊ±Ç„ÄÅÂêå‰∏ÄÂûãÂºèÂà§ÂÆöË¶ÅÊ±Ç,ÊèèËø∞‰∫ÜÁõ∏Â∫îÁöÑËØïÈ™åÊñπÊ≥ï„ÄÇ\\nÊú¨Êñá‰ª∂ÈÄÇÁî®‰∫éM1 Á±ªÂèäN1 Á±ªËΩ¶ËæÜÁöÑËΩ¶ËΩΩ‰∫ãÊïÖÁ¥ßÊÄ•ÂëºÂè´Á≥ªÁªü„ÄÇ\"\
            ,\n            \"children\": [],\n            \"full_path\": \"1 ËåÉÂõ¥\"\n\
            \          },\n          {\n            \"chapter_id\": \"3\",\n     \
            \       \"chapter_title\": \"ÊúØËØ≠ÂíåÂÆö‰πâ\",\n            \"raw_text\": \"‰∏ãÂàóÊúØËØ≠ÂíåÂÆö‰πâÈÄÇÁî®‰∫éÊú¨Êñá‰ª∂„ÄÇ\"\
            ,\n            \"children\": [\n              {\n                \"chapter_id\"\
            : \"3.1\",\n                \"chapter_title\": \"ËΩ¶ËΩΩ‰∫ãÊïÖÁ¥ßÊÄ•ÂëºÂè´Á≥ªÁªü on-boardaccidentemergencycallsystem;AECS\"\
            ,\n                \"raw_text\": \"ÈÄöËøáËΩ¶ËæÜÂÜÖÈÉ®Á≠ñÁï•Âú®ÂèëÁîü‰∫ãÊïÖÊó∂Ëá™Âä®ÊøÄÊ¥ª,ÊàñÁî±ËΩ¶ÂÜÖ‰∫∫ÂëòËøõË°åÊâãÂä®Ëß¶ÂèëÂêé,Â∞ÜËΩ¶ËæÜÁöÑ‰ΩçÁΩÆÂèäËΩ¶ËæÜÁõ∏ÂÖ≥Áä∂ÊÄÅ‰ø°ÊÅØÂêåÊ≠•ÂèëÈÄÅÁªôÁ¥ßÊÄ•ÂëºÂè´ÊúçÂä°Âπ≥Âè∞Âπ∂Âª∫Á´ãËØ≠Èü≥ÈÄöËØùÁöÑÁ≥ªÁªü„ÄÇ\"\
            ,\n                \"children\": [],\n                \"full_path\": \"\
            3 ÊúØËØ≠ÂíåÂÆö‰πâ/3.1 ËΩ¶ËΩΩ‰∫ãÊïÖÁ¥ßÊÄ•ÂëºÂè´Á≥ªÁªü on-boardaccidentemergencycallsystem;AECS\"\n  \
            \            },\n        ]\n      }\n    ]\n  }\n]\n```\n\n## ËæìÂá∫Á§∫‰æã2\n\n\
            ```\n[\n{\n\"file\": \"regulation\",\n\"section\": \"MAIN\",\n\"chapters\"\
            : [\n{\n\"chapter_id\": \"1\",\n\"scope\": \"[ËåÉÂõ¥]\",\n\"topic_keywords\"\
            : [\n\"ËåÉÂõ¥\" // ËåÉÂõ¥Á≠âÁ´†ËäÇÔºåÊòØÂÖ±ÊÄßÁ´†ËäÇÔºåÂèØ‰Ωú‰∏∫topic\n],\n\"context_keywords\": [] // ËåÉÂõ¥Á≠âÁ´†ËäÇ‰∏çÂÅöÊèêÂèñ\n\
            },\n{\n\"chapter_id\": \"3\",\n\"scope\": \"[ÊúØËØ≠ÂíåÂÆö‰πâ]\",\n\"topic_keywords\"\
            : [\n\"ÊúØËØ≠ÂíåÂÆö‰πâ\"\n],\n\"context_keywords\": []\n},\n{\n\"chapter_id\": \"\
            3.1\",\n\"scope\": \"[ÊúØËØ≠ÂíåÂÆö‰πâ]-[ËΩ¶ËΩΩ‰∫ãÊïÖÁ¥ßÊÄ•ÂëºÂè´Á≥ªÁªü(AECS)]\",\n\"topic_keywords\"\
            : [\n\"ËΩ¶ËΩΩ‰∫ãÊïÖÁ¥ßÊÄ•ÂëºÂè´Á≥ªÁªü\",\n\"AECS\"\n],\n\"context_keywords\": []\n},\n]\n\
            }\n]\n```\n\n## ËæìÂÖ•Á§∫‰æã3\n\n```\n[\n  {\n    \"file\": \"regulation\",\n\
            \    \"sections\": [\n      {\n        \"section\": \"MAIN\",\n      \
            \  \"context\": \"\",\n        \"chapters\": [\n          {\n        \
            \    \"chapter_id\": \"7-\",\n            \"chapter_title\": \"CRITERIA\
            \ OF TECHNICAL CONFORMITY\",\n            \"raw_text\": \"\",\n      \
            \      \"children\": [\n             {\n                \"chapter_id\"\
            : \"7.2\",\n                \"chapter_title\": \"Vehicle subjected to\
            \ type test\",\n                \"raw_text\": \"\",\n                \"\
            children\": [\n                  {\n                    \"chapter_id\"\
            : \"7.2.1\",\n                    \"chapter_title\": \"\",\n         \
            \           \"raw_text\": \"The vehicle shall be considered complying\
            \ with the requirement specified in item \\n4.1of this standard, if the\
            \ measured mass of carbon monoxide and the combined mass of hydrocarbon\
            \ and oxides of nitrogen, are less than or  equal to 0.70 of theallowahle\
            \ limits mentioned in Tahle (1 ).\",\n                    \"children\"\
            : [],\n                    \"full_path\": \"7- CRITERIA OF TECHNICAL CONFORMITY/7.2\
            \ Vehicle subjected to type test/7.2.1 \"\n                  },\n    \
            \              {\n                    \"chapter_id\": \"7.2.2\",\n   \
            \                 \"chapter_title\": \"\",\n                    \"raw_text\"\
            : \"The test shall  be repeated  if in the initial test, the measured\
            \ masses of both the carbon monoxide and the combined value of hydrocarbons\
            \ and oxides of nitrogenare less than or equal to 0.85 of their allowable\
            \ limits and one of these values isgreater than 0.70 of its allowable\
            \ limit.\",\n                    \"children\": [],\n                 \
            \   \"full_path\": \"7- CRITERIA OF TECHNICAL CONFORMITY/7.2 Vehicle subjected\
            \ to type test/7.2.2 \"\n                  }\n                ],\n   \
            \             \"full_path\": \"7- CRITERIA OF TECHNICAL CONFORMITY/7.2\
            \ Vehicle subjected to type test\"\n              },\n            ],\n\
            \          },\n        ],\n      },\n    ],\n  }\n]\n```\n\n## ËæìÂá∫Á§∫‰æã3\n\
            \n```\n[\n  {\n    \"file\": \"regulation\",\n    \"section\": \"MAIN\"\
            ,\n    \"chapters\": [\n      {\n        \"chapter_id\": \"7-\",\n   \
            \     \"scope\": \"[technical conformity criteria]\",\n        \"topic_keywords\"\
            : [\n          \"technical conformity criteria\"\n        ],\n       \
            \ \"context_keywords\": []\n      },\n      {\n        \"chapter_id\"\
            : \"7.2\",\n        \"scope\": \"[technical conformity criteria]-[vehicle\
            \ type test]\",\n        \"topic_keywords\": [\n          \"vehicle type\
            \ test\"\n        ],\n        \"context_keywords\": []\n      },\n   \
            \   {\n        \"chapter_id\": \"7.2.1\",\n        \"scope\": \"[Vehicle\
            \ type test] - [Emission conformity determination] - [Mass limit compliance\
            \ condition]\",\n        \"topic_keywords\": [\"vehicle type test\", \"\
            pollutants emission compliance condition\", \"mass-based limits\"],\n\
            \        \"context_keywords\": [, \"allowable emission limits\", \"compliance\
            \ criteria\"]\n        \"context_keywords\": [\n          \"pollutants\"\
            , // Êù•Ëá™ÊúØËØ≠ÂÆö‰πâÁ´†ËäÇÔºåÂÆÉÂåÖÂê´‰∫Ü‰∏ÄÊ∞ßÂåñÁ¢≥Á≠â\n          \"carbon monoxide\",\n          \"\
            hydrocarbons\",\n          \"oxides of nitrogen\"\n        ]\n      },\n\
            \      {\n        \"chapter_id\": \"7.2.2\",\n        \"scope\": \"[Vehicle\
            \ type test] - [Emission conformity determination] - [Test repetition\
            \ condition]\",\n        \"topic_keywords\": [\"vehicle type test\", \"\
            test repetition\",\"allowable emission limits\",\"mass-based range\"],\n\
            \        \"context_keywords\": [\n          \"pollutants\", // Êù•Ëá™ÊúØËØ≠ÂÆö‰πâÁ´†ËäÇÔºåÂÆÉÂåÖÂê´‰∫Ü‰∏ÄÊ∞ßÂåñÁ¢≥Á≠â\n\
            \          \"carbon monoxide\",\n          \"hydrocarbons\",\n       \
            \   \"oxides of nitrogen\"\n        ]\n      }\n    ]\n  }\n]\n\n```\n\
            \n"
        - id: 2e89206c-ded1-462d-9db7-2832e587c8ad
          role: user
          text: '{{#1756557801310.item#}}'
        selected: false
        title: LLM 2
        type: llm
        variables: []
        vision:
          enabled: false
      height: 89
      id: '1756557804693'
      parentId: '1756557801310'
      position:
        x: 204
        y: 78.24573165283232
      positionAbsolute:
        x: 842
        y: 430.2457316528323
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
      zIndex: 1002
    - data:
        code: "import json\nimport ast\nimport re\nfrom typing import Dict, List,\
          \ Any\n\ndef parse_input(src: str) -> List[Dict[str, Any]]:\n    \"\"\"\
          È¢ÑÂ§ÑÁêÜÂπ∂Ëß£ÊûêËæìÂÖ•Â≠óÁ¨¶‰∏≤‰∏∫ÂàóË°®Â≠óÂÖ∏ÁªìÊûÑ\"\"\"\n    try:\n        # 1. Êåâ </think> ÂàáÂàÜÂπ∂ÂèñÂêéÂçäÊÆµ\n  \
          \      src = src.rsplit('</think>', 1)[-1].strip()\n\n        # 2. ÂÖàÂπ≤Êéâ deepseek\
          \ V3 ÂèØËÉΩÊèíÂÖ•ÁöÑ‚ÄúÊûÅÈÄü‚ÄùÊàñÂçïÁã¨ÁöÑ‚ÄúÊûÅ‚Äù\n        src = re.sub(r'ÊûÅÈÄü\\s*', '', src)   # ÂéªÊéâ‚ÄúÊûÅÈÄü‚ÄùÂèäÂÖ∂ÂêéÂèØËÉΩÁöÑÂ§ö‰ΩôÁ©∫ÁôΩ\n\
          \        src = re.sub(r'(?<!\\w)ÊûÅ(?!\\w)', '', src)  # ÂéªÊéâÂ≠§Á´ãÂá∫Áé∞ÁöÑ‚ÄúÊûÅ‚Äù\n\n  \
          \      # 3. Â∞ùËØïÂ§öÁßçËß£ÊûêË∑ØÂæÑ\n        try:\n            data = json.loads(src)\n\
          \        except json.JSONDecodeError:\n            try:\n              \
          \  if src.startswith('\\ufeff'):\n                    src = src[1:]\n  \
          \              data = ast.literal_eval(src)\n            except (SyntaxError,\
          \ ValueError):\n                try:\n                    processed = src.replace('\\\
          \\\"', '\"').replace('\\\\n', '\\n')\n                    data = json.loads(processed)\n\
          \                except Exception as e:\n                    raise ValueError(f\"\
          Êó†Ê≥ïËß£ÊûêÔºö{str(e)}\")\n\n        # 4. ‰øùËØÅËøîÂõû list\n        if isinstance(data,\
          \ dict):\n            return [data]\n        elif isinstance(data, list):\n\
          \            return data\n        else:\n            return []\n    except\
          \ Exception as e:\n        raise ValueError(f\"È¢ÑÂ§ÑÁêÜÂ§±Ë¥•: {str(e)}\")\n\n\n\
          def main(arg1: str) -> dict:\n    data = parse_input(arg1)\n    return {\n\
          \        \"result\": json.dumps(data, ensure_ascii=False),\n    }"
        code_language: python3
        desc: ''
        error_strategy: fail-branch
        isInIteration: true
        isInLoop: false
        iteration_id: '1756557801310'
        outputs:
          result:
            children: null
            type: string
        selected: false
        title: ‰ª£Á†ÅÊâßË°å 2
        type: code
        variables:
        - value_selector:
          - '1756557804693'
          - text
          value_type: string
          variable: arg1
      height: 89
      id: '1756557929661'
      parentId: '1756557801310'
      position:
        x: 508
        y: 80
      positionAbsolute:
        x: 1146
        y: 432
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
      zIndex: 1002
    - data:
        answer: '{{#1756613329065.output#}}'
        desc: ''
        selected: false
        title: Áõ¥Êé•ÂõûÂ§ç 2
        type: answer
        variables: []
      height: 104
      id: '1756558073574'
      position:
        x: 5718
        y: 352
      positionAbsolute:
        x: 5718
        y: 352
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        code: "import json\nimport re\nimport os\n\nimport json\nimport ast\nimport\
          \ re\nfrom typing import Dict, List, Any\n\ndef parse_input(src: str) ->\
          \ List[Dict[str, Any]]:\n    \"\"\"È¢ÑÂ§ÑÁêÜÂπ∂Ëß£ÊûêËæìÂÖ•Â≠óÁ¨¶‰∏≤‰∏∫ÂàóË°®Â≠óÂÖ∏ÁªìÊûÑ\"\"\"\n    try:\n\
          \        # ÂéªÈô§ LLM ÊÄùÁª¥ÈìæÊ†áËÆ∞\n        try: \n            src = re.sub(r'<think>.*?</think>',\
          \ '', src, flags=re.DOTALL).strip()\n        except:\n            pass\n\
          \n        try:\n            data = json.loads(src)\n        except json.JSONDecodeError:\n\
          \            try:\n                if src.startswith('\\ufeff'):\n     \
          \               src = src[1:]\n                data = ast.literal_eval(src)\n\
          \            except (SyntaxError, ValueError):\n                try:\n \
          \                   processed = src.replace('\\\\\"', '\"').replace('\\\\\
          n', '\\n')\n                    data = json.loads(processed)\n         \
          \       except Exception as e:\n                    raise ValueError(f\"\
          Êó†Ê≥ïËß£ÊûêÔºö{str(e)}\")\n\n        # ‰øùËØÅ‰∏∫ list\n        if isinstance(data, dict):\n\
          \            return [data]\n        elif isinstance(data, list):\n     \
          \       return data\n        else:\n            return []\n    except Exception\
          \ as e:\n        raise ValueError(f\"È¢ÑÂ§ÑÁêÜÂ§±Ë¥•: {str(e)}\")\n        # return\
          \ []\n\ndef find_chapters_and_children_by_ids(tree, target_file, target_section,\
          \ experiment_root_ids):\n    \"\"\"\n    Ê†πÊçÆ experiment_root_ids ÊâæÂà∞ÂØπÂ∫îÁöÑÁ´†ËäÇÂèäÂÖ∂ÊâÄÊúâÂ≠êÁ´†ËäÇ\n\
          \    \n    Args:\n        tree: ÂÆåÊï¥ÁöÑÁ´†ËäÇÊ†ëÁªìÊûÑ\n        target_file: ÁõÆÊ†áÊñá‰ª∂Âêç (Â¶Ç\
          \ \"regulation\")\n        target_section: ÁõÆÊ†ásectionÂêç (Â¶Ç \"ÈôÑÂΩïB\")\n    \
          \    experiment_root_ids: Ë¶ÅÊü•ÊâæÁöÑÁ´†ËäÇIDÂàóË°® (Â¶Ç [\"B.2.1.1\",\"B.2.4.1\"])\n   \
          \ \n    Returns:\n        List[Dict]: ÊØè‰∏™itemÂåÖÂê´ÊâæÂà∞ÁöÑÁ´†ËäÇÂèäÂÖ∂ÊâÄÊúâÂ≠êÁ´†ËäÇ\n    \"\"\"\n\
          \    \n    def get_chapter_with_all_children(chapter):\n        \"\"\"ÈÄíÂΩíËé∑ÂèñÁ´†ËäÇÂèäÂÖ∂ÊâÄÊúâÂ≠êÁ´†ËäÇ\"\
          \"\"\n        result = {\n            \"chapter_id\": chapter[\"chapter_id\"\
          ],\n            \"chapter_title\": chapter[\"chapter_title\"],\n       \
          \     \"raw_text\": chapter.get(\"raw_text\", \"\"),\n            \"full_path\"\
          : chapter.get(\"full_path\", \"\"),\n            \"children\": []\n    \
          \    }\n        \n        # Â¶ÇÊûúÊúâÂ≠êÁ´†ËäÇÔºåÈÄíÂΩíËé∑Âèñ\n        if \"children\" in chapter\
          \ and chapter[\"children\"]:\n            for child in chapter[\"children\"\
          ]:\n                result[\"children\"].append(get_chapter_with_all_children(child))\n\
          \        \n        return result\n    \n    def find_chapter_by_id(chapters,\
          \ target_id):\n        \"\"\"Âú®Á´†ËäÇÂàóË°®‰∏≠ÈÄíÂΩíÊü•ÊâæÊåáÂÆöIDÁöÑÁ´†ËäÇ\"\"\"\n        for chapter\
          \ in chapters:\n            if chapter[\"chapter_id\"] == target_id:\n \
          \               return chapter\n\n            # Âú®Â≠êÁ´†ËäÇ‰∏≠ÈÄíÂΩíÊü•Êâæ\n            if\
          \ \"children\" in chapter and chapter[\"children\"]:\n                found\
          \ = find_chapter_by_id(chapter[\"children\"], target_id)\n             \
          \   if found:\n                    return found\n        \n        return\
          \ None\n    \n    result = []\n    \n    # ÈÅçÂéÜÊ†ëÁªìÊûÑÔºåÊâæÂà∞ÁõÆÊ†áÊñá‰ª∂Âíåsection\n    for\
          \ file_item in tree:\n        if file_item[\"file\"] != target_file:\n \
          \           continue\n            \n        for section in file_item[\"\
          sections\"]:\n            if section[\"section\"] != target_section:\n \
          \               continue\n            \n            # Âú® find_chapters_and_children_by_ids\
          \ ÈáåÔºåÊâæÂà∞ÁõÆÊ†á section ÂêéÂä†Ôºö\n            if experiment_root_ids == [\"ALL\"]:\n\
          \                # ÊääÁ¨¨‰∏ÄÂ±ÇÊâÄÊúâ chapter_id ÂèñÂá∫Êù•\n                experiment_root_ids\
          \ = [ch[\"chapter_id\"] for ch in section.get(\"chapters\", [])]\n\n   \
          \         # Âú®ËØ•section‰∏≠Êü•ÊâæÊØè‰∏™experiment_root_id\n            for root_id in\
          \ experiment_root_ids:\n                found_chapter = find_chapter_by_id(section[\"\
          chapters\"], root_id)\n                if found_chapter:\n             \
          \       # Ëé∑ÂèñËØ•Á´†ËäÇÂèäÂÖ∂ÊâÄÊúâÂ≠êÁ´†ËäÇ\n                    chapter_with_children = get_chapter_with_all_children(found_chapter)\n\
          \                    result.append(chapter_with_children)\n            \
          \    else:\n                    print(f\"Ë≠¶Âëä: Êú™ÊâæÂà∞Á´†ËäÇID '{root_id}' Âú® {target_file}/{target_section}\
          \ ‰∏≠\")\n    \n    return result\n\ndef extract_experiment_chapters(tree,\
          \ experiment_info):\n    \"\"\"\n    Ê†πÊçÆÂÆûÈ™å‰ø°ÊÅØÊèêÂèñÂØπÂ∫îÁöÑÁ´†ËäÇ\n    \n    Args:\n  \
          \      tree: ÂÆåÊï¥ÁöÑÁ´†ËäÇÊ†ë\n        experiment_info: ÂåÖÂê´ file, section, experiment_root_ids\
          \ ÁöÑÂ≠óÂÖ∏\n    \n    Returns:\n        List[Dict]: ÂÆûÈ™åÁõ∏ÂÖ≥ÁöÑÁ´†ËäÇÂàóË°®\n    \"\"\"\n \
          \   return find_chapters_and_children_by_ids(\n        tree=tree,\n    \
          \    target_file = experiment_info.get(\"file\", \"\"),\n        target_section\
          \ = experiment_info.get(\"section\", \"\"),\n        experiment_root_ids\
          \ = experiment_info.get(\"experiment_root_ids\", [])\n    )\n\ndef main(arg1:\
          \ list[str], arg2: str, file_name) -> dict:\n    \"\"\"\n    Â§ÑÁêÜÂÆûÈ™åÊï∞ÊçÆÁöÑ‰∏ªÂáΩÊï∞\n\
          \    \n    Args:\n        arg1: JSONÂ≠óÁ¨¶‰∏≤ÂàóË°®ÔºåÊØè‰∏™ÂåÖÂê´ file, section, experiment_root_ids\n\
          \        arg2: Á´†ËäÇÊ†ëÁöÑJSONÂ≠óÁ¨¶‰∏≤\n    \n    Returns:\n        dict: Â§ÑÁêÜÂêéÁöÑÂÆûÈ™åÁ´†ËäÇÊï∞ÊçÆ\n\
          \    \"\"\"\n    import json\n    from collections import defaultdict\n\
          \    \n    # Ëß£ÊûêËæìÂÖ•Êï∞ÊçÆ\n    experiment_data = []\n    solved_tree = []\n  \
          \  for t in arg1:\n        if t is None:\n            continue\n       \
          \ items = json.loads(t)\n        for item in items:\n            experiment_data.append({\n\
          \                'file': item.get('file', ''),\n                'section':\
          \ item.get('section', ''),\n                'experiment_root_ids': item.get('experiment_root_ids',\
          \ [])\n            })\n            solved_tree.append({\n              \
          \  'file': item['file'],\n                'section': item['section'],\n\
          \                'chapters':item['chapters']\n            })\n    \n   \
          \ # # Êåâ file Âíå section ÂêàÂπ∂Áõ∏ÂêåÁöÑÈ°πÁõÆ\n    # merged_experiments = defaultdict(list)\n\
          \    # for item in experiment_data:\n    #     key = (item['file'], item['section'])\n\
          \    #     merged_experiments[key].extend(item['experiment_root_ids'])\n\
          \    \n    # # ÂéªÈáçÂπ∂ÁîüÊàêÊúÄÁªàÁöÑ experiment_info ÂàóË°®\n    # experiment_infos = []\n\
          \    # for (file, section), root_ids in merged_experiments.items():\n  \
          \  #     # ÂéªÈáç‰ΩÜ‰øùÊåÅÈ°∫Â∫è\n    #     unique_root_ids = []\n    #     seen = set()\n\
          \    #     for root_id in root_ids:\n    #         if root_id not in seen:\n\
          \    #             unique_root_ids.append(root_id)\n    #             seen.add(root_id)\n\
          \    #     if len(unique_root_ids):\n    #         experiment_infos.append({\n\
          \    #             'file': file,\n    #             'section': section,\n\
          \    #             'experiment_root_ids': unique_root_ids\n    #       \
          \  })\n    \n    # ‚úÖ ÊåâÂéüÂßã JSON Â≠óÁ¨¶‰∏≤Á≤íÂ∫¶ÂêàÂπ∂Ôºå‰∏çÊãÜ ID\n    experiment_infos = []\n\
          \    for item in experiment_data:\n        if item['experiment_root_ids']:\
          \  # Âè™‰øùÁïôÈùûÁ©∫ÁöÑ\n            experiment_infos.append({\n                'file':\
          \ item['file'],\n                'section': item['section'],\n         \
          \       'experiment_root_ids': item['experiment_root_ids']  # ‰øùÊåÅÂéüÊ†∑Ôºå‰∏çÊãÜ\n\
          \            })\n\n    # ‚úÖ Êåâ (file, section, tuple(root_ids)) ÂéªÈáç\n    seen\
          \ = set()\n    unique_experiments = []\n    for exp in experiment_infos:\n\
          \        key = (exp['file'], exp['section'], tuple(exp['experiment_root_ids']))\n\
          \        if key not in seen:\n            seen.add(key)\n            unique_experiments.append(exp)\n\
          \n    experiment_infos = unique_experiments\n\n    # 1. ÂÖàÊää solved_tree Êåâ\
          \ (file, section) ÂÅöÂàÜÁªÑ\n    merged_chapters = defaultdict(list)\n    for\
          \ node in solved_tree:\n        key = (node['file'], node['section'])\n\
          \        merged_chapters[key].extend(node['chapters'])\n\n    # 2. Âú®ÊØè‰∏™ÂàÜÁªÑÂÜÖÂØπ\
          \ chapters ÂéªÈáç‰∏î‰øùÊåÅÂéüÈ°∫Â∫è\n    final_tree = []\n    for (file, section), chapters\
          \ in merged_chapters.items():\n        seen_id = set()\n        unique_chapters\
          \ = []\n        for c in chapters:\n            cid = c.get(\"chapter_id\"\
          )\n            if cid not in seen_id:\n                unique_chapters.append(c)\n\
          \                seen_id.add(cid)\n        if unique_chapters:\n       \
          \     final_tree.append({\n                \"file\": file,\n           \
          \     \"section\": section,\n                \"chapters\": unique_chapters\n\
          \            })\n\n    # Ëß£ÊûêÁ´†ËäÇÊ†ë\n    regulation_tree = parse_input(arg2)\n\
          \    \n    # print(experiment_infos)\n\n\n    # Â§ÑÁêÜÊØè‰∏™ÂÆûÈ™å‰ø°ÊÅØÔºåÊèêÂèñÂØπÂ∫îÁöÑÁ´†ËäÇ\n    result\
          \ = []\n    for i, experiment_info in enumerate(experiment_infos):\n   \
          \     chapters_with_hierarchy = extract_experiment_chapters(regulation_tree,\
          \ experiment_info)\n        \n        result.append(json.dumps(chapters_with_hierarchy,\
          \ ensure_ascii=False))\n    \n    # ÂéªÊéâÊâ©Â±ïÂêçÔºåÂÜçÊãº .json\n    base_name = os.path.splitext(file_name)[0]\
          \          # ÂæóÂà∞Êó†ÂêéÁºÄÁöÑÁ∫ØÊñá‰ª∂Âêç\n    path = f\"/tmp/mydata/{base_name}/first_version.json\"\
          \n\n\n    os.makedirs(os.path.dirname(path), exist_ok=True)\n    with open(path,\
          \ \"w\", encoding=\"utf-8\") as f:\n        json.dump(final_tree, f, ensure_ascii=False,\
          \ indent=2)\n\n    return {\n        \"chapters\":result,\n        \"experiment_infos\"\
          :json.dumps(experiment_infos, ensure_ascii=False),\n        \"final_tree\"\
          :json.dumps(final_tree, ensure_ascii=False),\n    }"
        code_language: python3
        desc: ''
        outputs:
          chapters:
            children: null
            type: array[string]
          experiment_infos:
            children: null
            type: string
          final_tree:
            children: null
            type: string
        selected: false
        title: ‰ª£Á†ÅÊâßË°å 3
        type: code
        variables:
        - value_selector:
          - '1758096294198'
          - result
          value_type: array[string]
          variable: arg1
        - value_selector:
          - '1756550411122'
          - tree
          value_type: string
          variable: arg2
        - value_selector:
          - '1756550268945'
          - file
          - name
          value_type: file
          variable: file_name
      height: 53
      id: '1756563307317'
      position:
        x: 3022.491463305665
        y: 352
      positionAbsolute:
        x: 3022.491463305665
        y: 352
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        code: "\ndef main(arg1: str) -> dict:\n    return {\n        \"array\": [arg1],\n\
          \    }\n"
        code_language: python3
        desc: ''
        outputs:
          array:
            children: null
            type: array[string]
        selected: false
        title: ‰ª£Á†ÅÊâßË°å 4
        type: code
        variables:
        - value_selector:
          - '1756550268945'
          - array
          value_type: string
          variable: arg1
      height: 53
      id: '1756567749405'
      position:
        x: 30
        y: 747
      positionAbsolute:
        x: 30
        y: 747
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        code: "\ndef main(arg1: list[str]) -> dict:\n    return {\n        \"result\"\
          :json.dumps(arg1, ensure_ascii=False)\n    }\n"
        code_language: python3
        desc: ''
        outputs:
          result:
            children: null
            type: string
        selected: false
        title: ‰ª£Á†ÅÊâßË°å 5
        type: code
        variables:
        - value_selector:
          - '1756557801310'
          - output
          value_type: array[string]
          variable: arg1
      height: 53
      id: '1756568067080'
      position:
        x: 30
        y: 840
      positionAbsolute:
        x: 30
        y: 840
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        desc: ''
        error_handle_mode: continue-on-error
        height: 290
        is_parallel: true
        iterator_input_type: array[string]
        iterator_selector:
        - '1756563307317'
        - chapters
        output_selector:
        - '1757762184699'
        - output
        output_type: array[string]
        parallel_nums: 10
        selected: false
        start_node_id: 1756613329065start
        title: Ëø≠‰ª£ 2
        type: iteration
        width: 1724
      height: 290
      id: '1756613329065'
      position:
        x: 2722.491463305665
        y: 783.5500134032299
      positionAbsolute:
        x: 2722.491463305665
        y: 783.5500134032299
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 1724
      zIndex: 1
    - data:
        desc: ''
        isInIteration: true
        selected: false
        title: ''
        type: iteration-start
      draggable: false
      height: 48
      id: 1756613329065start
      parentId: '1756613329065'
      position:
        x: 60
        y: 100.5
      positionAbsolute:
        x: 2782.491463305665
        y: 884.0500134032299
      selectable: false
      sourcePosition: right
      targetPosition: left
      type: custom-iteration-start
      width: 44
      zIndex: 1002
    - data:
        context:
          enabled: true
          variable_selector:
          - '1756550411122'
          - context
        desc: ''
        isInIteration: true
        isInLoop: false
        iteration_id: '1756613329065'
        model:
          completion_params: {}
          mode: chat
          name: deepseek-reasoner
          provider: langgenius/deepseek/deepseek
        prompt_template:
        - id: 8e7bb797-e961-48ec-9029-47c638adcc67
          role: system
          text: "# ËßíËâ≤ÔºàRoleÔºâ\n‰Ω†ÊòØ‰∏ÄÂêç **Ê†áÂáÜÊµãËØïÁî®‰æãËÆæËÆ°‰∏ìÂÆ∂**ÔºåÁ≤æÈÄöÊ±ΩËΩ¶ÂèäÁõ∏ÂÖ≥È¢ÜÂüüÁöÑÊ†áÂáÜÂíåÊ≥ïËßÑÊñáÊ°£„ÄÇ  \n‰Ω†ÁöÑËÅåË¥£ÊòØÂ∞ÜÊ†áÂáÜÊàñÊ≥ïËßÑÊñá‰ª∂‰∏≠ÁöÑÊù°Ê¨æÂíåËØïÈ™åÊñπÊ≥ï\
            \ **‰∏•Ê†ºÊãÜËß£‰∏∫ÁªìÊûÑÂåñÊµãËØïÁî®‰æã**ÔºåËæìÂá∫ÁªìÊûúÁî®‰∫éÔºö  \n- Ê†áÂáÜÂíåÊ≥ïËßÑÁöÑÊØîÂØπÂàÜÊûê  \n- ÊµãËØïÊú∫ÊûÑËÆæÂ§áËÉΩÂäõÈ™åËØÅÂíåË¶ÜÁõñÁéáËØÑ‰º∞  \n-\
            \ Ëá™Âä®ÂåñÊµãËØïÁî®‰æãÁÆ°ÁêÜÁ≥ªÁªü  \nË¶ÅÊ±Ç‰ø°ÊÅØÂÆåÊï¥„ÄÅÊù°ÁêÜÊ∏ÖÊô∞„ÄÅÂ≠óÊÆµËßÑËåÉÂåñÔºåÁ°Æ‰øùÁªìÊûÑÂåñÁªìÊûúÂÖ∑Â§áÂèØËøΩÊ∫ØÊÄßÂíåÂèØÊâßË°åÊÄß„ÄÇ\n\n---\n\n# ËæìÂÖ•Ê†ºÂºèÔºàInput\
            \ FormatÔºâ\nËæìÂÖ•‰∏∫‰∏Ä‰∏™ JSON Êï∞ÁªÑÔºåÊØè‰∏™ÂÖÉÁ¥†Ë°®Á§∫‰∏Ä‰∏™Á´†ËäÇÔºö\n```json\n[\n  {\n    \"chapter_id\"\
            : \"string\",      // Á´†ËäÇÁºñÂè∑\n    \"chapter_title\": \"string\",   // Á´†ËäÇÊ†áÈ¢ò\n\
            \    \"raw_text\": \"string\",        // Êú¨Á´†ËäÇÊ≠£Êñá\n    \"children\": [  \
            \              // Â≠êÁ´†ËäÇÊï∞ÁªÑÔºàÈÄíÂΩíÁªìÊûÑÔºâ\n      { ...ÂêåÁªìÊûÑ... }\n    ],\n    \"full_path\"\
            : \"string\"        // ‰ªéÊ†πÂà∞ËØ•Á´†ËäÇÁöÑË∑ØÂæÑ\n  }\n]\n```\nËØ¥ÊòéÔºö\n- ‰Ω†‰πüÂèØ‰ª•‰ªé {{#context#}}\
            \ ‰∏≠ÁêÜËß£Ê≠§Ê†áÂáÜÁöÑÁõ∏ÂÖ≥‰ø°ÊÅØ\n- `raw_text` ÂèØËÉΩ‰∏∫Á©∫ÔºåÊ≠£ÊñáÂèØËÉΩÂú® `children` ÂÜÖ„ÄÇ\n- Âêå‰∏ÄÁ´†ËäÇÂèØÂåÖÂê´Â§ö‰∏™ÂÆûÈ™åÊñπÊ≥ïÔºõÊØè‰∏™ÂÆûÈ™åÊñπÊ≥ïÈúÄÂçïÁã¨ÊãÜËß£ÊàêÂØπË±°„ÄÇ‰∏çÂêåÁ´†ËäÇÈó¥ÂêÑËá™Áã¨Á´ãÔºå‰∏çË¶ÅÁõ∏‰∫íÊé®Êñ≠\n\
            - ‰∏Ä‰∏™ÂÆûÈ™åÊñπÊ≥ï‰∏≠Â≠òÂú®Â§öÁßçËØïÈ™åÊñπÊ°àÊó∂Ôºå‰πüË¶ÅÂàÜÂà´ÊãÜËß£‰∏∫Â§ö‰∏™ÂØπË±°ÔºàÂëΩÂêçÊó∂Âú® `test_name` ÂêéÊ∑ªÂä†ÊñπÊ°àÊ†áËØÜÔºåÂ¶Ç‚Äú(XXÊñπÊ°à)‚ÄùÔºâ„ÄÇ\n\n\
            ---\n\n# ËæìÂá∫Ê†ºÂºèÔºàOutput FormatÔºâ\nËæìÂá∫‰∏∫‰∏Ä‰∏™ JSON Êï∞ÁªÑÔºåÂØπÂ∫îËæìÂÖ•ÁöÑÁ´†ËäÇÊï∞ÁªÑÔºåÊØè‰∏™Á´†ËäÇÂØπÂ∫î‰∏Ä‰∏™Êï∞ÁªÑÔºåÊï∞ÁªÑ‰∏≠ÁöÑÊØè‰∏™ÂØπË±°‰ª£Ë°®‰∏Ä‰∏™Áã¨Á´ãÁöÑÂÆûÈ™åÊàñÂÆûÈ™åÊñπÊ°àÔºö\n\
            \n```json\n[\n  [\n    {\n      \"chapter_id\": \"string\",    // Êù•Ê∫êÁ´†ËäÇÁöÑchapter_idÔºå‰øùËØÅÂèØËøΩÊ∫ØÊÄß\n\
            \      \"test_name\": \"string\",     // ÊµãËØïÂêçÁß∞Ôºå‰ªéÁ´†ËäÇÊ†áÈ¢òÊàñË∑ØÂæÑÊèêÁÇºÔºåÁÆÄÊ¥Å‰∏îÂîØ‰∏Ä\n     \
            \ \"conditions\": [            // ÊµãËØïÊù°‰ª∂ÔºöËØï‰ª∂Áä∂ÊÄÅ„ÄÅÂÆâË£Ö„ÄÅÁéØÂ¢ÉÁ≠âÔºåÂÆåÊï¥ÂàÜÁÇπÂàóÂá∫\n        \"\
            string\"\n      ],\n      \"criteria\": [              // Âà§ÂÆöÊ†áÂáÜÔºöÊòéÁ°ÆÊ£ÄÊµãÈ°π+ËææÊ†áÊù°‰ª∂ÔºåÂàÜÁÇπÂàóÂá∫\n\
            \        \"string\"\n      ],\n      \"equipment\": [             // ÊâÄÈúÄËÆæÂ§áÂèäËßÑÊ†ºÔºå‰æø‰∫éËÆæÂ§áËÉΩÂäõÈ™åËØÅ\n\
            \        {\n          \"name\": \"string\",          // ËÆæÂ§áÂêçÁß∞\n       \
            \   \"specification\": \"string\"  // ËÆæÂ§áËßÑÊ†º/ÊÄßËÉΩ\n        }\n      ],\n \
            \     \"parameters\": [            // ÊèêÂèñÁöÑÂÖ≥ÈîÆÂèÇÊï∞\n        {\n          \"\
            item\": \"string\",          // ÂèÇÊï∞È°π\n          \"constraint\": \"string\"\
            ,    // Á∫¶ÊùüÔºö<=|<|=|>=|>|range_closed|range_open|enum|boolean\n        \
            \  \"value\": \"string|array|null\", // ÂèÇÊï∞ÂÄºÔºåÂèØ‰∏∫Êï∞Â≠ó„ÄÅËåÉÂõ¥ÊàñÊûö‰∏æ\n          \"unit\"\
            : \"string|null\",     // Âçï‰ΩçÔºåËã•Êó†Âàônull\n          \"source_text\": \"string\"\
            \    // ÂéüÊñáÂÆåÊï¥ÁâáÊÆµÔºå‰øùÁïôÁ¨¶Âè∑Ôºå‰æø‰∫éËøΩÊ∫Ø\n        }\n      ],\n      \"refs\": [     \
            \             // ÂºïÁî®‰ø°ÊÅØ\n        {\n          \"ref_type\": \"internal|external\"\
            , // ÂÜÖÈÉ®/Â§ñÈÉ®ÂºïÁî®\n          \"doc_id\": \"string|null\",         // Â§ñÈÉ®Ê†áÂáÜÁºñÂè∑ÔºõÂÜÖÈÉ®ÂºïÁî®Â°´null\n\
            \          \"target_id\": \"string\",           // ÂºïÁî®ÁõÆÊ†áÁºñÂè∑ÔºàÂ¶Ç‚ÄúB.2.1.1‚Äù„ÄÅ‚ÄúË°®B.1‚ÄùÔºâ\n\
            \          \"anchor_text\": \"string\"          // ÂºïÁî®ÁöÑÁÆÄË¶Å‰∏ä‰∏ãÊñáÊèèËø∞\n      \
            \  }\n      ]\n    }\n  ]\n]\n```\n\n---\n\n# Â≠óÊÆµËØ¥Êòé\n- **chapter_id**ÔºöÊ†áËÆ∞Êù•Ê∫êÁ´†ËäÇÔºå‰øùËØÅÂèØËøΩÊ∫ØÊÄß„ÄÇ\
            \  \n- **test_name**ÔºöÂîØ‰∏ÄÊ†áËØÜËØ•ÂÆûÈ™åÊñπÊ≥ïÔºåËã•Âêå‰∏ÄÂÆûÈ™åÊúâÂ§öÊñπÊ°àÔºåÂú®ÂêéÂä†Êã¨Âè∑Ê†áÊòéÊñπÊ°àÔºåÂ¶Ç‚Äú(ÊñπÊ°àA)‚Äù„ÄÇ  \n- **conditions**ÔºöÂÆåÊï¥ÂàÜÁÇπÂàóÂá∫ÊµãËØïÂâçÁΩÆÊù°‰ª∂„ÄÅÂÆâË£ÖÁä∂ÊÄÅ„ÄÅÁéØÂ¢ÉË¶ÅÊ±ÇÁ≠â„ÄÇ\
            \  \n- **criteria**ÔºöÊØèÊù°ÂøÖÈ°ªÂåÖÂê´Ê£ÄÊµãÂØπË±°‰∏éÂà§ÂÆöÊù°‰ª∂ÔºåÈÅøÂÖçÁ¨ºÁªüÊèèËø∞„ÄÇ  \n- **equipment**ÔºöÂÆåÊï¥ÂàóÂá∫ÊâÄÊúâËÆæÂ§áÂèäÂÖ≥ÈîÆËßÑÊ†º„ÄÇ\
            \  \n- **parameters**ÔºöÊèêÂèñÊâÄÊúâÂÆöÈáèÊåáÊ†áÔºå‰∏≠ÊñáÊï∞Â≠óËΩ¨ÈòøÊãâ‰ºØÊï∞Â≠ó„ÄÇ ‰∏çÂåÖÂê´Ë°®Ê†ºÂÜÖÁöÑÂèÇÊï∞\n- **refs**ÔºöÊèêÂèñÁ´†ËäÇÂÜÖÁöÑÊ†áÂáÜÂºïÁî®Ôºö\n\
            \  - ÂÜÖÈÉ®ÂºïÁî®ÔºöÊåáÂêëÊú¨Ê†áÂáÜÁöÑÁ´†ËäÇ„ÄÅË°®„ÄÅÂõæÔºå`ref_type=internal`Ôºå`doc_id=null`„ÄÇ\n  - Â§ñÈÉ®ÂºïÁî®ÔºöÊåáÂêëÂ§ñÈÉ®Ê†áÂáÜÔºå`ref_type=external`ÔºåÂ°´ÂÜôÊ†áÂáÜÁºñÂè∑„ÄÇ\n\
            \  - Â§ö‰∏™ÂºïÁî®ÂèØÂêàÂπ∂Êàê‰∏ÄÊù°ËÆ∞ÂΩïÔºàÂ¶Ç‚ÄúË°®B.1„ÄÅÂõæB.1‚ÄùÂèØ‰∏∫‰∏Ä‰∏™Êù°ÁõÆÔºâ„ÄÇ\n\n---\n\n# Â§ÑÁêÜÈÄªËæëÔºàChain-of-ThoughtÔºâ\n\
            1. **Ëß£ÊûêÁ´†ËäÇÂÜÖÂÆπ**ÔºöÈÄíÂΩíÈÅçÂéÜ`children`ÔºåÂà§Êñ≠ÊòØÂê¶Â≠òÂú®ÁúüÂÆûÂÆûÈ™å\n   - Ëã•‰∏ª‰ΩìÊòØË¶ÅÊ±ÇÔºå‰æãÂ¶ÇÔºö‚Äú4.4. 1.1 ÊåâÈôÑÂΩïBËøõË°åËØïÈ™åÂêé,AECSÂ∫îË¢´Ëá™Âä®Ëß¶Âèë,‰∏îÂèëÈÄÅÁöÑMSD‰∏≠Ëß¶ÂèëÁ±ªÂûãÂ∫î‰∏∫Ëá™Âä®Ëß¶Âèë‚ÄùÔºåÁªìÂêà‰∏ä‰∏ãÊñáÁªºÂêàËØÑ‰º∞\n\
            \   - Ëã•‰∏ª‰ΩìÊòØÂÆûÈ™åÔºåÂç≥ÂÆûÈôÖ‰∏äÊ≠§Â§ÑÁ°ÆÂÆûËøõË°åÂÆûÈ™åÔºå‰ΩÜÊòØÂπ∂‰∏çÂÆåÊï¥ÔºàÊØîÂ¶ÇÂÆûÈ™åÊ≠•È™§ÂèÇËßÅ‰ªñÂ§ÑÔºâÔºå‰ªçÂ∫îÊèêÂèñÂÆûÈ™å\n   - ÁΩÆ‰ø°Â∫¶ÂçÅÂàÜÈ´òË¶ÅÊ±Ç‰∏ª‰ΩìÔºà‰æãÂ¶ÇÔºöThis\
            \ Appendix describes the procedure to be used to verify the production\
            \ conformity for the Type I test when the manufacturer's production standard\
            \ deviation issatisfactoryÊòéÊòæÂπ∂‰∏çÊòØ‰∏Ä‰∏™ÂÆûÈ™å‰∏ª‰ΩìÔºâÔºå‰∏çÂÜçËøõË°åÂÆûÈ™åÊèêÂèñÔºåÁõ¥Êé•ËøîÂõû[]\n2. **ÊãÜÂàÜÂÆûÈ™åÊñπÊ≥ï‰∏éÊñπÊ°à**ÔºöÊ†πÊçÆÊèèËø∞ÈÄªËæë„ÄÅÂ∫èÂè∑„ÄÅa)/b)ÂàÜÈ°πÁ≠âÊãÜÊàêÂ§ö‰∏™ÂØπË±°„ÄÇ\
            \  \n3. **ÊèêÂèñÁ´†ËäÇID**ÔºöÂÜôÂÖ•ÊØè‰∏™ÂØπË±°ÁöÑ`chapter_id`Â≠óÊÆµ„ÄÇ  \n4. **ÊèêÂèñÂÆûÈ™åÂêçÁß∞**Ôºö‰ºòÂÖàÁ´†ËäÇÊ†áÈ¢òÔºåÁªìÂêàË∑ØÂæÑÁÆÄÂåñÔºåÂøÖË¶ÅÊó∂Ê†áÊòéÊñπÊ°à„ÄÇ\
            \  \n5. **Ëß£ÊûêÂÆûÈ™åÊù°‰ª∂**Ôºö\n   - ÂàóÂá∫ÂÆâË£ÖÊñπÂºè„ÄÅËØï‰ª∂Áä∂ÊÄÅ„ÄÅÁéØÂ¢ÉÊù°‰ª∂„ÄÅÁîµÊ∫êÁä∂ÊÄÅÁ≠â„ÄÇ\n   - Âç≥‰Ωø‰∏éÂèÇÊï∞ÈáçÂ§çÔºå‰πüÈúÄÂÆåÊï¥‰øùÁïôÔºå‰øùËØÅ‰∏ä‰∏ãÊñáÂÆåÊï¥ÊÄß„ÄÇ\
            \  \n6. **ÊèêÂèñÂà§ÂÆöÊ†áÂáÜ**Ôºö\n   - ÂàÜÁÇπÊèèËø∞Ê£ÄÊµãÈ°π+Âà§ÂÆöÊù°‰ª∂ÔºåÈÅøÂÖç‚ÄúÁ¨¶ÂêàË¶ÅÊ±Ç‚ÄùËøôÁßçÊ®°Á≥äË°®Ëø∞„ÄÇ  \n7. **ÊèêÂèñËÆæÂ§á**Ôºö\n\
            \   - ÊçïÊçâÊâÄÊúâÊèêÂèäÁöÑËÆæÂ§á‰∏éÂÖ≥ÈîÆËßÑÊ†º„ÄÇ  \n8. **ÊèêÂèñÂèÇÊï∞**Ôºö\n   - ÊâÄÊúâÂÆöÈáèÂÄº„ÄÅËåÉÂõ¥„ÄÅÁ≤æÂ∫¶Ë¶ÅÊ±ÇÁ≠âÔºå‰∏•Ê†ºÊ†áÊ≥®Á∫¶ÊùüÁ±ªÂûãÂíåÂçï‰Ωç„ÄÇ\n\
            \   - ‰∏≠ÊñáÊï∞Â≠óÈúÄËΩ¨ÈòøÊãâ‰ºØÊï∞Â≠ó„ÄÇ  \n9. **ÊèêÂèñÂºïÁî®**Ôºö\n   - Â∞ÜÂÜÖÈÉ®ÂíåÂ§ñÈÉ®ÂºïÁî®ÂçïÁã¨ËÆ∞ÂΩïÂú®`refs`Êï∞ÁªÑ‰∏≠„ÄÇ  \n10.\
            \ **ËæìÂá∫Ê†áÂáÜÂåñ**Ôºö\n    - Áº∫Â§±È°π‰øùÊåÅÂ≠óÊÆµ‰ΩÜÂ°´Á©∫ÂÄºÔºàÂ¶ÇÁ©∫Êï∞ÁªÑÊàñnullÔºâ„ÄÇ  \n    - ‰∏•Ê†ºJSONÁªìÊûÑÔºåÈîÆÂêçÂÖ®ÈÉ®Â∞èÂÜôËã±Êñá„ÄÇ\
            \ \n\n\n---\n\n# Ê≥®ÊÑè‰∫ãÈ°πÔºàNotesÔºâ\n\n- ËæìÂá∫ÂøÖÈ°ª‰∏•Ê†ºÁ¨¶ÂêàJSONÊ†ºÂºèÔºåÈîÆÂêçÁªü‰∏Ä‰∏∫Ëã±ÊñáÔºå**ÂÄºÂàô‰∏éËæìÂÖ•‰øùÊåÅ‰∏ÄËá¥**ÔºàËæìÂÖ•Ëã±ÊñáÔºåÂàô‰øùÁïôËã±ÊñáË°®ËææÔºâ\n\
            - ËæìÂÖ•Á´†ËäÇÂèØËÉΩÂåÖÂê´Â§ö‰∏™ÂÆûÈ™åÊñπÊ≥ïÔºåÊØè‰∏™ÊñπÊ≥ïÁã¨Á´ãÊàêÂØπË±°„ÄÇ\n- ÈúÄË¶ÅÁªºÂêàÂà§Êñ≠ËæìÂÖ•ÂÜÖÂÆπÊòØÂÆûÈ™åËøòÊòØË¶ÅÊ±ÇÔºå‰∏ÄËà¨Êù•ËØ¥Êó†Â≠©Â≠êÊó†Ê†áÈ¢òÁöÑ‰∫åÁ∫ß‰ª•‰∏ãÁ´†ËäÇÊòØË¶ÅÊ±ÇÁöÑÊ¶ÇÁéáËæÉÂ§ßÔºõÂèØ‰ª•ÂÖÅËÆ∏‰∏ª‰ΩìÊòØÂÆûÈ™å‰ΩÜÂÜÖÂÆπ‰∏çÂÆåÊï¥Ôºå‰∏çÂÖÅËÆ∏‰∏ª‰ΩìÊòØË¶ÅÊ±ÇÔºåÂà§Êñ≠‰∏ª‰ΩìÊòØË¶ÅÊ±ÇÊó∂ÈúÄÂÖ∑ÊúâËæÉÈ´òÁöÑÁΩÆ‰ø°Â∫¶ÔºåÂê¶Âàô‰øùÁïô‰∏∫ÂÆûÈ™å„ÄÇ\n\
            - Áº∫Â§±‰ø°ÊÅØÊó∂ËøîÂõûÁ©∫Êï∞ÁªÑ`[]`Êàñ`null`Ôºå‰∏çÁúÅÁï•Â≠óÊÆµ„ÄÇ\n- ‰∏çËß£ÈáäÊàñÂ±ïÂºÄÂºïÁî®ÁöÑÂ§ñÈÉ®Ê†áÂáÜÔºå‰ªÖÂéüÊñá‰øùÁïôÂú®`source_text`„ÄÇ\n\
            \n# Few-shotÁ§∫‰æã\n\n### ËæìÂÖ•\n```json\n[\n  {\n    \"chapter_id\": \"B.2.1.1\"\
            ,\n    \"chapter_title\": \"ÊªëÂè∞Ê≠£Èù¢Á¢∞ÊíûËØïÈ™å\",\n    \"raw_text\": \"\",\n   \
            \ \"children\": [\n      {\n        \"chapter_id\": \"B.2.1.1.1\",\n \
            \       \"chapter_title\": \"\",\n        \"raw_text\": \"Â∞ÜÁôΩËΩ¶Ë∫´ÊàñÂ∑•Ë£ÖÂõ∫ÂÆöÂú®Á¢∞ÊíûËØïÈ™åÊªëÂè∞‰∏ä,ÂÆâË£ÖÊñπÂêëÊ®°ÊãüÊ≠£Èù¢Á¢∞Êíû„ÄÇ\"\
            ,\n        \"children\": [],\n        \"full_path\": \"B.2 ËØïÈ™åÈ°πÁõÆ/B.2.1\
            \ Ê≠£Èù¢Á¢∞Êíû/B.2.1.1 ÊªëÂè∞Ê≠£Èù¢Á¢∞ÊíûËØïÈ™å/B.2.1.1.1 \"\n      },\n      {\n        \"chapter_id\"\
            : \"B.2.1.1.2\",\n        \"chapter_title\": \"\",\n        \"raw_text\"\
            : \"ÊªëÂè∞ÊåâÁÖß‰ª•‰∏ãÂä†ÈÄüÂ∫¶Ê≥¢ÂΩ¢‰πã‰∏ÄËøõË°åÁ¢∞ÊíûËØïÈ™å„ÄÇ a) ‰ΩøÁî®Âà∂ÈÄ†ÂïÜÊåáÂÆöÁöÑÂä†ÈÄüÂ∫¶Ê≥¢ÂΩ¢ËøõË°åËØïÈ™å,ÊåáÂÆöÁöÑÂä†ÈÄüÂ∫¶Ê≥¢ÂΩ¢Â∫î‰∏∫Âú®B.2.1.2‰∏≠ÊèèËø∞ÁöÑÂÆûËΩ¶Á¢∞ÊíûËØïÈ™åÊù°‰ª∂‰∏≠,ËΩ¶Ë∫´ÈùûÂèòÂΩ¢Âå∫ÂüüÈááÈõÜÁöÑÂä†ÈÄüÂ∫¶-Êó∂Èó¥Êõ≤Á∫ø,Âπ∂ÁªèËøáÊª§Ê≥¢Á≠âÁ∫ßCFC60\
            \ Êª§Ê≥¢Êàñ100Hz‰ΩéÈÄöÊª§Ê≥¢„ÄÇÂÆûÈôÖËØïÈ™åÁªìÊûúÊ≥¢ÂΩ¢ÁöÑÁßØÂàÜÈÄüÂ∫¶ÂèòÂåñÈáèŒîvs(t)Â∫îÂú®‰ªªÊÑèÊó∂Âàª,‰∏çË∂ÖËøáÊåáÂÆöÊ≥¢ÂΩ¢ÁöÑÁßØÂàÜÈÄüÂ∫¶ÂèòÂåñÈáè[Œîvt(t)¬±1]km/hÁöÑËåÉÂõ¥„ÄÇ\\\
            nb) ÊåâÂõæB.1 ÁöÑÊ†áÂáÜÂä†ÈÄüÂ∫¶ÈÄöÈÅìËåÉÂõ¥ÂíåË°®B.1 ÁöÑÂèÇÊï∞ËøõË°åÂä†ÈÄüÊàñÂáèÈÄü,ÂÖ∂ÈÄüÂ∫¶ÂèòÂåñÈáèŒîv ‰∏∫(25¬±1)km/h„ÄÇ\",\n      \
            \  \"children\": [],\n        \"full_path\": \"B.2 ËØïÈ™åÈ°πÁõÆ/B.2.1 Ê≠£Èù¢Á¢∞Êíû/B.2.1.1\
            \ ÊªëÂè∞Ê≠£Èù¢Á¢∞ÊíûËØïÈ™å/B.2.1.1.2 \"\n      }\n    ],\n    \"full_path\": \"B.2 ËØïÈ™åÈ°πÁõÆ/B.2.1\
            \ Ê≠£Èù¢Á¢∞Êíû/B.2.1.1 ÊªëÂè∞Ê≠£Èù¢Á¢∞ÊíûËØïÈ™å\"\n  }\n]\n```\n\n### ËæìÂá∫\n```json\n[\n  [\n  \
            \  {\n      \"chapter_id\": \"B.2.1.1\",\n      \"test_name\": \"ÊªëÂè∞Ê≠£Èù¢Á¢∞ÊíûËØïÈ™å(ÊñπÊ°àA)\"\
            ,\n      \"conditions\": [\n        \"Â∞ÜÁôΩËΩ¶Ë∫´ÊàñÂ∑•Ë£ÖÂõ∫ÂÆöÂú®Á¢∞ÊíûËØïÈ™åÊªëÂè∞‰∏äÔºåÂÆâË£ÖÊñπÂêëÊ®°ÊãüÊ≠£Èù¢Á¢∞Êíû„ÄÇ\"\
            ,\n        \"ËØïÈ™åÊªëÂè∞ÊåâÂà∂ÈÄ†ÂïÜÊåáÂÆöÁöÑÂä†ÈÄüÂ∫¶Ê≥¢ÂΩ¢ËøõË°åËØïÈ™å„ÄÇ\",\n        \"Âä†ÈÄüÂ∫¶Ê≥¢ÂΩ¢Êù•Ê∫êÔºöÂÆûËΩ¶Á¢∞ÊíûËØïÈ™åÊù°‰ª∂‰∏ãÔºåËΩ¶Ë∫´ÈùûÂèòÂΩ¢Âå∫ÂüüÈááÈõÜÁöÑÂä†ÈÄüÂ∫¶-Êó∂Èó¥Êõ≤Á∫ø„ÄÇ\"\
            \n      ],\n      \"criteria\": [\n        \"Ê£ÄÊü•Á¢∞ÊíûËß¶Âèë‰ø°Âè∑ÔºåÁ°Æ‰øù‰∏éMSD‰∏≠ËÆ∞ÂΩïÁöÑËß¶ÂèëÁ±ªÂûã‰∏ÄËá¥„ÄÇ\"\
            ,\n        \"ÁßØÂàÜÈÄüÂ∫¶ÂèòÂåñÈáèŒîvs(t)Âú®‰ªªÊÑèÊó∂Âàª‰∏çÂæóË∂ÖÂá∫ÊåáÂÆöÊ≥¢ÂΩ¢Œîvt(t)¬±1 km/h„ÄÇ\"\n      ],\n  \
            \    \"equipment\": [\n        {\n          \"name\": \"Á¢∞ÊíûËØïÈ™åÊªëÂè∞\",\n  \
            \        \"specification\": \"ÂèØÊ®°ÊãüÊ≠£Èù¢Á¢∞ÊíûÔºõÊîØÊåÅËá™ÂÆö‰πâÂä†ÈÄüÂ∫¶Ê≥¢ÂΩ¢ËæìÂÖ•\"\n        }\n    \
            \  ],\n      \"parameters\": [\n        {\n          \"item\": \"ÁßØÂàÜÈÄüÂ∫¶ÂèòÂåñÈáèŒîvs(t)\"\
            ,\n          \"constraint\": \"range_closed\",\n          \"value\": [\"\
            Œîvt(t)-1\", \"Œîvt(t)+1\"],\n          \"unit\": \"km/h\",\n          \"\
            source_text\": \"ÁßØÂàÜÈÄüÂ∫¶ÂèòÂåñÈáèŒîvs(t)Â∫îÂú®‰ªªÊÑèÊó∂Âàª,‰∏çË∂ÖËøáÊåáÂÆöÊ≥¢ÂΩ¢ÁöÑÁßØÂàÜÈÄüÂ∫¶ÂèòÂåñÈáè[Œîvt(t)¬±1]km/h\"\n\
            \        },\n        {\n          \"item\": \"Êª§Ê≥¢Á≠âÁ∫ß\",\n          \"constraint\"\
            : \"enum\",\n          \"value\": [\"CFC60\", \"100Hz‰ΩéÈÄö\"],\n        \
            \  \"unit\": null,\n          \"source_text\": \"Âπ∂ÁªèËøáÊª§Ê≥¢Á≠âÁ∫ßCFC60Êª§Ê≥¢Êàñ100Hz‰ΩéÈÄöÊª§Ê≥¢\"\
            \n        }\n      ],\n      \"refs\": [\n        {\n          \"ref_type\"\
            : \"internal\",\n          \"doc_id\": null,\n          \"target_id\"\
            : \"B.2.1.2\",\n          \"anchor_text\": \"ÂÆûËΩ¶Á¢∞ÊíûËØïÈ™åÊù°‰ª∂\"\n        }\n \
            \     ]\n    },\n    {\n      \"chapter_id\": \"B.2.1.1\",\n      \"test_name\"\
            : \"ÊªëÂè∞Ê≠£Èù¢Á¢∞ÊíûËØïÈ™å(ÊñπÊ°àB)\",\n      \"conditions\": [\n        \"Â∞ÜÁôΩËΩ¶Ë∫´ÊàñÂ∑•Ë£ÖÂõ∫ÂÆöÂú®Á¢∞ÊíûËØïÈ™åÊªëÂè∞‰∏äÔºåÂÆâË£ÖÊñπÂêëÊ®°ÊãüÊ≠£Èù¢Á¢∞Êíû„ÄÇ\"\
            ,\n        \"ÊåâÂõæB.1Ê†áÂáÜÂä†ÈÄüÂ∫¶ÈÄöÈÅìÂíåË°®B.1ÂèÇÊï∞ËøõË°åÊªëÂè∞Âä†ÈÄüÊàñÂáèÈÄü„ÄÇ\"\n      ],\n      \"criteria\"\
            : [\n        \"Ê£ÄÊü•Á¢∞ÊíûËß¶Âèë‰ø°Âè∑ÔºåÁ°Æ‰øù‰∏éMSD‰∏≠ËÆ∞ÂΩïÁöÑËß¶ÂèëÁ±ªÂûã‰∏ÄËá¥„ÄÇ\",\n        \"ÈÄüÂ∫¶ÂèòÂåñÈáèŒîvÂ∫î‰∏∫25¬±1\
            \ km/h„ÄÇ\"\n      ],\n      \"equipment\": [\n        {\n          \"name\"\
            : \"Á¢∞ÊíûËØïÈ™åÊªëÂè∞\",\n          \"specification\": \"Êª°Ë∂≥ÂõæB.1„ÄÅË°®B.1Âä†ÈÄüÂ∫¶Êõ≤Á∫øÂèÇÊï∞\"\n \
            \       }\n      ],\n      \"parameters\": [\n        {\n          \"\
            item\": \"ÈÄüÂ∫¶ÂèòÂåñÈáèŒîv\",\n          \"constraint\": \"range_closed\",\n  \
            \        \"value\": [\"24\", \"26\"],\n          \"unit\": \"km/h\",\n\
            \          \"source_text\": \"ÈÄüÂ∫¶ÂèòÂåñÈáèŒîv‰∏∫(25¬±1)km/h\"\n        },\n     \
            \ ],\n      \"refs\": [\n        {\n          \"ref_type\": \"internal\"\
            ,\n          \"doc_id\": null,\n          \"target_id\": \"ÂõæB.1\",\n \
            \         \"anchor_text\": \"Ê†áÂáÜÂä†ÈÄüÂ∫¶ÈÄöÈÅì\"\n        },\n        {\n      \
            \    \"ref_type\": \"internal\",\n          \"doc_id\": null,\n      \
            \    \"target_id\": \"Ë°®B.1\",\n          \"anchor_text\": \"Ê†áÂáÜÂä†ÈÄüÂ∫¶ÂèÇÊï∞\"\n\
            \        }\n      ]\n    }\n  ]\n]\n```"
        - id: 8ee6a7aa-9eb1-44fd-8cf5-1d32d1a65fea
          role: user
          text: '{{#1756613329065.item#}}'
        selected: false
        title: LLM 2
        type: llm
        variables: []
        vision:
          enabled: false
      height: 89
      id: '1756613344845'
      parentId: '1756613329065'
      position:
        x: 205.75426834716745
        y: 81.75426834716768
      positionAbsolute:
        x: 2928.2457316528325
        y: 865.3042817503975
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
      zIndex: 1002
    - data:
        code: "import json\nimport ast\nimport re\nfrom typing import Dict, List,\
          \ Any\n\ndef parse_input(src: str) -> List[Dict[str, Any]]:\n    \"\"\"\
          È¢ÑÂ§ÑÁêÜÂπ∂Ëß£ÊûêËæìÂÖ•Â≠óÁ¨¶‰∏≤‰∏∫ÂàóË°®Â≠óÂÖ∏ÁªìÊûÑ\"\"\"\n    try:\n        # 1. Êåâ </think> ÂàáÂàÜÂπ∂ÂèñÂêéÂçäÊÆµ\n  \
          \      src = src.rsplit('</think>', 1)[-1].strip()\n\n        # 2. ÂÖàÂπ≤Êéâ deepseek\
          \ V3 ÂèØËÉΩÊèíÂÖ•ÁöÑ‚ÄúÊûÅÈÄü‚ÄùÊàñÂçïÁã¨ÁöÑ‚ÄúÊûÅ‚Äù\n        src = re.sub(r'ÊûÅÈÄü\\s*', '', src)   # ÂéªÊéâ‚ÄúÊûÅÈÄü‚ÄùÂèäÂÖ∂ÂêéÂèØËÉΩÁöÑÂ§ö‰ΩôÁ©∫ÁôΩ\n\
          \        src = re.sub(r'(?<!\\w)ÊûÅ(?!\\w)', '', src)  # ÂéªÊéâÂ≠§Á´ãÂá∫Áé∞ÁöÑ‚ÄúÊûÅ‚Äù\n\n  \
          \      # 3. Â∞ùËØïÂ§öÁßçËß£ÊûêË∑ØÂæÑ\n        try:\n            data = json.loads(src)\n\
          \        except json.JSONDecodeError:\n            try:\n              \
          \  if src.startswith('\\ufeff'):\n                    src = src[1:]\n  \
          \              data = ast.literal_eval(src)\n            except (SyntaxError,\
          \ ValueError):\n                try:\n                    processed = src.replace('\\\
          \\\"', '\"').replace('\\\\n', '\\n')\n                    data = json.loads(processed)\n\
          \                except Exception as e:\n                    raise ValueError(f\"\
          Êó†Ê≥ïËß£ÊûêÔºö{str(e)}\")\n\n        # 4. ‰øùËØÅËøîÂõû list\n        if isinstance(data,\
          \ dict):\n            return [data]\n        elif isinstance(data, list):\n\
          \            return data\n        else:\n            return []\n    except\
          \ Exception as e:\n        raise ValueError(f\"È¢ÑÂ§ÑÁêÜÂ§±Ë¥•: {str(e)}\")\n\n\n\
          def main(arg1: str) -> dict:\n    data = parse_input(arg1)\n    return {\n\
          \        \"result\": json.dumps(data, ensure_ascii=False),\n    }"
        code_language: python3
        desc: ''
        error_strategy: fail-branch
        isInIteration: true
        isInLoop: false
        iteration_id: '1756613329065'
        outputs:
          result:
            children: null
            type: string
        selected: false
        title: ‰ª£Á†ÅÊâßË°å 6
        type: code
        variables:
        - value_selector:
          - '1756613344845'
          - text
          value_type: string
          variable: arg1
      height: 89
      id: '1756613456815'
      parentId: '1756613329065'
      position:
        x: 509.75426834716745
        y: 81.75426834716768
      positionAbsolute:
        x: 3232.2457316528325
        y: 865.3042817503975
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
      zIndex: 1002
    - data:
        code: "import os, json, traceback, subprocess, platform\n\ndef main(arg1:\
          \ list[str], arg2: str, file_name) -> dict:\n    \"\"\"\n    Â∞ÜËØïÈ™åÊï∞ÊçÆ‰∏éÁ´†ËäÇ‰ø°ÊÅØ‰∏Ä‰∏ÄÂØπÂ∫îÔºåÁîüÊàêÁõÆÊ†áÊ†ºÂºè\n\
          \    [\n        {\n            \"file\": \"regulation\",\n            \"\
          section\": \"ÈôÑÂΩïB\",\n            \"experiments\": [...]\n        },\n  \
          \      ...\n    ]\n    \"\"\"\n    # 1. ÂÖàÊää arg2 ÁöÑÂ≠óÁ¨¶‰∏≤ÂèçÂ∫èÂàóÂåñÊàêÂàóË°®\n    sec_list\
          \ = json.loads(arg2)\n\n    # 2. ‰æùÊ¨°Â§ÑÁêÜ arg1 ‰∏≠ÁöÑÊØè‰∏™ÂÖÉÁ¥†\n    result = []\n   \
          \ for sec, exp_str in zip(sec_list, arg1):\n        # ÊääÂΩìÂâçËØïÈ™åÂ≠óÁ¨¶‰∏≤ÂèçÂ∫èÂàóÂåñÊàêÂàóË°®\n\
          \        experiments = json.loads(exp_str)\n\n        # 3. ÊãºÊàêÁõÆÊ†áÂ≠óÂÖ∏\n    \
          \    result.append({\n            \"file\": sec[\"file\"],\n           \
          \ \"section\": sec[\"section\"],\n            \"experiments\": experiments\n\
          \        })\n        \n\n\n    # ÂéªÊéâÊâ©Â±ïÂêçÔºåÂÜçÊãº .json\n    base_name = os.path.splitext(file_name)[0]\
          \          # ÂæóÂà∞Êó†ÂêéÁºÄÁöÑÁ∫ØÊñá‰ª∂Âêç\n    path = f\"/tmp/mydata/{base_name}/experiment.json\"\
          \n\n    # 3. Ë∞ÉËØï‰ø°ÊÅØ\n    log = []\n    log.append(f\"[PWD] ÂΩìÂâçÂ∑•‰ΩúÁõÆÂΩï: {os.getcwd()}\"\
          )\n    log.append(f\"[PATH] ÁªùÂØπË∑ØÂæÑ: {os.path.abspath(path)}\")\n    log.append(f\"\
          [DIR] Áà∂ÁõÆÂΩïÊòØÂê¶Â≠òÂú®: {os.path.exists(os.path.dirname(path))}\")\n    log.append(f\"\
          [VOLUMES] ÊåÇËΩΩ‰ø°ÊÅØ: {subprocess.getoutput('mount | grep mydata')}\")\n\n   \
          \ # 4. ÂÜôÊñá‰ª∂\n    try:\n        os.makedirs(os.path.dirname(path), exist_ok=True)\n\
          \        with open(path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(result,\
          \ f, ensure_ascii=False, indent=2)\n        log.append(\"[SUCCESS] Êñá‰ª∂Â∑≤ÂÜôÂÖ•\"\
          )\n    except Exception as e:\n        log.append(f\"[ERROR] {type(e).__name__}:\
          \ {e}\")\n        log.append(traceback.format_exc())\n\n    # 5. ÂÜçÊ£ÄÊü•‰∏ÄÊ¨°\n\
          \    log.append(f\"[EXISTS] Êñá‰ª∂ÊòØÂê¶Â≠òÂú®: {os.path.exists(path)}\")\n    if os.path.exists(path):\n\
          \        log.append(f\"[SIZE] Êñá‰ª∂Â§ßÂ∞è: {os.path.getsize(path)} Â≠óËäÇ\")\n\n  \
          \  return {\"debug\": \"\\n\".join(log), \"result\": json.dumps(result,\
          \ ensure_ascii=False)}"
        code_language: python3
        desc: ''
        outputs:
          debug:
            children: null
            type: string
          result:
            children: null
            type: string
        selected: false
        title: ‰ª£Á†ÅÊâßË°å 7
        type: code
        variables:
        - value_selector:
          - '1756613329065'
          - output
          value_type: array[string]
          variable: arg1
        - value_selector:
          - '1756563307317'
          - experiment_infos
          value_type: string
          variable: arg2
        - value_selector:
          - '1756550268945'
          - file
          - name
          value_type: file
          variable: file_name
      height: 53
      id: '1756618044071'
      position:
        x: 4506.491463305665
        y: 783.5500134032299
      positionAbsolute:
        x: 4506.491463305665
        y: 783.5500134032299
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        code: "import json\nimport os\nfrom collections import defaultdict\n\ndef\
          \ build_chapter_index(tree_data):\n    \"\"\"ÊûÑÂª∫chapterÁ¥¢ÂºïÔºåkey‰∏∫(file, section,\
          \ chapter_id)Ôºåvalue‰∏∫chapterÂØπË±°ÁöÑÂºïÁî®\"\"\"\n    chapter_index = {}\n    section_index\
          \ = {}\n    \n    def add_chapter_recursive(chapter, file_name, section_name):\n\
          \        \"\"\"ÈÄíÂΩíÊ∑ªÂä†Á´†ËäÇÂèäÂÖ∂Â≠êÁ´†ËäÇÂà∞Á¥¢Âºï‰∏≠\"\"\"\n        if 'chapter_id' in chapter:\n\
          \            chapter_id = chapter['chapter_id']\n            chapter_key\
          \ = (file_name, section_name, chapter_id)\n            chapter_index[chapter_key]\
          \ = chapter\n        \n        # ÈÄíÂΩíÂ§ÑÁêÜÂ≠êÁ´†ËäÇ\n        if 'children' in chapter:\n\
          \            for child_chapter in chapter['children']:\n               \
          \ add_chapter_recursive(child_chapter, file_name, section_name)\n    \n\
          \    for file_data in tree_data:\n        file_name = file_data['file']\n\
          \        for section in file_data['sections']:\n            section_name\
          \ = section['section']\n            section_key = (file_name, section_name)\n\
          \            section_index[section_key] = section\n            \n      \
          \      # Â§ÑÁêÜÊØè‰∏™È°∂Á∫ßÁ´†ËäÇ\n            for chapter in section['chapters']:\n   \
          \             add_chapter_recursive(chapter, file_name, section_name)\n\
          \    \n    return chapter_index, section_index\n\ndef merge_final_tree_data(chapter_index,\
          \ final_tree_data):\n    \"\"\"Â∞Üfinal_treeÁöÑÊï∞ÊçÆËûçÂêàÂà∞tree‰∏≠\"\"\"\n    merged_count\
          \ = 0\n    not_found_count = 0\n    \n    for final_section in final_tree_data:\n\
          \        file_name = final_section['file']\n        section_name = final_section['section']\n\
          \        \n        for final_chapter in final_section['chapters']:\n   \
          \         if 'chapter_id' not in final_chapter:\n                continue\n\
          \                \n            chapter_id = final_chapter['chapter_id']\n\
          \            chapter_key = (file_name, section_name, chapter_id)\n     \
          \       \n            # O(1)Êü•Êâæ\n            tree_chapter = chapter_index.get(chapter_key)\n\
          \            \n            if tree_chapter:\n                # Áõ¥Êé•‰øÆÊîπÂºïÁî®ÁöÑÂØπË±°\n\
          \                tree_chapter['parameters'] = final_chapter.get('paramaters',\
          \ [])\n                tree_chapter['topic_keywords'] = final_chapter.get('topic_keywords',\
          \ [])\n                tree_chapter['context_keywords'] = final_chapter.get('context_keywords',\
          \ [])\n                tree_chapter['refs'] = final_chapter.get('refs',\
          \ [])\n                tree_chapter['table_headers'] = final_chapter.get('table_headers',\
          \ [])\n                merged_count += 1\n            else:\n          \
          \      not_found_count += 1\n    \n    return merged_count, not_found_count\n\
          \ndef merge_result_data(chapter_index, result_data):\n    \"\"\"Â∞ÜresultÁöÑexperimentsÊï∞ÊçÆËûçÂêàÂà∞tree‰∏≠\"\
          \"\"\n    merged_count = 0\n    not_found_count = 0\n    \n    for result_section\
          \ in result_data:\n        file_name = result_section['file']\n        section_name\
          \ = result_section['section']\n        experiments_groups = result_section.get('experiments',\
          \ [])\n        \n        # ÈÅçÂéÜÊØè‰∏™ÂÆûÈ™åÁªÑ\n        for experiments_group in experiments_groups:\n\
          \            if not experiments_group:\n                continue\n     \
          \           \n            # Ëé∑ÂèñÁ¨¨‰∏Ä‰∏™ÂÆûÈ™åÁöÑchapter_id‰Ωú‰∏∫ËØ•ÁªÑÁöÑÂÆö‰ΩçÊ†áËØÜ\n            first_experiment\
          \ = experiments_group[0]\n            chapter_id = first_experiment.get('chapter_id')\n\
          \            \n            if not chapter_id:\n                not_found_count\
          \ += 1\n                continue\n            \n            chapter_key\
          \ = (file_name, section_name, chapter_id)\n            # O(1)Êü•ÊâæÂØπÂ∫îÁöÑÁ´†ËäÇ\n \
          \           chapter = chapter_index.get(chapter_key)\n            \n   \
          \         if chapter:\n                # Â∞ÜËØ•ÁªÑÂÆûÈ™åÊ∑ªÂä†Âà∞ÂØπÂ∫îÁ´†ËäÇÁöÑexperimentsÂ≠óÊÆµ\n  \
          \              if 'experiments' not in chapter:\n                    chapter['experiments']\
          \ = []\n                chapter['experiments'].extend(experiments_group)\n\
          \                merged_count += 1\n            else:\n                not_found_count\
          \ += 1\n    \n    return merged_count, not_found_count\n\ndef main(tree:\
          \ str, final_tree: str, file_name, result: str) -> dict:\n    # Âä†ËΩΩJSONÊï∞ÊçÆ\n\
          \    tree_data = json.loads(tree)\n    final_tree_data = json.loads(final_tree)\n\
          \    result_data = json.loads(result)\n    # result_data = []\n    \n  \
          \  # ÊûÑÂª∫Á¥¢Âºï\n    chapter_index, section_index = build_chapter_index(tree_data)\n\
          \    \n    # ËûçÂêàÊï∞ÊçÆ\n    chapter_merged, chapter_not_found = merge_final_tree_data(chapter_index,\
          \ final_tree_data)\n    experiment_merged, experiment_not_found = merge_result_data(chapter_index,\
          \ result_data)\n    \n    # ÁªüËÆ°‰ø°ÊÅØ\n    total_chapters = len(chapter_index)\n\
          \    chapters_with_params = 0\n    chapters_with_keywords = 0\n    chapters_with_experiments\
          \ = 0\n    \n    for chapter in chapter_index.values():\n        if chapter.get('parameters'):\n\
          \            chapters_with_params += 1\n        if chapter.get('topic_keywords'):\n\
          \            chapters_with_keywords += 1\n        if chapter.get('experiments'):\n\
          \            chapters_with_experiments += 1\n    \n    # ÂêàÂπ∂ÂêéÁöÑÊï∞ÊçÆÂ∞±ÊòØ‰øÆÊîπÂêéÁöÑtree_data\n\
          \    merged_tree = tree_data\n\n    # ÂéªÊéâÊâ©Â±ïÂêçÔºåÂÜçÊãº .json\n    base_name = os.path.splitext(file_name)[0]\
          \          # ÂæóÂà∞Êó†ÂêéÁºÄÁöÑÁ∫ØÊñá‰ª∂Âêç\n    path = f\"/tmp/mydata/{base_name}/{base_name}.json\"\
          \n    os.makedirs(os.path.dirname(path), exist_ok=True)\n    with open(path,\
          \ \"w\", encoding=\"utf-8\") as f:\n        json.dump(merged_tree, f, ensure_ascii=False,\
          \ indent=2)\n    \n    return {\n        \"result\": f\"ËûçÂêàÂÆåÊàêÔºÅÁªüËÆ°‰ø°ÊÅØ: ÊÄªÁ´†ËäÇÊï∞:\
          \ {total_chapters}, ÂèÇÊï∞Á´†ËäÇ: {chapters_with_params}, ÂÖ≥ÈîÆËØçÁ´†ËäÇ: {chapters_with_keywords},\
          \ ËØïÈ™åÁ´†ËäÇ: {chapters_with_experiments}, ËûçÂêàÊàêÂäü: Á´†ËäÇÊï∞ÊçÆ{chapter_merged}‰∏™, ËØïÈ™åÊï∞ÊçÆ{experiment_merged}‰∏™\"\
          \n    }"
        code_language: python3
        desc: ''
        outputs:
          result:
            children: null
            type: string
        selected: false
        title: ‰ª£Á†ÅÊâßË°å 8
        type: code
        variables:
        - value_selector:
          - '1756550411122'
          - tree
          value_type: string
          variable: tree
        - value_selector:
          - '1756563307317'
          - final_tree
          value_type: string
          variable: final_tree
        - value_selector:
          - '1756550268945'
          - file
          - name
          value_type: file
          variable: file_name
        - value_selector:
          - '1756618044071'
          - result
          value_type: string
          variable: result
      height: 53
      id: '1756629493937'
      position:
        x: 4810.491463305665
        y: 783.5500134032299
      positionAbsolute:
        x: 4810.491463305665
        y: 783.5500134032299
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        context:
          enabled: false
          variable_selector: []
        desc: ''
        isInIteration: true
        isInLoop: false
        iteration_id: '1756557801310'
        model:
          completion_params: {}
          mode: chat
          name: deepseek-reasoner
          provider: langgenius/deepseek/deepseek
        prompt_template:
        - id: fb74cb72-3859-4cad-ac1e-cdbb9834f569
          role: system
          text: "# ËßíËâ≤ÔºàRoleÔºâ\n‰Ω†ÊòØ‰∏ÄÂêç‰∏•Ê†ºÁöÑ JSON ËæìÂá∫‰øÆÂ§çÂô®ÔºàJSON FixerÔºâ„ÄÇ  \n‰Ω†ÁöÑËÅåË¥£ÊòØÔºö\n- Êé•Êî∂‰∏Ä‰∏™ÂèØËÉΩÊ†ºÂºèÈîôËØØÁöÑ\
            \ JSON Â≠óÁ¨¶‰∏≤\n- ‰∏•Ê†º‰øÆÂ§çÂÖ∂‰∏≠ÁöÑËØ≠Ê≥ïÈóÆÈ¢òÊàñÁªìÊûÑÈóÆÈ¢ò„ÄÇ\n- ËæìÂá∫ÂÆåÂÖ®Á¨¶ÂêàÈ¢ÑÊúüÊ†ºÂºèÁöÑ JSON ÂØπË±°„ÄÇ\n\n---\n\n# ËæìÂÖ•ÔºàInputÔºâ\n\
            ‰Ω†Â∞ÜÊî∂Âà∞Ôºö\n**raw_output**ÔºöÊ®°ÂûãÂéüÂßãËæìÂá∫Ôºå‰Ω†ÈúÄË¶ÅÂøΩÁï•‰ªé‚Äù<think>‚ÄúÂà∞\"</think>\"ÁöÑÈÉ®ÂàÜÔºåÂè™‰øùÁïôÈîôËØØ JSON„ÄÇ\n\
            \nÁ§∫‰æãËæìÂÖ•Ôºö\nerror: Expecting property name enclosed in double quotes: line\
            \ 3 column 5 (char 25)\nraw_text: {file: \"doc1\", section: \"main\",\
            \ chapters: [ ... ]}\n\n---\n\n# ËæìÂá∫Ê†ºÂºèÔºàOutputÔºâ\nËØ∑ËæìÂá∫‰∏Ä‰∏™ **ÂÆåÊï¥ JSON ÂØπË±°**ÔºåÂøÖÈ°ª‰∏•Ê†ºÁ¨¶Âêà‰ª•‰∏ãÁªìÊûÑÔºà‰∏çË¶ÅËæìÂá∫Â§ö‰ΩôÁöÑËß£ÈáäÊÄßÊñáÂ≠óÔºâÔºö\
            \  \n```json\n{\n  \"file\": \"string\",\n  \"section\": \"string\",\n\
            \  \"chapters\":[\n    {\n      \"chapter_id\": \"string\",\n      \"\
            scope\":\"string\",\n      \"topic_keywords\": [\"string\", \"...\"],\n\
            \      \"context_keywords\": [\"string\", \"...\"]\n    }\n  ]\n}\n```\n\
            \n# Ê≥®ÊÑè‰∫ãÈ°π\n\n1. ‰Ω†ÈúÄË¶Å‰∏•Ê†ºÊåâÁÖßËæìÂá∫Ê†ºÂºèË¶ÅÊ±ÇËøõË°å‰øÆÂ§çÔºåÂèØÈÄöËøáËæìÂÖ•jsonËøõË°åÊé®Êñ≠ÔºåÁ°Æ‰øù JSON Â≠óÁ¨¶‰∏≤ÂèØË¢´ `json.loads()`\
            \ Ëß£ÊûêÈÄöËøáÂç≥ÂèØ\n2. ‰∏•Ê†ºËæìÂá∫‰∏Ä‰∏™ÂêàÊ≥ï JSON ÂØπË±°Ôºå‰∏çÂæóËæìÂá∫Â§ö‰ΩôÊñáÂ≠ó„ÄÅÊ≥®ÈáäÊàñ Markdown„ÄÇ\n3. Â¶ÇÊûúÊüêÂ≠óÊÆµÊó†ÂÜÖÂÆπÔºåËØ∑‰ΩøÁî®\
            \ `null` ÊàñÁ©∫Êï∞ÁªÑ `[]`„ÄÇ\n4. ÂøÖÈ°ªË°•ÂÖ®Áº∫Â§±Â≠óÊÆµÔºå‰∏çÂÖÅËÆ∏ÁúÅÁï•‰ªª‰ΩïÂ≠óÊÆµ„ÄÇ"
        - id: c68f56a1-5429-4a52-b480-a030d0c3f431
          role: user
          text: 'error_type:

            {{#1756557929661.error_type#}}


            error_message:

            {{#1756557929661.error_message#}}


            raw_text:

            {{#1756557804693.text#}}

            '
        selected: false
        title: LLM 3
        type: llm
        variables: []
        vision:
          enabled: false
      height: 89
      id: '1756698267184'
      parentId: '1756557801310'
      position:
        x: 812
        y: 154.5
      positionAbsolute:
        x: 1450
        y: 506.5
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
      zIndex: 1002
    - data:
        desc: ''
        isInIteration: true
        isInLoop: false
        iteration_id: '1756557801310'
        output_type: string
        selected: false
        title: ÂèòÈáèËÅöÂêàÂô®
        type: variable-aggregator
        variables:
        - - '1756557929661'
          - result
        - - '1756698848564'
          - result
      height: 129
      id: '1756698812908'
      parentId: '1756557801310'
      position:
        x: 1420
        y: 65
      positionAbsolute:
        x: 2058
        y: 417
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
      zIndex: 1002
    - data:
        code: "import json\nimport ast\nimport re\nfrom typing import Dict, List,\
          \ Any\n\ndef parse_input(src: str) -> List[Dict[str, Any]]:\n    \"\"\"\
          È¢ÑÂ§ÑÁêÜÂπ∂Ëß£ÊûêËæìÂÖ•Â≠óÁ¨¶‰∏≤‰∏∫ÂàóË°®Â≠óÂÖ∏ÁªìÊûÑ\"\"\"\n    try:\n        # 1. Êåâ </think> ÂàáÂàÜÂπ∂ÂèñÂêéÂçäÊÆµ\n  \
          \      src = src.rsplit('</think>', 1)[-1].strip()\n\n        # 2. ÂÖàÂπ≤Êéâ deepseek\
          \ V3 ÂèØËÉΩÊèíÂÖ•ÁöÑ‚ÄúÊûÅÈÄü‚ÄùÊàñÂçïÁã¨ÁöÑ‚ÄúÊûÅ‚Äù\n        src = re.sub(r'ÊûÅÈÄü\\s*', '', src)   # ÂéªÊéâ‚ÄúÊûÅÈÄü‚ÄùÂèäÂÖ∂ÂêéÂèØËÉΩÁöÑÂ§ö‰ΩôÁ©∫ÁôΩ\n\
          \        src = re.sub(r'(?<!\\w)ÊûÅ(?!\\w)', '', src)  # ÂéªÊéâÂ≠§Á´ãÂá∫Áé∞ÁöÑ‚ÄúÊûÅ‚Äù\n\n  \
          \      # 3. Â∞ùËØïÂ§öÁßçËß£ÊûêË∑ØÂæÑ\n        try:\n            data = json.loads(src)\n\
          \        except json.JSONDecodeError:\n            try:\n              \
          \  if src.startswith('\\ufeff'):\n                    src = src[1:]\n  \
          \              data = ast.literal_eval(src)\n            except (SyntaxError,\
          \ ValueError):\n                try:\n                    processed = src.replace('\\\
          \\\"', '\"').replace('\\\\n', '\\n')\n                    data = json.loads(processed)\n\
          \                except Exception as e:\n                    raise ValueError(f\"\
          Êó†Ê≥ïËß£ÊûêÔºö{str(e)}\")\n\n        # 4. ‰øùËØÅËøîÂõû list\n        if isinstance(data,\
          \ dict):\n            return [data]\n        elif isinstance(data, list):\n\
          \            return data\n        else:\n            return []\n    except\
          \ Exception as e:\n        raise ValueError(f\"È¢ÑÂ§ÑÁêÜÂ§±Ë¥•: {str(e)}\")\n\n\n\
          def main(arg1: str) -> dict:\n    data = parse_input(arg1)\n    return {\n\
          \        \"result\": json.dumps(data, ensure_ascii=False),\n    }"
        code_language: python3
        desc: ''
        isInIteration: true
        isInLoop: false
        iteration_id: '1756557801310'
        outputs:
          result:
            children: null
            type: string
        selected: false
        title: ‰ª£Á†ÅÊâßË°å 9
        type: code
        variables:
        - value_selector:
          - '1756698267184'
          - text
          value_type: string
          variable: arg1
      height: 53
      id: '1756698848564'
      parentId: '1756557801310'
      position:
        x: 1116
        y: 172.5
      positionAbsolute:
        x: 1754
        y: 524.5
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
      zIndex: 1002
    - data:
        context:
          enabled: false
          variable_selector: []
        desc: ''
        isInIteration: true
        isInLoop: false
        iteration_id: '1756613329065'
        model:
          completion_params: {}
          mode: chat
          name: deepseek-reasoner
          provider: langgenius/deepseek/deepseek
        prompt_template:
        - id: 3b1b8e69-21ca-4e8c-9eb0-e45279111fc6
          role: system
          text: "# ËßíËâ≤ÔºàRoleÔºâ\n‰Ω†ÊòØ‰∏ÄÂêç‰∏•Ê†ºÁöÑ JSON ËæìÂá∫‰øÆÂ§çÂô®ÔºàJSON FixerÔºâ„ÄÇ  \n‰Ω†ÁöÑËÅåË¥£ÊòØÔºö\n- Êé•Êî∂‰∏Ä‰∏™ÂèØËÉΩÊ†ºÂºèÈîôËØØÁöÑ\
            \ JSON Â≠óÁ¨¶‰∏≤\n- ‰∏•Ê†º‰øÆÂ§çÂÖ∂‰∏≠ÁöÑËØ≠Ê≥ïÈóÆÈ¢òÊàñÁªìÊûÑÈóÆÈ¢ò„ÄÇ\n- ËæìÂá∫ÂÆåÂÖ®Á¨¶ÂêàÈ¢ÑÊúüÊ†ºÂºèÁöÑ JSON ÂØπË±°„ÄÇ\n\n---\n\n# ËæìÂÖ•ÔºàInputÔºâ\n\
            ‰Ω†Â∞ÜÊî∂Âà∞Ôºö\n**raw_output**ÔºöÊ®°ÂûãÂéüÂßãËæìÂá∫Ôºå‰Ω†ÈúÄË¶ÅÂøΩÁï•‰ªé‚Äù<think>‚ÄúÂà∞\"</think>\"ÁöÑÈÉ®ÂàÜÔºåÂè™‰øùÁïôÈîôËØØ JSON„ÄÇ\n\
            \nÁ§∫‰æãËæìÂÖ•Ôºö\nerror: Expecting property name enclosed in double quotes: line\
            \ 3 column 5 (char 25)\nraw_text: {file: \"doc1\", section: \"main\",\
            \ chapters: [ ... ]}\n\n---\n\n# ËæìÂá∫Ê†ºÂºèÔºàOutputÔºâ\nËØ∑ËæìÂá∫‰∏Ä‰∏™ **ÂÆåÊï¥ JSON ÂØπË±°**ÔºåÂøÖÈ°ª‰∏•Ê†ºÁ¨¶Âêà‰ª•‰∏ãÁªìÊûÑÔºà‰∏çË¶ÅËæìÂá∫Â§ö‰ΩôÁöÑËß£ÈáäÊÄßÊñáÂ≠óÔºâÔºö\
            \  \n```json\n[\n  [\n    {\n      \"chapter_id\": \"string\",    // Êù•Ê∫êÁ´†ËäÇÁöÑchapter_idÔºå‰øùËØÅÂèØËøΩÊ∫ØÊÄß\n\
            \      \"test_name\": \"string\",     // ÊµãËØïÂêçÁß∞Ôºå‰ªéÁ´†ËäÇÊ†áÈ¢òÊàñË∑ØÂæÑÊèêÁÇºÔºåÁÆÄÊ¥Å‰∏îÂîØ‰∏Ä\n     \
            \ \"conditions\": [            // ÊµãËØïÊù°‰ª∂ÔºöËØï‰ª∂Áä∂ÊÄÅ„ÄÅÂÆâË£Ö„ÄÅÁéØÂ¢ÉÁ≠âÔºåÂÆåÊï¥ÂàÜÁÇπÂàóÂá∫\n        \"\
            string\"\n      ],\n      \"criteria\": [              // Âà§ÂÆöÊ†áÂáÜÔºöÊòéÁ°ÆÊ£ÄÊµãÈ°π+ËææÊ†áÊù°‰ª∂ÔºåÂàÜÁÇπÂàóÂá∫\n\
            \        \"string\"\n      ],\n      \"equipment\": [             // ÊâÄÈúÄËÆæÂ§áÂèäËßÑÊ†ºÔºå‰æø‰∫éËÆæÂ§áËÉΩÂäõÈ™åËØÅ\n\
            \        {\n          \"name\": \"string\",          // ËÆæÂ§áÂêçÁß∞\n       \
            \   \"specification\": \"string\"  // ËÆæÂ§áËßÑÊ†º/ÊÄßËÉΩ\n        }\n      ],\n \
            \     \"parameters\": [            // ÊèêÂèñÁöÑÂÖ≥ÈîÆÂèÇÊï∞\n        {\n          \"\
            item\": \"string\",          // ÂèÇÊï∞È°π\n          \"constraint\": \"string\"\
            ,    // Á∫¶ÊùüÔºö<=|<|=|>=|>|range_closed|range_open|enum|boolean\n        \
            \  \"value\": \"string|array|null\", // ÂèÇÊï∞ÂÄºÔºåÂèØ‰∏∫Êï∞Â≠ó„ÄÅËåÉÂõ¥ÊàñÊûö‰∏æ\n          \"unit\"\
            : \"string|null\",     // Âçï‰ΩçÔºåËã•Êó†Âàônull\n          \"source_text\": \"string\"\
            \    // ÂéüÊñáÂÆåÊï¥ÁâáÊÆµÔºå‰øùÁïôÁ¨¶Âè∑Ôºå‰æø‰∫éËøΩÊ∫Ø\n        }\n      ],\n      \"refs\": [     \
            \             // ÂºïÁî®‰ø°ÊÅØ\n        {\n          \"ref_type\": \"internal|external\"\
            , // ÂÜÖÈÉ®/Â§ñÈÉ®ÂºïÁî®\n          \"doc_id\": \"string|null\",         // Â§ñÈÉ®Ê†áÂáÜÁºñÂè∑ÔºõÂÜÖÈÉ®ÂºïÁî®Â°´null\n\
            \          \"target_id\": \"string\",           // ÂºïÁî®ÁõÆÊ†áÁºñÂè∑ÔºàÂ¶Ç‚ÄúB.2.1.1‚Äù„ÄÅ‚ÄúË°®B.1‚ÄùÔºâ\n\
            \          \"anchor_text\": \"string\"          // ÂºïÁî®ÁöÑÁÆÄË¶Å‰∏ä‰∏ãÊñáÊèèËø∞\n      \
            \  }\n      ]\n    }\n  ]\n]\n```\n\n# Ê≥®ÊÑè‰∫ãÈ°π\n\n1. ‰Ω†ÈúÄË¶Å‰∏•Ê†ºÊåâÁÖßËæìÂá∫Ê†ºÂºèË¶ÅÊ±ÇËøõË°å‰øÆÂ§çÔºåÂèØÈÄöËøáËæìÂÖ•jsonËøõË°åÊé®Êñ≠ÔºåÁ°Æ‰øù\
            \ JSON Â≠óÁ¨¶‰∏≤ÂèØË¢´ `json.loads()` Ëß£ÊûêÈÄöËøáÂç≥ÂèØ\n2. ‰∏•Ê†ºËæìÂá∫‰∏Ä‰∏™ÂêàÊ≥ï JSON ÂØπË±°Ôºå‰∏çÂæóËæìÂá∫Â§ö‰ΩôÊñáÂ≠ó„ÄÅÊ≥®ÈáäÊàñ Markdown„ÄÇ\n\
            3. Â¶ÇÊûúÊüêÂ≠óÊÆµÊó†ÂÜÖÂÆπÔºåËØ∑‰ΩøÁî® `null` ÊàñÁ©∫Êï∞ÁªÑ `[]`„ÄÇ\n4. ÂøÖÈ°ªË°•ÂÖ®Áº∫Â§±Â≠óÊÆµÔºå‰∏çÂÖÅËÆ∏ÁúÅÁï•‰ªª‰ΩïÂ≠óÊÆµ„ÄÇ"
        - id: e9d87d24-8cf0-41a1-b1d5-c976fc1c181e
          role: user
          text: 'error_type:

            {{#1756613456815.error_type#}}


            error_message:

            {{#1756613456815.error_message#}}


            raw_text:

            {{#1756613344845.text#}}

            '
        selected: false
        title: LLM 4
        type: llm
        variables: []
        vision:
          enabled: false
      height: 89
      id: '1757762170810'
      parentId: '1756613329065'
      position:
        x: 805.8389388549558
        y: 178.32128815214674
      positionAbsolute:
        x: 3528.330402160621
        y: 961.8713015553766
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
      zIndex: 1002
    - data:
        code: "import json\nimport ast\nimport re\nfrom typing import Dict, List,\
          \ Any\n\ndef parse_input(src: str) -> List[Dict[str, Any]]:\n    \"\"\"\
          È¢ÑÂ§ÑÁêÜÂπ∂Ëß£ÊûêËæìÂÖ•Â≠óÁ¨¶‰∏≤‰∏∫ÂàóË°®Â≠óÂÖ∏ÁªìÊûÑ\"\"\"\n    try:\n        # 1. Êåâ </think> ÂàáÂàÜÂπ∂ÂèñÂêéÂçäÊÆµ\n  \
          \      src = src.rsplit('</think>', 1)[-1].strip()\n\n        # 2. ÂÖàÂπ≤Êéâ deepseek\
          \ V3 ÂèØËÉΩÊèíÂÖ•ÁöÑ‚ÄúÊûÅÈÄü‚ÄùÊàñÂçïÁã¨ÁöÑ‚ÄúÊûÅ‚Äù\n        src = re.sub(r'ÊûÅÈÄü\\s*', '', src)   # ÂéªÊéâ‚ÄúÊûÅÈÄü‚ÄùÂèäÂÖ∂ÂêéÂèØËÉΩÁöÑÂ§ö‰ΩôÁ©∫ÁôΩ\n\
          \        src = re.sub(r'(?<!\\w)ÊûÅ(?!\\w)', '', src)  # ÂéªÊéâÂ≠§Á´ãÂá∫Áé∞ÁöÑ‚ÄúÊûÅ‚Äù\n\n  \
          \      # 3. Â∞ùËØïÂ§öÁßçËß£ÊûêË∑ØÂæÑ\n        try:\n            data = json.loads(src)\n\
          \        except json.JSONDecodeError:\n            try:\n              \
          \  if src.startswith('\\ufeff'):\n                    src = src[1:]\n  \
          \              data = ast.literal_eval(src)\n            except (SyntaxError,\
          \ ValueError):\n                try:\n                    processed = src.replace('\\\
          \\\"', '\"').replace('\\\\n', '\\n')\n                    data = json.loads(processed)\n\
          \                except Exception as e:\n                    raise ValueError(f\"\
          Êó†Ê≥ïËß£ÊûêÔºö{str(e)}\")\n\n        # 4. ‰øùËØÅËøîÂõû list\n        if isinstance(data,\
          \ dict):\n            return [data]\n        elif isinstance(data, list):\n\
          \            return data\n        else:\n            return []\n    except\
          \ Exception as e:\n        raise ValueError(f\"È¢ÑÂ§ÑÁêÜÂ§±Ë¥•: {str(e)}\")\n\n\n\
          def main(arg1: str) -> dict:\n    data = parse_input(arg1)\n    return {\n\
          \        \"result\": json.dumps(data, ensure_ascii=False),\n    }"
        code_language: python3
        desc: ''
        isInIteration: true
        isInLoop: false
        iteration_id: '1756613329065'
        outputs:
          result:
            children: null
            type: string
        selected: false
        title: ‰ª£Á†ÅÊâßË°å 10
        type: code
        variables:
        - value_selector:
          - '1757762170810'
          - text
          value_type: string
          variable: arg1
      height: 53
      id: '1757762176397'
      parentId: '1756613329065'
      position:
        x: 1114.5881365502833
        y: 180.4906291677239
      positionAbsolute:
        x: 3837.0795998559483
        y: 964.0406425709538
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
      zIndex: 1002
    - data:
        desc: ''
        isInIteration: true
        isInLoop: false
        iteration_id: '1756613329065'
        output_type: string
        selected: false
        title: ÂèòÈáèËÅöÂêàÂô® 2
        type: variable-aggregator
        variables:
        - - '1757762176397'
          - result
        - - '1756613456815'
          - result
      height: 129
      id: '1757762184699'
      parentId: '1756613329065'
      position:
        x: 1374.2622913938985
        y: 82.33412502691726
      positionAbsolute:
        x: 4096.753754699564
        y: 865.8841384301471
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
      zIndex: 1002
    - data:
        author: Dale
        desc: ''
        height: 88
        selected: false
        showAuthor: true
        text: '{"root":{"children":[{"children":[{"detail":0,"format":0,"mode":"normal","style":"","text":"ÊèêÂèñÂÖ≥ÈîÆËØçÁ≠âÂ≠óÊÆµ","type":"text","version":1}],"direction":"ltr","format":"","indent":0,"type":"paragraph","version":1,"textFormat":0,"textStyle":""}],"direction":"ltr","format":"","indent":0,"type":"root","version":1}}'
        theme: blue
        title: ''
        type: ''
        width: 240
      height: 88
      id: '1757850678552'
      position:
        x: 1299.8917635483372
        y: 203.92833256073843
      positionAbsolute:
        x: 1299.8917635483372
        y: 203.92833256073843
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom-note
      width: 240
    - data:
        author: Dale
        desc: ''
        height: 88
        selected: false
        showAuthor: true
        text: '{"root":{"children":[{"children":[{"detail":0,"format":0,"mode":"normal","style":"","text":"È¢ÑÂ§ÑÁêÜpdf‰∏∫jsonÊ†ºÂºè","type":"text","version":1}],"direction":"ltr","format":"","indent":0,"type":"paragraph","version":1,"textFormat":0,"textStyle":""}],"direction":"ltr","format":"","indent":0,"type":"root","version":1}}'
        theme: blue
        title: ''
        type: ''
        width: 240
      height: 88
      id: '1757850708484'
      position:
        x: 334.15313855200105
        y: 224.57661618703446
      positionAbsolute:
        x: 334.15313855200105
        y: 224.57661618703446
      sourcePosition: right
      targetPosition: left
      type: custom-note
      width: 240
    - data:
        author: Dale
        desc: ''
        height: 88
        selected: false
        showAuthor: true
        text: '{"root":{"children":[{"children":[{"detail":0,"format":0,"mode":"normal","style":"","text":"ÊèêÂèñÂÆûÈ™å","type":"text","version":1}],"direction":"ltr","format":"","indent":0,"type":"paragraph","version":1,"textFormat":0,"textStyle":""}],"direction":"ltr","format":"","indent":0,"type":"root","version":1}}'
        theme: blue
        title: ''
        type: ''
        width: 240
      height: 88
      id: '1757850722503'
      position:
        x: 3469.8642784687045
        y: 643.462102402726
      positionAbsolute:
        x: 3469.8642784687045
        y: 643.462102402726
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom-note
      width: 240
    - data:
        code: "import json\nimport os\nfrom collections import defaultdict\n\ndef\
          \ build_chapter_index(tree_data):\n    \"\"\"ÊûÑÂª∫chapterÁ¥¢ÂºïÔºåkey‰∏∫(file, section,\
          \ chapter_id)Ôºåvalue‰∏∫chapterÂØπË±°ÁöÑÂºïÁî®\"\"\"\n    chapter_index = {}\n    section_index\
          \ = {}\n    \n    def add_chapter_recursive(chapter, file_name, section_name):\n\
          \        \"\"\"ÈÄíÂΩíÊ∑ªÂä†Á´†ËäÇÂèäÂÖ∂Â≠êÁ´†ËäÇÂà∞Á¥¢Âºï‰∏≠\"\"\"\n        if 'chapter_id' in chapter:\n\
          \            chapter_id = chapter['chapter_id']\n            chapter_key\
          \ = (file_name, section_name, chapter_id)\n            chapter_index[chapter_key]\
          \ = chapter\n        \n        # ÈÄíÂΩíÂ§ÑÁêÜÂ≠êÁ´†ËäÇ\n        if 'children' in chapter:\n\
          \            for child_chapter in chapter['children']:\n               \
          \ add_chapter_recursive(child_chapter, file_name, section_name)\n    \n\
          \    for file_data in tree_data:\n        file_name = file_data['file']\n\
          \        for section in file_data['sections']:\n            section_name\
          \ = section['section']\n            section_key = (file_name, section_name)\n\
          \            section_index[section_key] = section\n            \n      \
          \      # Â§ÑÁêÜÊØè‰∏™È°∂Á∫ßÁ´†ËäÇ\n            for chapter in section['chapters']:\n   \
          \             add_chapter_recursive(chapter, file_name, section_name)\n\
          \    \n    return chapter_index, section_index\n\ndef merge_final_tree_data(chapter_index,\
          \ final_tree_data):\n    \"\"\"Â∞Üfinal_treeÁöÑÊï∞ÊçÆËûçÂêàÂà∞tree‰∏≠\"\"\"\n    merged_count\
          \ = 0\n    not_found_count = 0\n    \n    for final_section in final_tree_data:\n\
          \        file_name = final_section['file']\n        section_name = final_section['section']\n\
          \        \n        for final_chapter in final_section['chapters']:\n   \
          \         if 'chapter_id' not in final_chapter:\n                continue\n\
          \                \n            chapter_id = final_chapter['chapter_id']\n\
          \            chapter_key = (file_name, section_name, chapter_id)\n     \
          \       \n            # O(1)Êü•Êâæ\n            tree_chapter = chapter_index.get(chapter_key)\n\
          \            \n            if tree_chapter:\n                # Áõ¥Êé•‰øÆÊîπÂºïÁî®ÁöÑÂØπË±°\n\
          \                tree_chapter['scope'] = final_chapter.get('scope', \"\"\
          )\n                tree_chapter['topic_keywords'] = final_chapter.get('topic_keywords',\
          \ [])\n                tree_chapter['context_keywords'] = final_chapter.get('context_keywords',\
          \ [])\n                tree_chapter['parameters'] = final_chapter.get('parameters',\
          \ [])\n                tree_chapter['refs'] = final_chapter.get('refs',\
          \ [])\n                tree_chapter['table_headers'] = final_chapter.get('table_headers',\
          \ [])\n                merged_count += 1\n            else:\n          \
          \      not_found_count += 1\n    \n    return merged_count, not_found_count\n\
          \ndef merge_result_data(chapter_index, result_data):\n    \"\"\"Â∞ÜresultÁöÑexperimentsÊï∞ÊçÆËûçÂêàÂà∞tree‰∏≠\"\
          \"\"\n    merged_count = 0\n    not_found_count = 0\n    \n    for result_section\
          \ in result_data:\n        file_name = result_section['file']\n        section_name\
          \ = result_section['section']\n        experiments_groups = result_section.get('experiments',\
          \ [])\n        \n        # ÈÅçÂéÜÊØè‰∏™ÂÆûÈ™åÁªÑ\n        for experiments_group in experiments_groups:\n\
          \            if not experiments_group:\n                continue\n     \
          \           \n            # Ëé∑ÂèñÁ¨¨‰∏Ä‰∏™ÂÆûÈ™åÁöÑchapter_id‰Ωú‰∏∫ËØ•ÁªÑÁöÑÂÆö‰ΩçÊ†áËØÜ\n            first_experiment\
          \ = experiments_group[0]\n            chapter_id = first_experiment.get('chapter_id')\n\
          \            \n            if not chapter_id:\n                not_found_count\
          \ += 1\n                continue\n            \n            chapter_key\
          \ = (file_name, section_name, chapter_id)\n            # O(1)Êü•ÊâæÂØπÂ∫îÁöÑÁ´†ËäÇ\n \
          \           chapter = chapter_index.get(chapter_key)\n            \n   \
          \         if chapter:\n                # Â∞ÜËØ•ÁªÑÂÆûÈ™åÊ∑ªÂä†Âà∞ÂØπÂ∫îÁ´†ËäÇÁöÑexperimentsÂ≠óÊÆµ\n  \
          \              if 'experiments' not in chapter:\n                    chapter['experiments']\
          \ = []\n                chapter['experiments'].extend(experiments_group)\n\
          \                merged_count += 1\n            else:\n                not_found_count\
          \ += 1\n    \n    return merged_count, not_found_count\n\ndef main(tree:\
          \ str, final_tree: str, file_name) -> dict:\n    # Âä†ËΩΩJSONÊï∞ÊçÆ\n    tree_data\
          \ = json.loads(tree)\n    final_tree_data = json.loads(final_tree)\n   \
          \ # result_data = json.loads(result)\n    result_data = []\n    \n    #\
          \ ÊûÑÂª∫Á¥¢Âºï\n    chapter_index, section_index = build_chapter_index(tree_data)\n\
          \    \n    # ËûçÂêàÊï∞ÊçÆ\n    chapter_merged, chapter_not_found = merge_final_tree_data(chapter_index,\
          \ final_tree_data)\n    experiment_merged, experiment_not_found = merge_result_data(chapter_index,\
          \ result_data)\n    \n    # ÁªüËÆ°‰ø°ÊÅØ\n    total_chapters = len(chapter_index)\n\
          \    chapters_with_params = 0\n    chapters_with_keywords = 0\n    chapters_with_experiments\
          \ = 0\n    \n    for chapter in chapter_index.values():\n        if chapter.get('parameters'):\n\
          \            chapters_with_params += 1\n        if chapter.get('topic_keywords'):\n\
          \            chapters_with_keywords += 1\n        if chapter.get('experiments'):\n\
          \            chapters_with_experiments += 1\n    \n    # ÂêàÂπ∂ÂêéÁöÑÊï∞ÊçÆÂ∞±ÊòØ‰øÆÊîπÂêéÁöÑtree_data\n\
          \    merged_tree = tree_data\n\n    # ÂéªÊéâÊâ©Â±ïÂêçÔºåÂÜçÊãº .json\n    base_name = os.path.splitext(file_name)[0]\
          \          # ÂæóÂà∞Êó†ÂêéÁºÄÁöÑÁ∫ØÊñá‰ª∂Âêç\n    path = f\"/tmp/mydata/{base_name}/{base_name}.json\"\
          \n    os.makedirs(os.path.dirname(path), exist_ok=True)\n    with open(path,\
          \ \"w\", encoding=\"utf-8\") as f:\n        json.dump(merged_tree, f, ensure_ascii=False,\
          \ indent=2)\n    \n    return {\n        \"result\": f\"ËûçÂêàÂÆåÊàêÔºÅÁªüËÆ°‰ø°ÊÅØ: ÊÄªÁ´†ËäÇÊï∞:\
          \ {total_chapters}, ÂèÇÊï∞Á´†ËäÇ: {chapters_with_params}, ÂÖ≥ÈîÆËØçÁ´†ËäÇ: {chapters_with_keywords},\
          \ ËØïÈ™åÁ´†ËäÇ: {chapters_with_experiments}, ËûçÂêàÊàêÂäü: Á´†ËäÇÊï∞ÊçÆ{chapter_merged}‰∏™, ËØïÈ™åÊï∞ÊçÆ{experiment_merged}‰∏™\"\
          \n    }"
        code_language: python3
        desc: ''
        outputs:
          result:
            children: null
            type: string
        selected: false
        title: ‰ª£Á†ÅÊâßË°å 8 (1)
        type: code
        variables:
        - value_selector:
          - '1756550411122'
          - tree
          value_type: string
          variable: tree
        - value_selector:
          - '1756563307317'
          - final_tree
          value_type: string
          variable: final_tree
        - value_selector:
          - '1756550268945'
          - file
          - name
          value_type: file
          variable: file_name
      height: 53
      id: '17580960600210'
      position:
        x: 5410.491463305665
        y: 352
      positionAbsolute:
        x: 5410.491463305665
        y: 352
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        desc: ''
        error_handle_mode: continue-on-error
        height: 304
        is_parallel: true
        iterator_input_type: array[string]
        iterator_selector:
        - '1756550411122'
        - array
        output_selector:
        - '1758105758637'
        - output
        output_type: array[string]
        parallel_nums: 10
        selected: false
        start_node_id: 1758096258752start
        title: Ëø≠‰ª£ 1B
        type: iteration
        width: 1724
      height: 304
      id: '17580962587520'
      position:
        x: 638
        y: 731.7304907470502
      positionAbsolute:
        x: 638
        y: 731.7304907470502
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 1724
      zIndex: 1
    - data:
        context:
          enabled: false
          variable_selector: []
        desc: ''
        isInIteration: true
        isInLoop: false
        iteration_id: '17580962587520'
        model:
          completion_params: {}
          mode: chat
          name: deepseek-reasoner
          provider: langgenius/deepseek/deepseek
        prompt_template:
        - id: b8df94d2-1037-41d9-8649-be6e2790a7d5
          role: system
          text: "# ËßíËâ≤\n\n‰Ω†ÊòØ‰∏ÄÂêçÊäÄÊúØÊ†áÂáÜÁü•ËØÜÂ∑•Á®ã‰∏ìÂÆ∂Ôºå‰∏ìÊ≥®‰∫éÂ∞ÜÊ±ΩËΩ¶ÂèäÁõ∏ÂÖ≥È¢ÜÂüüÁöÑÊ†áÂáÜÂíåÊ≥ïËßÑÊñáÊ°£ËΩ¨Âåñ‰∏∫ÂèØÁªìÊûÑÂåñËß£ÊûêÁöÑÊï∞ÊçÆËµÑ‰∫ßÔºåÁî®‰∫éÁü•ËØÜÂõæË∞±Âª∫Á´ã„ÄÇ\n\
            \n‰Ω†ÁöÑ‰ªªÂä°ÊòØÂü∫‰∫éËæìÂÖ•ÁöÑÊ†áÂáÜÊñáÊ°£ JSON Ê†ëÁªìÊûÑÔºåÁ≤æÂáÜÊäΩÂèñÁõ∏ÂÖ≥‰ø°ÊÅØ„ÄÇ \n\n---\n\n# ËæìÂÖ•ÂÜÖÂÆπ\n\n‰Ω†Â∞ÜÊî∂Âà∞‰∏Ä‰∏™ JSON Êï∞ÁªÑÔºåÂåÖÂê´‰∏Ä‰∏™ÂØπË±°ÔºåÂÖ∂ÁªìÊûÑÂ¶Ç‰∏ãÔºö\n\
            \n- `file`: Êñá‰ª∂Ê†áËØÜÔºàÂ¶Ç \"regulation\"„ÄÅ\"ANNEX 1\"Ôºâ\n- `sections`: ÂùóÊï∞ÁªÑÔºåÊØè‰∏™ÂÖÉÁ¥†ÂåÖÂê´Ôºö\n\
            \  - `section`: Ê†áËØÜÂΩìÂâçÂùóÔºàÂ¶Ç \"MAIN\", \"APPENDIX 1\"Ôºâ\n  - `chapters`: Á´†ËäÇÊï∞ÁªÑÔºåÊØè‰∏™ÂÖÉÁ¥†ÂåÖÂê´Ôºö\n\
            \    - `chapter_id`: Á´†ËäÇÁºñÂè∑Ôºà‰øùÊåÅÂéüÊ†∑Ôºâ\n    - `chapter_title`: Á´†ËäÇÊ†áÈ¢ò\n    - `raw_text`:\
            \ ËØ•ËäÇÁöÑÁ∫ØÊñáÊú¨ÂÜÖÂÆπ\n    - `children`: Â≠êÊù°Ê¨æÊï∞ÁªÑÔºàÁªìÊûÑ‰∏éÁà∂Á∫ßÁõ∏ÂêåÔºâ\n    - `full_path`: ÂÆåÊï¥Ë∑ØÂæÑÔºàÂèØÈÄâÔºâ\n\
            \n---\n\n# ËæìÂá∫Ê†ºÂºè\n\nËæìÂá∫ÂøÖÈ°ªÊòØÁ∫ØÂáÄ„ÄÅÂÆåÊï¥ÁöÑJSONÂØπË±°ÔºåÁ°Æ‰øùÂèØ‰ª•Ë¢´`json.loads()`Áõ¥Êé•Ëß£ÊûêÔºå‰∏îÂøÖÈ°ªÂÆåÂÖ®Á¨¶Âêà‰ª•‰∏ãÁªìÊûÑÔºö\n\
            \n```json\n{\n  \"file\": \"string\",\n  \"section\": \"string\",\n  \"\
            chapters\":[\n    {\n      \"chapter_id\": \"string\",\n      \"parameters\"\
            : [\n        {\n          \"item\": \"string\",\n          \"constraint\"\
            : \"<|<=|>|>=|=|¬±|range\",\n          \"value\": \"string|null\",\n  \
            \        \"relative_to\": \"string|null\",\n          \"unit\": \"string|null\"\
            ,\n        }\n      ],\n      \"refs\": [\n        {\n          \"ref_type\"\
            : \"table|graph|clause|external\",\n          \"doc_id\": \"string|null\"\
            ,\n          \"target_id\": \"string\",\n          \"anchor_text\": \"\
            string\"\n        }\n      ],\n      \"table_headers\": [\"string\", \"\
            ...\"]\n    }\n  ],\n  \"experiment_root_ids\": [\"string\", \"...\"],\n\
            }\n\n```\n\n# Â§ÑÁêÜÈÄªËæëÔºàÈìæÂºèÊÄùËÄÉÔºâ\n\n1. **ÈÅçÂéÜÁ´†ËäÇÊ†ë**\n\n   ÈÅçÂéÜÊØè‰∏™ `chapter_id` ÂèäÂÖ∂Â≠êÁ´†ËäÇÔºåÂàÜÂà´Â§ÑÁêÜÔºåÁ°Æ‰øùËæìÂá∫‰∏≠**ÊØè‰∏™Á´†ËäÇ**ÈÉΩÁã¨Á´ãÊàêÊù°„ÄÇÊúÄÂêéÊâßË°åÊ≠•È™§5\n\
            \n2. **ÂèÇÊï∞ÊèêÂèñ (`parameters`)**\n\n   - ÊèêÂèñÊ†áÂáÜ‰∏≠ÁöÑÂÆöÈáèÊåáÊ†áÊàñÁ∫¶ÊùüÊù°‰ª∂ÔºåÊØèÊù°ÂèÇÊï∞Áã¨Á´ãÊàêÂØπË±°Ôºõ**Âè™Ë¶ÅÊ≤°ÊúâÂ≠©Â≠ê‰∏îÊ≤°ÊúâÊ†áÈ¢òÁöÑÁ´†ËäÇ‰∏≠ÔºåÂá∫Áé∞ÂèÇÊï∞(Êï∞Â≠óÂΩ¢Âºè)ÔºåÂàôÂøÖÊèêÂèñÔºåË°®Ê†ºÂÜÖÂèÇÊï∞Èô§Â§ñ**\n\
            \   - `item`Ôºö‰øùÊåÅÂéüÊñáÊèèËø∞ÔºåÊäΩÂèñÁ∫¶ÊùüÁöÑÂÖ≥ÈîÆÂØπË±°ÔºåÊØîÂ¶ÇÔºö‚Äúcarbon monoxide‚Äù\n   - `constraint`ÔºöÊîØÊåÅ\
            \ `<`, `<=`, `>`, `>=`, `=`,`¬±`Á≠âÂ∏∏ËßÅÊï∞Â≠¶Ë°®ËææÔºå`range`Ë°®Á§∫Âå∫Èó¥„ÄÇ\n   - `value`Ôºö‰øùÊåÅÊï∞Â≠óÊàñËåÉÂõ¥Ôºà`[1.0,1.5]`Á≠âÔºâÔºå‰∏çÂ∏¶Âçï‰ΩçÔºåÂÄçÊï∞ÂíåÁôæÂàÜÊØîÁ≠âÁªü‰∏ÄÁî®Â∞èÊï∞ÔºàÊØîÂ¶ÇÂ∞è‰∫é1.1ÂÄçÂíå‰∏çË∂ÖËøáÁôæÂàÜ‰πãÂçÅÔºåÂú®Ê≠§Â§ÑÂùáÁî®1.1Ôºâ\n\
            \   - `relative_to`ÔºöÂü∫ÂáÜÂÄºÔºåÊØîÂ¶Ç1.5¬±0.1‰∏≠ÁöÑ1.5ÔºåÂèØ‰∏∫null\n   - `unit`ÔºöÁªü‰∏ÄËã±ÊñáÂçï‰Ωç\n\n\
            3. **ÂºïÁî®ÊèêÂèñ (`refs`)**\n\n   - ÊèêÂèñÂÜÖÈÉ®ÂºïÁî®ÔºàÊú¨Ê†áÂáÜÂÜÖÁ´†ËäÇ/Ë°®Ê†º/ÂõæÁâáÔºâ‰∏éÂ§ñÈÉ®ÂºïÁî®ÔºàÂ§ñÈÉ®Ê†áÂáÜÁºñÂè∑ÔºâÔºåÂπ∂Âå∫ÂàÜ `ref_type`„ÄÇ\n\
            \   - `doc_id` Â°´ÂÜôÊ†áÂáÜÁºñÂè∑ÔºàÂ¶ÇÊúâÔºâÔºåÂÜÖÈÉ®ÂºïÁî®Â°´ `null`„ÄÇ\n   - `target_id` ‰øùÁïôÂºïÁî®ÁõÆÊ†áÁºñÂè∑ÔºàÂ¶Ç‚ÄúB.2.1.1‚Äù„ÄÅ‚ÄúË°®B.1‚ÄùÔºâ„ÄÇ\n\
            \   - `anchor_text` Âú®‰∏ä‰∏ãÊñáÊèêÂèñÁÆÄË¶ÅÊñáÊú¨ÔºåËØ¥ÊòéÂºïÁî®ÁöÑÂÜÖÂÆπÔºåÁ°Æ‰øùÂèØÁ≤æÁ°ÆÂÆö‰Ωç„ÄÇ\n   - ÂéüÊñáÁõ∏Âêå‰ΩçÁΩÆÂá∫Áé∞ÁöÑÂºïÁî®ÔºåÂèØÂè™ÁîüÊàê‰∏ÄÊù°ÂÜÖÂÆπÔºåÂêåÊ†∑‰øùÊåÅÂπ∂ÂàóÂç≥ÂèØ„ÄÇ‰æãÂ¶ÇÔºöÊåâA„ÄÅBËøõË°åÂÆûÈ™åÔºåÊèêÂèñÊó∂ÂèØÂ∞ÜA„ÄÅBÂπ∂ÂàóÔºåËÄå‰∏çÁî®ÁîüÊàê‰∏§Êù°ÂÜÖÂÆπ„ÄÇ\n\
            \n4. **Ë°®Ê†ºË°®Â§¥ (`table_headers`)**\n\n   - Â¶ÇÊûúÁ´†ËäÇÂåÖÂê´Ë°®Ê†ºÂºïÁî®Ôºå‰ªÖÊèêÂèñË°®Â§¥Â≠óÊÆµ„ÄÇ\n\n5. **ÂÆûÈ™åÁ´†ËäÇËØÜÂà´\
            \ (`experiment_root_ids`)**\n\n   - ÂêéÁª≠‰ºöÊ†πÊçÆÊ≠§ÁªìÊûúÔºå‰ªéÂØπÂ∫îÁöÑÁ´†ËäÇÂÜÖÂÆπ‰∏≠ÊèêÂèñÂÆûÈ™åÔºåÊïÖÊ†πËäÇÁÇπÂèØ‰ª•ÂÖÅËÆ∏Âêë‰∏äÂ±ÇÂ¶•ÂçèÔºå‰∏çÂèØËøáÂ∫¶ÁªÜÂåñ\n\
            \     \n   - Âà§Êñ≠Á´†ËäÇÂÜÖÂÆπÊòØÂê¶‰∏∫ÂÆûÈ™åÁ´†ËäÇÔºåÊ†áËÆ∞ÂÖ∂ `chapter_id` ‰∏∫ÂÆûÈ™åÊ†πËäÇÁÇπ„ÄÇÊúâ‰ª•‰∏ã‰∏âÁßçÊÉÖÂÜµÔºö\n   \n   \
            \  - Ê†áÈ¢ò‰∏≠Áõ¥Êé•ÂåÖÂê´‰∫ÜÂÖ≥ÈîÆÂ≠ó‰æãÂ¶ÇÂÆûÈ™å„ÄÅtestÁ≠âÔºå‰ΩÜÈúÄÊ≥®ÊÑèÂå∫ÂàÜÂÆûÈ™åÂíåÂÆûÈ™åÁöÑÈÉ®ÂàÜÔºåÊØîÂ¶Ç‚Äú6.1 ÂÆûÈ™åÊù°‰ª∂‚Äù„ÄÅ‚Äú6.2 ÂÆûÈ™åÊñπÊ≥ï‚Äù„ÄÅ‚Äú6 Ëá™Ê£ÄËØïÈ™åÊñπÊ≥ï‚ÄùÔºåÂàô‚Äú6‚ÄùÂ∫îËØ•‰Ωú‰∏∫ÂÆûÈ™åÊ†πËäÇÁÇπ\n\
            \     - Ê≠£Êñá‰∏≠ÂåÖÂê´‰∫Ü‚ÄúÊåâXXÂÆûÈ™å‚ÄùÁ≠âÂÜÖÂÆπ\n   \n       - Ê†πÊçÆÊé®Êñ≠ÔºåËØ•Á´†ËäÇÂèäÂÖ∂Â≠êÁ´†ËäÇÂùáÂõ¥ÁªïÊüê‰∏™ÂÆûÈ™åÂ±ïÂºÄ\n   \n\
            \   \n      - Ëã•‰∏Ä‰∏™Á´†ËäÇÊòØÂÆûÈ™åÁ´†ËäÇÔºåÂàôÊâÄÊúâÂ≠êÁ´†ËäÇÈÉΩÁÆóÂÆûÈ™åÂÜÖÂÆπÔºåÊó†ÈúÄÂçïÁã¨ÈáçÂ§çÊ†áÊ≥®„ÄÇ‰ΩÜËã•ÊòØÂ≠êÁ´†ËäÇÂåÖÂê´‰∫Ü‰∏çÂêåÁöÑÂÆûÈ™åÔºåÂàôÈúÄË¶ÅÂØπÊØè‰∏™ÂÆûÈ™åËøõË°åÊ†áËÆ∞ÔºåËÄå‰∏çÊòØÂΩìÂâçÁ´†ËäÇ\n\
            \   \n   - Ëã•ÁªºÂêàÂà§ÂÆö‰∏Ä‰∏™section‰ªÖÂõ¥Áªï‰∏Ä‰∏™ÂÆûÈ™åÂ±ïÂºÄÔºà‰æãÂ¶ÇÔºöÈôÑÂΩïD Ëá™Ê£ÄÂÆûÈ™åÊñπÊ≥ïÔºâÔºåÂàô‰ª•ÁâπÊÆäÊ†áËÆ∞ALL‰Ωú‰∏∫ÁªìÊûúÔºà‰æãÂ¶ÇÔºöexperiment_root_idsÔºö[\"\
            ALL\"]ÔºåÁ¶ÅÊ≠¢‰ªÖ‰øùÁïô‚ÄúD‚ÄùÂØºËá¥‰ª£Á†ÅÊó†Ê≥ïÂåπÈÖçÔºâ\n   \n\n------\n\n# Ê≥®ÊÑè‰∫ãÈ°π\n\n- ÂØπ‰∫éÊ≤°ÊúâÂ≠©Â≠êÔºåÊ≤°ÊúâÊ†áÈ¢òÁöÑÁ´†ËäÇÔºå**Âè™Ë¶ÅÂá∫Áé∞ÂèÇÊï∞(Êï∞Â≠óÁ≠âÂΩ¢ÂºèÔºåÊØîÂ¶Ç1.1ÂÄçÔºå10\
            \ percentÁ≠â)/Ë°®Ê†º/ÂºïÁî®ÔºåÂàôÂøÖÊèêÂèñÔºåË°®Ê†ºÂÜÖÂèÇÊï∞Èô§Â§ñ**\n- Êó†Ê≥ïÊèêÂèñÁöÑÂ≠óÊÆµÂèØ‰∏çÂÅöËæìÂá∫ÔºõÂΩì‰∏î‰ªÖÂΩìÊâÄÊúâÂ≠óÊÆµÂùáÊó†Ê≥ïÊèêÂèñÊó∂ÔºåÂøΩÁï•ËØ•Á´†ËäÇ„ÄÇ\n\
            - ÊâÄÊúâÂºïÁî®ÂøÖÈ°ªËÉΩÂú®ÂéüÊñá‰∏≠Á≤æÂáÜÂÆö‰ΩçÔºå‰∏îÂøÖÈ°ªÊòØÂÆûÈôÖÂºïÁî®ÊâçËÉΩÊèêÂèñrefsÂ≠óÊÆµÔºåÂç≥‰∏ä‰∏ãÊñáÊúâÁ±ª‰ºº‰∫é‚ÄúÂèÇËßÅXX‚ÄùÊàñËÄÖ‚ÄúÊåâXXÂÆûÈ™å‚ÄùÁ≠âË°®Ëø∞ÔºåËã•ÊòØÂ≠§Á´ã„ÄÅÁ™ÅÂÖÄÂá∫Áé∞ÂàôÂèØ‰ª•ÁêÜËß£‰∏∫È°µÁúâË¢´ÈîôËØØËß£ÊûêÔºåÊàñËÄÖ‚ÄúËßÑËåÉÊÄßÂºïÁî®Êñá‰ª∂‚ÄùÁ´†ËäÇ‰∏≠ÂØπÂºïÁî®Êñá‰ª∂ÁöÑÁΩóÂàó\n\
            \n\n# Few-shot Á§∫‰æã\n\n## ËæìÂÖ•Á§∫‰æã1\n\n```\n[\n  {\n    \"file\": \"regulation\"\
            ,\n    \"sections\": [\n      {\n        \"section\": \"ÈôÑÂΩïB\",\n     \
            \   \"context\": \"(ËßÑËåÉÊÄß)Ëá™Âä®Ëß¶ÂèëËØïÈ™åÊñπÊ≥ï\",\n        \"chapters\": [\n       \
            \   {\n          {\n            \"chapter_id\": \"B.2\",\n           \
            \ \"chapter_title\": \"ËØïÈ™åÈ°πÁõÆ\",\n            \"raw_text\": \"\",\n    \
            \        \"children\": [\n              {\n                \"chapter_id\"\
            : \"B.2.1\",\n                \"chapter_title\": \"Ê≠£Èù¢Á¢∞Êíû\",\n         \
            \       \"raw_text\": \"\",\n                \"children\": [\n       \
            \           {\n                    \"chapter_id\": \"B.2.1.1\",\n    \
            \                \"chapter_title\": \"ÊªëÂè∞Ê≠£Èù¢Á¢∞ÊíûËØïÈ™å\",\n                  \
            \  \"raw_text\": \"\",\n                    \"children\": [\n        \
            \              {\n                        \"chapter_id\": \"B.2.1.1.1\"\
            ,\n                        \"chapter_title\": \"\",\n                \
            \        \"raw_text\": \"Â∞ÜÁôΩËΩ¶Ë∫´ÊàñÂ∑•Ë£ÖÂõ∫ÂÆöÂú®Á¢∞ÊíûËØïÈ™åÊªëÂè∞‰∏ä,ÂÆâË£ÖÊñπÂêëÊ®°ÊãüÊ≠£Èù¢Á¢∞Êíû„ÄÇ \",\n         \
            \               \"children\": [],\n                        \"full_path\"\
            : \"B.2 ËØïÈ™åÈ°πÁõÆ/B.2.1 Ê≠£Èù¢Á¢∞Êíû/B.2.1.1 ÊªëÂè∞Ê≠£Èù¢Á¢∞ÊíûËØïÈ™å/B.2.1.1.1 \"\n              \
            \        },\n                      {\n                        \"chapter_id\"\
            : \"B.2.1.1.2\",\n                        \"chapter_title\": \"\",\n \
            \                       \"raw_text\": \"ÊªëÂè∞ÊåâÁÖß‰ª•‰∏ãÂä†ÈÄüÂ∫¶Ê≥¢ÂΩ¢‰πã‰∏ÄËøõË°åÁ¢∞ÊíûËØïÈ™å„ÄÇ a) ‰ΩøÁî®Âà∂ÈÄ†ÂïÜÊåáÂÆöÁöÑÂä†ÈÄüÂ∫¶Ê≥¢ÂΩ¢ËøõË°åËØïÈ™å,ÊåáÂÆöÁöÑÂä†ÈÄüÂ∫¶Ê≥¢ÂΩ¢Â∫î‰∏∫Âú®B.2.1.2‰∏≠ÊèèËø∞ÁöÑÂÆûËΩ¶Á¢∞ÊíûËØïÈ™åÊù°‰ª∂‰∏≠,ËΩ¶Ë∫´ÈùûÂèòÂΩ¢Âå∫ÂüüÈááÈõÜÁöÑÂä†ÈÄüÂ∫¶-Êó∂Èó¥Êõ≤Á∫ø,Âπ∂ÁªèËøáÊª§Ê≥¢Á≠âÁ∫ßCFC60\
            \ Êª§Ê≥¢Êàñ100Hz‰ΩéÈÄöÊª§Ê≥¢„ÄÇÂÆûÈôÖËØïÈ™åÁªìÊûúÊ≥¢ÂΩ¢ÁöÑÁßØÂàÜÈÄüÂ∫¶ÂèòÂåñÈáèŒîvs( t)Â∫îÂú®‰ªªÊÑèÊó∂Âàª,‰∏çË∂ÖËøáÊåáÂÆöÊ≥¢ÂΩ¢ÁöÑÁßØÂàÜÈÄüÂ∫¶ÂèòÂåñÈáè[Œîvt( t)¬±1]km/hÁöÑËåÉÂõ¥„ÄÇ\\\
            nb) ÊåâÂõæB.1 ÁöÑÊ†áÂáÜÂä†ÈÄüÂ∫¶ÈÄöÈÅìËåÉÂõ¥ÂíåË°®B.1 ÁöÑÂèÇÊï∞ËøõË°åÂä†ÈÄüÊàñÂáèÈÄü,ÂÖ∂ÈÄüÂ∫¶ÂèòÂåñÈáèŒîv ‰∏∫\\n(25¬±1)km/h„ÄÇ\\nGB45672‚Äî2025ÂõæB.1\
            \ Ê≠£Èù¢Á¢∞ÊíûËá™Âä®Ëß¶ÂèëÂä†ÈÄüÂ∫¶ÈÄöÈÅìË°®B.1 Ê≠£Èù¢Á¢∞ÊíûËá™Âä®Ëß¶ÂèëÂä†ÈÄüÂ∫¶ÂèÇÊï∞\\nÁÇπ\\nÊó∂Èó¥t\\nms\\nÂä†ÈÄüÂ∫¶‰∏ãÈôê(√óg) ÁÇπ Êó∂Èó¥tms\\\
            nÂä†ÈÄüÂ∫¶‰∏äÈôê(√óg) A 15 0 E 0 3 B 45 10 F 40 17 C 60 10 G 63 17 D 85 0 H 105 0\"\
            ,\n                        \"children\": [],\n                       \
            \ \"full_path\": \"B.2 ËØïÈ™åÈ°πÁõÆ/B.2.1 Ê≠£Èù¢Á¢∞Êíû/B.2.1.1 ÊªëÂè∞Ê≠£Èù¢Á¢∞ÊíûËØïÈ™å/B.2.1.1.2 \"\n\
            \                      }\n                    ],\n                   \
            \ \"full_path\": \"B.2 ËØïÈ™åÈ°πÁõÆ/B.2.1 Ê≠£Èù¢Á¢∞Êíû/B.2.1.1 ÊªëÂè∞Ê≠£Èù¢Á¢∞ÊíûËØïÈ™å\"\n          \
            \        },\n                ],\n              },\n            ],\n  \
            \        }\n        ]\n      }\n    ]\n  }\n]\n```\n\n## ËæìÂá∫Á§∫‰æã1\n\n```\n\
            \  {\n      \"file\": \"regulation\",\n      \"section\": \"ÈôÑÂΩïB\",\n \
            \     \"experiment_root_ids\": [\"B.2.1.1\"]\n      \"chapters\":[\n \
            \    {\n      \"chapter_id\": \"B.2.1.1.2\",\n      \"paramaters\": [\n\
            \        {\n          \"item\": \"ÈÄüÂ∫¶ÂèòÂåñÈáèŒîv\",\n          \"constraint\"\
            : \"=\",\n          \"value\": \"25\",\n          \"unit\": \"km/h\",\n\
            \          \"source_text\": \"ÂÖ∂ÈÄüÂ∫¶ÂèòÂåñÈáèŒîv‰∏∫(25¬±1)km/h\"\n        }\n     \
            \ ],\n      \"refs\": [\n        {\n          \"ref_type\": \"clause\"\
            ,\n          \"doc_id\": null,\n          \"target_id\": \"B.2.1.2\",\n\
            \          \"anchor_text\": \"ÂÆûËΩ¶Á¢∞ÊíûËØïÈ™åÊù°‰ª∂-Âä†ÈÄüÂ∫¶Ê≥¢ÂΩ¢\"\n        },\n        {\n\
            \          \"ref_type\": \"graph\",\n          \"doc_id\": null,\n   \
            \       \"target_id\": \"ÂõæB.1\",\n          \"anchor_text\": \"\"\n  \
            \      },\n        {\n          \"ref_type\": \"table\",\n          \"\
            doc_id\": null,\n          \"target_id\": \"Ë°®B.1\",\n          \"anchor_text\"\
            : \"\"\n        },\n      ],\n      \"table_headers\": [\"ÁÇπ\", \"Êó∂Èó¥t(ms)\"\
            , \"Âä†ÈÄüÂ∫¶‰∏ãÈôê(√óg)\", \"Âä†ÈÄüÂ∫¶‰∏äÈôê(√óg)\"]\n    }\n      ]\n  }\n\n```\n\n## ËæìÂÖ•Á§∫‰æã2\n\
            \n```\n[\n  {\n    \"file\": \"regulation\",\n    \"sections\": [\n  \
            \    {\n        \"section\": \"MAIN\",\n        \"context\": \"\",\n \
            \       \"chapters\": [\n          {\n            \"chapter_id\": \"7-\"\
            ,\n            \"chapter_title\": \"CRITERIA OF TECHNICAL CONFORMITY\"\
            ,\n            \"raw_text\": \"\",\n            \"children\": [\n    \
            \         {\n                \"chapter_id\": \"7.2\",\n              \
            \  \"chapter_title\": \"Vehicle subjected to type test\",\n          \
            \      \"raw_text\": \"\",\n                \"children\": [\n        \
            \          {\n                    \"chapter_id\": \"7.2.1\",\n       \
            \             \"chapter_title\": \"\",\n                    \"raw_text\"\
            : \"The vehicle shall be considered complying with the requirement specified\
            \ in item \\n4.1of this standard, if the measured mass of carbon monoxide\
            \ and the combined mass of hydrocarbon and oxides of nitrogen, are less\
            \ than or  equal to 0.70 of theallowahle limits mentioned in Tahle (1\
            \ ).\",\n                    \"children\": [],\n                    \"\
            full_path\": \"7- CRITERIA OF TECHNICAL CONFORMITY/7.2 Vehicle subjected\
            \ to type test/7.2.1 \"\n                  },\n                  {\n \
            \                   \"chapter_id\": \"7.2.2\",\n                    \"\
            chapter_title\": \"\",\n                    \"raw_text\": \"The test shall\
            \  be repeated  if in the initial test, the measured masses of both the\
            \ carbon monoxide and the combined value of hydrocarbons and oxides of\
            \ nitrogenare less than or equal to 0.85 of their allowable limits and\
            \ one of these values isgreater than 0.70 of its allowable limit.\",\n\
            \                    \"children\": [],\n                    \"full_path\"\
            : \"7- CRITERIA OF TECHNICAL CONFORMITY/7.2 Vehicle subjected to type\
            \ test/7.2.2 \"\n                  },\n                  {\n         \
            \           \"chapter_id\": \"7.2.6\",\n                    \"chapter_title\"\
            : \"\",\n                    \"raw_text\": \"The vehicle shall be considered\
            \ non-complying with the requirement mentioned in item 4.1  if more than\
            \ one of the measured masses of both the carbon monoxide and the combined\
            \ value of hydrocarbons and oxides of nitrogen in the three tests exceed\
            \ 1.1 times the allowable limit for the pollutant.\",\n              \
            \      \"children\": [],\n                    \"full_path\": \"7- CRITERIA\
            \ OF TECHNICAL CONFORMITY/7.2 Vehicle subjected to type test/7.2.6 \"\
            ,\n                  }\n                ],\n                \"full_path\"\
            : \"7- CRITERIA OF TECHNICAL CONFORMITY/7.2 Vehicle subjected to type\
            \ test\"\n              },\n            ],\n          },\n        ],\n\
            \      },\n    ],\n  }\n]\n```\n\n## ËæìÂá∫Á§∫‰æã2\n\n```\n{\n\"file\": \"regulation\"\
            ,\n\"section\": \"MAIN\",\n\"experiment_root_ids\": [\n\"7.2\"\n],\n\"\
            chapters\": [\n{\n\"chapter_id\": \"7.2.1\",\n\"parameters\": [\n{\n\"\
            item\": \"measured mass of carbon monoxide\",\n\"constraint\": \"<=\"\
            ,\n\"value\": \"0.70\",\n\"relative_to\": \"allowable limit\",\n\"unit\"\
            : \"times\"\n},\n{\n\"item\": \"combined mass of hydrocarbons and oxides\
            \ of nitrogen\",\n\"constraint\": \"<=\",\n\"value\": \"0.70\",\n\"relative_to\"\
            : \"allowable limit\",\n\"unit\": \"times\"\n}\n],\n\"refs\": [\n{\n\"\
            ref_type\": \"clause\",\n\"doc_id\": null,\n\"target_id\": \"4.1\",\n\"\
            anchor_text\": \"item 4.1 of this standard\"\n}\n],\n},\n{\n\"chapter_id\"\
            : \"7.2.2\",\n\"parameters\": [\n{\n\"item\": \"measured mass of carbon\
            \ monoxide\",\n\"constraint\": \"<=\",\n\"value\": \"0.85\",\n\"relative_to\"\
            : \"allowable limit\",\n\"unit\": \"times\"\n},\n{\n\"item\": \"combined\
            \ mass of hydrocarbons and oxides of nitrogen\",\n\"constraint\": \"<=\"\
            ,\n\"value\": \"0.85\",\n\"relative_to\": \"allowable limit\",\n\"unit\"\
            : \"times\"\n},\n{\n\"item\": \"one of the measured masses (carbon monoxide\
            \ or combined hydrocarbons+NOx)\",\n\"constraint\": \">\",\n\"value\"\
            : \"0.70\",\n\"relative_to\": \"allowable limit\",\n\"unit\": \"times\"\
            \n}\n],\n},\n{\n\"chapter_id\": \"7.2.6\",\n\"parameters\": [\n{\n\"item\"\
            : \"measured mass of carbon monoxide (per test)\",\n\"constraint\": \"\
            >\",\n\"value\": \"1.1\",\n\"relative_to\": \"allowable limit\",\n\"unit\"\
            : \"times\"\n},\n{\n\"item\": \"combined value of hydrocarbons and oxides\
            \ of nitrogen (per test)\",\n\"constraint\": \">\",\n\"value\": \"1.1\"\
            ,\n\"relative_to\": \"allowable limit\",\n\"unit\": \"times\"\n}\n],\n\
            }\n}\n]\n}\n```"
        - id: 2e89206c-ded1-462d-9db7-2832e587c8ad
          role: user
          text: '{{#17580962587520.item#}}'
        selected: false
        title: LLM 1B
        type: llm
        variables: []
        vision:
          enabled: false
      height: 89
      id: '1758096258752017580962587520'
      parentId: '17580962587520'
      position:
        x: 204
        y: 78.24573165283243
      positionAbsolute:
        x: 842
        y: 809.9762223998827
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
      zIndex: 1002
    - data:
        code: "import json\nimport ast\nimport re\nfrom typing import Dict, List,\
          \ Any\n\ndef parse_input(src: str) -> List[Dict[str, Any]]:\n    \"\"\"\
          È¢ÑÂ§ÑÁêÜÂπ∂Ëß£ÊûêËæìÂÖ•Â≠óÁ¨¶‰∏≤‰∏∫ÂàóË°®Â≠óÂÖ∏ÁªìÊûÑ\"\"\"\n    try:\n        # 1. Êåâ </think> ÂàáÂàÜÂπ∂ÂèñÂêéÂçäÊÆµ\n  \
          \      src = src.rsplit('</think>', 1)[-1].strip()\n\n        # 2. ÂÖàÂπ≤Êéâ deepseek\
          \ V3 ÂèØËÉΩÊèíÂÖ•ÁöÑ‚ÄúÊûÅÈÄü‚ÄùÊàñÂçïÁã¨ÁöÑ‚ÄúÊûÅ‚Äù\n        src = re.sub(r'ÊûÅÈÄü\\s*', '', src)   # ÂéªÊéâ‚ÄúÊûÅÈÄü‚ÄùÂèäÂÖ∂ÂêéÂèØËÉΩÁöÑÂ§ö‰ΩôÁ©∫ÁôΩ\n\
          \        src = re.sub(r'(?<!\\w)ÊûÅ(?!\\w)', '', src)  # ÂéªÊéâÂ≠§Á´ãÂá∫Áé∞ÁöÑ‚ÄúÊûÅ‚Äù\n\n  \
          \      # 3. Â∞ùËØïÂ§öÁßçËß£ÊûêË∑ØÂæÑ\n        try:\n            data = json.loads(src)\n\
          \        except json.JSONDecodeError:\n            try:\n              \
          \  if src.startswith('\\ufeff'):\n                    src = src[1:]\n  \
          \              data = ast.literal_eval(src)\n            except (SyntaxError,\
          \ ValueError):\n                try:\n                    processed = src.replace('\\\
          \\\"', '\"').replace('\\\\n', '\\n')\n                    data = json.loads(processed)\n\
          \                except Exception as e:\n                    raise ValueError(f\"\
          Êó†Ê≥ïËß£ÊûêÔºö{str(e)}\")\n\n        # 4. ‰øùËØÅËøîÂõû list\n        if isinstance(data,\
          \ dict):\n            return [data]\n        elif isinstance(data, list):\n\
          \            return data\n        else:\n            return []\n    except\
          \ Exception as e:\n        raise ValueError(f\"È¢ÑÂ§ÑÁêÜÂ§±Ë¥•: {str(e)}\")\n\n\n\
          def main(arg1: str) -> dict:\n    data = parse_input(arg1)\n    return {\n\
          \        \"resultB\": json.dumps(data, ensure_ascii=False),\n    }"
        code_language: python3
        desc: ''
        error_strategy: fail-branch
        isInIteration: true
        isInLoop: false
        iteration_id: '17580962587520'
        outputs:
          resultB:
            children: null
            type: string
        selected: false
        title: ‰ª£Á†ÅÊâßË°å 12
        type: code
        variables:
        - value_selector:
          - '1758096258752017580962587520'
          - text
          value_type: string
          variable: arg1
      height: 89
      id: '1758096258752017580962587521'
      parentId: '17580962587520'
      position:
        x: 508
        y: 78.24573165283243
      positionAbsolute:
        x: 1146
        y: 809.9762223998827
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
      zIndex: 1002
    - data:
        desc: ''
        isInIteration: true
        selected: false
        title: ''
        type: iteration-start
      draggable: false
      height: 48
      id: 1758096258752start
      parentId: '17580962587520'
      position:
        x: 24
        y: 68
      positionAbsolute:
        x: 662
        y: 799.7304907470502
      selectable: false
      sourcePosition: right
      targetPosition: left
      type: custom-iteration-start
      width: 44
      zIndex: 1002
    - data:
        code: "import json\nfrom typing import List, Dict, Any\n\ndef merge_item(base:\
          \ Dict[str, Any], extra: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"\
          Â∞Ü extra ÂêàÂπ∂Âà∞ base\"\"\"\n    merged = dict(base)\n\n    # ÂêàÂπ∂ experiment_root_ids\n\
          \    if \"experiment_root_ids\" in extra:\n        merged_ids = merged.get(\"\
          experiment_root_ids\", [])\n        merged[\"experiment_root_ids\"] = list(set(merged_ids\
          \ + extra[\"experiment_root_ids\"]))\n\n    # ÂêàÂπ∂ chapters\n    base_chapters\
          \ = {c[\"chapter_id\"]: c for c in merged.get(\"chapters\", [])}\n    for\
          \ c in extra.get(\"chapters\", []):\n        cid = c[\"chapter_id\"]\n \
          \       if cid in base_chapters:\n            chapter = base_chapters[cid]\n\
          \            # ÂêàÂπ∂ÂàóË°®Â≠óÊÆµ\n            for k in [\"parameters\", \"refs\", \"\
          table_headers\"]:\n                if k in c:\n                    chapter.setdefault(k,\
          \ [])\n                    # Â¶ÇÊûúÊòØ list of dict\n                    if c[k]\
          \ and isinstance(c[k][0], dict):\n                        existing_keys\
          \ = [json.dumps(x, sort_keys=True) for x in chapter[k]]\n              \
          \          for x in c[k]:\n                            sx = json.dumps(x,\
          \ sort_keys=True)\n                            if sx not in existing_keys:\n\
          \                                chapter[k].append(x)\n                \
          \    else:\n                        for x in c[k]:\n                   \
          \         if x not in chapter[k]:\n                                chapter[k].append(x)\n\
          \        else:\n            base_chapters[cid] = c\n    merged[\"chapters\"\
          ] = list(base_chapters.values())\n    return merged\n\ndef main(arg1: list[str],\
          \ arg2: list[str], global_scope: str) -> dict:\n    if len(arg1) != len(arg2):\n\
          \        raise ValueError(\"arg1 ‰∏é arg2 ÈïøÂ∫¶ÂøÖÈ°ª‰∏ÄËá¥\")\n\n    result = []\n \
          \   for s1, s2 in zip(arg1, arg2):\n        try:\n            data1 = json.loads(s1)\n\
          \        except:\n            data1 = []\n        try:\n            data2\
          \ = json.loads(s2)\n        except:\n            data2 = []\n\n        #\
          \ Âª∫Á´ã file+section ÁöÑÊò†Â∞Ñ\n        base_map = {(d[\"file\"], d[\"section\"]):\
          \ d for d in data1}\n        for d in data2:\n            key = (d[\"file\"\
          ], d[\"section\"])\n            if key in base_map:\n                base_map[key]\
          \ = merge_item(base_map[key], d)\n            else:\n                base_map[key]\
          \ = d\n\n        # result.append(json.dumps(list(base_map.values()), ensure_ascii=False))\n\
          \n\n        merged_list = list(base_map.values())\n        for sec in merged_list:\n\
          \            for ch in sec.get(\"chapters\", []):\n                if \"\
          scope\" in ch:\n                    ch[\"scope\"] = '[' + global_scope +\
          \ ']-' + ch[\"scope\"]\n\n        result.append(json.dumps(merged_list,\
          \ ensure_ascii=False))\n\n    return {\"result\": result}\n"
        code_language: python3
        desc: ''
        outputs:
          result:
            children: null
            type: array[string]
        selected: false
        title: ‰ª£Á†ÅÊâßË°å 14
        type: code
        variables:
        - value_selector:
          - '1756557801310'
          - output
          value_type: array[string]
          variable: arg1
        - value_selector:
          - '17580962587520'
          - output
          value_type: array[string]
          variable: arg2
        - value_selector:
          - '1758622742335'
          - global_scope
          value_type: string
          variable: global_scope
      height: 53
      id: '1758096294198'
      position:
        x: 2569.0205524344196
        y: 352
      positionAbsolute:
        x: 2569.0205524344196
        y: 352
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        context:
          enabled: false
          variable_selector: []
        desc: ''
        isInIteration: true
        isInLoop: false
        iteration_id: '17580962587520'
        model:
          completion_params: {}
          mode: chat
          name: deepseek-reasoner
          provider: langgenius/deepseek/deepseek
        prompt_template:
        - id: 988d6e78-9e92-4ede-bab7-9dcc0073feb4
          role: system
          text: "# ËßíËâ≤ÔºàRoleÔºâ\n‰Ω†ÊòØ‰∏ÄÂêç‰∏•Ê†ºÁöÑ JSON ËæìÂá∫‰øÆÂ§çÂô®ÔºàJSON FixerÔºâ„ÄÇ  \n‰Ω†ÁöÑËÅåË¥£ÊòØÔºö\n- Êé•Êî∂‰∏Ä‰∏™ÂèØËÉΩÊ†ºÂºèÈîôËØØÁöÑ\
            \ JSON Â≠óÁ¨¶‰∏≤\n- ‰∏•Ê†º‰øÆÂ§çÂÖ∂‰∏≠ÁöÑËØ≠Ê≥ïÈóÆÈ¢òÊàñÁªìÊûÑÈóÆÈ¢ò„ÄÇ\n- ËæìÂá∫ÂÆåÂÖ®Á¨¶ÂêàÈ¢ÑÊúüÊ†ºÂºèÁöÑ JSON ÂØπË±°„ÄÇ\n\n---\n\n# ËæìÂÖ•ÔºàInputÔºâ\n\
            ‰Ω†Â∞ÜÊî∂Âà∞Ôºö\n**raw_output**ÔºöÊ®°ÂûãÂéüÂßãËæìÂá∫Ôºå‰Ω†ÈúÄË¶ÅÂøΩÁï•‰ªé‚Äù<think>‚ÄúÂà∞\"</think>\"ÁöÑÈÉ®ÂàÜÔºåÂè™‰øùÁïôÈîôËØØ JSON„ÄÇ\n\
            \nÁ§∫‰æãËæìÂÖ•Ôºö\nerror: Expecting property name enclosed in double quotes: line\
            \ 3 column 5 (char 25)\nraw_text: {file: \"doc1\", section: \"main\",\
            \ chapters: [ ... ]}\n\n---\n\n# ËæìÂá∫Ê†ºÂºèÔºàOutputÔºâ\nËØ∑ËæìÂá∫‰∏Ä‰∏™ **ÂÆåÊï¥ JSON ÂØπË±°**ÔºåÂøÖÈ°ª‰∏•Ê†ºÁ¨¶Âêà‰ª•‰∏ãÁªìÊûÑÔºà‰∏çË¶ÅËæìÂá∫Â§ö‰ΩôÁöÑËß£ÈáäÊÄßÊñáÂ≠óÔºâÔºö\
            \  \n```json\n{\n  \"file\": \"string\",\n  \"section\": \"string\",\n\
            \  \"experiment_root_ids\": [\"string\", \"...\"],\n  \"chapters\":[\n\
            \    {\n      \"chapter_id\": \"string\",\n      \"parameters\": [\n \
            \       {\n          \"item\": \"string\",\n          \"constraint\":\
            \ \"<=|<|=|>=|>|range_closed|range_open|enum|boolean\",\n          \"\
            value\": \"string|array|null\",\n          \"unit\": \"string|null\",\n\
            \          \"source_text\": \"string\"\n        }\n      ],\n      \"\
            refs\": [\n        {\n          \"ref_type\": \"table|graph|clause|external\"\
            ,\n          \"doc_id\": \"string|null\",\n          \"target_id\": \"\
            string\",\n          \"anchor_text\": \"string\"\n        }\n      ],\n\
            \      \"table_headers\": [\"string\", \"...\"]\n    }\n  ]\n}\n\n```\n\
            \n# Ê≥®ÊÑè‰∫ãÈ°π\n\n1. ‰Ω†ÈúÄË¶Å‰∏•Ê†ºÊåâÁÖßËæìÂá∫Ê†ºÂºèË¶ÅÊ±ÇËøõË°å‰øÆÂ§çÔºåÂèØÈÄöËøáËæìÂÖ•jsonËøõË°åÊé®Êñ≠ÔºåÁ°Æ‰øù JSON Â≠óÁ¨¶‰∏≤ÂèØË¢´ `json.loads()`\
            \ Ëß£ÊûêÈÄöËøáÂç≥ÂèØ\n2. ‰∏•Ê†ºËæìÂá∫‰∏Ä‰∏™ÂêàÊ≥ï JSON ÂØπË±°Ôºå‰∏çÂæóËæìÂá∫Â§ö‰ΩôÊñáÂ≠ó„ÄÅÊ≥®ÈáäÊàñ Markdown„ÄÇ\n3. Â¶ÇÊûúÊüêÂ≠óÊÆµÊó†ÂÜÖÂÆπÔºåËØ∑‰ΩøÁî®\
            \ `null` ÊàñÁ©∫Êï∞ÁªÑ `[]`„ÄÇ\n4. ÂøÖÈ°ªË°•ÂÖ®Áº∫Â§±Â≠óÊÆµÔºå‰∏çÂÖÅËÆ∏ÁúÅÁï•‰ªª‰ΩïÂ≠óÊÆµ„ÄÇ"
        - id: 8fd45a2e-1ede-4756-bf64-fbfe42302cff
          role: user
          text: 'error_type:

            {{#1758096258752017580962587521.error_type#}}


            error_message:

            {{#1758096258752017580962587521.error_message#}}


            raw_text:

            {{#1758096258752017580962587520.text#}}

            '
        - id: f02c2f0b-66b2-4a3f-8960-ea7c573f4a35
          role: assistant
          text: ''
        selected: false
        title: LLM 6
        type: llm
        variables: []
        vision:
          enabled: false
      height: 89
      id: '1758105741313'
      parentId: '17580962587520'
      position:
        x: 812
        y: 188.76463752439133
      positionAbsolute:
        x: 1450
        y: 920.4951282714416
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
      zIndex: 1002
    - data:
        code: "import json\nimport ast\nimport re\nfrom typing import Dict, List,\
          \ Any\n\ndef parse_input(src: str) -> List[Dict[str, Any]]:\n    \"\"\"\
          È¢ÑÂ§ÑÁêÜÂπ∂Ëß£ÊûêËæìÂÖ•Â≠óÁ¨¶‰∏≤‰∏∫ÂàóË°®Â≠óÂÖ∏ÁªìÊûÑ\"\"\"\n    try:\n        # 1. Êåâ </think> ÂàáÂàÜÂπ∂ÂèñÂêéÂçäÊÆµ\n  \
          \      src = src.rsplit('</think>', 1)[-1].strip()\n\n        # 2. ÂÖàÂπ≤Êéâ deepseek\
          \ V3 ÂèØËÉΩÊèíÂÖ•ÁöÑ‚ÄúÊûÅÈÄü‚ÄùÊàñÂçïÁã¨ÁöÑ‚ÄúÊûÅ‚Äù\n        src = re.sub(r'ÊûÅÈÄü\\s*', '', src)   # ÂéªÊéâ‚ÄúÊûÅÈÄü‚ÄùÂèäÂÖ∂ÂêéÂèØËÉΩÁöÑÂ§ö‰ΩôÁ©∫ÁôΩ\n\
          \        src = re.sub(r'(?<!\\w)ÊûÅ(?!\\w)', '', src)  # ÂéªÊéâÂ≠§Á´ãÂá∫Áé∞ÁöÑ‚ÄúÊûÅ‚Äù\n\n  \
          \      # 3. Â∞ùËØïÂ§öÁßçËß£ÊûêË∑ØÂæÑ\n        try:\n            data = json.loads(src)\n\
          \        except json.JSONDecodeError:\n            try:\n              \
          \  if src.startswith('\\ufeff'):\n                    src = src[1:]\n  \
          \              data = ast.literal_eval(src)\n            except (SyntaxError,\
          \ ValueError):\n                try:\n                    processed = src.replace('\\\
          \\\"', '\"').replace('\\\\n', '\\n')\n                    data = json.loads(processed)\n\
          \                except Exception as e:\n                    raise ValueError(f\"\
          Êó†Ê≥ïËß£ÊûêÔºö{str(e)}\")\n\n        # 4. ‰øùËØÅËøîÂõû list\n        if isinstance(data,\
          \ dict):\n            return [data]\n        elif isinstance(data, list):\n\
          \            return data\n        else:\n            return []\n    except\
          \ Exception as e:\n        raise ValueError(f\"È¢ÑÂ§ÑÁêÜÂ§±Ë¥•: {str(e)}\")\n\n\n\
          def main(arg1: str) -> dict:\n    data = parse_input(arg1)\n    return {\n\
          \        \"resultB\": json.dumps(data, ensure_ascii=False),\n    }"
        code_language: python3
        desc: ''
        isInIteration: true
        isInLoop: false
        iteration_id: '17580962587520'
        outputs:
          resultB:
            children: null
            type: string
        selected: false
        title: ‰ª£Á†ÅÊâßË°å 14
        type: code
        variables:
        - value_selector:
          - '1758105741313'
          - text
          value_type: string
          variable: arg1
      height: 53
      id: '1758105753732'
      parentId: '17580962587520'
      position:
        x: 1107.228658264162
        y: 192.27317421872647
      positionAbsolute:
        x: 1745.228658264162
        y: 924.0036649657767
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
      zIndex: 1002
    - data:
        desc: ''
        isInIteration: true
        isInLoop: false
        iteration_id: '17580962587520'
        output_type: string
        selected: false
        title: ÂèòÈáèËÅöÂêàÂô® 3
        type: variable-aggregator
        variables:
        - - '1758105753732'
          - resultB
        - - '1758096258752017580962587521'
          - resultB
      height: 129
      id: '1758105758637'
      parentId: '17580962587520'
      position:
        x: 1418.2457316528325
        y: 65
      positionAbsolute:
        x: 2056.2457316528325
        y: 796.7304907470502
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
      zIndex: 1002
    - data:
        context:
          enabled: true
          variable_selector:
          - '1756550411122'
          - context
        desc: ''
        model:
          completion_params: {}
          mode: chat
          name: deepseek-reasoner
          provider: langgenius/deepseek/deepseek
        prompt_template:
        - id: 57cf1789-3bae-4a65-87a4-dbbd9959758f
          role: system
          text: '# ËßíËâ≤

            ‰Ω†ÊòØ‰∏Ä‰ΩçÊ∑±ËÄï‰∫éÊäÄÊúØÊ†áÂáÜÈ¢ÜÂüüÁöÑËµÑÊ∑±‰∏ìÂÆ∂ÔºåÊã•ÊúâÂ§öÂπ¥ÁöÑÊñáÊ°£ÂàÜÊûêÁªèÈ™åÔºåÊìÖÈïø‰ªéÂ§çÊùÇÁöÑ„ÄÅÂÖÖÊª°Ë°å‰∏öÊúØËØ≠ÁöÑÊñáÊú¨‰∏≠ÔºåÁ≤æÂáÜÂú∞ÊèêÁÇºÂá∫ÊúÄÊ†∏ÂøÉÁöÑ‰∏ªÈ¢ò„ÄÇ


            ‰Ω†ÁöÑ‰ªªÂä°ÊòØÂàÜÊûêÊàëÊèê‰æõÁöÑÊ±ΩËΩ¶È¢ÜÂüüÁöÑÊäÄÊúØÊ†áÂáÜÊñáÊ°£ÁöÑÂºïË®ÄÊàñËåÉÂõ¥ÔºàScopeÔºâÈÉ®ÂàÜÔºåÂπ∂ËØÜÂà´Âá∫Ëøô‰ªΩÊñáÊ°£ÊúÄÊ†∏ÂøÉ„ÄÅÊúÄÈ¶ñË¶ÅÁöÑËßÑÂÆöÂØπË±°Êàñ‰∏ªÈ¢ò„ÄÇ


            # ËæìÂá∫Ë¶ÅÊ±Ç

            1.  **È´òÂ∫¶Ê¶ÇÊã¨**: ÁªìÊûúÂøÖÈ°ªÊòØ‰∏Ä‰∏™ÁÆÄÊ¥ÅÁöÑÂêçËØçÊÄßÁü≠ËØ≠„ÄÇ

            2.  **Ê†∏ÂøÉ‰∏ªÈ¢ò**: Ëøô‰∏™Áü≠ËØ≠ÂøÖÈ°ªËÉΩ‰Ωú‰∏∫Êï¥‰∏™ÊñáÊ°£ÁöÑ‚ÄúÂÖ®Â±ÄËåÉÂõ¥‚ÄùÊ†áÁ≠æ„ÄÇ

            3.  **Ê†ºÂºè‰∏•Ê†º**: **‰ªÖËæìÂá∫Ëøô‰∏™ÂêçËØçÊÄßÁü≠ËØ≠**ÔºåÁªùÂØπ‰∏çË¶ÅÂåÖÂê´‰ªª‰ΩïËß£Èáä„ÄÅÂâçÁºÄÔºàÂ¶Ç‚Äú‰∏ªÈ¢òÊòØÔºö‚ÄùÔºâÊàñ‰ªª‰ΩïÂÖ∂‰ªñÂ§ö‰ΩôÁöÑÊñáÂ≠ó„ÄÇ


            # fewshot


            **Á§∫‰æã 1:**


            **ËæìÂÖ•ÊñáÊú¨:**

            ```

            This Regulation applies to:

            Front and rear position lamps and stop lamps for vehicles of categories
            L, M, N, O and T1; and,

            End-outline marker lamps for vehicles of categories M, N, O and T.

            ```


            **‰Ω†ÁöÑËæìÂá∫:**

            ```

            Front and rear position lamps, stop lamps and End-outline marker lamps

            ```


            ---


            **Á§∫‰æã 2:**


            **ËæìÂÖ•ÊñáÊú¨:**

            ```

            This Regulation applies to vehicles of categories M, N, and to their trailers
            (category O)1 with regard to the installation of lighting and light-signalling
            devices.

            ```


            **‰Ω†ÁöÑËæìÂá∫:**

            ```

            Lighting and light-signalling devices installation

            ```


            ---


            **Á§∫‰æã 3:**


            **ËæìÂÖ•ÊñáÊú¨:**

            ```

            Êú¨Êñá‰ª∂ËßÑÂÆö‰∫ÜËΩ¶ËΩΩ‰∫ãÊïÖÁ¥ßÊÄ•ÂëºÂè´Á≥ªÁªüÁöÑÊäÄÊúØË¶ÅÊ±Ç„ÄÅÂêå‰∏ÄÂûãÂºèÂà§ÂÆöË¶ÅÊ±Ç,ÊèèËø∞‰∫ÜÁõ∏Â∫îÁöÑËØïÈ™åÊñπÊ≥ï„ÄÇ

            Êú¨Êñá‰ª∂ÈÄÇÁî®‰∫éM1Á±ªÂèäN1Á±ªËΩ¶ËæÜÁöÑËΩ¶ËΩΩ‰∫ãÊïÖÁ¥ßÊÄ•ÂëºÂè´Á≥ªÁªü„ÄÇ

            ```


            **‰Ω†ÁöÑËæìÂá∫:**

            ```

            ËΩ¶ËΩΩ‰∫ãÊïÖÁ¥ßÊÄ•ÂëºÂè´Á≥ªÁªü

            ```


            ---


            # Â∑•‰Ωú

            Áé∞Âú®ÔºåËØ∑Â§ÑÁêÜ‰ª•‰∏ãËæìÂÖ•ÂÜÖÂÆπÔºö

            {{#context#}}'
        selected: false
        title: LLM 7
        type: llm
        variables: []
        vision:
          enabled: false
      height: 89
      id: '1758595213024'
      position:
        x: 653.8702599223778
        y: 1086.85928286599
      positionAbsolute:
        x: 653.8702599223778
        y: 1086.85928286599
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        code: "import json\nimport ast\nimport re\nfrom typing import Dict, List,\
          \ Any\n\n\ndef main(arg1: str) -> dict:\n    return {\n        \"global_scope\"\
          : arg1.rsplit('</think>', 1)[-1].strip(),\n    }"
        code_language: python3
        desc: ''
        outputs:
          global_scope:
            children: null
            type: string
        selected: false
        title: ‰ª£Á†ÅÊâßË°å 15
        type: code
        variables:
        - value_selector:
          - '1758595213024'
          - text
          value_type: string
          variable: arg1
      height: 53
      id: '1758622742335'
      position:
        x: 2116.2512368817224
        y: 1075.504816742838
      positionAbsolute:
        x: 2116.2512368817224
        y: 1075.504816742838
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    viewport:
      x: 349.04847037377806
      y: 142.26830271156803
      zoom: 0.5963849333908374
