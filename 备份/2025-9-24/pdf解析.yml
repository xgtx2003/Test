app:
  description: 'ä¿ç•™å¼•ç”¨ã€é¢å‘ä¸¤ä¸ªåœºæ™¯

    '
  icon: ğŸ¤–
  icon_background: '#FFEAD5'
  mode: advanced-chat
  name: pdfè§£æ
  use_icon_as_answer_icon: false
dependencies:
- current_identifier: null
  type: marketplace
  value:
    marketplace_plugin_unique_identifier: langgenius/deepseek:0.0.6@dd589dc093c8084925858034ab5ec1fdf0d33819f43226c2f8c4a749a9acbbb2
kind: app
version: 0.3.0
workflow:
  conversation_variables: []
  environment_variables: []
  features:
    file_upload:
      allowed_file_extensions:
      - .JPG
      - .JPEG
      - .PNG
      - .GIF
      - .WEBP
      - .SVG
      allowed_file_types:
      - image
      allowed_file_upload_methods:
      - local_file
      - remote_url
      enabled: false
      fileUploadConfig:
        audio_file_size_limit: 50
        batch_count_limit: 5
        file_size_limit: 15
        image_file_size_limit: 10
        video_file_size_limit: 100
        workflow_file_upload_limit: 10
      image:
        enabled: false
        number_limits: 3
        transfer_methods:
        - local_file
        - remote_url
      number_limits: 3
    opening_statement: ''
    retriever_resource:
      enabled: true
    sensitive_word_avoidance:
      enabled: false
    speech_to_text:
      enabled: false
    suggested_questions: []
    suggested_questions_after_answer:
      enabled: false
    text_to_speech:
      enabled: false
      language: ''
      voice: ''
  graph:
    edges:
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '1756557801310'
        sourceType: iteration-start
        targetType: llm
      id: 1756557801310start-source-1756557804693-target
      selected: false
      source: 1756557801310start
      sourceHandle: source
      target: '1756557804693'
      targetHandle: target
      type: custom
      zIndex: 1002
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '1756557801310'
        sourceType: llm
        targetType: code
      id: 1756557804693-source-1756557929661-target
      selected: false
      source: '1756557804693'
      sourceHandle: source
      target: '1756557929661'
      targetHandle: target
      type: custom
      zIndex: 1002
    - data:
        isInLoop: false
        sourceType: start
        targetType: code
      id: 1756550268945-source-1756550411122-target
      selected: false
      source: '1756550268945'
      sourceHandle: source
      target: '1756550411122'
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '1756613329065'
        sourceType: iteration-start
        targetType: llm
      id: 1756613329065start-source-1756613344845-target
      selected: false
      source: 1756613329065start
      sourceHandle: source
      target: '1756613344845'
      targetHandle: target
      type: custom
      zIndex: 1002
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '1756613329065'
        sourceType: llm
        targetType: code
      id: 1756613344845-source-1756613456815-target
      selected: false
      source: '1756613344845'
      sourceHandle: source
      target: '1756613456815'
      targetHandle: target
      type: custom
      zIndex: 1002
    - data:
        isInIteration: false
        isInLoop: false
        sourceType: iteration
        targetType: code
      id: 1756613329065-source-1756618044071-target
      selected: false
      source: '1756613329065'
      sourceHandle: source
      target: '1756618044071'
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '1756557801310'
        sourceType: llm
        targetType: code
      id: 1756698267184-source-1756698848564-target
      selected: false
      source: '1756698267184'
      sourceHandle: source
      target: '1756698848564'
      targetHandle: target
      type: custom
      zIndex: 1002
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '1756613329065'
        sourceType: code
        targetType: llm
      id: 1756613456815-fail-branch-1757762170810-target
      selected: false
      source: '1756613456815'
      sourceHandle: fail-branch
      target: '1757762170810'
      targetHandle: target
      type: custom
      zIndex: 1002
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '1756613329065'
        sourceType: llm
        targetType: code
      id: 1757762170810-source-1757762176397-target
      selected: false
      source: '1757762170810'
      sourceHandle: source
      target: '1757762176397'
      targetHandle: target
      type: custom
      zIndex: 1002
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '1756613329065'
        sourceType: code
        targetType: variable-aggregator
      id: 1757762176397-source-1757762184699-target
      selected: false
      source: '1757762176397'
      sourceHandle: source
      target: '1757762184699'
      targetHandle: target
      type: custom
      zIndex: 1002
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '1756613329065'
        sourceType: code
        targetType: variable-aggregator
      id: 1756613456815-source-1757762184699-target
      selected: false
      source: '1756613456815'
      sourceHandle: source
      target: '1757762184699'
      targetHandle: target
      type: custom
      zIndex: 1002
    - data:
        isInLoop: false
        sourceType: code
        targetType: code
      id: 1756618044071-source-1756629493937-target
      selected: false
      source: '1756618044071'
      sourceHandle: source
      target: '1756629493937'
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInLoop: false
        sourceType: code
        targetType: answer
      id: 17580960600210-source-1756558073574-target
      selected: false
      source: '17580960600210'
      sourceHandle: source
      target: '1756558073574'
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInLoop: false
        sourceType: code
        targetType: code
      id: 1756563307317-source-17580960600210-target
      selected: false
      source: '1756563307317'
      sourceHandle: source
      target: '17580960600210'
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '1756557801310'
        sourceType: iteration-start
        targetType: llm
      id: 1758096258752start-source-1758096258752017580962587520-target
      selected: false
      source: 1758096258752start
      sourceHandle: source
      target: '1758096258752017580962587520'
      targetHandle: target
      type: custom
      zIndex: 1002
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '1756557801310'
        sourceType: llm
        targetType: code
      id: 1758096258752017580962587520-source-1758096258752017580962587521-target
      selected: false
      source: '1758096258752017580962587520'
      sourceHandle: source
      target: '1758096258752017580962587521'
      targetHandle: target
      type: custom
      zIndex: 1002
    - data:
        isInIteration: false
        isInLoop: false
        sourceType: code
        targetType: code
      id: 1758096294198-source-1756563307317-target
      source: '1758096294198'
      sourceHandle: source
      target: '1756563307317'
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '1756557801310'
        sourceType: code
        targetType: llm
      id: 1756557929661-fail-branch-1756698267184-target
      source: '1756557929661'
      sourceHandle: fail-branch
      target: '1756698267184'
      targetHandle: target
      type: custom
      zIndex: 1002
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '1756557801310'
        sourceType: code
        targetType: variable-aggregator
      id: 1756698848564-source-1756698812908-target
      source: '1756698848564'
      sourceHandle: source
      target: '1756698812908'
      targetHandle: target
      type: custom
      zIndex: 1002
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '1756557801310'
        sourceType: code
        targetType: variable-aggregator
      id: 1756557929661-source-1756698812908-target
      source: '1756557929661'
      sourceHandle: source
      target: '1756698812908'
      targetHandle: target
      type: custom
      zIndex: 1002
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '17580962587520'
        sourceType: llm
        targetType: code
      id: 1758105741313-source-1758105753732-target
      source: '1758105741313'
      sourceHandle: source
      target: '1758105753732'
      targetHandle: target
      type: custom
      zIndex: 1002
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '17580962587520'
        sourceType: code
        targetType: variable-aggregator
      id: 1758105753732-source-1758105758637-target
      source: '1758105753732'
      sourceHandle: source
      target: '1758105758637'
      targetHandle: target
      type: custom
      zIndex: 1002
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '17580962587520'
        sourceType: code
        targetType: llm
      id: 1758096258752017580962587521-fail-branch-1758105741313-target
      source: '1758096258752017580962587521'
      sourceHandle: fail-branch
      target: '1758105741313'
      targetHandle: target
      type: custom
      zIndex: 1002
    - data:
        isInIteration: true
        isInLoop: false
        iteration_id: '17580962587520'
        sourceType: code
        targetType: variable-aggregator
      id: 1758096258752017580962587521-source-1758105758637-target
      source: '1758096258752017580962587521'
      sourceHandle: source
      target: '1758105758637'
      targetHandle: target
      type: custom
      zIndex: 1002
    - data:
        isInLoop: false
        sourceType: iteration
        targetType: code
      id: 1756557801310-source-1758096294198-target
      source: '1756557801310'
      sourceHandle: source
      target: '1758096294198'
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInLoop: false
        sourceType: iteration
        targetType: code
      id: 17580962587520-source-1758096294198-target
      source: '17580962587520'
      sourceHandle: source
      target: '1758096294198'
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInIteration: false
        isInLoop: false
        sourceType: llm
        targetType: code
      id: 1758595213024-source-1758622742335-target
      source: '1758595213024'
      sourceHandle: source
      target: '1758622742335'
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInIteration: false
        isInLoop: false
        sourceType: code
        targetType: code
      id: 1758622742335-source-1758096294198-target
      source: '1758622742335'
      sourceHandle: source
      target: '1758096294198'
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInLoop: false
        sourceType: code
        targetType: llm
      id: 1756550411122-source-1758595213024-target
      source: '1756550411122'
      sourceHandle: source
      target: '1758595213024'
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInLoop: false
        sourceType: code
        targetType: iteration
      id: 1756550411122-source-17580962587520-target
      source: '1756550411122'
      sourceHandle: source
      target: '17580962587520'
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInLoop: false
        sourceType: code
        targetType: iteration
      id: 1756550411122-source-1756557801310-target
      source: '1756550411122'
      sourceHandle: source
      target: '1756557801310'
      targetHandle: target
      type: custom
      zIndex: 0
    nodes:
    - data:
        desc: ''
        selected: false
        title: å¼€å§‹
        type: start
        variables:
        - allowed_file_extensions: []
          allowed_file_types:
          - document
          allowed_file_upload_methods:
          - local_file
          - remote_url
          label: file
          max_length: 48
          options: []
          required: false
          type: file
          variable: file
        - label: context
          max_length: 100000
          options: []
          required: false
          type: paragraph
          variable: context
        - label: array
          max_length: 1000000
          options: []
          required: false
          type: paragraph
          variable: array
        - label: regulation
          max_length: 1000000
          options: []
          required: false
          type: paragraph
          variable: regulation
        - label: result
          max_length: 1000000
          options: []
          required: false
          type: paragraph
          variable: result
      height: 193
      id: '1756550268945'
      position:
        x: 30
        y: 352
      positionAbsolute:
        x: 30
        y: 352
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        answer: '{{#1758595213024.text#}}'
        desc: ''
        selected: false
        title: ç›´æ¥å›å¤
        type: answer
        variables: []
      height: 104
      id: answer
      position:
        x: 945.7859968508628
        y: 1443.3008747276417
      positionAbsolute:
        x: 945.7859968508628
        y: 1443.3008747276417
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        code: "import fitz  # PyMuPDF\nimport re\nimport json\nimport os\nfrom typing\
          \ import List, Dict, Tuple\nfrom collections import defaultdict\n\n# chapter_patterns\
          \ = [\n#     re.compile(r'^(é™„\\s*å½•\\s*[A-Z])\\s+(.+)$'),\n#     re.compile(r'^([A-Z](?:\\\
          .\\d+)+)\\s+(.+)$'),\n#     re.compile(r'^(\\d+(?:\\.\\d+)*)(\\s+)(.+)$'),\n\
          # ]\n\n# chapter_patterns = [\n#     re.compile(r'^(APPENDIX\\s+[A-Z0-9]+)$',\
          \ re.I),          # APPENDIX A / APPENDIX 1\n#     re.compile(r'^([A-Z](?:\\\
          .\\d+)+)\\s+(.+)$'),\n#     re.compile(r'^(\\d+(?:\\.\\d+)*\\.?)\\s+(.+)$'),\
          \                     # 1.1. Title\n# ]\n\n# æ—§çš„ç« èŠ‚æ¨¡å¼ï¼ˆå·²æ³¨é‡Šï¼‰\n# chapter_patterns\
          \ = [\n#     re.compile(r'^(é™„\\s*å½•\\s*[A-Z0-9])$'), # é™„ å½• B\n#     re.compile(r'^((APPENDIX|ANNEX|ATTACHMENT)\\\
          s+(?:[A-Z0-9]+|\\([A-Z0-9]+\\)))$', re.I),  # ANNEX A / ANNEX 1\n#     re.compile(r'^([A-Z]\\\
          .)\\s+(.+)$'),                                # A. Title (å•ç‹¬å­—æ¯ç« èŠ‚)\n#   \
          \  re.compile(r'^([A-Z](?:\\.\\d+)+\\.?)\\s+(.+)$'),                   \
          \  # A.1. Title / A.1.1. Title\n#     re.compile(r'^(\\d+(?:\\.\\d+)*\\\
          .?)\\s+(.+)$'),                       # 1.1. Title\n#     re.compile(r'^(\\\
          d+(?:-\\d+)*-)\\s+(.+)$'),                          # 1- Title / 1-2- Title\n\
          # ]\n\n# æ–°çš„åˆå¹¶åçš„ç« èŠ‚æ¨¡å¼\nchapter_patterns = [\n    # 1. ä¸­æ–‡é™„å½•ï¼šé™„å½•A, é™„ å½• B\n  \
          \  re.compile(r'^(é™„\\s*å½•\\s*[A-Z0-9])$'),\n    \n    # 2. è‹±æ–‡é™„å½•ï¼šAPPENDIX\
          \ A, ANNEX A, ATTACHMENT A\n    re.compile(r'^((APPENDIX|ANNEX|ATTACHMENT)\\\
          s+(?:[A-Z0-9]+|\\([A-Z0-9]+\\)))$', re.I),\n    \n    # 3. å­—æ¯ç« èŠ‚ï¼ˆæ”¯æŒç‚¹å’Œæ¨ªçº¿åˆ†éš”ç¬¦ï¼‰ï¼šA.\
          \ Title, A.1. Title, A-1- Title\n    re.compile(r'^([A-Z](?:[.\\-]\\d+)*[.\\\
          -]?)\\s+(.+)$'),\n    \n    # 4. æ•°å­—ç« èŠ‚ï¼ˆæ”¯æŒç‚¹å’Œæ¨ªçº¿åˆ†éš”ç¬¦ï¼‰ï¼š1. Title, 1.1. Title, 1-\
          \ Title, 1-2- Title\n    re.compile(r'^(\\d+(?:[.\\-]\\d+)*[.\\-]?)\\s+(.+)$'),\n\
          ]\n\ndef detect_document_language(lines: List[str]) -> str:\n    \"\"\"\n\
          \    æ£€æµ‹æ–‡æ¡£è¯­è¨€ï¼šä¸­æ–‡æˆ–è‹±æ–‡\n    :param lines: æ–‡æ¡£çš„æ‰€æœ‰è¡Œ\n    :return: 'zh' è¡¨ç¤ºä¸­æ–‡ï¼Œ'en'\
          \ è¡¨ç¤ºè‹±æ–‡\n    \"\"\"\n    chinese_char_count = 0\n    total_chars = 0\n  \
          \  \n    # é‡‡æ ·å‰1000è¡Œæˆ–å…¨éƒ¨è¡Œ\n    sample_lines = lines[:1000] if len(lines) >\
          \ 1000 else lines\n    \n    for line in sample_lines:\n        for char\
          \ in line:\n            total_chars += 1\n            if '\\u4e00' <= char\
          \ <= '\\u9fff':  # ä¸­æ–‡å­—ç¬¦\n                chinese_char_count += 1\n    \n\
          \    # åªè¦æœ‰ä¸­æ–‡å­—ç¬¦å°±è®¤ä¸ºæ˜¯ä¸­æ–‡æ–‡æ¡£\n    if chinese_char_count > 0:\n        return 'zh'\n\
          \    else:\n        return 'en'\n\n# ä¸­æ–‡ç« èŠ‚max_chapter_num=50\n# å…¨æ–‡é¦–å…ˆæ£€æµ‹æ˜¯ä¸­æ–‡è¿˜æ˜¯è‹±æ–‡\n\
          def detect_chapter(line: str, max_chapter_num=1000, language='en', number_analysis=None):\n\
          \    clean_line = line.strip()\n    if not clean_line:\n        return None\n\
          \n    for pattern in chapter_patterns:\n        m = pattern.match(clean_line)\n\
          \        if m:\n            chapter_id = m.group(1).strip()\n          \
          \  chapter_title = m.group(len(m.groups())).strip() if m.group(len(m.groups()))\
          \ else \"\"\n            if re.match(r'^(é™„\\s*å½•\\s*[A-Z0-9])$', chapter_id):\n\
          \                # å»æ‰ä¸­é—´çš„ç©ºæ ¼\n                chapter_id = chapter_id.replace(\"\
          \ \", \"\")\n                # chapter_id = chapter_id[-1]\n           \
          \ # ---- åŸºç¡€è¿‡æ»¤ ----\n            first_num = None\n            if chapter_id.upper().startswith(\"\
          APPENDIX\"):\n                suffix = chapter_id[len(\"APPENDIX\"):].strip(\"\
          \ ()\")\n                if suffix.isdigit():\n                    first_num\
          \ = int(suffix)\n            else:\n                m_num = re.match(r'^(\\\
          d+)', chapter_id)\n                if m_num:\n                    first_num\
          \ = int(m_num.group(1))\n\n            if first_num is not None and number_analysis\
          \ is not None:\n                # ä½¿ç”¨æ™ºèƒ½æ•°å­—èŒƒå›´åˆ¤æ–­\n                min_reasonable\
          \ = number_analysis.get(\"min_reasonable\", 1)\n                max_reasonable\
          \ = number_analysis.get(\"max_reasonable\", max_chapter_num)\n         \
          \       \n                # ç‰¹æ®Šå¤„ç†æ³•è§„ç¼–å·æ¨¡å¼\n                if number_analysis.get(\"\
          regulation_mode\", False):\n                    regulation_number = number_analysis.get(\"\
          regulation_number\")\n                    if first_num != regulation_number:\n\
          \                        return None  # ä¸æ˜¯æ³•è§„ç¼–å·ï¼Œè¿‡æ»¤æ‰\n                else:\n\
          \                    # æ­£å¸¸ç« èŠ‚ç¼–å·èŒƒå›´æ£€æŸ¥\n                    if first_num < min_reasonable\
          \ or first_num > max_reasonable:\n                        return None  #\
          \ æ•°å­—èŒƒå›´ä¸åˆç†\n            elif first_num is not None:\n                # å…œåº•é€»è¾‘ï¼šä½¿ç”¨ä¼ ç»Ÿçš„max_chapter_num\n\
          \                if first_num < 1 or first_num > max_chapter_num:\n    \
          \                return None  # æ•°å­—èŒƒå›´ä¸åˆç†\n\n            # ---- å†…å®¹ç‰¹å¾è¿‡æ»¤ ----\n\
          \            # 1) æ ‡é¢˜å¿…é¡»åŒ…å«å­—æ¯æˆ–ä¸­æ–‡\n            if not re.search(r'[A-Za-z\\\
          u4e00-\\u9fff]', chapter_title):\n                return None\n\n      \
          \      # 2) å»æ‰çº¯æ•°å­—è¡¨æ ¼è¡Œ\n            if re.fullmatch(r'[\\d\\s\\.\\-]+', chapter_title):\n\
          \                return None\n\n            # 3) è¡¨æ ¼å†…å®¹è¿‡æ»¤ - æ£€æµ‹æ˜æ˜¾çš„è¡¨æ ¼æ•°æ®æ¨¡å¼\n\
          \            # å¦‚æœæ ‡é¢˜åŒ…å«å¤§é‡æ•°å­—ã€ç©ºæ ¼å’Œå°‘é‡å­—æ¯çš„ç»„åˆï¼Œå¯èƒ½æ˜¯è¡¨æ ¼æ•°æ®\n            if re.search(r'^\\\
          d+\\s+\\d+.*[A-Z]\\s+\\d+\\s+\\d+', chapter_title):  # å¦‚ \"10 0 E 0 16\"\
          \n                return None\n            \n            # æ£€æµ‹è¡¨æ ¼è¡Œæ¨¡å¼ï¼šå•ä¸ªå­—æ¯\
          \ + æ•°å­—ç»„åˆ\n            if re.fullmatch(r'[A-Z]\\s*\\d+.*', chapter_title)\
          \ and len(chapter_title.split()) >= 3:\n                # å¦‚æœæ ‡é¢˜æ˜¯ \"A 10 0\"\
          \ è¿™æ ·çš„æ ¼å¼ï¼Œå¾ˆå¯èƒ½æ˜¯è¡¨æ ¼æ•°æ®\n                parts = chapter_title.split()\n      \
          \          if len(parts) >= 3 and all(part.isdigit() or part in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\
          \ for part in parts[:3]):\n                    return None\n\n         \
          \   # 4) æ£€æµ‹åæ ‡ç‚¹æˆ–å‚æ•°è¡¨æ ¼ï¼šå¦‚ \"A15 0 E 0 3\"\n            if re.search(r'^[A-Z]\\\
          d+\\s+\\d+\\s+[A-Z]\\s+\\d+\\s+\\d+', chapter_title):\n                return\
          \ None\n\n            # 5) è¡Œå¤ªçŸ­\n            if len(clean_line) < 4 and not\
          \ chapter_id.upper().startswith(\"APPENDIX\") and not chapter_id.startswith(\"\
          é™„å½•\"):\n                return None\n\n            # 6) è¿‡æ»¤æ˜æ˜¾çš„è¡¨æ ¼æ ‡é¢˜ç»„åˆ\n  \
          \          if len(chapter_id) == 1 and chapter_id.isupper():\n         \
          \       # å•ä¸ªå¤§å†™å­—æ¯ä½œä¸ºç« èŠ‚IDï¼Œæ£€æŸ¥æ ‡é¢˜æ˜¯å¦åƒè¡¨æ ¼æ•°æ®\n                if re.search(r'\\d+.*\\\
          d+', chapter_title) and len(chapter_title.split()) <= 6:\n             \
          \       return None\n\n            return {\n                \"chapter_id\"\
          : chapter_id,\n                \"chapter_title\": chapter_title\n      \
          \      }\n\n    return None\n\ndef build_tree(chapter_list: List[Dict])\
          \ -> List[Dict]:\n    id_map = {}\n    root = []\n\n    # å…ˆæ³¨å†Œæ‰€æœ‰èŠ‚ç‚¹\n    for\
          \ chap in chapter_list:\n        chap[\"children\"] = []\n        # ç»Ÿä¸€å»æ‰æœ«å°¾ç‚¹å’Œæ¨ªçº¿ä½œä¸º\
          \ key\n        key = chap[\"chapter_id\"].rstrip('.-')\n        id_map[key]\
          \ = chap\n\n    # ä¸ºæ¯ä¸ªèŠ‚ç‚¹åˆ›å»ºç¼ºå¤±çš„çˆ¶èŠ‚ç‚¹ï¼ˆåªé’ˆå¯¹ä¸‰çº§åŠä»¥ä¸Šæ ‡é¢˜ï¼‰\n    for chap in chapter_list:\n\
          \        cid = chap[\"chapter_id\"].rstrip('.')\n        parts = cid.split('.')\n\
          \        \n        # åªæœ‰ä¸‰çº§åŠä»¥ä¸Šæ ‡é¢˜æ‰åˆ›å»ºä¸­é—´çˆ¶èŠ‚ç‚¹\n        if len(parts) >= 3:\n  \
          \          # åˆ›å»ºæ‰€æœ‰ç¼ºå¤±çš„ä¸­é—´çˆ¶çº§èŠ‚ç‚¹ï¼ˆä½†ä¸åŒ…æ‹¬é¡¶çº§çˆ¶èŠ‚ç‚¹ï¼‰\n            for i in range(2, len(parts)):\
          \  # ä»ç¬¬äºŒçº§å¼€å§‹åˆ›å»ºï¼Œè·³è¿‡é¡¶çº§\n                parent_key = '.'.join(parts[:i])\n \
          \               if parent_key not in id_map:\n                    # åˆ›å»ºç¼ºå¤±çš„çˆ¶èŠ‚ç‚¹\n\
          \                    parent_node = {\n                        \"chapter_id\"\
          : parent_key + \".\",\n                        \"chapter_title\": \"\",\n\
          \                        \"raw_text\": \"\",\n                        \"\
          children\": []\n                    }\n                    id_map[parent_key]\
          \ = parent_node\n\n    # æ„å»ºæ ‘ç»“æ„\n    for chap in chapter_list:\n        cid\
          \ = chap[\"chapter_id\"].rstrip('.')\n        parts = cid.split('.')\n\n\
          \        # æ ¹èŠ‚ç‚¹åˆ¤æ–­\n        if cid.startswith(\"APPENDIX\"):\n           \
          \ root.append(chap)\n        elif cid.startswith(\"é™„å½•\") or len(parts) ==\
          \ 1:\n            root.append(chap)\n        else:\n            parent_key\
          \ = '.'.join(parts[:-1])\n            parent = id_map.get(parent_key)\n\
          \            if parent:\n                parent[\"children\"].append(chap)\n\
          \            else:\n                # å¦‚æœçˆ¶èŠ‚ç‚¹ä¸å­˜åœ¨ï¼Œå¯¹äºäºŒçº§æ ‡é¢˜ï¼Œç›´æ¥ä½œä¸ºæ ¹èŠ‚ç‚¹\n        \
          \        if len(parts) == 2:\n                    root.append(chap)\n  \
          \              # ä¸‰çº§åŠä»¥ä¸Šæ ‡é¢˜æ²¡æœ‰çˆ¶èŠ‚ç‚¹æ—¶ï¼Œä¸åšå¤„ç†ï¼ˆå› ä¸ºå‰é¢å·²ç»åˆ›å»ºäº†çˆ¶èŠ‚ç‚¹ï¼‰\n\n\n    # å°†åˆ›å»ºçš„ä¸­é—´èŠ‚ç‚¹ä¹Ÿæ·»åŠ åˆ°æœ€ç»ˆçš„ç« èŠ‚åˆ—è¡¨ä¸­ï¼Œä½†åªæœ‰é‚£äº›æœ‰å­èŠ‚ç‚¹çš„\n\
          \    created_parents = []\n    for key, node in id_map.items():\n      \
          \  if node not in chapter_list and len(node[\"children\"]) > 0:\n      \
          \      created_parents.append(node)\n    \n    # å¯¹åˆ›å»ºçš„çˆ¶èŠ‚ç‚¹ä¹Ÿè¿›è¡Œæ ‘ç»“æ„æ„å»º\n    for\
          \ parent in created_parents:\n        cid = parent[\"chapter_id\"].rstrip('.')\n\
          \        parts = cid.split('.')\n        \n        if len(parts) == 1:\n\
          \            root.append(parent)\n        else:\n            parent_key\
          \ = '.'.join(parts[:-1])\n            grandparent = id_map.get(parent_key)\n\
          \            if grandparent and parent not in grandparent[\"children\"]:\n\
          \                grandparent[\"children\"].append(parent)\n            elif\
          \ len(parts) == 1:  # è¿™æ˜¯ä¸€çº§ç« èŠ‚\n                if parent not in root:\n \
          \                   root.append(parent)\n    \n    return root\n\ndef build_full_path(chapters:\
          \ List[Dict], path_prefix=\"\"):\n    for chap in chapters:\n        if\
          \ path_prefix:\n            chap[\"full_path\"] = f\"{path_prefix}/{chap['chapter_id']}\
          \ {chap['chapter_title']}\"\n        else:\n            chap[\"full_path\"\
          ] = f\"{chap['chapter_id']} {chap['chapter_title']}\"\n        if chap.get(\"\
          children\"):\n            build_full_path(chap[\"children\"], chap[\"full_path\"\
          ])\n\ndef fullwidth_to_halfwidth(text: str) -> str:\n    result = []\n \
          \   for char in text:\n        code = ord(char)\n        if 0xFF01 <= code\
          \ <= 0xFF5E:\n            result.append(chr(code - 0xFEE0))\n        else:\n\
          \            result.append(char)\n    return ''.join(result)\n\ndef build_term_dict(raw_text:\
          \ str) -> Dict[str, str]:\n    text = re.sub(r'\\n+', '\\n', raw_text.strip())\n\
          \    pattern = re.compile(\n        r'^\\d+\\.\\d+\\n'\n        r'(?P<cn>[^\\\
          n]*?)\\s*'\n        r'(?P<en>[A-Za-z].*?)\\s*(?=\\n)',\n        re.MULTILINE\n\
          \    )\n\n    term_map = {}\n    for m in pattern.finditer(text):\n    \
          \    cn = m.group(\"cn\").strip()\n        en = m.group(\"en\").strip()\n\
          \        if cn and en:\n            term_map[cn] = en\n    return term_map\n\
          \ndef extract_terms_with_abbr_from_terms_section(raw_text: str) -> Dict[str,\
          \ Dict[str, str]]:\n    \"\"\"\n    æå–æœ¯è¯­ç« èŠ‚ä¸­çš„ä¸­è‹±æ–‡æœ¯è¯­åŠç¼©å†™\n    è¿”å›æ ¼å¼ï¼š\n    {\n\
          \      \"ä¸­æ–‡æœ¯è¯­\": {\n         \"en\": \"è‹±æ–‡æœ¯è¯­\",\n         \"abbr\": \"ç¼©å†™ï¼ˆå¦‚æœ‰ï¼‰\"\
          \n      }\n    }\n    \"\"\"\n    term_map = {}\n    text = re.sub(r'\\\
          n+', '\\n', raw_text.strip())\n\n    pattern = re.compile(\n        r'(?P<cn>[\\\
          u4e00-\\u9fffï¼ˆï¼‰()Â·\\s]{2,})'        # ä¸­æ–‡éƒ¨åˆ†\n        r'\\s*'            \
          \                             # å¯é€‰ç©ºæ ¼\n        r'(?P<en>[A-Za-z][A-Za-z\\\
          s\\-/]*)'              # è‹±æ–‡æœ¯è¯­\n        r'(?:[;ï¼›:ï¼š]?\\s*(?P<abbr>[A-Z0-9Â·]+))?',\
          \       # å¯é€‰ç¼©å†™\n        re.MULTILINE\n    )\n\n\n    for m in pattern.finditer(text):\n\
          \        cn = m.group(\"cn\").strip()\n        en = m.group(\"en\").strip()\n\
          \        abbr = m.group(\"abbr\").strip() if m.group(\"abbr\") else \"\"\
          \n\n        term_map[cn] = {\"en\": en}\n        if abbr:\n            term_map[cn][\"\
          abbr\"] = abbr\n\n    return term_map\n\ndef extract_abbr_terms_from_symbols_section(raw_text:\
          \ str) -> Dict[str, Dict[str, str]]:\n    \"\"\"\n    æå–â€œç¬¦å·å’Œç¼©ç•¥è¯­â€ç« èŠ‚çš„ä¸­è‹±ç¼©å†™æ˜ å°„ï¼Œè¿”å›ä»¥ä¸­æ–‡ä¸ºé”®çš„ç»“æ„ï¼š\n\
          \    {\n        \"ä¸­æ–‡\": {\n            \"abbr\": \"ç¼©å†™\",\n            \"\
          en\": \"è‹±æ–‡é‡Šä¹‰\"\n        }\n    }\n    \"\"\"\n    abbr_map = {}\n    # æ¸…ç†æ–‡æœ¬\n\
          \    text = re.sub(r'\\n+', '\\n', raw_text.strip())\n\n    # åŒ¹é…æ¨¡å¼ï¼šACLR:\
          \ é‚»é“æ³„æ¼åŠŸç‡æ¯” (Adjacent Channel Leakage Power Ratio)\n    pattern = re.compile(\n\
          \        r'(?P<abbr>[A-Za-z0-9Â·\\-_]+)\\s*[:ï¼š]?\\s*'\n        r'(?P<cn>[\\\
          u4e00-\\u9fffÂ·]+)'\n        r'(?:[ï¼ˆï¼‰()]*\\s*(?P<en>[A-Za-z\\s/\\-]+)\\s*[ï¼ˆï¼‰()]*)?'\n\
          \    )\n\n    for m in pattern.finditer(text):\n        abbr = m.group(\"\
          abbr\").strip()\n        cn = m.group(\"cn\").strip(\"ï¼ˆï¼‰()\").strip()\n\
          \        en = m.group(\"en\").strip() if m.group(\"en\") else \"\"\n\n \
          \       if cn:\n            abbr_map[cn] = {}\n            if abbr:\n  \
          \              abbr_map[cn][\"abbr\"] = abbr\n            if en:\n     \
          \           abbr_map[cn][\"en\"] = en\n\n    return abbr_map\n\ndef should_merge_crossline(prev_text,\
          \ curr_text, prev_bbox, curr_bbox):\n    \"\"\"\n    åˆ¤æ–­æ˜¯å¦éœ€è¦æŠŠå½“å‰è¡Œåˆå¹¶åˆ°ä¸Šä¸€è¡Œ\n\
          \    \"\"\"\n    text_stripped = curr_text.strip()\n\n    # æ¨¡å¼åŒ¹é…ï¼šè¡¨æ ¼æ ‡é¢˜ã€ç¼–å·æ ‡é¢˜ç­‰\n\
          \    if re.match(r'^è¡¨\\s*\\d+', text_stripped):\n        return True\n\n\
          \    # å‚ç›´è·ç¦»å¾ˆå°ï¼ˆè¯´æ˜æ˜¯è§†è§‰ä¸Šçš„åŒä¸€è¡Œï¼‰\n    prev_y = prev_bbox[1]\n    curr_y = curr_bbox[1]\n\
          \    line_height = prev_bbox[3] - prev_bbox[1]\n    if abs(curr_y - prev_y)\
          \ < 0.3 * line_height:\n        return True\n\n    return False\n\ndef fix_broken_chapters(lines:\
          \ list[str]) -> list[str]:\n    def normalize_chapter_spaces(s: str) ->\
          \ str:\n        line = s.strip()\n        \n        # 1. ä¿ç•™åŸæ¥çš„é€»è¾‘ï¼šä¿®å¤ç‚¹åé¢çš„ç©ºæ ¼ï¼Œé€‚ç”¨äºæ‰€æœ‰æƒ…å†µ\
          \ (A. 1, 7. 1)\n        line = re.sub(r'\\.\\s+(?=\\d)', '.', line)\n  \
          \      \n        # 2. ä¿®å¤æ•°å­—/å­—æ¯å’Œç‚¹ä¹‹é—´çš„ç©ºæ ¼ï¼š7 .1 -> 7.1, A .1 -> A.1\n        line\
          \ = re.sub(r'([A-Za-z0-9]+)\\s+(\\.\\d+)', r'\\1\\2', line)\n        \n\
          \        # 3. ä¿®å¤å¤æ‚çš„å¤šçº§ç©ºæ ¼ï¼š7 . 1 . 2 -> 7.1.2\n        # éœ€è¦å¾ªç¯å¤„ç†ï¼Œç›´åˆ°æ²¡æœ‰æ›´å¤šå˜åŒ–\n\
          \        max_iterations = 10  # é˜²æ­¢æ— é™å¾ªç¯\n        iterations = 0\n       \
          \ prev_line = \"\"\n        while prev_line != line and iterations < max_iterations:\n\
          \            prev_line = line\n            # å¤„ç†å„ç§ç©ºæ ¼ç»„åˆï¼Œæ”¯æŒå­—æ¯å’Œæ•°å­—å¼€å¤´\n      \
          \      line = re.sub(r'([A-Za-z0-9]+)\\s*\\.\\s*(\\d+)', r'\\1.\\2', line)\n\
          \            iterations += 1\n        \n        # 4. ä¿®å¤OCRå¸¸è§é”™è¯¯ï¼šæ•°å­—å¼€å¤´çš„ç« èŠ‚\n\
          \        line = re.sub(r'(\\d+\\.\\d+)\\.\\s*l\\b', r'\\1.1', line)\n  \
          \      line = re.sub(r'([A-Za-z0-9]+)\\.l\\.(\\d+)', r'\\1.1.\\2', line)\n\
          \        line = re.sub(r'^l\\.(\\d+)', r'1.\\1', line)\n        \n     \
          \   # 5. ä¿®å¤å­—æ¯å¼€å¤´ç« èŠ‚çš„OCRé”™è¯¯ï¼šB.l -> B.1, A.O -> A.0, C.I -> C.1\n        line\
          \ = re.sub(r'^([A-Z])\\.l\\b', r'\\1.1', line)\n        line = re.sub(r'^([A-Z])\\\
          .l\\.(\\d+)', r'\\1.1.\\2', line)\n        line = re.sub(r'^([A-Z])\\.O\\\
          .(\\d+)', r'\\1.0.\\2', line)\n        line = re.sub(r'^([A-Z])\\.I\\.(\\\
          d+)', r'\\1.1.\\2', line)\n        \n        # 6. ä¿®å¤å…¶ä»–OCRé”™è¯¯ï¼šO -> 0, I ->\
          \ 1\n        line = re.sub(r'([A-Za-z0-9]+)\\.O\\.(\\d+)', r'\\1.0.\\2',\
          \ line)\n        line = re.sub(r'([A-Za-z0-9]+)\\.I\\.(\\d+)', r'\\1.1.\\\
          2', line)\n        \n        return line\n\n    lines = [normalize_chapter_spaces(line)\
          \ for line in lines]\n\n    return lines\n\ndef process_gb_terms_format(lines:\
          \ List[str]) -> List[str]:\n    \"\"\"\n    å¤„ç†å›½æ ‡æœ¯è¯­å®šä¹‰æ ¼å¼ï¼š\n    å°† \"3.1\" (ä¸‹ä¸€è¡Œ)\
          \ \"ä¸­æ–‡æœ¯è¯­ è‹±æ–‡æœ¯è¯­\" åˆå¹¶ä¸º \"3.1 ä¸­æ–‡æœ¯è¯­ è‹±æ–‡æœ¯è¯­\"\n    \"\"\"\n    result = []\n   \
          \ i = 0\n    \n    while i < len(lines):\n        current_line = lines[i].strip()\n\
          \        \n        # æ£€æµ‹æ˜¯å¦æ˜¯æœ¯è¯­å®šä¹‰ç¼–å·ï¼šçº¯æ•°å­—.æ•°å­—æ ¼å¼ï¼Œä¸”ä¸‹ä¸€è¡ŒåŒ…å«ä¸­æ–‡+è‹±æ–‡ï¼Œæˆ–è€…ç¬¬äºŒè¡Œæ˜¯ä¸­æ–‡ï¼Œç¬¬ä¸‰è¡Œæ˜¯è‹±æ–‡\n\
          \        if (i + 1 < len(lines) and \n            re.match(r'^\\d+\\.\\\
          d+$', current_line) and\n            current_line.startswith('3.')):  #\
          \ é€šå¸¸æœ¯è¯­ç« èŠ‚æ˜¯ç¬¬3ç« \n            \n            next_line = lines[i + 1].strip()\n\
          \            \n            # æ£€æŸ¥ä¸‹ä¸€è¡Œæ˜¯å¦ç¬¦åˆ: ä¸­æ–‡ + ç©ºæ ¼ + è‹±æ–‡ çš„æ¨¡å¼\n            if\
          \ re.search(r'[\\u4e00-\\u9fa5].*[A-Za-z]', next_line):\n              \
          \  # åˆå¹¶æˆæ ‡é¢˜æ ¼å¼\n                merged_line = f\"{current_line} {next_line}\"\
          \n                result.append(merged_line)\n                i += 2  #\
          \ è·³è¿‡ä¸‹ä¸€è¡Œ\n                continue\n\n            # æ£€æŸ¥ç¬¬äºŒè¡Œæ˜¯å¦æ˜¯ä¸­æ–‡ï¼Œç¬¬ä¸‰è¡Œæ˜¯å¦æ˜¯è‹±æ–‡\n\
          \            if (i + 2 < len(lines) and\n                re.search(r'[\\\
          u4e00-\\u9fa5]', lines[i + 1].strip()) and\n                re.search(r'[A-Za-z]',\
          \ lines[i + 2].strip())):\n                merged_line = f\"{current_line}\
          \ {lines[i + 1].strip()} {lines[i + 2].strip()}\"\n                result.append(merged_line)\n\
          \                i += 3  # è·³è¿‡åä¸¤è¡Œ\n                continue\n\n        result.append(current_line)\n\
          \        i += 1\n    \n    return result\n\ndef extract_full_text_with_filter(pdf_path:\
          \ str, top_crop=0.08, bottom_crop=0.08):\n    doc = fitz.open(pdf_path)\n\
          \    all_lines = []\n\n    prev_line_text = None\n    prev_bbox = None\n\
          \n\n\n    for page in doc:\n\n        h = page.rect.height\n        clip_rect\
          \ = fitz.Rect(0, h * top_crop, page.rect.width, h * (1 - bottom_crop))\n\
          \        page_dict = page.get_text(\"dict\", clip=clip_rect)\n\n       \
          \ for block in page_dict[\"blocks\"]:\n            if block[\"type\"] !=\
          \ 0:  # åªå¤„ç†æ–‡æœ¬\n                continue\n\n            for line in block[\"\
          lines\"]:\n                # 1. æŒ‰xåæ ‡åˆå¹¶åŒä¸€è¡Œçš„span\n                spans =\
          \ sorted(line[\"spans\"], key=lambda s: s[\"bbox\"][0])\n              \
          \  merged = \"\"\n                last_x = None\n                for sp\
          \ in spans:\n                    x0, x1 = sp[\"bbox\"][0], sp[\"bbox\"][2]\n\
          \                    width = max(1.0, x1 - x0)\n                    avg_char_w\
          \ = width / max(len(sp[\"text\"]), 1)\n\n                    if last_x is\
          \ not None:\n                        gap = x0 - last_x\n               \
          \         if gap > max(avg_char_w * 0.5, 3.0):\n                       \
          \     merged += \" \"\n                    merged += sp[\"text\"]\n    \
          \                last_x = x1\n\n                merged = merged.strip()\n\
          \                curr_bbox = line[\"bbox\"]\n\n                # 2. è·¨è¡Œæ™ºèƒ½åˆå¹¶åˆ¤å®š\n\
          \                if prev_line_text is not None:\n                    if\
          \ should_merge_crossline(prev_line_text, merged, prev_bbox, curr_bbox):\n\
          \                        prev_line_text += \" \" + merged\n            \
          \            prev_bbox = (\n                            prev_bbox[0],\n\
          \                            prev_bbox[1],\n                           \
          \ max(prev_bbox[2], curr_bbox[2]),\n                            max(prev_bbox[3],\
          \ curr_bbox[3])\n                        )\n                        continue\n\
          \                    else:\n                        all_lines.append(prev_line_text)\n\
          \n                prev_line_text = merged\n                prev_bbox = curr_bbox\n\
          \n    # æœ€åä¸€è¡Œ\n    if prev_line_text:\n        all_lines.append(prev_line_text)\n\
          \n    # è¿›è¡Œå…¨è§’å­—ç¬¦è½¬åŠè§’å­—ç¬¦\n    all_lines = [fullwidth_to_halfwidth(line.strip())\
          \ for line in all_lines]\n\n    # è¿›è¡Œç« èŠ‚ç¼–å·ä¿®å¤\n    normalized = fix_broken_chapters(all_lines)\n\
          \    \n    # \U0001F195 å›½æ ‡æœ¯è¯­å®šä¹‰æ ¼å¼å¤„ç†\n    normalized = process_gb_terms_format(normalized)\n\
          \n    # å†™å‡ºæ–‡ä»¶ä¸è¿”å›\n    with open('extracted_full_text.txt', \"w\", encoding=\"\
          utf-8\") as f:\n        f.write(\"\\n\".join(normalized))\n\n    return\
          \ normalized\n\ndef detect_chapter_pattern(chapters: List[Dict]) -> str:\n\
          \    \"\"\"\n    æ£€æµ‹æ–‡æ¡£çš„ç« èŠ‚æ¨¡å¼ï¼š\n    - 'alpha_first': å­—æ¯ç« èŠ‚åœ¨å‰ (A, A.1, A.2, B,\
          \ B.1, 1, 2, ...)\n    - 'numeric_first': æ•°å­—ç« èŠ‚åœ¨å‰ (1, 2, ..., A, A.1, A.2,\
          \ B, B.1, ...)\n    \"\"\"\n    alpha_indices = []\n    numeric_indices\
          \ = []\n    \n    for i, ch in enumerate(chapters):\n        chapter_id\
          \ = ch[\"chapter_id\"].strip()\n        if re.match(r'^[A-Z](\\.\\d+)*\\\
          .?$', chapter_id):\n            alpha_indices.append(i)\n        elif re.match(r'^\\\
          d+(\\.\\d+)*\\.?$', chapter_id):\n            numeric_indices.append(i)\n\
          \    \n    if not alpha_indices or not numeric_indices:\n        return\
          \ 'numeric_first'  # é»˜è®¤æ•°å­—ä¼˜å…ˆ\n    \n    # æ¯”è¾ƒç¬¬ä¸€ä¸ªå­—æ¯ç« èŠ‚å’Œç¬¬ä¸€ä¸ªæ•°å­—ç« èŠ‚çš„ä½ç½®\n    first_alpha\
          \ = min(alpha_indices)\n    first_numeric = min(numeric_indices)\n    \n\
          \    if first_alpha < first_numeric:\n        return 'alpha_first'\n   \
          \ else:\n        return 'numeric_first'\n\ndef parse_chapter_id(chapter_id:\
          \ str, pattern: str = 'numeric_first') -> List[int]:\n    \"\"\"\n    æ ¹æ®æ–‡æ¡£æ¨¡å¼è§£æç« èŠ‚ID\n\
          \    :param chapter_id: ç« èŠ‚IDå­—ç¬¦ä¸²\n    :param pattern: æ–‡æ¡£æ¨¡å¼ ('alpha_first'\
          \ æˆ– 'numeric_first')\n    \"\"\"\n    chapter_id = chapter_id.strip()\n\n\
          \    # å­—æ¯ç« èŠ‚æ ¼å¼ - æ”¯æŒç‚¹å’Œæ¨ªçº¿åˆ†éš”ç¬¦\n    if re.fullmatch(r'[A-Z](?:[.\\-]\\d+)*[.\\\
          -]?', chapter_id):\n        # ç»Ÿä¸€å¤„ç†ç‚¹å’Œæ¨ªçº¿åˆ†éš”ç¬¦\n        normalized = re.sub(r'[.\\\
          -]+', '.', chapter_id).rstrip('.')\n        parts = normalized.split('.')\n\
          \        letter = parts[0]\n        \n        if pattern == 'alpha_first':\n\
          \            # å­—æ¯åœ¨å‰æ¨¡å¼ï¼šA=1, B=2, C=3, ...\n            letter_value = ord(letter)\
          \ - ord('A') + 1\n        else:\n            # æ•°å­—åœ¨å‰æ¨¡å¼ï¼šå­—æ¯ç« èŠ‚æ”¾åœ¨æ•°å­—ç« èŠ‚ä¹‹å\n   \
          \         # å‡è®¾æœ€å¤šæœ‰100ä¸ªæ•°å­—ç« èŠ‚ï¼Œå­—æ¯ä»101å¼€å§‹\n            letter_value = ord(letter)\
          \ - ord('A') + 101\n        \n        try:\n            rest = [int(p) for\
          \ p in parts[1:]] if len(parts) > 1 else []\n            return [letter_value]\
          \ + rest\n        except ValueError:\n            return []\n\n    # æ•°å­—ç« èŠ‚æ ¼å¼\
          \ - æ”¯æŒç‚¹å’Œæ¨ªçº¿åˆ†éš”ç¬¦\n    elif re.fullmatch(r'\\d+(?:[.\\-]\\d+)*[.\\-]?', chapter_id):\n\
          \        try:\n            # ç»Ÿä¸€å¤„ç†ç‚¹å’Œæ¨ªçº¿åˆ†éš”ç¬¦\n            normalized = re.sub(r'[.\\\
          -]+', '.', chapter_id).rstrip('.')\n            parts = normalized.split('.')\n\
          \            numeric_parts = [int(p) for p in parts]\n            \n   \
          \         if pattern == 'alpha_first':\n                # å­—æ¯åœ¨å‰æ¨¡å¼ï¼šæ•°å­—ç« èŠ‚æ”¾åœ¨å­—æ¯ç« èŠ‚ä¹‹å\n\
          \                # å‡è®¾æœ€å¤šæœ‰26ä¸ªå­—æ¯ç« èŠ‚ï¼Œæ•°å­—ä»27å¼€å§‹\n                numeric_parts[0]\
          \ += 26\n            # æ•°å­—åœ¨å‰æ¨¡å¼ï¼šä¿æŒåŸæœ‰æ•°å­—\n            \n            return numeric_parts\n\
          \        except ValueError:\n            return []\n\n    return []\n\n\
          def is_chapter_a_before_b(a: list[int], b: list[int]) -> bool:\n    for\
          \ i in range(min(len(a), len(b))):\n        if a[i] < b[i]:\n          \
          \  return True\n        elif a[i] > b[i]:\n            return False\n  \
          \  return len(a) < len(b)\n\ndef is_reasonable_chapter_jump(prev_id: List[int],\
          \ curr_id: List[int]) -> bool:\n    \"\"\"\n    åˆ¤æ–­ç« èŠ‚è·³è·ƒæ˜¯å¦åˆç†ï¼Œæ›´å®½æ¾çš„ç­–ç•¥ï¼š\n   \
          \ ä¸»è¦è¿‡æ»¤æ‰æ˜æ˜¾ä¸åˆç†çš„è·³è·ƒï¼Œä½†å…è®¸æ­£å¸¸çš„ç« èŠ‚ç»“æ„\n    å¯¹å­—æ¯é“¾å’Œæ•°å­—é“¾éƒ½è¿›è¡Œåˆç†æ€§åˆ¤æ–­\n    \"\"\"\n    if not\
          \ prev_id or not curr_id:\n        return True  # å¦‚æœæ— æ³•è§£æï¼Œé»˜è®¤å…è®¸\n    \n  \
          \  # å¦‚æœæ˜¯ä¸åŒå±‚çº§ï¼Œä¸€èˆ¬éƒ½æ˜¯åˆç†çš„ï¼ˆå¦‚ 1. -> 1.1 æˆ– 1.1 -> 2.ï¼‰\n    if len(prev_id) != len(curr_id):\n\
          \        return True\n    \n    # åŒå±‚çº§çš„æƒ…å†µä¸‹ï¼Œæ£€æŸ¥è·³è·ƒå¹…åº¦\n    if len(prev_id) ==\
          \ 1:  # ä¸€çº§ç« èŠ‚\n        prev_num = prev_id[0]\n        curr_num = curr_id[0]\n\
          \        diff = curr_num - prev_num\n        \n        # åˆ¤æ–­æ˜¯å¦ä¸ºå­—æ¯ç« èŠ‚ï¼ˆç¼–ç èŒƒå›´101-126å¯¹åº”A-Zï¼‰\n\
          \        if prev_num >= 101 and curr_num >= 101:  # å­—æ¯ç« èŠ‚\n            #\
          \ å­—æ¯è·³è·ƒæ£€æŸ¥ï¼šä¸å…è®¸è·¨è¶Šè¶…è¿‡2ä¸ªå­—æ¯ï¼ˆå¦‚Bè·³åˆ°Eä»¥ä¸Šï¼‰\n            return 1 <= diff <= 2\n     \
          \   else:  # æ•°å­—ç« èŠ‚\n            return 1 <= diff <= 5  # å…è®¸è·³è·ƒ1-5ç« ï¼ˆè¿‡æ»¤æ‰ä»5è·³åˆ°100è¿™ç§æ˜æ˜¾é”™è¯¯çš„ï¼‰\n\
          \    \n    elif len(prev_id) == 2:  # äºŒçº§ç« èŠ‚\n        # å¦‚æœç¬¬ä¸€çº§ç›¸åŒï¼Œæ£€æŸ¥ç¬¬äºŒçº§çš„è·³è·ƒ\n\
          \        if prev_id[0] == curr_id[0]:\n            prev_num = prev_id[1]\n\
          \            curr_num = curr_id[1]\n            diff = curr_num - prev_num\n\
          \            \n            # åˆ¤æ–­ç¬¬ä¸€çº§æ˜¯å¦ä¸ºå­—æ¯ç« èŠ‚\n            if prev_id[0] >=\
          \ 101:  # å­—æ¯ç« èŠ‚çš„å­çº§\n                return 1 <= diff <= 5  # å­—æ¯ç« èŠ‚çš„å­çº§è·³è·ƒç¨å¾®å®½æ¾ä¸€äº›\n\
          \            else:  # æ•°å­—ç« èŠ‚çš„å­çº§\n                return 1 <= diff <= 10  #\
          \ äºŒçº§ç« èŠ‚å…è®¸æ›´å¤§è·³è·ƒ\n        else:\n            # ä¸åŒçš„ä¸€çº§ç« èŠ‚ï¼Œéƒ½åˆç†\n            return\
          \ True\n    \n    else:  # ä¸‰çº§åŠä»¥ä¸Šç« èŠ‚\n        # å¯¹äºæ·±å±‚æ¬¡ç« èŠ‚ï¼Œæ›´å®½æ¾ä¸€äº›\n        return\
          \ True\n\ndef analyze_chapter_number_distribution(chapters: List[Dict])\
          \ -> Dict[str, int]:\n    \"\"\"\n    åˆ†æç« èŠ‚ç¼–å·çš„æ•°å­—åˆ†å¸ƒï¼Œç¡®å®šåˆç†çš„æ•°å­—èŒƒå›´\n    è¿”å›: {\"\
          min_reasonable\": æœ€å°åˆç†æ•°å­—, \"max_reasonable\": æœ€å¤§åˆç†æ•°å­—, \"primary_range\"\
          : ä¸»è¦æ•°å­—èŒƒå›´}\n    \"\"\"\n    first_numbers = []\n    \n    for ch in chapters:\n\
          \        chapter_id = ch[\"chapter_id\"].strip()\n        # æå–ç¬¬ä¸€ä¸ªæ•°å­—\n  \
          \      m_num = re.match(r'^(\\d+)', chapter_id)\n        if m_num:\n   \
          \         first_numbers.append(int(m_num.group(1)))\n        # å¤„ç†APPENDIXåè·Ÿæ•°å­—çš„æƒ…å†µ\n\
          \        elif chapter_id.upper().startswith(\"APPENDIX\"):\n           \
          \ suffix = chapter_id[len(\"APPENDIX\"):].strip(\" ()\")\n            if\
          \ suffix.isdigit():\n                first_numbers.append(int(suffix))\n\
          \    \n    if not first_numbers:\n        return {\"min_reasonable\": 1,\
          \ \"max_reasonable\": 50, \"primary_range\": (1, 50)}\n    \n    first_numbers.sort()\n\
          \    \n    # åˆ†ææ•°å­—åˆ†å¸ƒæ¨¡å¼\n    from collections import Counter\n    counter\
          \ = Counter(first_numbers)\n    \n    # å¦‚æœå¤§å¤šæ•°ç« èŠ‚éƒ½æ˜¯åŒä¸€ä¸ªæ•°å­—å¼€å¤´ï¼ˆå¦‚60.1, 60.2, 60.3...ï¼‰ï¼Œè¿™å¯èƒ½æ˜¯æ³•è§„ç¼–å·\n\
          \    most_common = counter.most_common(1)[0]\n    most_common_num, most_common_count\
          \ = most_common\n    \n    # å¦‚æœæŸä¸ªæ•°å­—å‡ºç°æ¬¡æ•°è¶…è¿‡æ€»æ•°çš„60%ï¼Œä¸”è¿™ä¸ªæ•°å­—å¤§äº30ï¼Œå¯èƒ½æ˜¯æ³•è§„ç¼–å·æ¨¡å¼\n  \
          \  if most_common_count > len(first_numbers) * 0.6 and most_common_num >\
          \ 30:\n        print(f\"æ£€æµ‹åˆ°å¯èƒ½çš„æ³•è§„ç¼–å·æ¨¡å¼: {most_common_num}.x (å‡ºç°{most_common_count}æ¬¡)\"\
          )\n        # åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå…è®¸è¿™ä¸ªç‰¹å®šçš„æ³•è§„ç¼–å·\n        return {\n            \"min_reasonable\"\
          : most_common_num, \n            \"max_reasonable\": most_common_num, \n\
          \            \"primary_range\": (most_common_num, most_common_num),\n  \
          \          \"regulation_mode\": True,\n            \"regulation_number\"\
          : most_common_num\n        }\n    \n    # æ­£å¸¸çš„ç« èŠ‚ç¼–å·æ¨¡å¼\n    min_num = min(first_numbers)\n\
          \    max_num = max(first_numbers)\n    \n    # å¦‚æœæ•°å­—èŒƒå›´å¾ˆå°ï¼ˆ<= 50ï¼‰ï¼Œè®¤ä¸ºæ˜¯æ­£å¸¸ç« èŠ‚\n\
          \    if max_num <= 50:\n        return {\n            \"min_reasonable\"\
          : max(1, min_num), \n            \"max_reasonable\": min(50, max_num + 5),\
          \  # å…è®¸å°‘é‡è¶…å‡º\n            \"primary_range\": (min_num, max_num)\n       \
          \ }\n    \n    # å¦‚æœæ•°å­—èŒƒå›´å¾ˆå¤§ï¼Œå¯èƒ½åŒ…å«é¡µç ç­‰å¹²æ‰°ï¼Œé‡‡ç”¨æ›´ä¿å®ˆç­–ç•¥\n    # æ‰¾åˆ°æœ€å¯†é›†çš„æ•°å­—åŒºé—´\n    gaps\
          \ = []\n    for i in range(len(first_numbers) - 1):\n        gaps.append(first_numbers[i\
          \ + 1] - first_numbers[i])\n    \n    # å¦‚æœæœ‰æ˜æ˜¾çš„å¤§è·³è·ƒï¼ˆ>20ï¼‰ï¼Œå¯èƒ½å‰é¢æ˜¯æ­£å¸¸ç« èŠ‚ï¼Œåé¢æ˜¯é¡µç ç­‰\n\
          \    large_gap_idx = -1\n    for i, gap in enumerate(gaps):\n        if\
          \ gap > 20:\n            large_gap_idx = i\n            break\n    \n  \
          \  if large_gap_idx != -1:\n        # å–è·³è·ƒå‰çš„æ•°å­—ä½œä¸ºåˆç†èŒƒå›´\n        reasonable_max\
          \ = first_numbers[large_gap_idx]\n        return {\n            \"min_reasonable\"\
          : max(1, min_num), \n            \"max_reasonable\": reasonable_max,\n \
          \           \"primary_range\": (min_num, reasonable_max)\n        }\n  \
          \  \n    # é»˜è®¤ä¿å®ˆç­–ç•¥\n    return {\"min_reasonable\": 1, \"max_reasonable\"\
          : 50, \"primary_range\": (1, 50)}\n\ndef evaluate_content_richness_and_decide_abandonment(chapters:\
          \ List[Dict], chain_indices: List[int]) -> bool:\n    \"\"\"è¯„ä¼°ç« èŠ‚é“¾çš„å†…å®¹ä¸°åº¦ï¼Œå¹¶å†³å®šæ˜¯å¦æ”¾å¼ƒå½“å‰é“¾\"\
          \"\"\n    ############## å†…å®¹ä¸°åº¦è¯„ä¼°é€»è¾‘ #################\n    def evaluate_content_richness(chain_chapters:\
          \ List[Dict]) -> Dict:\n        \"\"\"è¯„ä¼°ç« èŠ‚é“¾çš„å†…å®¹ä¸°åº¦\"\"\"\n        total_chars\
          \ = 0\n        chapters_with_content = 0\n        total_chapters = len(chain_chapters)\n\
          \        \n        # è®¡ç®—å¹³å‡æ ‡é¢˜é•¿åº¦\n        title_lengths = [len(ch.get('chapter_title',\
          \ '').strip()) for ch in chain_chapters]\n        avg_title_length = sum(title_lengths)\
          \ / len(title_lengths) if title_lengths else 0\n        \n        # ç»Ÿè®¡æœ‰å®è´¨å†…å®¹çš„ç« èŠ‚\n\
          \        for ch in chain_chapters:\n            raw_text = ch.get('raw_text',\
          \ '').strip()\n            title = ch.get('chapter_title', '').strip()\n\
          \            \n            # è®¡ç®—æ€»å­—ç¬¦æ•°ï¼ˆæ ‡é¢˜+æ­£æ–‡ï¼‰\n            total_chars += len(title)\
          \ + len(raw_text)\n            \n            # åˆ¤æ–­æ˜¯å¦æœ‰å®è´¨å†…å®¹\n            if\
          \ (len(raw_text) > 20 or  # æ­£æ–‡è¶…è¿‡20å­—ç¬¦\n                len(title) > 30 or\
          \     # æ ‡é¢˜è¶…è¿‡30å­—ç¬¦ï¼ˆå¯èƒ½æ˜¯æ®µè½ï¼‰\n                '.' in title or ',' in title or\
          \ 'ï¼›' in title or 'ã€‚' in title):  # åŒ…å«æ ‡ç‚¹ç¬¦å·\n                chapters_with_content\
          \ += 1\n        \n        # è®¡ç®—å„ç§æŒ‡æ ‡\n        avg_chars_per_chapter = total_chars\
          \ / total_chapters if total_chapters > 0 else 0\n        content_ratio =\
          \ chapters_with_content / total_chapters if total_chapters > 0 else 0\n\
          \        \n        # æ£€æµ‹æ˜¯å¦ä¸ºè¡¨æ ¼/è¡¨å•ç»“æ„ï¼ˆå¤§é‡çŸ­æ ‡é¢˜ï¼Œæ— å®è´¨å†…å®¹ï¼‰\n        short_titles = sum(1\
          \ for length in title_lengths if length <= 20)\n        short_title_ratio\
          \ = short_titles / total_chapters if total_chapters > 0 else 0\n       \
          \ \n        return {\n            'total_chars': total_chars,\n        \
          \    'avg_chars_per_chapter': avg_chars_per_chapter,\n            'content_ratio':\
          \ content_ratio,\n            'avg_title_length': avg_title_length,\n  \
          \          'short_title_ratio': short_title_ratio,\n            'total_chapters':\
          \ total_chapters,\n            'chapters_with_content': chapters_with_content\n\
          \        }\n    \n    # è¯„ä¼°å½“å‰æœ€é•¿é“¾çš„å†…å®¹ä¸°åº¦\n    chain_chapters = [chapters[i]\
          \ for i in chain_indices]\n    richness_metrics = evaluate_content_richness(chain_chapters)\n\
          \    \n    print(f\"å†…å®¹ä¸°åº¦è¯„ä¼°: {richness_metrics}\")\n    # å†³ç­–é€»è¾‘ï¼šå¦‚æœç« èŠ‚æ•°é‡å¤šä½†å†…å®¹è´«ä¹ï¼Œæ”¾å¼ƒå½“å‰é“¾\n\
          \    should_abandon_chain = (\n        richness_metrics['total_chapters']\
          \ > 10 and (  # ç« èŠ‚æ•°é‡å¤šçš„æƒ…å†µä¸‹\n            (richness_metrics['avg_chars_per_chapter']\
          \ < 30 and  # å¹³å‡å­—ç¬¦æ•°å¾ˆå°‘\n                richness_metrics['content_ratio']\
          \ < 0.4) or         # ä¸”æœ‰å®è´¨å†…å®¹æ¯”ä¾‹ä½\n                \n            (richness_metrics['short_title_ratio']\
          \ > 0.6 and     # æˆ–è€…çŸ­æ ‡é¢˜æ¯”ä¾‹å¾ˆé«˜\n                richness_metrics['avg_chars_per_chapter']\
          \ < 50)     # ä¸”å¹³å‡å­—ç¬¦æ•°ä¸å¤š\n        )\n    )\n    \n    return should_abandon_chain\n\
          \ndef find_longest_chapter_chain_with_append(chapters: List[Dict], language:\
          \ str = 'en') -> Tuple[List[Dict], str]:\n    # å…ˆæ£€æµ‹ç« èŠ‚æ¨¡å¼\n    pattern = detect_chapter_pattern(chapters)\n\
          \    print(f\"æ£€æµ‹åˆ°ç« èŠ‚æ¨¡å¼: {pattern}\")\n    \n    # \U0001F195 åˆ†æç« èŠ‚æ•°å­—åˆ†å¸ƒ\n \
          \   number_analysis = analyze_chapter_number_distribution(chapters)\n  \
          \  print(f\"ç« èŠ‚æ•°å­—åˆ†æç»“æœ: {number_analysis}\")\n    \n    # ç”¨æ£€æµ‹åˆ°çš„æ¨¡å¼é‡æ–°è§£æç« èŠ‚ID\n\
          \    parsed_ids = [parse_chapter_id(ch[\"chapter_id\"], pattern) for ch\
          \ in chapters]\n    # print(f'ç¬¬ä¸€ä¸ªç« èŠ‚: {chapters[0]}')\n    n = len(chapters)\n\
          \n    # ç¬¬ä¸€æ­¥ï¼šè¿‡æ»¤æ‰æ˜æ˜¾ä¸åˆç†çš„ç« èŠ‚ï¼ˆå¦‚è¯¯è¯†åˆ«çš„æ•°å­—ï¼‰\n    valid_indices = []\n    for i in range(n):\n\
          \        if not parsed_ids[i]:\n            continue\n            \n   \
          \     # æ£€æŸ¥æ˜¯å¦æ˜¯æ˜æ˜¾çš„è¯¯è¯†åˆ«\n        chapter_text = chapters[i][\"chapter_id\"]\
          \ + \" \" + chapters[i][\"chapter_title\"]\n        \n        # è¿‡æ»¤æ˜æ˜¾çš„æµ‹é‡å•ä½ã€é¢‘ç‡èŒƒå›´ã€çº¯æ•°å­—ç­‰\n\
          \        if re.search(r'\\b\\d+\\s*(MHz|GHz|Hz|kHz|dB|V|mV|ÂµV|A|mA|ÂµA|W|mW|Î©|%|Â°C|Â°F|mm|cm|m|km|kg|g|mg|ms|s|min|h|rpm|bar|Pa|kPa|MPa)\\\
          b', chapter_text, re.I):\n            continue\n        if re.search(r'\\\
          d+\\s*MHz\\s*[~-]\\s*\\d+\\s*MHz', chapter_text, re.I):\n            continue\n\
          \        if re.match(r'^\\d+\\s*$', chapters[i][\"chapter_title\"].strip()):\
          \  # æ ‡é¢˜æ˜¯çº¯æ•°å­—\n            continue\n        if len(chapters[i][\"chapter_title\"\
          ].strip()) < 2:  # æ ‡é¢˜å¤ªçŸ­\n            continue\n            \n        # \U0001F195\
          \ è¡¨æ ¼æ•°æ®ç‰¹å¾è¿‡æ»¤\n        chapter_title = chapters[i][\"chapter_title\"].strip()\n\
          \        chapter_id = chapters[i][\"chapter_id\"].strip()\n        \n  \
          \      # æ£€æµ‹è¡¨æ ¼è¡Œæ¨¡å¼ï¼šå•ä¸ªå­—æ¯ + ä¸»è¦æ˜¯æ•°å­—çš„æ ‡é¢˜\n        if (len(chapter_id) == 1 and chapter_id.isupper()\
          \ and \n            re.search(r'^\\d+.*\\d+', chapter_title) and \n    \
          \        len([x for x in chapter_title.split() if x.isdigit()]) >= 2):\n\
          \            continue\n            \n        # æ£€æµ‹åæ ‡ç‚¹æ ¼å¼ï¼šå¦‚ \"10 0 E 0 16\"\
          \n        if re.match(r'^\\d+\\s+\\d+\\s+[A-Z]\\s+\\d+\\s+\\d+', chapter_title):\n\
          \            continue\n            \n        # æ£€æµ‹å‚æ•°è¡¨æ ¼æ ¼å¼ï¼šå¦‚ \"34 65 F 25 77\"\
          \n        title_parts = chapter_title.split()\n        if (len(title_parts)\
          \ >= 4 and \n            sum(1 for part in title_parts if part.isdigit())\
          \ >= 3 and\n            sum(1 for part in title_parts if len(part) == 1\
          \ and part.isupper()) >= 1):\n            continue\n            \n     \
          \   # æ£€æµ‹å›¾è¡¨æ ‡æ³¨è¯´æ˜ï¼šå•ä¸ªå­—æ¯ + ä»¥ç ´æŠ˜å·å¼€å¤´çš„æ ‡é¢˜\n        if (len(chapter_id) == 1 and chapter_id.isupper()\
          \ and \n            chapter_title.startswith('â€”â€”â€”')):\n            continue\n\
          \            \n        valid_indices.append(i)\n    \n    # ç¬¬äºŒæ­¥ï¼šéªŒè¯å­—æ¯ç« èŠ‚çš„åˆç†æ€§ï¼ˆé’ˆå¯¹ä¸­è‹±æ–‡å·®å¼‚åŒ–å¤„ç†ï¼‰\n\
          \    if valid_indices:\n        # æ£€æŸ¥æ˜¯å¦åŒ…å«å­—æ¯ç« èŠ‚\n        alpha_chapters = []\n\
          \        for i, idx in enumerate(valid_indices):\n            chapter_id\
          \ = chapters[idx][\"chapter_id\"].strip()\n            if re.match(r'^[A-Z](?:\\\
          .\\d+)*\\.?$', chapter_id):\n                alpha_chapters.append((i, idx,\
          \ chapter_id[0]))  # (åœ¨valid_indicesä¸­çš„ä½ç½®, åŸå§‹ç´¢å¼•, é¦–å­—æ¯)\n        \n       \
          \ # å¦‚æœæœ‰å­—æ¯ç« èŠ‚ï¼Œè¿›è¡Œåˆç†æ€§éªŒè¯\n        if alpha_chapters:\n            if language\
          \ == 'en':\n                # è‹±æ–‡æ–‡æ¡£ï¼šè¦æ±‚å­—æ¯ç« èŠ‚å¿…é¡»ä»Aå¼€å¤´\n                first_alpha_letter\
          \ = alpha_chapters[0][2]\n                if first_alpha_letter != 'A':\n\
          \                    print(f\"è‹±æ–‡æ–‡æ¡£å­—æ¯ç« èŠ‚ä¸ä»¥Aå¼€å¤´ï¼Œè·³è¿‡: ç¬¬ä¸€ä¸ªå­—æ¯ç« èŠ‚æ˜¯ {alpha_chapters[0][2]}\"\
          )\n                    # ç§»é™¤æ‰€æœ‰å­—æ¯ç« èŠ‚\n                    alpha_indices_set\
          \ = {item[1] for item in alpha_chapters}\n                    valid_indices\
          \ = [idx for idx in valid_indices if idx not in alpha_indices_set]\n   \
          \         else:\n                # ä¸­æ–‡æ–‡æ¡£ï¼šæ£€æŸ¥å­—æ¯ç« èŠ‚çš„ä¸€è‡´æ€§ï¼ˆåŒä¸€é™„å½•åº”è¯¥ä»¥åŒä¸€å­—æ¯å¼€å¤´ï¼‰\n    \
          \            from collections import Counter\n                alpha_letters\
          \ = [item[2] for item in alpha_chapters]\n                letter_counter\
          \ = Counter(alpha_letters)\n                most_common_letter, most_common_count\
          \ = letter_counter.most_common(1)[0]\n                \n               \
          \ # å¦‚æœæŸä¸ªå­—æ¯å‡ºç°æ¬¡æ•°è¶…è¿‡60%ï¼Œè®¤ä¸ºè¿™æ˜¯ä¸»è¦çš„é™„å½•å­—æ¯\n                if most_common_count >\
          \ len(alpha_chapters) * 0.6:\n                    print(f\"ä¸­æ–‡æ–‡æ¡£æ£€æµ‹åˆ°ä¸»è¦é™„å½•å­—æ¯:\
          \ {most_common_letter} (å‡ºç°{most_common_count}æ¬¡)\")\n                   \
          \ # ä¿ç•™ä¸ä¸»è¦å­—æ¯ä¸€è‡´çš„ç« èŠ‚ï¼Œç§»é™¤å…¶ä»–å­—æ¯ç« èŠ‚\n                    keep_alpha_indices = {item[1]\
          \ for item in alpha_chapters if item[2] == most_common_letter}\n       \
          \             remove_alpha_indices = {item[1] for item in alpha_chapters\
          \ if item[2] != most_common_letter}\n                    valid_indices =\
          \ [idx for idx in valid_indices if idx not in remove_alpha_indices]\n  \
          \                  if remove_alpha_indices:\n                        removed_letters\
          \ = {chapters[idx][\"chapter_id\"].strip()[0] for idx in remove_alpha_indices}\n\
          \                        print(f\"ç§»é™¤ä¸ä¸€è‡´çš„å­—æ¯ç« èŠ‚: {removed_letters}\")\n   \
          \             else:\n                    # å¦‚æœæ²¡æœ‰æ˜æ˜¾çš„ä¸»è¦å­—æ¯ï¼Œä¿æŒåŸæœ‰é€»è¾‘ï¼ˆå¯èƒ½æ˜¯æ··åˆæƒ…å†µï¼‰\n\
          \                    print(f\"ä¸­æ–‡æ–‡æ¡£å­—æ¯ç« èŠ‚åˆ†å¸ƒè¾ƒå‡åŒ€: {dict(letter_counter)}\")\n\
          \                    # ä¸åšç‰¹æ®Šå¤„ç†ï¼Œä¿ç•™æ‰€æœ‰å­—æ¯ç« èŠ‚\n\n    # ç¬¬ä¸‰æ­¥ï¼šä»åå¾€å‰æ„å»ºæœ€é•¿é“¾\n    dp =\
          \ [1] * len(valid_indices)\n    next_link = [-1] * len(valid_indices)  #\
          \ æ”¹ä¸ºè®°å½•ä¸‹ä¸€ä¸ªèŠ‚ç‚¹\n    max_len = 0\n    max_idx = -1\n\n    # ä»åå¾€å‰éå†\n    for\
          \ i in range(len(valid_indices) - 1, -1, -1):\n        curr_idx = valid_indices[i]\n\
          \        curr_parsed = parsed_ids[curr_idx]\n        \n        # æ‰¾åœ¨å½“å‰èŠ‚ç‚¹ä¹‹åçš„æ‰€æœ‰èŠ‚ç‚¹\n\
          \        for j in range(i + 1, len(valid_indices)):\n            next_idx\
          \ = valid_indices[j]\n            next_parsed = parsed_ids[next_idx]\n \
          \           \n            # æ£€æŸ¥å½“å‰èŠ‚ç‚¹æ˜¯å¦å¯ä»¥è¿åˆ°ä¸‹ä¸€ä¸ªèŠ‚ç‚¹\n            if (is_chapter_a_before_b(curr_parsed,\
          \ next_parsed) and \n                is_reasonable_chapter_jump(curr_parsed,\
          \ next_parsed)):\n                if dp[j] + 1 > dp[i]:\n              \
          \      dp[i] = dp[j] + 1\n                    next_link[i] = j\n       \
          \ \n        if dp[i] > max_len:\n            max_len = dp[i]\n         \
          \   max_idx = i\n\n    # ç¬¬å››æ­¥ï¼šå¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆç†çš„é“¾ï¼Œé€€å›åˆ°ç®€å•çš„é¡ºåºè¿‡æ»¤\n    if max_len < 2:\n\
          \        # ç®€å•æŒ‰ç« èŠ‚ç¼–å·é¡ºåºè¿‡æ»¤\n        filtered_chapters = simple_chapter_filter(chapters)\n\
          \        \n        # å¦‚æœè¿‡æ»¤åè¿˜æ˜¯æ²¡æœ‰ç« èŠ‚ï¼Œå°†æ‰€æœ‰å†…å®¹æ”¾å…¥è·³è¿‡çš„å†…å®¹ä¸­\n        if not filtered_chapters:\n\
          \            all_content = []\n            for ch in chapters:\n       \
          \         content = f\"{ch['chapter_id']} {ch['chapter_title']}\"\n    \
          \            if ch.get('raw_text'):\n                    content += \" \"\
          \ + ch['raw_text']\n                all_content.append(content)\n      \
          \      skipped_text = \"\\n\".join(all_content)\n            return [],\
          \ skipped_text\n        \n        return filtered_chapters, \"\"\n\n   \
          \ # å›æº¯å‡ºä¸»é“¾ç´¢å¼•ï¼ˆä»å‰å¾€åçš„æ­£ç¡®é¡ºåºï¼‰\n    chain_indices = []\n    idx = max_idx\n    while\
          \ idx != -1:\n        chain_indices.append(valid_indices[idx])\n       \
          \ idx = next_link[idx]\n    \n    print(f\"ä»åå¾€å‰ç”Ÿæˆçš„æœ€é•¿é“¾: é•¿åº¦={len(chain_indices)},\
          \ ä½ç½®={chain_indices[:5]}{'...' if len(chain_indices)>5 else ''}\")\n   \
          \ \n    # æœ€é•¿é“¾çš„ç¬¬ä¸€ä¸ªç« èŠ‚ç´¢å¼•\n    first_chain_idx = chain_indices[0]\n    \n  \
          \  # ç”Ÿæˆè·³è¿‡çš„å†…å®¹ï¼ˆæœ€é•¿é“¾ç¬¬ä¸€ä¸ªç« èŠ‚ä¹‹å‰çš„æ‰€æœ‰å†…å®¹ï¼‰\n    skipped_chapters = chapters[:first_chain_idx]\n\
          \    skipped_text = \"\\n\".join([f\"{ch['chapter_id']} {ch['chapter_title']}\
          \ {ch.get('raw_text','')}\" for ch in skipped_chapters])\n    \n    chain_set\
          \ = set(chain_indices)\n    \n    # \U0001F195 æ·»åŠ å†…å®¹ä¸°åº¦æ£€æŸ¥\n    if language\
          \ == 'en' and evaluate_content_richness_and_decide_abandonment(chapters,\
          \ chain_indices):\n        # å°†æ‰€æœ‰ç« èŠ‚å†…å®¹åˆå¹¶ä¸ºè·³è¿‡çš„å†…å®¹\n        print(\"=====================================\"\
          )\n        print(\"å†…å®¹ä¸°åº¦è¯„ä¼°å†³å®šæ”¾å¼ƒå½“å‰é“¾ï¼Œè¿”å›ç©ºç»“æœ\")\n        print(\"=====================================\"\
          )\n        all_content = []\n        for ch in chapters:\n            content\
          \ = f\"{ch['chapter_id']} {ch['chapter_title']}\"\n            if ch.get('raw_text'):\n\
          \                content += \" \" + ch['raw_text']\n            all_content.append(content)\n\
          \        skipped_text = \"\\n\".join(all_content)\n        \n        return\
          \ [], skipped_text\n    \n    # æœ€ç»ˆç»“æœæ„å»º\n    result = []\n    last_valid\
          \ = None\n    for i, chap in enumerate(chapters):\n        if i in chain_set:\n\
          \            result.append(chap)\n            last_valid = chap\n      \
          \  elif i >= first_chain_idx:  # åªå¤„ç†æœ€é•¿é“¾å¼€å§‹ä¹‹åçš„ç« èŠ‚\n            if last_valid:\n\
          \                content_to_add = \"\\n\" + chap[\"chapter_id\"] + chap[\"\
          chapter_title\"]\n                if chap.get(\"raw_text\"):\n         \
          \           content_to_add += \" \" + chap[\"raw_text\"]\n             \
          \   last_valid[\"raw_text\"] += content_to_add\n\n    # åˆ¤æ–­ç« èŠ‚æ ‡é¢˜æ˜¯å¦åº”è¯¥åˆå¹¶åˆ°æ­£æ–‡ä¸­\n\
          \    for chap in result:\n        should_merge = False\n        \n     \
          \   # ä¸­æ–‡å¤„ç†ï¼šåŒ…å«ä¸­æ–‡ä¸”æœ‰ä¸­æ–‡é€—å·ï¼Œå¥å·ï¼Œå†’å·ï¼Œæˆ–è€…é•¿åº¦å¤§äº30å­—ç¬¦\n        if re.search(r'[\\u4e00-\\\
          u9fa5]', chap[\"chapter_title\"]):\n            # è·å–ç« èŠ‚ç¼–å·çš„ç¬¬ä¸€ä¸ªæ•°å­—ï¼Œå‰ä¸‰ç« è·³è¿‡åˆå¹¶åˆ¤æ–­\n\
          \            first_num = None\n            chapter_id = chap[\"chapter_id\"\
          ].strip('.-')\n            if re.match(r'^\\d+', chapter_id):\n        \
          \        first_num = int(re.match(r'^\\d+', chapter_id).group())\n     \
          \       # å‰ä¸‰ç« è·³è¿‡åˆå¹¶åˆ¤æ–­\n            if first_num is not None and first_num\
          \ <= 3:\n                continue          \n            if re.search(r'[ï¼Œã€‚ï¼š,:]',\
          \ chap[\"chapter_title\"]) or len(chap[\"chapter_title\"]) > 30:\n     \
          \           should_merge = True\n        \n        # è‹±æ–‡å¤„ç†ï¼šæ›´æ™ºèƒ½çš„åˆ¤æ–­é€»è¾‘\n   \
          \     else:\n            # å¦‚æœå…¨å¤§å†™ï¼Œåˆ™è‚¯å®šæ˜¯æ ‡é¢˜\n            if chap[\"chapter_title\"\
          ].isupper():\n                continue\n            # 1. å¦‚æœraw_textä»¥å°å†™å­—æ¯å¼€å¤´ï¼Œå¯èƒ½æ˜¯æ ‡é¢˜çš„å»¶ç»­\n\
          \            if len(chap[\"raw_text\"]) and chap[\"raw_text\"][0].islower():\n\
          \                should_merge = True\n            # 2. å¦‚æœchapter_titleåŒ…å«å®Œæ•´å¥å­çš„ç‰¹å¾\n\
          \            elif re.search(r'[,;!?]', chap[\"chapter_title\"]):\n     \
          \           should_merge = True\n            # 3. å¦‚æœchapter_titleå¾ˆé•¿ï¼ˆè¶…è¿‡50ä¸ªå­—ç¬¦ï¼‰ï¼Œå¯èƒ½æ˜¯æ®µè½æ–‡æœ¬\n\
          \            elif len(chap[\"chapter_title\"]) > 50:\n                should_merge\
          \ = True\n        \n        if should_merge:\n            chap[\"raw_text\"\
          ] = chap[\"chapter_title\"] + ' ' + chap[\"raw_text\"]\n            chap[\"\
          chapter_title\"] = \"\"\n\n    return result, skipped_text\n\ndef simple_chapter_filter(chapters:\
          \ List[Dict]) -> List[Dict]:\n    \"\"\"\n    ç®€å•çš„ç« èŠ‚è¿‡æ»¤ç­–ç•¥ï¼šå½“æœ€é•¿é“¾ç®—æ³•å¤±æ•ˆæ—¶çš„å¤‡ç”¨æ–¹æ¡ˆ\n\
          \    \"\"\"\n    # æ£€æµ‹ç« èŠ‚æ¨¡å¼\n    pattern = detect_chapter_pattern(chapters)\n\
          \    \n    result = []\n    parsed_ids = [parse_chapter_id(ch[\"chapter_id\"\
          ], pattern) for ch in chapters]\n    \n    for i, chap in enumerate(chapters):\n\
          \        parsed_id = parsed_ids[i]\n        \n        # åŸºæœ¬åˆç†æ€§æ£€æŸ¥\n      \
          \  if not parsed_id:\n            # æ— æ³•è§£æçš„ç« èŠ‚ï¼Œè¿½åŠ åˆ°ä¸Šä¸€ä¸ªæœ‰æ•ˆç« èŠ‚\n            if result:\n\
          \                content_to_add = \"\\n\" + chap[\"chapter_id\"] + chap[\"\
          chapter_title\"]\n                if chap.get(\"raw_text\"):\n         \
          \           content_to_add += \" \" + chap[\"raw_text\"]\n             \
          \   result[-1][\"raw_text\"] += content_to_add\n            continue\n \
          \       \n        # æ£€æŸ¥ç« èŠ‚ç¼–å·æ˜¯å¦åœ¨åˆç†èŒƒå›´å†…\n        first_num = parsed_id[0]\n \
          \       \n        # æ ¹æ®æ¨¡å¼è°ƒæ•´åˆç†æ€§æ£€æŸ¥\n        if pattern == 'alpha_first':\n\
          \            # å­—æ¯åœ¨å‰ï¼šA=1, B=2, ..., 1=27, 2=28, ...\n            if 1 <=\
          \ first_num <= 50:  # åˆç†èŒƒå›´ï¼š26ä¸ªå­—æ¯ + 20ä¸ªæ•°å­—ç« èŠ‚\n                result.append(chap)\n\
          \            else:\n                # ä¸åˆç†çš„ç« èŠ‚ï¼Œè¿½åŠ åˆ°ä¸Šä¸€ä¸ªæœ‰æ•ˆç« èŠ‚\n              \
          \  if result:\n                    content_to_add = \"\\n\" + chap[\"chapter_id\"\
          ] + chap[\"chapter_title\"]\n                    if chap.get(\"raw_text\"\
          ):\n                        content_to_add += \" \" + chap[\"raw_text\"\
          ]\n                    result[-1][\"raw_text\"] += content_to_add\n    \
          \    else:\n            # æ•°å­—åœ¨å‰ï¼š1, 2, ..., A=101, B=102, ...\n          \
          \  if (1 <= first_num <= 20) or (101 <= first_num <= 126):  # æ•°å­—ç« èŠ‚æˆ–å­—æ¯ç« èŠ‚\n\
          \                result.append(chap)\n            else:\n              \
          \  # ä¸åˆç†çš„ç« èŠ‚ï¼Œè¿½åŠ åˆ°ä¸Šä¸€ä¸ªæœ‰æ•ˆç« èŠ‚\n                if result:\n                   \
          \ content_to_add = \"\\n\" + chap[\"chapter_id\"] + chap[\"chapter_title\"\
          ]\n                    if chap.get(\"raw_text\"):\n                    \
          \    content_to_add += \" \" + chap[\"raw_text\"]\n                    result[-1][\"\
          raw_text\"] += content_to_add\n    \n    return result\n    \n    return\
          \ result\n\ndef split_sections_by_attachment(chapters: List[Dict]) -> List[Dict]:\n\
          \    \"\"\"\n    å°†æ•´ä¸ªæ–‡æ¡£æŒ‰é™„ä»¶ï¼ˆANNEXï¼‰åˆ‡åˆ†ã€‚\n    é¡¶å±‚ file: regulation / ANNEX n\n\
          \    æ”¹è¿›ï¼šåˆå¹¶è¿ç»­çš„ç›¸åŒé™„ä»¶æ ‡é¢˜\n    \"\"\"\n    sections = []\n    current_section\
          \ = {\n        \"section\": \"regulation\",  # é»˜è®¤ä¸»æ–‡æ¡£\n        \"chapters\"\
          : []\n    }\n\n    annex_pattern = re.compile(r'^(ANNEX|ATTACHMENT)\\s+([A-Z0-9]+)',\
          \ re.I)\n\n    for chap in chapters:\n        match = annex_pattern.match(chap['chapter_id'])\n\
          \        if match:\n            annex_name = match.group(1).upper() + \"\
          \ \" + match.group(2)  # æ ‡å‡†åŒ–åç§°ï¼Œå¦‚ \"ANNEX 1\"\n            \n           \
          \ # æ£€æŸ¥æ˜¯å¦ä¸å½“å‰ section çš„åç§°ç›¸åŒ\n            if current_section[\"section\"] !=\
          \ \"regulation\" and current_section[\"section\"].upper() == annex_name:\n\
          \                # ç›¸åŒçš„é™„ä»¶ï¼Œç›´æ¥æ·»åŠ åˆ°å½“å‰ sectionï¼Œè·³è¿‡é‡å¤çš„æ ‡é¢˜ç« èŠ‚\n                if chap.get('chapter_title')\
          \ or chap.get('raw_text', '').strip():\n                    current_section[\"\
          chapters\"].append(chap)\n                # å¦‚æœæ˜¯ç©ºçš„é‡å¤æ ‡é¢˜ç« èŠ‚ï¼ˆåªæœ‰chapter_idæ²¡æœ‰å†…å®¹ï¼‰ï¼Œåˆ™è·³è¿‡\n\
          \            else:\n                # ä¸åŒçš„é™„ä»¶ï¼Œä¿å­˜å½“å‰å—å¹¶æ–°å»º\n                if\
          \ current_section[\"chapters\"]:\n                    sections.append(current_section)\n\
          \                # æ–°å»ºé™„ä»¶å—\n                current_section = {\n        \
          \            \"section\": annex_name,\n                    \"chapters\"\
          : [chap] if (chap.get('chapter_title') or chap.get('raw_text', '').strip())\
          \ else []\n                }\n        else:\n            current_section[\"\
          chapters\"].append(chap)\n\n    if current_section[\"chapters\"]:\n    \
          \    sections.append(current_section)\n\n    return sections\n\ndef split_sections_by_appendix(chapters):\n\
          \    sections = []\n    current_section = {\"section\": \"MAIN\", \"chapters\"\
          : []}\n\n    for ch in chapters:\n        # æ£€æµ‹ APPENDIX å¼€å¤´çš„é¡¶å±‚æ ‡é¢˜ï¼Œæˆ–è€…é™„å½•\n \
          \       appendix_match = re.match(r'^(APPENDIX\\s+(?:[A-Z0-9]+|\\([A-Z0-9]+\\\
          )))$', ch['chapter_id'], re.IGNORECASE)\n        annex_match = ch[\"chapter_id\"\
          ].startswith(\"é™„å½•\")\n        \n        if appendix_match or annex_match:\n\
          \            # æ ‡å‡†åŒ–é™„å½•åç§°\n            if appendix_match:\n               \
          \ appendix_name = appendix_match.group(1).upper()\n            else:\n \
          \               appendix_name = ch['chapter_id'].strip()\n            \n\
          \            # æ£€æŸ¥æ˜¯å¦ä¸å½“å‰ section çš„åç§°ç›¸åŒ\n            if current_section[\"\
          section\"] != \"MAIN\" and current_section[\"section\"].upper() == appendix_name:\n\
          \                # ç›¸åŒçš„é™„å½•ï¼Œç›´æ¥æ·»åŠ åˆ°å½“å‰ sectionï¼ˆå¦‚æœæœ‰å®é™…å†…å®¹ï¼‰\n                if ch.get('chapter_title')\
          \ or ch.get('raw_text', '').strip():\n                    current_section[\"\
          chapters\"].append(ch)\n                # å¦‚æœæ˜¯ç©ºçš„é‡å¤æ ‡é¢˜ç« èŠ‚ï¼Œåˆ™è·³è¿‡\n            else:\n\
          \                # ä¸åŒçš„é™„å½•ï¼Œå…ˆä¿å­˜å½“å‰å—\n                if current_section[\"chapters\"\
          ]:\n                    sections.append(current_section)\n             \
          \   # æ–°å»ºé™„å½•å—\n                current_section = {\n                    \"\
          section\": appendix_name,\n                    \"chapters\": [ch] if (ch.get('chapter_title')\
          \ or ch.get('raw_text', '').strip()) else []\n                }\n      \
          \  else:\n            current_section[\"chapters\"].append(ch)\n\n    #\
          \ æœ«å°¾å—åŠ å…¥\n    if current_section[\"chapters\"]:\n        sections.append(current_section)\n\
          \n    # # æ‰“å°æå–çš„æ‰€æœ‰ç« èŠ‚æ ‡é¢˜\n    # for sec in sections:\n    #     print(f\"Section:\
          \ {sec['section']}\")\n    #     for chap in sec[\"chapters\"]:\n    # \
          \        print(f\"  Chapter ID: {chap['chapter_id']}, Title: {chap['chapter_title']}\"\
          )\n\n    return sections\n\ndef process_sections_with_lis(chapters, language='en'):\n\
          \    # å…ˆæ‹†åˆ†æˆæ­£æ–‡å’Œå¤šä¸ªé™„å½•\n    sections = split_sections_by_appendix(chapters)\n\
          \n    # æ¯ä¸ªéƒ¨åˆ†å†…éƒ¨å•ç‹¬è·‘æœ€é•¿é“¾\n    processed_sections = []\n    for sec in sections:\n\
          \        valid_chaps, skipped_content = find_longest_chapter_chain_with_append(sec[\"\
          chapters\"], language)\n        processed_sections.append({\n          \
          \  \"section\": sec[\"section\"],\n            \"context\": skipped_content,\
          \  # æ·»åŠ è¢«è·³è¿‡çš„å†…å®¹\n            \"chapters\": valid_chaps\n        })\n\n   \
          \ return processed_sections\n\ndef filter_start_of_main(chapters: List[Dict])\
          \ -> Tuple[List[Dict], str]:\n    \"\"\"\n    æ‰¾åˆ°ç¬¬ä¸€ä¸ªæ­£æ–‡ç« èŠ‚ä½œä¸ºèµ·ç‚¹ï¼Œè·³è¿‡ç›®å½•\n    \"\
          \"\"\n    start_index = 0\n    for i, chap in enumerate(chapters):\n   \
          \     chapter_id = chap.get(\"chapter_id\", \"\").strip()\n        # æ­£æ–‡ä¸»é“¾æˆ–é™„ä»¶å†…éƒ¨ç« èŠ‚ï¼šæ•°å­—å¼€å¤´æˆ–å­—æ¯å¼€å¤´\n\
          \        if chapter_id in {\"1\", \"1-\", \"1.\", \"A\", \"A.\", \"A.1\"\
          }:\n            # SCOPE / GENERAL / INTRO ç­‰éƒ½ç®—æ­£æ–‡èµ·ç‚¹\n            title_upper\
          \ = chap.get(\"chapter_title\", \"\").upper()\n            if any(k in title_upper\
          \ for k in [\"SCOPE\", \"GENERAL\", \"INTRO\", \"æ€»åˆ™\", \"èŒƒå›´\", \"LEGISLATIVE\"\
          , \"FUNCTION\"]):\n                start_index = i\n                break\n\
          \                \n        # ä¹Ÿæ£€æŸ¥æ ‡å‡†çš„ç« èŠ‚å¼€å¤´æ¨¡å¼\n        if re.match(r'^[A-Z](\\\
          .\\d+)*\\.?$', chapter_id) or re.match(r'^\\d+(\\.\\d+)*\\.?$', chapter_id):\n\
          \            title_upper = chap.get(\"chapter_title\", \"\").upper()\n \
          \           if any(k in title_upper for k in [\"SCOPE\", \"GENERAL\", \"\
          INTRO\", \"æ€»åˆ™\", \"èŒƒå›´\", \"LEGISLATIVE\", \"FUNCTION\"]):\n            \
          \    start_index = i\n                break\n                \n    # print(f'chapters[str]:\
          \ {chapters[start_index]}')\n    filtered_chapters = chapters[start_index:]\n\
          \    skipped_content = chapters[:start_index]\n    skipped_text = \"\\n\"\
          .join([f\"{ch['chapter_id']} {ch['chapter_title']} {ch.get('raw_text','')}\"\
          \ for ch in skipped_content])\n\n    return filtered_chapters, skipped_text\n\
          \n\ndef smart_paragraph_join(lines: List[str], language: str = 'en') ->\
          \ str:\n    \"\"\"\n    æ™ºèƒ½æ®µè½åˆå¹¶ï¼šåªåœ¨æ®µè½ç»“æŸæ—¶æ¢è¡Œ\n    \"\"\"\n    if not lines:\n\
          \        return \"\"\n    \n    result = []\n    current_paragraph = []\n\
          \    \n    for i, line in enumerate(lines):\n        line = line.strip()\n\
          \        if not line:  # ç©ºè¡Œç›´æ¥è·³è¿‡\n            continue\n            \n  \
          \      # æ£€æŸ¥æ˜¯å¦æ˜¯æ®µè½ç»“æŸçš„æ ‡å¿—\n        is_paragraph_end = False\n        \n    \
          \    # 1. ä»¥æ ‡ç‚¹ç¬¦å·ç»“å°¾ï¼ˆä¸­è‹±æ–‡ï¼‰\n        if re.search(r'[ã€‚ï¼ï¼Ÿï¼›ï¼š.!?;:]$', line):\n\
          \            is_paragraph_end = True\n            \n        # 2. æ£€æŸ¥ä¸‹ä¸€è¡Œæ˜¯å¦æ˜¯æ–°æ®µè½çš„å¼€å§‹\n\
          \        if i + 1 < len(lines):\n            next_line = lines[i + 1].strip()\n\
          \            # ä¸‹ä¸€è¡Œæ˜¯ç« èŠ‚æ ‡é¢˜ã€åˆ—è¡¨é¡¹ã€æˆ–æ˜æ˜¾çš„æ®µè½å¼€å§‹\n            if (detect_chapter(next_line)\
          \ or\n                re.match(r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\\d]+[ã€\\.\\)]', next_line)\
          \ or  # åˆ—è¡¨é¡¹\n                re.match(r'^[ï¼ˆ(]\\d+[ï¼‰)]', next_line) or  #\
          \ ç¼–å·é¡¹\n                re.match(r'^[â€”â€”\\-â€”]+', next_line)):  # ç ´æŠ˜å·å¼€å¤´\n \
          \               is_paragraph_end = True\n        \n        # 3. è¡¨æ ¼ç›¸å…³å†…å®¹ä¿æŒåŸæœ‰æ¢è¡Œ\n\
          \        if ('è¡¨' in line and re.search(r'è¡¨\\s*[A-Z0-9]', line)) or \\\n\
          \           re.match(r'^[|\\s]*[A-Za-z0-9\\u4e00-\\u9fa5]+[|\\s]*$', line):\
          \  # ç®€å•è¡¨æ ¼è¡Œæ£€æµ‹\n            current_paragraph.append(line)\n            is_paragraph_end\
          \ = True\n        else:\n            current_paragraph.append(line)\n  \
          \      \n        # å¦‚æœæ˜¯æ®µè½ç»“æŸï¼Œå°†å½“å‰æ®µè½åˆå¹¶å¹¶åŠ å…¥ç»“æœ\n        if is_paragraph_end:\n\
          \            if current_paragraph:\n                # ä¸­æ–‡çš„æ¢è¡Œä¸ç”¨ç©ºæ ¼è¿æ¥ï¼Œè‹±æ–‡çš„ç”¨ç©ºæ ¼è¿æ¥\n\
          \                if language == 'zh':\n                    paragraph_text\
          \ = ''.join(current_paragraph).strip()\n                else:\n        \
          \            paragraph_text = ' '.join(current_paragraph).strip()\n    \
          \            if paragraph_text:\n                    result.append(paragraph_text)\n\
          \                current_paragraph = []\n    \n    # å¤„ç†æœ€åå‰©ä½™çš„æ®µè½\n    if current_paragraph:\n\
          \        paragraph_text = ' '.join(current_paragraph).strip()\n        if\
          \ paragraph_text:\n            result.append(paragraph_text)\n    \n   \
          \ return '\\n'.join(result)\n\ndef parse_pdf_to_chapter_tree(pdf_path: str)\
          \ -> Tuple[List[Dict], Dict[str, str]]:\n    \"\"\"\n    ä» PDF ä¸­æå–ç« èŠ‚æ ‘å’Œæœ¯è¯­æ˜ å°„\n\
          \    :param pdf_path: PDF æ–‡ä»¶è·¯å¾„\n    :return: (ç« èŠ‚æ ‘, æœ¯è¯­æ˜ å°„)\n    \"\"\"\n \
          \   cleaned_lines = extract_full_text_with_filter(pdf_path)\n\n    # \U0001F195\
          \ æ£€æµ‹æ–‡æ¡£è¯­è¨€\n    language = detect_document_language(cleaned_lines)\n    max_chapter_num\
          \ = 50 if language == 'zh' else 1000\n    print(f\"æ£€æµ‹åˆ°æ–‡æ¡£è¯­è¨€: {'ä¸­æ–‡' if language\
          \ == 'zh' else 'è‹±æ–‡'}, max_chapter_num={max_chapter_num}\")\n\n    # \U0001F195\
          \ ç¬¬ä¸€è½®ï¼šç²—ç•¥æå–æ‰€æœ‰å¯èƒ½çš„ç« èŠ‚ï¼Œç”¨äºåˆ†ææ•°å­—åˆ†å¸ƒ\n    preliminary_chapters = []\n    current =\
          \ {\n        \"chapter_id\": \"\",\n        \"chapter_title\": \"\",\n \
          \       \"raw_text\": \"\"\n    }\n    buffer = []\n\n    for line in cleaned_lines:\n\
          \        # ç¬¬ä¸€è½®ä½¿ç”¨å®½æ¾çš„æ•°å­—èŒƒå›´è¿›è¡Œç²—æå–\n        chapter_info = detect_chapter(line,\
          \ max_chapter_num=1000, language=language, number_analysis=None)\n\n   \
          \     if chapter_info:\n            if current:\n                current[\"\
          raw_text\"] = smart_paragraph_join(buffer, language)\n                preliminary_chapters.append(current)\n\
          \                buffer = []\n            current = {\n                \"\
          chapter_id\": chapter_info[\"chapter_id\"],\n                \"chapter_title\"\
          : chapter_info[\"chapter_title\"],\n                \"raw_text\": \"\"\n\
          \            }\n        else:\n            buffer.append(line)\n\n    if\
          \ current:\n        current[\"raw_text\"] = smart_paragraph_join(buffer,\
          \ language)\n        preliminary_chapters.append(current)\n\n    # \U0001F195\
          \ åˆ†æç« èŠ‚æ•°å­—åˆ†å¸ƒ\n    number_analysis = analyze_chapter_number_distribution(preliminary_chapters)\n\
          \    print(f\"æ•°å­—åˆ†å¸ƒåˆ†æ: {number_analysis}\")\n\n    # \U0001F195 ç¬¬äºŒè½®ï¼šä½¿ç”¨åˆ†æç»“æœé‡æ–°ç²¾ç¡®æå–ç« èŠ‚\n\
          \    chapters = []\n    current = {\n        \"chapter_id\": \"\",\n   \
          \     \"chapter_title\": \"\",\n        \"raw_text\": \"\"\n    }\n    buffer\
          \ = []\n\n    for line in cleaned_lines:\n        chapter_info = detect_chapter(line,\
          \ max_chapter_num=max_chapter_num, language=language, number_analysis=number_analysis)\n\
          \n        if chapter_info:\n            if current:\n                # ä½¿ç”¨æ™ºèƒ½æ®µè½åˆå¹¶è€Œä¸æ˜¯ç®€å•çš„\
          \ \\n è¿æ¥\n                current[\"raw_text\"] = smart_paragraph_join(buffer,\
          \ language)\n                chapters.append(current)\n                buffer\
          \ = []\n            current = {\n                \"chapter_id\": chapter_info[\"\
          chapter_id\"],\n                \"chapter_title\": chapter_info[\"chapter_title\"\
          ],\n                \"raw_text\": \"\"\n            }\n        else:\n \
          \           buffer.append(line)\n\n    if current:\n        current[\"raw_text\"\
          ] = smart_paragraph_join(buffer, language)\n        chapters.append(current)\n\
          \n    # 1ï¸âƒ£ å…ˆæŒ‰é™„ä»¶åˆ‡åˆ†é¡¶å±‚\n    attachment_sections = split_sections_by_attachment(chapters)\n\
          \n    tree = []\n\n    for top_sec in attachment_sections:\n        \n \
          \       # 2ï¸âƒ£ æ¯ä¸ªé¡¶å±‚å—å†æŒ‰é™„å½•åˆ‡åˆ†\n        sections = split_sections_by_appendix(top_sec[\"\
          chapters\"])\n        section_tree_list = []\n\n        for sec in sections:\n\
          \            # filtered_chapters, skipped_text = filter_start_of_main(sec[\"\
          chapters\"])\n            # 3ï¸âƒ£ å¯¹æ¯ä¸ªéƒ¨åˆ†å†…éƒ¨ä¿ç•™æœ€é•¿é“¾\n            valid_chaps_in_sec,\
          \ skipped_text = find_longest_chapter_chain_with_append(sec[\"chapters\"\
          ], language)\n            \n            # \U0001F195 å¦‚æœæœ€é•¿é“¾æå–ç»“æœä¸ºç©ºï¼Œåˆ›å»ºè™šæ‹Ÿçš„ALLç« èŠ‚\n\
          \            if not valid_chaps_in_sec:\n                # å°†contextå†…å®¹ä½œä¸ºè™šæ‹Ÿç« èŠ‚çš„rawtext\n\
          \                virtual_chapter = {\n                    \"chapter_id\"\
          : \"ALL\",\n                    \"chapter_title\": \"\",\n             \
          \       \"raw_text\": skipped_text,\n                    \"full_path\":\
          \ \"ALL\",\n                    \"children\": []\n                }\n  \
          \              tree_in_sec = [virtual_chapter]\n                # æ¸…ç©ºcontextï¼Œå› ä¸ºå†…å®¹å·²ç»æ”¾å…¥è™šæ‹Ÿç« èŠ‚\n\
          \                skipped_text = \"\"\n            else:\n              \
          \  tree_in_sec = build_tree(valid_chaps_in_sec)\n                build_full_path(tree_in_sec)\n\
          \            \n            # æ’å…¥é”®å€¼å¯¹ section\n            section_tree_list.append({\n\
          \                \"section\": sec[\"section\"],\n                \"context\"\
          : skipped_text,\n                \"chapters\": tree_in_sec,\n          \
          \  })\n\n        # 4ï¸âƒ£ æ„å»ºé¡¶å±‚æ ‘\n        tree.append({\n            \"file\"\
          : top_sec[\"section\"],  # regulation æˆ– ANNEX n\n            \"sections\"\
          : section_tree_list,\n        })\n\n    term_map = {}\n\n    for chap in\
          \ chapters:\n        title = chap.get(\"chapter_title\", \"\")\n       \
          \ if \"æœ¯è¯­\" in title:\n            # æå–æœ¯è¯­\n            terms = extract_terms_with_abbr_from_terms_section(chap[\"\
          chapter_title\"])\n            term_map.update(terms)\n            for child\
          \ in chap.get(\"children\", []):\n                terms = extract_terms_with_abbr_from_terms_section(child[\"\
          chapter_title\"])\n                term_map.update(terms)\n        elif\
          \ \"ç¼©ç•¥\" in title:\n            # æå–ç¼©ç•¥è¯­\n            abbr_terms = extract_abbr_terms_from_symbols_section(chap[\"\
          chapter_title\"] + chap[\"raw_text\"])\n            term_map.update(abbr_terms)\n\
          \            for child in chap.get(\"children\", []):\n                abbr_terms\
          \ = extract_abbr_terms_from_symbols_section(child[\"chapter_title\"] + child[\"\
          raw_text\"])\n                term_map.update(abbr_terms)\n\n    return\
          \ tree, term_map\n\nimport re\nfrom collections import defaultdict\nfrom\
          \ typing import List, Dict\nimport pdfplumber\n\ndef _build_page_lines_from_words(page,\
          \ y_tol=3):\n    \"\"\"\n    ç”¨ page.extract_words() æ„å»ºè¡Œï¼šæŒ‰ top åˆ†æ¡¶ï¼ˆy_tol å®¹å·®ï¼‰ï¼Œæ¯è¡ŒæŒ‰\
          \ x0 æ’åºå¹¶åˆå¹¶æ–‡æœ¬ï¼Œ\n    è¿”å›åˆ—è¡¨ï¼š{'text', 'y', 'x0', 'x1'}\n    \"\"\"\n    words\
          \ = page.extract_words()  # è¿”å›æ¯ä¸ª word å¸¦ x0,x1,top,bottom,text\n    if not\
          \ words:\n        return []\n\n    # æŒ‰ top, x0 æ’åº\n    words = sorted(words,\
          \ key=lambda w: (w['top'], w['x0']))\n\n    lines = []\n    cur = None\n\
          \    for w in words:\n        top = w['top']\n        x0 = float(w['x0'])\n\
          \        x1 = float(w['x1'])\n        text = w['text']\n\n        if cur\
          \ is None:\n            cur = {'text': text, 'y': top, 'x0': x0, 'x1': x1}\n\
          \            continue\n\n        if abs(top - cur['y']) <= y_tol:\n    \
          \        # åŒä¸€è¡Œï¼ŒæŒ‰ x é¡ºåºè¿æ¥ï¼ˆextract_words å·²æŒ‰ x æ’åºï¼Œä½†ä»åšä¿é™©ï¼‰\n            if x0\
          \ < cur['x1'] + 1:\n                # é‡å æˆ–ç´§é‚»ï¼Œç›´æ¥ç”¨ç©ºæ ¼éš”å¼€ï¼ˆé¿å…æŠŠè¯ç²˜åœ¨ä¸€èµ·ï¼‰\n        \
          \        cur['text'] = cur['text'] + ' ' + text\n            else:\n   \
          \             cur['text'] = cur['text'] + ' ' + text\n            cur['x1']\
          \ = max(cur['x1'], x1)\n            cur['x0'] = min(cur['x0'], x0)\n   \
          \         # keep y as average to be robust\n            cur['y'] = (cur['y']\
          \ + top) / 2.0\n        else:\n            lines.append(cur)\n         \
          \   cur = {'text': text, 'y': top, 'x0': x0, 'x1': x1}\n    if cur:\n  \
          \      lines.append(cur)\n    return lines\n\ndef _find_table_title_near_bbox(page,\
          \ table_bbox, max_above=60, y_tol=4):\n    \"\"\"\n    åœ¨è¡¨æ ¼ä¸Šæ–¹ max_above pt\
          \ çš„èŒƒå›´å†…æ‰¾å¯èƒ½çš„æ ‡é¢˜ï¼š\n    - å…ˆä» page.extract_words() ä¸­ç­›é€‰å‡ºè¯¥å‚ç›´å¸¦å†…å¹¶ä¸”ä¸è¡¨æ ¼æ°´å¹³æœ‰é‡å çš„ words\n\
          \    - æŒ‰ top/x0 åˆ†ç»„æˆè¡Œå¹¶æ‹¼æ¥ï¼Œè¿”å›æ‹¼å®Œçš„å­—ç¬¦ä¸²ï¼ˆå¯èƒ½åŒ…å«ç¼–å·ï¼‰\n    \"\"\"\n    table_top = float(table_bbox[1])\n\
          \    table_x0, table_x1 = float(table_bbox[0]), float(table_bbox[2])\n \
          \   words = page.extract_words()\n    if not words:\n        return None\n\
          \n    # ç­›é€‰ï¼šå‚ç›´åœ¨ (table_top - max_above, table_top + 10) èŒƒå›´å†…ï¼Œ\n    # åŒæ—¶æ°´å¹³ä¸Šè‡³å°‘ä¸è¡¨æ ¼å·¦å³æ‰©å±•\
          \ 50pt æœ‰é‡å ï¼ˆé˜²æ­¢å®Œå…¨é å·¦çš„æ ‡é¢˜è¢«å¿½ç•¥ï¼‰\n    relevant = []\n    margin_x = 60\n    for\
          \ w in words:\n        w_top = float(w['top'])\n        if not (table_top\
          \ - max_above <= w_top <= table_top + 10):\n            continue\n     \
          \   w_x0 = float(w['x0']); w_x1 = float(w['x1'])\n        # ä¸è¡¨æ ¼æ°´å¹³æŠ•å½±æœ‰é‡å  æˆ–\
          \ åœ¨è¡¨æ ¼å·¦ä¾§æ¥è¿‘ä½ç½®\n        if (w_x1 >= table_x0 - margin_x and w_x0 <= table_x1\
          \ + margin_x) or w_x0 < table_x0:\n            relevant.append(w)\n\n  \
          \  if not relevant:\n        return None\n\n    # å°†è¿™äº› words æŒ‰ top/x0 æ’åºï¼Œåˆ†è¡Œå¹¶åˆå¹¶\n\
          \    relevant = sorted(relevant, key=lambda w: (w['top'], w['x0']))\n  \
          \  lines = []\n    cur = None\n    for w in relevant:\n        top = w['top'];\
          \ x0 = float(w['x0']); x1 = float(w['x1']); txt = w['text']\n        if\
          \ cur is None:\n            cur = {'text': txt, 'y': top, 'x0': x0, 'x1':\
          \ x1}\n        else:\n            if abs(top - cur['y']) <= y_tol:\n   \
          \             cur['text'] = cur['text'] + ' ' + txt\n                cur['x1']\
          \ = max(cur['x1'], x1)\n                cur['x0'] = min(cur['x0'], x0)\n\
          \                cur['y'] = (cur['y'] + top) / 2.0\n            else:\n\
          \                lines.append(cur)\n                cur = {'text': txt,\
          \ 'y': top, 'x0': x0, 'x1': x1}\n    if cur:\n        lines.append(cur)\n\
          \n    # ç°åœ¨æŠŠè¿™äº›è¡Œæ‹¼æˆæœ€ç»ˆæ ‡é¢˜ï¼šæŒ‰ y ä»ä¸Šåˆ°ä¸‹ã€æŒ‰ x0 ä»å·¦åˆ°å³è¿æ¥\n    # ä½†ä¼˜å…ˆé€‰æ‹©åŒ…å« \"è¡¨\" çš„è¡Œæˆ–ä»¥ \"è¡¨\"\
          \ å¼€å¤´çš„è¡ŒåŠå…¶ç›¸é‚»è¡Œ\n    full = \" \".join([ln['text'] for ln in lines]).strip()\n\
          \n    # ä¼˜å…ˆç­–ç•¥ï¼šæ‰¾åˆ°åŒ…å«\"è¡¨\"çš„è¡Œï¼ˆæœ€é è¿‘è¡¨æ ¼çš„é‚£ä¸€è¡Œä¼˜å…ˆï¼‰\n    cand = None\n    candidates =\
          \ []\n    for ln in lines:\n        if 'è¡¨' in ln['text'] or 'Table' in ln['text']:\n\
          \            candidates.append((abs(table_top - ln['y']), ln))\n    if candidates:\n\
          \        # å–è·ç¦»æœ€å°çš„é‚£ä¸€è¡Œä½œä¸ºæ ¸å¿ƒï¼Œç„¶åæŠŠåŒä¸€ y å¸¦å†…çš„å…¶ä»–è¡Œåˆå¹¶ï¼ˆå·¦å³æ‰©å±•ï¼‰\n        candidates.sort(key=lambda\
          \ x: x[0])\n        core_y = candidates[0][1]['y']\n        # åˆå¹¶ä¸ core_y\
          \ æ¥è¿‘çš„æ‰€æœ‰è¡Œ\n        merge_parts = [ln['text'] for ln in lines if abs(ln['y']\
          \ - core_y) <= max(y_tol, 10)]\n        cand = \" \".join(merge_parts).strip()\n\
          \    else:\n        # æœªæ‰¾åˆ°åŒ…å«\"è¡¨\"çš„æ˜ç¡®è¡Œï¼Œå°±é€€å›ç”¨ fullï¼ˆå¯èƒ½å°±æ˜¯æ•´æ®µæ ‡é¢˜ï¼‰\n        cand =\
          \ full if full else None\n\n    # print(f\"æ‰¾åˆ°æ ‡é¢˜å€™é€‰: {cand}\")\n    # è¿›ä¸€æ­¥æ¸…ç†ï¼šæŠŠå¤šä½™ç©ºæ ¼ã€è¿ç»­å¤šä½™æ ‡ç‚¹æ•´ç†ä¸€ä¸‹\n\
          \    if cand:\n        cand = re.sub(r'\\s+', ' ', cand).strip()\n     \
          \   cand = re.sub(r'\\s*([ï¼Œ,ï¼š:ï¼›;])\\s*', r'\\1 ', cand)\n    return cand\n\
          \ndef extract_tables_from_pdf(pdf_path: str) -> List[Dict]:\n    \"\"\"\n\
          \    æ”¹è¿›ç‰ˆï¼šä»PDFä¸­æå–æ‰€æœ‰è¡¨æ ¼åŠå…¶æ ‡è¯†ï¼Œå°½é‡æ¢å¤ 'è¡¨F.1' è¿™ç±»ç¼–å·\n    è¿”å›æ ¼å¼: [{\"table_id\": \"\
          è¡¨X.x æ ‡é¢˜\", \"table_content\": äºŒç»´æ•°ç»„}, ...]\n    \"\"\"\n    all_tables =\
          \ []\n\n    with pdfplumber.open(pdf_path) as pdf:\n        for page_num,\
          \ page in enumerate(pdf.pages):\n            # ä½¿ç”¨ extract_words æ„å»ºé¡µé¢è¡Œï¼ˆä¹Ÿä¿ç•™åŸå§‹\
          \ words ç”¨äºæ›´ç²¾ç»†åˆ¤æ–­ï¼‰\n            page_lines = _build_page_lines_from_words(page,\
          \ y_tol=3)\n\n            # æå–å¹¶æ’åºè¡¨æ ¼\n            tables = page.find_tables()\n\
          \            if not tables:\n                continue\n            tables\
          \ = sorted(tables, key=lambda t: t.bbox[1])\n\n            for table_idx,\
          \ table in enumerate(tables):\n                # è¿‡æ»¤ï¼šåªæœ‰ 1 åˆ—çš„ç›´æ¥ä¸¢å¼ƒ\n      \
          \          table_data = table.extract()\n                if not table_data:\n\
          \                    continue\n                sample_row = table_data[0]\n\
          \                if len(sample_row) <= 1:\n                    continue\n\
          \n                cleaned_data = [\n                    [cell.replace('\\\
          n', ' ').strip() if cell else \"\" for cell in row]\n                  \
          \  for row in table_data\n                ]\n\n                # ä¼˜å…ˆé€šè¿‡ page_lines\
          \ æ‰¾æ ‡é¢˜\n                table_top = table.bbox[1]\n                best_line\
          \ = None\n                min_gap = float('inf')\n\n                # å…ˆå°è¯•æ›´ç¨³å¥çš„æ–¹å¼ï¼šä»\
          \ words åŒºåŸŸæ”¶é›†å¹¶æ‹¼æ¥æ ‡é¢˜\n                cand = _find_table_title_near_bbox(page,\
          \ table.bbox, max_above=60, y_tol=4)\n                if cand:\n       \
          \             best_line = cand\n                else:\n                \
          \    # é€€å›åˆ°åŸå…ˆé€»è¾‘ï¼šåœ¨ page_lines ä¸­æ‰¾åŒ…å« \"è¡¨\" çš„è¡Œï¼ˆè·ç¦»æœ€è¿‘çš„ï¼‰\n                    for\
          \ item in page_lines:\n                        if (\"è¡¨\" in item['text']\
          \ or 'Table' in item['text']) and 0 < (table_top - item['y']) < 60:\n  \
          \                          gap = table_top - item['y']\n               \
          \             if gap < min_gap:\n                                min_gap\
          \ = gap\n                                best_line = item['text']\n\n  \
          \              # å…œåº•ï¼šå¦‚æœè¿˜æ˜¯æ²¡æœ‰ç¼–å·ï¼Œæ£€æŸ¥è¡¨æ ¼ç¬¬ä¸€è¡Œçš„å•å…ƒæ ¼é‡Œæ˜¯å¦æœ‰â€œè¡¨Xâ€æ ·å¼\n                if best_line\
          \ is None:\n                    first_row = cleaned_data[0]\n          \
          \          # æŠŠç¬¬ä¸€è¡Œæ‰€æœ‰å•å…ƒæ ¼æ‹¼èµ·æ¥æŸ¥æ‰¾â€œè¡¨â€å…³é”®è¯\n                    joined_first = \"\
          \ \".join(first_row).strip()\n                    if re.search(r'è¡¨\\s*[A-Z0-9]\\\
          .?\\d*', joined_first) or joined_first.startswith('è¡¨'):\n              \
          \          best_line = joined_first\n\n                # å¦‚æœæ‰¾åˆ°äº†æ ‡é¢˜åˆ™æ–°å¢ï¼Œå¦åˆ™ç”¨å…œåº•\
          \ id\n                if best_line:\n                    # è¿›ä¸€æ­¥åšå°æ¸…æ´—ï¼šå°† \"\
          è¡¨  F.1\" ç­‰ä¸­é—´å¤šä½™ç©ºæ ¼å»æ‰ï¼ˆä¿ç•™è¡¨å­—å’Œç¼–å·ï¼‰\n                    best_line = re.sub(r'è¡¨\\\
          s+([A-Za-z0-9])', r'è¡¨\\1', best_line)\n                    all_tables.append({\n\
          \                        \"table_id\": best_line,\n                    \
          \    \"table_content\": cleaned_data\n                    })\n         \
          \       else:\n                    table_id = f\"è¡¨-é¡µ{page_num + 1}-è¡¨{table_idx\
          \ + 1}\"\n                    all_tables.append({\n                    \
          \    \"table_id\": table_id,\n                        \"table_content\"\
          : cleaned_data\n                    })\n\n    return all_tables\n\n\nimport\
          \ tempfile\nimport requests\nfrom urllib.parse import urlparse\n\ndef resolve_pdf_path(pdf_path:\
          \ str) -> str:\n    # å¦‚æœæ˜¯ URLï¼Œå°±ä¸‹è½½åˆ°ä¸´æ—¶æ–‡ä»¶å¤¹\n    if pdf_path.startswith(\"http://\"\
          ) or pdf_path.startswith(\"https://\"):\n        response = requests.get(pdf_path)\n\
          \        response.raise_for_status()\n        suffix = os.path.splitext(urlparse(pdf_path).path)[-1]\n\
          \        with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as\
          \ tmp_file:\n            tmp_file.write(response.content)\n            return\
          \ tmp_file.name\n    else:\n        return pdf_path\n\ndef count_leaf_nodes(chapters):\n\
          \    \"\"\"é€’å½’è®¡ç®—ç« èŠ‚æ ‘ä¸­çš„å¶å­èŠ‚ç‚¹æ•°é‡\"\"\"\n    total = 0\n    for chapter in chapters:\n\
          \        if chapter.get(\"children\") and len(chapter[\"children\"]) > 0:\n\
          \            # æœ‰å­èŠ‚ç‚¹ï¼Œé€’å½’è®¡ç®—\n            total += count_leaf_nodes(chapter[\"\
          children\"])\n        else:\n            # å¶å­èŠ‚ç‚¹\n            total += 1\n\
          \    return total\n\ndef extract_chapters_by_id(tree, chapter_ids):\n  \
          \  \"\"\"ä»æ ‘ç»“æ„ä¸­æå–æŒ‡å®šIDçš„ç« èŠ‚ï¼Œä¿æŒåŸæœ‰å±‚çº§ç»“æ„\"\"\"\n    result = []\n    \n    for file_item\
          \ in tree:\n        # åªå¤„ç† file ä¸º regulation çš„å†…å®¹\n        if file_item[\"\
          file\"] != \"regulation\":\n            continue\n            \n       \
          \ new_file = {\n            \"file\": file_item[\"file\"],\n           \
          \ \"sections\": []\n        }\n        \n        for section in file_item[\"\
          sections\"]:\n            if section[\"section\"] != \"MAIN\":\n       \
          \         continue\n            new_section = {\n                \"section\"\
          : section[\"section\"],\n                # \"context\": section[\"context\"\
          ],\n                \"chapters\": []\n            }\n            \n    \
          \        # æå–åŒ¹é…çš„ç« èŠ‚\n            for chapter in section[\"chapters\"]:\n\
          \                if chapter[\"chapter_id\"] in chapter_ids:\n          \
          \          new_section[\"chapters\"].append(chapter)\n            \n   \
          \         # åªæœ‰å½“sectionæœ‰ç« èŠ‚æ—¶æ‰æ·»åŠ \n            if new_section[\"chapters\"]:\n\
          \                new_file[\"sections\"].append(new_section)\n        \n\
          \        # åªæœ‰å½“fileæœ‰sectionsæ—¶æ‰æ·»åŠ \n        if new_file[\"sections\"]:\n  \
          \          result.append(new_file)\n    \n    return result\n\ndef get_first_three_chapters_from_main(tree)\
          \ -> set:\n    \"\"\"\n    å–å‰ 5 ä¸ªä¸€çº§ç« èŠ‚é‡Œï¼Œ\n    æ ‡é¢˜å« scope/èŒƒå›´ æˆ– definition/æœ¯è¯­\
          \ çš„ chapter_idï¼Œ\n    æœ‰å‡ ä¸ªæ‹¿å‡ ä¸ªï¼Œä¸é™åˆ¶å”¯ä¸€ã€‚\n    \"\"\"\n    main_chaps = []\n  \
          \  for file_item in tree:\n        if file_item[\"file\"] != \"regulation\"\
          :\n            continue\n        for sec in file_item[\"sections\"]:\n \
          \           if sec[\"section\"] == \"MAIN\":\n                # åªä¿ç•™ä¸€çº§\n\
          \                main_chaps = [\n                    ch for ch in sec[\"\
          chapters\"]\n                    if re.fullmatch(r\"^\\d+$|^[A-Z]$|^(APPENDIX|ANNEX)\\\
          s+[A-Z0-9]+$\",\n                                    ch.get(\"chapter_id\"\
          , \"\").strip().rstrip(\".-\"), re.I)\n                ]\n             \
          \   break\n        break\n\n    wanted = set()\n    for ch in main_chaps[:5]:\
          \          # çœ‹å‰ 5 ä¸ªä¸€çº§\n        t = ch.get(\"chapter_title\", \"\").lower()\n\
          \        if any(k in t for k in (\"scope\", \"èŒƒå›´\", \"definition\", \"æœ¯è¯­\"\
          )):\n            wanted.add(ch[\"chapter_id\"])\n    return wanted\n\ndef\
          \ group_main_chapters(tree):\n    \"\"\"\n    æ¯ä¸ªä¸€çº§ç« èŠ‚å•ç‹¬ä½œä¸ºä¸€ç»„\n    \"\"\"\n\
          \    groups = []\n\n    # æ‰¾åˆ° regulation file çš„ MAIN section\n    for file_item\
          \ in tree:\n        if file_item[\"file\"] != \"regulation\":\n        \
          \    continue\n\n        for section in file_item[\"sections\"]:\n     \
          \       if section[\"section\"] == \"MAIN\":\n                for chapter\
          \ in section[\"chapters\"]:\n                    groups.append([chapter[\"\
          chapter_id\"]])\n                break\n        break\n\n    return groups\n\
          \ndef get_non_main_sections(tree):\n    \"\"\"è·å–æ‰€æœ‰é MAIN çš„ sections\"\"\"\
          \n    non_main_sections = []\n    \n    for file_item in tree:\n       \
          \ if file_item[\"file\"] != \"regulation\":\n            continue\n    \
          \        \n        for section in file_item[\"sections\"]:\n           \
          \ if section[\"section\"] != \"MAIN\":\n                non_main_sections.append(section[\"\
          section\"])\n    \n    return non_main_sections\n\ndef create_section_tree(tree,\
          \ section_name):\n    \"\"\"åˆ›å»ºåŒ…å«æŒ‡å®š section çš„å®Œæ•´æ ‘ç»“æ„\"\"\"\n    result = []\n\
          \    \n    for file_item in tree:\n        if file_item[\"file\"] != \"\
          regulation\":\n            continue\n            \n        new_file = {\n\
          \            \"file\": file_item[\"file\"],\n            \"sections\": []\n\
          \        }\n        \n        for section in file_item[\"sections\"]:\n\
          \            if section[\"section\"] == section_name:\n                new_file[\"\
          sections\"].append(section)\n                break\n        \n        if\
          \ new_file[\"sections\"]:\n            result.append(new_file)\n       \
          \     break\n    \n    return result\n\ndef count_leaf_nodes(chapters):\n\
          \    \"\"\"é€’å½’ç»Ÿè®¡ç« èŠ‚æ ‘çš„å¶å­èŠ‚ç‚¹æ•°é‡\"\"\"\n    count = 0\n    for chapter in chapters:\n\
          \        if chapter.get(\"children\"):\n            count += count_leaf_nodes(chapter[\"\
          children\"])\n        else:\n            count += 1\n    return count\n\n\
          import copy\n\ndef split_array_items_if_needed(array_items, max_leaf_nodes=150):\n\
          \    \"\"\"\n    éå†æœ€ç»ˆçš„ array_itemsï¼Œå¦‚æœæŸä¸ª item è¶…è¿‡ max_leaf_nodesï¼Œ\n    åˆ™æŒ‰äºŒçº§ç« èŠ‚åˆ†ç»„ï¼ˆä¿è¯äºŒçº§ç« èŠ‚å®Œæ•´ä¿ç•™ï¼Œä¸èƒ½æ‹†å¼€ï¼‰\n\
          \    \"\"\"\n    new_array = []\n    for item in array_items:\n        tree\
          \ = json.loads(item)\n        chapters = tree[0][\"sections\"][0][\"chapters\"\
          ] if tree else []\n        leaf_count = count_leaf_nodes(chapters)\n\n \
          \       if leaf_count <= max_leaf_nodes:\n            new_array.append(item)\n\
          \        else:\n            big_chapter = chapters[0]\n            if big_chapter.get(\"\
          children\"):  # äºŒçº§ç« èŠ‚å­˜åœ¨\n                group = []\n                group_count\
          \ = 0\n                for child in big_chapter[\"children\"]:  # éå† 6.1ã€6.2\
          \ ...\n                    child_count = count_leaf_nodes([child])\n\n \
          \                   # å¦‚æœåŠ ä¸Šå½“å‰ child è¶…è¿‡é™åˆ¶ï¼Œåˆ™æŠŠä¹‹å‰çš„ group ä¿å­˜\n               \
          \     if group_count + child_count > max_leaf_nodes and group:\n       \
          \                 new_tree = copy.deepcopy(tree)\n                     \
          \   new_tree[0][\"sections\"][0][\"chapters\"] = [\n                   \
          \         copy.deepcopy(big_chapter)\n                        ]\n      \
          \                  new_tree[0][\"sections\"][0][\"chapters\"][0][\"children\"\
          ] = group\n                        new_array.append(json.dumps(new_tree,\
          \ ensure_ascii=False))\n\n                        # å¼€å¯æ–°ç»„\n             \
          \           group = []\n                        group_count = 0\n\n    \
          \                # æŠŠ child æ”¾è¿›å½“å‰ç»„\n                    group.append(child)\n\
          \                    group_count += child_count\n\n                # ä¿å­˜æœ€åä¸€ç»„\n\
          \                if group:\n                    new_tree = copy.deepcopy(tree)\n\
          \                    new_tree[0][\"sections\"][0][\"chapters\"] = [\n  \
          \                      copy.deepcopy(big_chapter)\n                    ]\n\
          \                    new_tree[0][\"sections\"][0][\"chapters\"][0][\"children\"\
          ] = group\n                    new_array.append(json.dumps(new_tree, ensure_ascii=False))\n\
          \            else:\n                # æ²¡æœ‰äºŒçº§å­ç« èŠ‚ï¼Œæ— æ³•å†æ‹†ï¼Œä¿æŒåŸæ ·\n              \
          \  new_array.append(item)\n\n    return new_array\n\n\ndef main(url: str)\
          \ -> dict:\n    pdf_path = resolve_pdf_path(url)  # å…¼å®¹ URL å’Œæœ¬åœ°è·¯å¾„\n\n   \
          \ # ç« èŠ‚æ ‘\n    tree, term_map = parse_pdf_to_chapter_tree(pdf_path)\n    \n\
          \    # æå–å‰ä¸‰ç« çš„ chapter_idï¼ˆä» MAIN sectionï¼‰\n    first_three_chapter_ids =\
          \ get_first_three_chapters_from_main(tree)\n    \n    # æå–å‰ä¸‰ç« ä½œä¸ºcontext\n\
          \    context_tree = extract_chapters_by_id(tree, first_three_chapter_ids)\n\
          \    \n    # æŒ‰å¶å­èŠ‚ç‚¹æ•°é‡åˆ†ç»„ MAIN section çš„å…¶ä½™ç« èŠ‚\n    main_chapter_groups = group_main_chapters(tree)\n\
          \    \n    # # è·å–æ‰€æœ‰é MAIN çš„ sections\n    # non_main_sections = get_non_main_sections(tree)\n\
          \    \n    # ä¸ºæ¯ä¸ª MAIN ç« èŠ‚ç»„å’Œé MAIN section åˆ›å»ºå®Œæ•´çš„æ ‘ç»“æ„\n    array_items = []\n\
          \    \n    # å¤„ç† MAIN section çš„ç« èŠ‚ç»„\n    for group_chapter_ids in main_chapter_groups:\n\
          \        group_tree = extract_chapters_by_id(tree, set(group_chapter_ids))\n\
          \        if group_tree:  # ç¡®ä¿ç»„ä¸ä¸ºç©º\n            array_items.append(json.dumps(group_tree,\
          \ ensure_ascii=False))\n    \n    # # å¤„ç†é MAIN sectionsï¼ˆæ¯ä¸ªé™„å½•ä½œä¸ºä¸€ä¸ªç‹¬ç«‹çš„ itemï¼‰\n\
          \    # for section_name in non_main_sections:\n    #     section_tree =\
          \ create_section_tree(tree, section_name)\n    #     if section_tree:  #\
          \ ç¡®ä¿sectionä¸ä¸ºç©º\n    #         array_items.append(json.dumps(section_tree,\
          \ ensure_ascii=False))\n\n    array_items = split_array_items_if_needed(array_items,\
          \ max_leaf_nodes=150)\n\n\n    return {\n        \"tree\":json.dumps(tree,\
          \ ensure_ascii=False),\n        \"context\": json.dumps(context_tree, ensure_ascii=False),\n\
          \        \"array\": array_items\n    }"
        code_language: python3
        desc: ''
        outputs:
          array:
            children: null
            type: array[string]
          context:
            children: null
            type: string
          tree:
            children: null
            type: string
        selected: false
        title: ä»£ç æ‰§è¡Œ
        type: code
        variables:
        - value_selector:
          - '1756550268945'
          - file
          - url
          value_type: file
          variable: url
      height: 53
      id: '1756550411122'
      position:
        x: 334
        y: 352
      positionAbsolute:
        x: 334
        y: 352
      selected: true
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        desc: ''
        error_handle_mode: continue-on-error
        height: 304
        is_parallel: true
        iterator_input_type: array[string]
        iterator_selector:
        - '1756550411122'
        - array
        output_selector:
        - '1756698812908'
        - output
        output_type: array[string]
        parallel_nums: 10
        selected: false
        start_node_id: 1756557801310start
        title: è¿­ä»£ 1A
        type: iteration
        width: 1724
      height: 304
      id: '1756557801310'
      position:
        x: 638
        y: 352
      positionAbsolute:
        x: 638
        y: 352
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 1724
      zIndex: 1
    - data:
        desc: ''
        isInIteration: true
        selected: false
        title: ''
        type: iteration-start
      draggable: false
      height: 48
      id: 1756557801310start
      parentId: '1756557801310'
      position:
        x: 60
        y: 100.5
      positionAbsolute:
        x: 698
        y: 452.5
      selectable: false
      sourcePosition: right
      targetPosition: left
      type: custom-iteration-start
      width: 44
      zIndex: 1002
    - data:
        context:
          enabled: true
          variable_selector:
          - '1756550411122'
          - context
        desc: ''
        isInIteration: true
        isInLoop: false
        iteration_id: '1756557801310'
        model:
          completion_params: {}
          mode: chat
          name: deepseek-reasoner
          provider: langgenius/deepseek/deepseek
        prompt_template:
        - id: b8df94d2-1037-41d9-8649-be6e2790a7d5
          role: system
          text: "# è§’è‰²\n\nä½ æ˜¯ä¸€åæŠ€æœ¯æ ‡å‡†çŸ¥è¯†å·¥ç¨‹ä¸“å®¶ï¼Œä¸“æ³¨äºå°†æ±½è½¦åŠç›¸å…³é¢†åŸŸçš„æ ‡å‡†å’Œæ³•è§„æ–‡æ¡£è½¬åŒ–ä¸ºå¯ç»“æ„åŒ–è§£æçš„æ•°æ®èµ„äº§ï¼Œç”¨äºæ„å»ºçŸ¥è¯†å›¾è°±ã€‚\
            \  \n\nä½ çš„ä»»åŠ¡æ˜¯åŸºäºè¾“å…¥çš„æ ‡å‡†æ–‡æ¡£ JSON æ ‘ç»“æ„ï¼Œç²¾å‡†æŠ½å–ç›¸å…³ä¿¡æ¯ï¼Œä»¥æ”¯æŒè·¨æ ‡å‡†æ¡æ¬¾æ¯”å¯¹ã€æ³•è§„ä¸€è‡´æ€§éªŒè¯ã€è®¾å¤‡èƒ½åŠ›è¯„ä¼°ç­‰ä¸‹æ¸¸åº”ç”¨\
            \ ï¼Œä¿è¯è·¨æ ‡å‡†å¯æ¯”æ€§ï¼ˆä¸åŒæ–‡ä»¶é—´åŒç±»æ¡æ¬¾èƒ½å¯¹é½ï¼‰\n\n---\n\n# è¾“å…¥å†…å®¹\n\nä½ å°†æ”¶åˆ°ä¸€ä¸ª JSON æ•°ç»„ï¼ŒåŒ…å«ä¸€ä¸ªå¯¹è±¡ï¼Œå…¶ç»“æ„å¦‚ä¸‹ï¼š\n\
            \n- `file`: æ–‡ä»¶æ ‡è¯†ï¼ˆå¦‚ \"regulation\"ã€\"ANNEX 1\"ï¼‰\n- `sections`: å—æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼š\n\
            \  - `section`: æ ‡è¯†å½“å‰å—ï¼ˆå¦‚ \"MAIN\", \"APPENDIX 1\"ï¼‰\n  - `chapters`: ç« èŠ‚æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼š\n\
            \    - `chapter_id`: ç« èŠ‚ç¼–å·ï¼ˆä¿æŒåŸæ ·ï¼‰\n    - `chapter_title`: ç« èŠ‚æ ‡é¢˜\n    - `raw_text`:\
            \ è¯¥èŠ‚çš„çº¯æ–‡æœ¬å†…å®¹\n    - `children`: å­æ¡æ¬¾æ•°ç»„ï¼ˆç»“æ„ä¸çˆ¶çº§ç›¸åŒï¼‰\n    - `full_path`: å®Œæ•´è·¯å¾„ï¼ˆå¯é€‰ï¼‰\n\
            \n> åŒæ—¶æä¾›èƒŒæ™¯ä¸Šä¸‹æ–‡ï¼ˆ{{#context#}}ï¼‰ï¼ŒåŒ…å«èŒƒå›´ã€æœ¯è¯­å®šä¹‰ç­‰ç« èŠ‚ï¼Œä¾¿äºç†è§£ã€‚\n---\n\n# è¾“å‡ºæ ¼å¼\n\nè¾“å‡ºå¿…é¡»æ˜¯çº¯å‡€ã€å®Œæ•´çš„JSONå¯¹è±¡ï¼Œç¡®ä¿å¯ä»¥è¢«`json.loads()`ç›´æ¥è§£æï¼Œç»“æ„å¦‚ä¸‹ï¼š\n\
            \n```json\n{\n  \"file\": \"string\",\n  \"section\": \"string\",\n  \"\
            chapters\":[\n    {\n      \"chapter_id\": \"string\",\n      \"scope\"\
            : \"string\",\n      \"topic_keywords\": [\"string\", \"...\"],\n    \
            \  \"context_keywords\": [\"string\", \"...\"]\n    }\n  ]\n}\n```\n\n\
            # å¤„ç†é€»è¾‘ï¼ˆé“¾å¼æ€è€ƒï¼‰\n\nè®©æˆ‘ä»¬ä¸€æ­¥ä¸€æ­¥æ€è€ƒï¼š\n\n1. **éå†ç« èŠ‚æ ‘**\n\n   éå†æ¯ä¸ª `chapter_id` åŠå…¶å­ç« èŠ‚ï¼Œä¾æ¬¡è¿›è¡Œåç»­æ“ä½œï¼Œæ˜ç¡®æå–çš„ç›®çš„æ˜¯ä¸ºäº†è·¨æ ‡å‡†åŒ¹é…(bge-m3å’Œbge-v2-reranker-m3)ã€‚\n\
            \n2. **è¯†åˆ«æ ¸å¿ƒä¸»ä½“**\n   \n    -   **è¿™æ˜¯æœ€é‡è¦çš„ä¸€æ­¥ï¼** é¦–å…ˆé—®è‡ªå·±ï¼šâ€œè¿™ä¸ªæ¡æ¬¾åœ¨è§„å®šæˆ–æè¿°ä»€ä¹ˆä¸œè¥¿ï¼Ÿâ€ï¼Œå¯ä»¥ç»“åˆèƒŒæ™¯ä¸Šä¸‹æ–‡ä¸­çš„èŒƒå›´ç« èŠ‚ï¼Œäº†è§£æ ¸å¿ƒä¸»ä½“æ˜¯ä»€ä¹ˆï¼Œæ¯”å¦‚â€è½¦è½½ç´§æ€¥å‘¼æ•‘ç³»ç»Ÿâ€œï¼Œâ€ç¯çš„å®‰è£…â€œç­‰ï¼Œç„¶åä»ç¬¬ä¸€å±‚ç« èŠ‚åˆ°å½“å‰ç« èŠ‚ï¼Œå±‚å±‚ç†æ¸…è„‰ç»œï¼Œå†ç»“åˆ`rawtext`ï¼Œå¿…è¦æ—¶å‚è€ƒ`children`ä¸­éƒ½è®²äº†ä»€ä¹ˆ\n\
            \    -   ä¾‹å¦‚ï¼šâ€è½¦è½½ç´§æ€¥å‘¼å«ç³»ç»Ÿçš„è‡ªåŠ¨è§¦å‘å®éªŒä¸­çš„æ»‘å°æ­£é¢ç¢°æ’è¯•éªŒçš„å®‰è£…æ­¥éª¤â€œï¼Œâ€åˆ¹è½¦ç¯çš„å®‰è£…ä½ç½®çš„å®½åº¦è¦æ±‚â€œï¼Œâ€æ°”ä½“æ±¡æŸ“ç‰©çš„æ’æ”¾è´¨é‡é™åˆ¶â€œ\n\
            \    -   ç„¶åå¸¦ç€å¯¹æ ¸å¿ƒä¸»ä½“çš„ç†è§£ï¼Œç»§ç»­ä¸‹é¢çš„æ­¥éª¤\n    \n3. **scopeæå–**\n\n   - å°†è¯†åˆ«åˆ°çš„æ ¸å¿ƒä¸»ä½“ï¼Œè§„èŒƒä¸ºç±»ä¼¼äºâ€[åˆ¶åŠ¨ç¯]\
            \ - [å®‰è£…] - [å®½åº¦,é«˜åº¦è¦æ±‚]â€œçš„æ ¼å¼ï¼Œä½œä¸ºscope\n   \n4. **topic_keywordsæå–**\n\n   -\
            \ 1â€“6 ä¸ªå…³é”®è¯ï¼Œç”¨äºå…·ä½“æ¦‚æ‹¬è¯¥å±‚ä¸»é¢˜\n   - æ¦‚æ‹¬å½“å‰æ¡æ¬¾çš„ç›´æ¥è§„å®šå¯¹è±¡ï¼Œæ¯”å¦‚å½“å‰ç« èŠ‚è®²äº†ä»€ä¹ˆæ±¡æŸ“ç‰©æ’æ”¾çš„è´¨é‡åœ¨ä»€ä¹ˆåŒºé—´æ—¶éœ€è¦è¿›è¡Œç¬¬äºŒæ¬¡å®éªŒï¼Œå¯ä»¥æ‹†åˆ†å…³é”®è¯ä¸º\"\
            test repetition condition\"ã€\"Gaseous pollutantsâ€ã€â€œmass-based emission\
            \ limits\"ï¼Œä½†ä¸å…è®¸ä»…æœ‰ç±»ä¼¼äºâ€æ–¹æ³•â€œè¿™ç§æè¿°ï¼ˆæ— æ³•ç†è§£å’ŒåŒ¹é…ï¼‰ï¼Œä¹Ÿå°±æ˜¯è¯´ä¸€å®šè¦èƒ½ç†è§£æ˜¯å¯¹ä»€ä¹ˆçš„è§„å®šæˆ–æè¿°ï¼Œä½†ä¸ºäº†è¡¨è¾¾çš„ç®€æ´ï¼Œå¯ä»¥æ‹†åˆ†æˆå¤šä¸ªå…³é”®è¯ã€‚\n\
            \   - éå¶å­èŠ‚ç‚¹ï¼šå¯ä»¥æ˜¯ä¸€ä¸ªç®€æ´ä¸»é¢˜è¯ï¼Œç”šè‡³ä¸ºç©ºï¼›å¶å­èŠ‚ç‚¹ï¼šåº”ç»†åŒ–åˆ°å…·ä½“è¦æ±‚æˆ–å®éªŒã€‚\n   - å¯¹é˜é‡Šæœ¯è¯­æˆ–å®šä¹‰çš„å­ç« èŠ‚ï¼Œä»…å°†è¯¥æœ¯è¯­ä½œä¸ºtopicï¼›èŒƒå›´ã€å¼•ç”¨æ€§æ–‡ä»¶å’Œæ“ä½œæ‰‹å†Œç­‰å…±æ€§ç« èŠ‚ä»…æå–æœ‰æ•ˆæ¯”å¯¹å†…å®¹ï¼Œä¾‹å¦‚â€œèŒƒå›´â€\n\
            \n5. **context_keywordsæå–**\n   \n   - 0â€“6 ä¸ªå…³é”®è¯ï¼Œè¿™éƒ¨åˆ†æ˜¯åº”ç”¨äºä¸»ä½“çš„**è¡Œä¸ºã€æ¡ä»¶ã€å±æ€§æˆ–æ–¹æ³•**ï¼Œæè¿°å®éªŒå¯¹è±¡ã€ç¯å¢ƒã€æ¡ä»¶ã€æœ¯è¯­å…³è”ç­‰ã€‚\n\
            \   - åœ¨ç¡®å®šäº†ä¸»ä½“ä¹‹åï¼Œå†é—®è‡ªå·±ï¼šâ€œè¿™ä¸ªæ¡æ¬¾å¯¹ä¸»ä½“**åšäº†ä»€ä¹ˆ**ï¼Ÿæˆ–è€…æè¿°äº†å®ƒçš„**ä»€ä¹ˆç‰¹æ€§**ï¼Ÿâ€\n    -   æ¯”å¦‚æ±¡æŸ“ç‰©æ¶‰åŠåˆ°ä¸€æ°§åŒ–ç¢³ã€ä¸€æ°§åŒ–æ°®ç­‰ï¼Œå°±å¯ä»¥æå–å‡ºæ¥ï¼Œè¿˜æœ‰â€å†·å¯åŠ¨â€œè¿™ç§å®éªŒæ¡ä»¶ï¼Œâ€è¡°å‡å› å­â€œç­‰å®éªŒç»†èŠ‚ç­‰ç­‰ï¼Œä¹Ÿå°±æ˜¯å¯ä»¥è¿›ä¸€æ­¥æé«˜åŒ¹é…å‡†ç¡®åº¦çš„å†…å®¹\n\
            \   - è‹¥ä¸Šä¸‹æ–‡å®šä¹‰äº†æœ¯è¯­ï¼Œåˆ™è¦ä¿ç•™ä¸æœ¯è¯­å…³è”çš„å…³é”®è¯ï¼ˆå¦‚ â€œGaseous pollutants â†’ CO, HC, NOxâ€ï¼‰ã€‚\n\
            \   - èŒƒå›´ã€å¼•ç”¨æ€§æ–‡ä»¶ç­‰çº²é¢†æ€§ç« èŠ‚ä¸åšæå–\n   \n6. **æ— æ•ˆä¿¡æ¯å‰”é™¤**\n\n   - å‰”é™¤å†…å®¹ï¼š\n        -\
            \   å…·ä½“çš„æ•°å€¼å’Œå•ä½ (å¦‚ \"10 per cent\", \"25 km/h\", \"CFC60\")ã€‚\n        - \
            \  å¯¹å…¶ä»–æ–‡ä»¶ã€ç« èŠ‚ã€å›¾è¡¨çš„å¼•ç”¨ (å¦‚ \"paragraph 5.3.1.4\", \"Table 1\", \"å›¾ B.1\",\"\
            Annex 1\",\"Appendix 1\",\"GSO standards\"ç­‰)ã€‚\n        -   å®½æ³›ã€æ— æ„ä¹‰çš„è¯ (å¦‚\
            \ \"è¦æ±‚\", \"è§„å®š\", \"æ–¹æ³•\", \"æµ‹è¯•\")ã€‚\n\n\n---\n\n# Few-shot ç¤ºä¾‹\n\n## è¾“å…¥ç¤ºä¾‹1\n\
            \n```\n[\n  {\n    \"file\": \"regulation\",\n    \"sections\": [\n  \
            \    {\n        \"section\": \"é™„å½•B\",\n        \"context\": \"(è§„èŒƒæ€§)è‡ªåŠ¨è§¦å‘è¯•éªŒæ–¹æ³•\"\
            ,\n        \"chapters\": [\n          {\n          {\n            \"chapter_id\"\
            : \"B.2\",\n            \"chapter_title\": \"è¯•éªŒé¡¹ç›®\",\n            \"raw_text\"\
            : \"\",\n            \"children\": [\n              {\n              \
            \  \"chapter_id\": \"B.2.1\",\n                \"chapter_title\": \"æ­£é¢ç¢°æ’\"\
            ,\n                \"raw_text\": \"\",\n                \"children\":\
            \ [\n                  {\n                    \"chapter_id\": \"B.2.1.1\"\
            ,\n                    \"chapter_title\": \"æ»‘å°æ­£é¢ç¢°æ’è¯•éªŒ\",\n            \
            \        \"raw_text\": \"\",\n                    \"children\": [\n  \
            \                    {\n                        \"chapter_id\": \"B.2.1.1.1\"\
            ,\n                        \"chapter_title\": \"\",\n                \
            \        \"raw_text\": \"å°†ç™½è½¦èº«æˆ–å·¥è£…å›ºå®šåœ¨ç¢°æ’è¯•éªŒæ»‘å°ä¸Š,å®‰è£…æ–¹å‘æ¨¡æ‹Ÿæ­£é¢ç¢°æ’ã€‚ \",\n         \
            \               \"children\": [],\n                        \"full_path\"\
            : \"B.2 è¯•éªŒé¡¹ç›®/B.2.1 æ­£é¢ç¢°æ’/B.2.1.1 æ»‘å°æ­£é¢ç¢°æ’è¯•éªŒ/B.2.1.1.1 \"\n              \
            \        },\n                      {\n                        \"chapter_id\"\
            : \"B.2.1.1.2\",\n                        \"chapter_title\": \"\",\n \
            \                       \"raw_text\": \"æ»‘å°æŒ‰ç…§ä»¥ä¸‹åŠ é€Ÿåº¦æ³¢å½¢ä¹‹ä¸€è¿›è¡Œç¢°æ’è¯•éªŒã€‚ a) ä½¿ç”¨åˆ¶é€ å•†æŒ‡å®šçš„åŠ é€Ÿåº¦æ³¢å½¢è¿›è¡Œè¯•éªŒ,æŒ‡å®šçš„åŠ é€Ÿåº¦æ³¢å½¢åº”ä¸ºåœ¨B.2.1.2ä¸­æè¿°çš„å®è½¦ç¢°æ’è¯•éªŒæ¡ä»¶ä¸­,è½¦èº«éå˜å½¢åŒºåŸŸé‡‡é›†çš„åŠ é€Ÿåº¦-æ—¶é—´æ›²çº¿,å¹¶ç»è¿‡æ»¤æ³¢ç­‰çº§CFC60\
            \ æ»¤æ³¢æˆ–100Hzä½é€šæ»¤æ³¢ã€‚å®é™…è¯•éªŒç»“æœæ³¢å½¢çš„ç§¯åˆ†é€Ÿåº¦å˜åŒ–é‡Î”vs( t)åº”åœ¨ä»»æ„æ—¶åˆ»,ä¸è¶…è¿‡æŒ‡å®šæ³¢å½¢çš„ç§¯åˆ†é€Ÿåº¦å˜åŒ–é‡[Î”vt( t)Â±1]km/hçš„èŒƒå›´ã€‚\\\
            nb) æŒ‰å›¾B.1 çš„æ ‡å‡†åŠ é€Ÿåº¦é€šé“èŒƒå›´å’Œè¡¨B.1 çš„å‚æ•°è¿›è¡ŒåŠ é€Ÿæˆ–å‡é€Ÿ,å…¶é€Ÿåº¦å˜åŒ–é‡Î”v ä¸º\\n(25Â±1)km/hã€‚\",\n   \
            \                     \"children\": [],\n                        \"full_path\"\
            : \"B.2 è¯•éªŒé¡¹ç›®/B.2.1 æ­£é¢ç¢°æ’/B.2.1.1 æ»‘å°æ­£é¢ç¢°æ’è¯•éªŒ/B.2.1.1.2 \"\n              \
            \        }\n                    ],\n                    \"full_path\"\
            : \"B.2 è¯•éªŒé¡¹ç›®/B.2.1 æ­£é¢ç¢°æ’/B.2.1.1 æ»‘å°æ­£é¢ç¢°æ’è¯•éªŒ\"\n                  },\n   \
            \             ],\n              },\n            ],\n          }\n    \
            \    ]\n      }\n    ]\n  }\n]\n```\n\n## è¾“å‡ºç¤ºä¾‹1\n\n```\n[\n{\n\"file\"\
            : \"regulation\",\n\"section\": \"é™„å½•B\",\n\"chapters\": [\n{\n\"chapter_id\"\
            : \"B.2\",\n\"scope\": \"[è‡ªåŠ¨è§¦å‘è¯•éªŒæ–¹æ³•]-[è¯•éªŒé¡¹ç›®]\",\n\"topic_keywords\": [\n\
            \"è‡ªåŠ¨è§¦å‘è¯•éªŒé¡¹ç›®\"\n],\n\"context_keywords\": []\n},\n{\n\"chapter_id\": \"\
            B.2.1\",\n\"scope\": \"[è‡ªåŠ¨è§¦å‘è¯•éªŒæ–¹æ³•]-[æ­£é¢ç¢°æ’]\",\n\"topic_keywords\": [\n\"\
            è‡ªåŠ¨è§¦å‘è¯•éªŒ\"\n\"æ­£é¢ç¢°æ’\"\n],\n\"context_keywords\": []\n},\n{\n\"chapter_id\"\
            : \"B.2.1.1\",\n\"scope\": \"[è‡ªåŠ¨è§¦å‘è¯•éªŒæ–¹æ³•]-[æ­£é¢ç¢°æ’]-[æ»‘å°æ­£é¢ç¢°æ’è¯•éªŒ]\",\n\"topic_keywords\"\
            : [\n\"è‡ªåŠ¨è§¦å‘è¯•éªŒ\"\n\"æ»‘å°æ­£é¢ç¢°æ’è¯•éªŒ\"\n],\n\"context_keywords\": []\n},\n{\n\"\
            chapter_id\": \"B.2.1.1.1\",\n\"scope\": \"[è‡ªåŠ¨è§¦å‘è¯•éªŒæ–¹æ³•]-[æ­£é¢ç¢°æ’]-[æ»‘å°æ­£é¢ç¢°æ’è¯•éªŒ]\"\
            ,\n\"topic_keywords\": [\n\"æ»‘å°æ­£é¢ç¢°æ’è¯•éªŒ\",\n\"å®‰è£…æ­¥éª¤\",\n\"å®‰è£…æ–¹å‘\",\n],\n\"\
            context_keywords\": [\n\"ç™½è½¦èº«\",\n\"å·¥è£…\",\n\"ç¢°æ’è¯•éªŒæ»‘å°\"\n]\n},\n{\n\"chapter_id\"\
            : \"B.2.1.1.2\",\n\"scope\": \"[è‡ªåŠ¨è§¦å‘è¯•éªŒæ–¹æ³•]-[æ­£é¢ç¢°æ’]-[æ»‘å°æ­£é¢ç¢°æ’è¯•éªŒ]\",\n\"topic_keywords\"\
            : [\n\"ç¢°æ’è¯•éªŒ\",\n\"å®éªŒè¦æ±‚\"\n],\n\"context_keywords\": [\n\"åŠ é€Ÿåº¦æ³¢å½¢\",\n\"\
            é€Ÿåº¦å˜åŒ–é‡\",\n\"æ»¤æ³¢ç­‰çº§\",\n\"ä½é€šæ»¤æ³¢\",\n\"åŠ é€Ÿåº¦é€šé“èŒƒå›´\"\n]\n}\n]\n}\n]\n```\n\n##\
            \ è¾“å…¥ç¤ºä¾‹2\n\n```\n[\n  {\n    \"file\": \"regulation\",\n    \"sections\"\
            : [\n      {\n        \"section\": \"MAIN\",\n        \"context\": \"\"\
            ,\n        \"chapters\": [\n          {\n            \"chapter_id\": \"\
            1\",\n            \"chapter_title\": \"èŒƒå›´\",\n            \"raw_text\"\
            : \"æœ¬æ–‡ä»¶è§„å®šäº†è½¦è½½äº‹æ•…ç´§æ€¥å‘¼å«ç³»ç»Ÿçš„æŠ€æœ¯è¦æ±‚ã€åŒä¸€å‹å¼åˆ¤å®šè¦æ±‚,æè¿°äº†ç›¸åº”çš„è¯•éªŒæ–¹æ³•ã€‚\\næœ¬æ–‡ä»¶é€‚ç”¨äºM1 ç±»åŠN1 ç±»è½¦è¾†çš„è½¦è½½äº‹æ•…ç´§æ€¥å‘¼å«ç³»ç»Ÿã€‚\"\
            ,\n            \"children\": [],\n            \"full_path\": \"1 èŒƒå›´\"\n\
            \          },\n          {\n            \"chapter_id\": \"3\",\n     \
            \       \"chapter_title\": \"æœ¯è¯­å’Œå®šä¹‰\",\n            \"raw_text\": \"ä¸‹åˆ—æœ¯è¯­å’Œå®šä¹‰é€‚ç”¨äºæœ¬æ–‡ä»¶ã€‚\"\
            ,\n            \"children\": [\n              {\n                \"chapter_id\"\
            : \"3.1\",\n                \"chapter_title\": \"è½¦è½½äº‹æ•…ç´§æ€¥å‘¼å«ç³»ç»Ÿ on-boardaccidentemergencycallsystem;AECS\"\
            ,\n                \"raw_text\": \"é€šè¿‡è½¦è¾†å†…éƒ¨ç­–ç•¥åœ¨å‘ç”Ÿäº‹æ•…æ—¶è‡ªåŠ¨æ¿€æ´»,æˆ–ç”±è½¦å†…äººå‘˜è¿›è¡Œæ‰‹åŠ¨è§¦å‘å,å°†è½¦è¾†çš„ä½ç½®åŠè½¦è¾†ç›¸å…³çŠ¶æ€ä¿¡æ¯åŒæ­¥å‘é€ç»™ç´§æ€¥å‘¼å«æœåŠ¡å¹³å°å¹¶å»ºç«‹è¯­éŸ³é€šè¯çš„ç³»ç»Ÿã€‚\"\
            ,\n                \"children\": [],\n                \"full_path\": \"\
            3 æœ¯è¯­å’Œå®šä¹‰/3.1 è½¦è½½äº‹æ•…ç´§æ€¥å‘¼å«ç³»ç»Ÿ on-boardaccidentemergencycallsystem;AECS\"\n  \
            \            },\n        ]\n      }\n    ]\n  }\n]\n```\n\n## è¾“å‡ºç¤ºä¾‹2\n\n\
            ```\n[\n{\n\"file\": \"regulation\",\n\"section\": \"MAIN\",\n\"chapters\"\
            : [\n{\n\"chapter_id\": \"1\",\n\"scope\": \"[èŒƒå›´]\",\n\"topic_keywords\"\
            : [\n\"èŒƒå›´\" // èŒƒå›´ç­‰ç« èŠ‚ï¼Œæ˜¯å…±æ€§ç« èŠ‚ï¼Œå¯ä½œä¸ºtopic\n],\n\"context_keywords\": [] // èŒƒå›´ç­‰ç« èŠ‚ä¸åšæå–\n\
            },\n{\n\"chapter_id\": \"3\",\n\"scope\": \"[æœ¯è¯­å’Œå®šä¹‰]\",\n\"topic_keywords\"\
            : [\n\"æœ¯è¯­å’Œå®šä¹‰\"\n],\n\"context_keywords\": []\n},\n{\n\"chapter_id\": \"\
            3.1\",\n\"scope\": \"[æœ¯è¯­å’Œå®šä¹‰]-[è½¦è½½äº‹æ•…ç´§æ€¥å‘¼å«ç³»ç»Ÿ(AECS)]\",\n\"topic_keywords\"\
            : [\n\"è½¦è½½äº‹æ•…ç´§æ€¥å‘¼å«ç³»ç»Ÿ\",\n\"AECS\"\n],\n\"context_keywords\": []\n},\n]\n\
            }\n]\n```\n\n## è¾“å…¥ç¤ºä¾‹3\n\n```\n[\n  {\n    \"file\": \"regulation\",\n\
            \    \"sections\": [\n      {\n        \"section\": \"MAIN\",\n      \
            \  \"context\": \"\",\n        \"chapters\": [\n          {\n        \
            \    \"chapter_id\": \"7-\",\n            \"chapter_title\": \"CRITERIA\
            \ OF TECHNICAL CONFORMITY\",\n            \"raw_text\": \"\",\n      \
            \      \"children\": [\n             {\n                \"chapter_id\"\
            : \"7.2\",\n                \"chapter_title\": \"Vehicle subjected to\
            \ type test\",\n                \"raw_text\": \"\",\n                \"\
            children\": [\n                  {\n                    \"chapter_id\"\
            : \"7.2.1\",\n                    \"chapter_title\": \"\",\n         \
            \           \"raw_text\": \"The vehicle shall be considered complying\
            \ with the requirement specified in item \\n4.1of this standard, if the\
            \ measured mass of carbon monoxide and the combined mass of hydrocarbon\
            \ and oxides of nitrogen, are less than or  equal to 0.70 of theallowahle\
            \ limits mentioned in Tahle (1 ).\",\n                    \"children\"\
            : [],\n                    \"full_path\": \"7- CRITERIA OF TECHNICAL CONFORMITY/7.2\
            \ Vehicle subjected to type test/7.2.1 \"\n                  },\n    \
            \              {\n                    \"chapter_id\": \"7.2.2\",\n   \
            \                 \"chapter_title\": \"\",\n                    \"raw_text\"\
            : \"The test shall  be repeated  if in the initial test, the measured\
            \ masses of both the carbon monoxide and the combined value of hydrocarbons\
            \ and oxides of nitrogenare less than or equal to 0.85 of their allowable\
            \ limits and one of these values isgreater than 0.70 of its allowable\
            \ limit.\",\n                    \"children\": [],\n                 \
            \   \"full_path\": \"7- CRITERIA OF TECHNICAL CONFORMITY/7.2 Vehicle subjected\
            \ to type test/7.2.2 \"\n                  }\n                ],\n   \
            \             \"full_path\": \"7- CRITERIA OF TECHNICAL CONFORMITY/7.2\
            \ Vehicle subjected to type test\"\n              },\n            ],\n\
            \          },\n        ],\n      },\n    ],\n  }\n]\n```\n\n## è¾“å‡ºç¤ºä¾‹3\n\
            \n```\n[\n  {\n    \"file\": \"regulation\",\n    \"section\": \"MAIN\"\
            ,\n    \"chapters\": [\n      {\n        \"chapter_id\": \"7-\",\n   \
            \     \"scope\": \"[technical conformity criteria]\",\n        \"topic_keywords\"\
            : [\n          \"technical conformity criteria\"\n        ],\n       \
            \ \"context_keywords\": []\n      },\n      {\n        \"chapter_id\"\
            : \"7.2\",\n        \"scope\": \"[technical conformity criteria]-[vehicle\
            \ type test]\",\n        \"topic_keywords\": [\n          \"vehicle type\
            \ test\"\n        ],\n        \"context_keywords\": []\n      },\n   \
            \   {\n        \"chapter_id\": \"7.2.1\",\n        \"scope\": \"[Vehicle\
            \ type test] - [Emission conformity determination] - [Mass limit compliance\
            \ condition]\",\n        \"topic_keywords\": [\"vehicle type test\", \"\
            pollutants emission compliance condition\", \"mass-based limits\"],\n\
            \        \"context_keywords\": [, \"allowable emission limits\", \"compliance\
            \ criteria\"]\n        \"context_keywords\": [\n          \"pollutants\"\
            , // æ¥è‡ªæœ¯è¯­å®šä¹‰ç« èŠ‚ï¼Œå®ƒåŒ…å«äº†ä¸€æ°§åŒ–ç¢³ç­‰\n          \"carbon monoxide\",\n          \"\
            hydrocarbons\",\n          \"oxides of nitrogen\"\n        ]\n      },\n\
            \      {\n        \"chapter_id\": \"7.2.2\",\n        \"scope\": \"[Vehicle\
            \ type test] - [Emission conformity determination] - [Test repetition\
            \ condition]\",\n        \"topic_keywords\": [\"vehicle type test\", \"\
            test repetition\",\"allowable emission limits\",\"mass-based range\"],\n\
            \        \"context_keywords\": [\n          \"pollutants\", // æ¥è‡ªæœ¯è¯­å®šä¹‰ç« èŠ‚ï¼Œå®ƒåŒ…å«äº†ä¸€æ°§åŒ–ç¢³ç­‰\n\
            \          \"carbon monoxide\",\n          \"hydrocarbons\",\n       \
            \   \"oxides of nitrogen\"\n        ]\n      }\n    ]\n  }\n]\n\n```\n\
            \n"
        - id: 2e89206c-ded1-462d-9db7-2832e587c8ad
          role: user
          text: '{{#1756557801310.item#}}'
        selected: false
        title: LLM 2
        type: llm
        variables: []
        vision:
          enabled: false
      height: 89
      id: '1756557804693'
      parentId: '1756557801310'
      position:
        x: 204
        y: 78.24573165283232
      positionAbsolute:
        x: 842
        y: 430.2457316528323
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
      zIndex: 1002
    - data:
        code: "import json\nimport ast\nimport re\nfrom typing import Dict, List,\
          \ Any\n\ndef parse_input(src: str) -> List[Dict[str, Any]]:\n    \"\"\"\
          é¢„å¤„ç†å¹¶è§£æè¾“å…¥å­—ç¬¦ä¸²ä¸ºåˆ—è¡¨å­—å…¸ç»“æ„\"\"\"\n    try:\n        # 1. æŒ‰ </think> åˆ‡åˆ†å¹¶å–ååŠæ®µ\n  \
          \      src = src.rsplit('</think>', 1)[-1].strip()\n\n        # 2. å…ˆå¹²æ‰ deepseek\
          \ V3 å¯èƒ½æ’å…¥çš„â€œæé€Ÿâ€æˆ–å•ç‹¬çš„â€œæâ€\n        src = re.sub(r'æé€Ÿ\\s*', '', src)   # å»æ‰â€œæé€Ÿâ€åŠå…¶åå¯èƒ½çš„å¤šä½™ç©ºç™½\n\
          \        src = re.sub(r'(?<!\\w)æ(?!\\w)', '', src)  # å»æ‰å­¤ç«‹å‡ºç°çš„â€œæâ€\n\n  \
          \      # 3. å°è¯•å¤šç§è§£æè·¯å¾„\n        try:\n            data = json.loads(src)\n\
          \        except json.JSONDecodeError:\n            try:\n              \
          \  if src.startswith('\\ufeff'):\n                    src = src[1:]\n  \
          \              data = ast.literal_eval(src)\n            except (SyntaxError,\
          \ ValueError):\n                try:\n                    processed = src.replace('\\\
          \\\"', '\"').replace('\\\\n', '\\n')\n                    data = json.loads(processed)\n\
          \                except Exception as e:\n                    raise ValueError(f\"\
          æ— æ³•è§£æï¼š{str(e)}\")\n\n        # 4. ä¿è¯è¿”å› list\n        if isinstance(data,\
          \ dict):\n            return [data]\n        elif isinstance(data, list):\n\
          \            return data\n        else:\n            return []\n    except\
          \ Exception as e:\n        raise ValueError(f\"é¢„å¤„ç†å¤±è´¥: {str(e)}\")\n\n\n\
          def main(arg1: str) -> dict:\n    data = parse_input(arg1)\n    return {\n\
          \        \"result\": json.dumps(data, ensure_ascii=False),\n    }"
        code_language: python3
        desc: ''
        error_strategy: fail-branch
        isInIteration: true
        isInLoop: false
        iteration_id: '1756557801310'
        outputs:
          result:
            children: null
            type: string
        selected: false
        title: ä»£ç æ‰§è¡Œ 2
        type: code
        variables:
        - value_selector:
          - '1756557804693'
          - text
          value_type: string
          variable: arg1
      height: 89
      id: '1756557929661'
      parentId: '1756557801310'
      position:
        x: 508
        y: 80
      positionAbsolute:
        x: 1146
        y: 432
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
      zIndex: 1002
    - data:
        answer: '{{#1756613329065.output#}}'
        desc: ''
        selected: false
        title: ç›´æ¥å›å¤ 2
        type: answer
        variables: []
      height: 104
      id: '1756558073574'
      position:
        x: 5718
        y: 352
      positionAbsolute:
        x: 5718
        y: 352
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        code: "import json\nimport re\nimport os\n\nimport json\nimport ast\nimport\
          \ re\nfrom typing import Dict, List, Any\n\ndef parse_input(src: str) ->\
          \ List[Dict[str, Any]]:\n    \"\"\"é¢„å¤„ç†å¹¶è§£æè¾“å…¥å­—ç¬¦ä¸²ä¸ºåˆ—è¡¨å­—å…¸ç»“æ„\"\"\"\n    try:\n\
          \        # å»é™¤ LLM æ€ç»´é“¾æ ‡è®°\n        try: \n            src = re.sub(r'<think>.*?</think>',\
          \ '', src, flags=re.DOTALL).strip()\n        except:\n            pass\n\
          \n        try:\n            data = json.loads(src)\n        except json.JSONDecodeError:\n\
          \            try:\n                if src.startswith('\\ufeff'):\n     \
          \               src = src[1:]\n                data = ast.literal_eval(src)\n\
          \            except (SyntaxError, ValueError):\n                try:\n \
          \                   processed = src.replace('\\\\\"', '\"').replace('\\\\\
          n', '\\n')\n                    data = json.loads(processed)\n         \
          \       except Exception as e:\n                    raise ValueError(f\"\
          æ— æ³•è§£æï¼š{str(e)}\")\n\n        # ä¿è¯ä¸º list\n        if isinstance(data, dict):\n\
          \            return [data]\n        elif isinstance(data, list):\n     \
          \       return data\n        else:\n            return []\n    except Exception\
          \ as e:\n        raise ValueError(f\"é¢„å¤„ç†å¤±è´¥: {str(e)}\")\n        # return\
          \ []\n\ndef find_chapters_and_children_by_ids(tree, target_file, target_section,\
          \ experiment_root_ids):\n    \"\"\"\n    æ ¹æ® experiment_root_ids æ‰¾åˆ°å¯¹åº”çš„ç« èŠ‚åŠå…¶æ‰€æœ‰å­ç« èŠ‚\n\
          \    \n    Args:\n        tree: å®Œæ•´çš„ç« èŠ‚æ ‘ç»“æ„\n        target_file: ç›®æ ‡æ–‡ä»¶å (å¦‚\
          \ \"regulation\")\n        target_section: ç›®æ ‡sectionå (å¦‚ \"é™„å½•B\")\n    \
          \    experiment_root_ids: è¦æŸ¥æ‰¾çš„ç« èŠ‚IDåˆ—è¡¨ (å¦‚ [\"B.2.1.1\",\"B.2.4.1\"])\n   \
          \ \n    Returns:\n        List[Dict]: æ¯ä¸ªitemåŒ…å«æ‰¾åˆ°çš„ç« èŠ‚åŠå…¶æ‰€æœ‰å­ç« èŠ‚\n    \"\"\"\n\
          \    \n    def get_chapter_with_all_children(chapter):\n        \"\"\"é€’å½’è·å–ç« èŠ‚åŠå…¶æ‰€æœ‰å­ç« èŠ‚\"\
          \"\"\n        result = {\n            \"chapter_id\": chapter[\"chapter_id\"\
          ],\n            \"chapter_title\": chapter[\"chapter_title\"],\n       \
          \     \"raw_text\": chapter.get(\"raw_text\", \"\"),\n            \"full_path\"\
          : chapter.get(\"full_path\", \"\"),\n            \"children\": []\n    \
          \    }\n        \n        # å¦‚æœæœ‰å­ç« èŠ‚ï¼Œé€’å½’è·å–\n        if \"children\" in chapter\
          \ and chapter[\"children\"]:\n            for child in chapter[\"children\"\
          ]:\n                result[\"children\"].append(get_chapter_with_all_children(child))\n\
          \        \n        return result\n    \n    def find_chapter_by_id(chapters,\
          \ target_id):\n        \"\"\"åœ¨ç« èŠ‚åˆ—è¡¨ä¸­é€’å½’æŸ¥æ‰¾æŒ‡å®šIDçš„ç« èŠ‚\"\"\"\n        for chapter\
          \ in chapters:\n            if chapter[\"chapter_id\"] == target_id:\n \
          \               return chapter\n\n            # åœ¨å­ç« èŠ‚ä¸­é€’å½’æŸ¥æ‰¾\n            if\
          \ \"children\" in chapter and chapter[\"children\"]:\n                found\
          \ = find_chapter_by_id(chapter[\"children\"], target_id)\n             \
          \   if found:\n                    return found\n        \n        return\
          \ None\n    \n    result = []\n    \n    # éå†æ ‘ç»“æ„ï¼Œæ‰¾åˆ°ç›®æ ‡æ–‡ä»¶å’Œsection\n    for\
          \ file_item in tree:\n        if file_item[\"file\"] != target_file:\n \
          \           continue\n            \n        for section in file_item[\"\
          sections\"]:\n            if section[\"section\"] != target_section:\n \
          \               continue\n            \n            # åœ¨ find_chapters_and_children_by_ids\
          \ é‡Œï¼Œæ‰¾åˆ°ç›®æ ‡ section ååŠ ï¼š\n            if experiment_root_ids == [\"ALL\"]:\n\
          \                # æŠŠç¬¬ä¸€å±‚æ‰€æœ‰ chapter_id å–å‡ºæ¥\n                experiment_root_ids\
          \ = [ch[\"chapter_id\"] for ch in section.get(\"chapters\", [])]\n\n   \
          \         # åœ¨è¯¥sectionä¸­æŸ¥æ‰¾æ¯ä¸ªexperiment_root_id\n            for root_id in\
          \ experiment_root_ids:\n                found_chapter = find_chapter_by_id(section[\"\
          chapters\"], root_id)\n                if found_chapter:\n             \
          \       # è·å–è¯¥ç« èŠ‚åŠå…¶æ‰€æœ‰å­ç« èŠ‚\n                    chapter_with_children = get_chapter_with_all_children(found_chapter)\n\
          \                    result.append(chapter_with_children)\n            \
          \    else:\n                    print(f\"è­¦å‘Š: æœªæ‰¾åˆ°ç« èŠ‚ID '{root_id}' åœ¨ {target_file}/{target_section}\
          \ ä¸­\")\n    \n    return result\n\ndef extract_experiment_chapters(tree,\
          \ experiment_info):\n    \"\"\"\n    æ ¹æ®å®éªŒä¿¡æ¯æå–å¯¹åº”çš„ç« èŠ‚\n    \n    Args:\n  \
          \      tree: å®Œæ•´çš„ç« èŠ‚æ ‘\n        experiment_info: åŒ…å« file, section, experiment_root_ids\
          \ çš„å­—å…¸\n    \n    Returns:\n        List[Dict]: å®éªŒç›¸å…³çš„ç« èŠ‚åˆ—è¡¨\n    \"\"\"\n \
          \   return find_chapters_and_children_by_ids(\n        tree=tree,\n    \
          \    target_file = experiment_info.get(\"file\", \"\"),\n        target_section\
          \ = experiment_info.get(\"section\", \"\"),\n        experiment_root_ids\
          \ = experiment_info.get(\"experiment_root_ids\", [])\n    )\n\ndef main(arg1:\
          \ list[str], arg2: str, file_name) -> dict:\n    \"\"\"\n    å¤„ç†å®éªŒæ•°æ®çš„ä¸»å‡½æ•°\n\
          \    \n    Args:\n        arg1: JSONå­—ç¬¦ä¸²åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å« file, section, experiment_root_ids\n\
          \        arg2: ç« èŠ‚æ ‘çš„JSONå­—ç¬¦ä¸²\n    \n    Returns:\n        dict: å¤„ç†åçš„å®éªŒç« èŠ‚æ•°æ®\n\
          \    \"\"\"\n    import json\n    from collections import defaultdict\n\
          \    \n    # è§£æè¾“å…¥æ•°æ®\n    experiment_data = []\n    solved_tree = []\n  \
          \  for t in arg1:\n        if t is None:\n            continue\n       \
          \ items = json.loads(t)\n        for item in items:\n            experiment_data.append({\n\
          \                'file': item.get('file', ''),\n                'section':\
          \ item.get('section', ''),\n                'experiment_root_ids': item.get('experiment_root_ids',\
          \ [])\n            })\n            solved_tree.append({\n              \
          \  'file': item['file'],\n                'section': item['section'],\n\
          \                'chapters':item['chapters']\n            })\n    \n   \
          \ # # æŒ‰ file å’Œ section åˆå¹¶ç›¸åŒçš„é¡¹ç›®\n    # merged_experiments = defaultdict(list)\n\
          \    # for item in experiment_data:\n    #     key = (item['file'], item['section'])\n\
          \    #     merged_experiments[key].extend(item['experiment_root_ids'])\n\
          \    \n    # # å»é‡å¹¶ç”Ÿæˆæœ€ç»ˆçš„ experiment_info åˆ—è¡¨\n    # experiment_infos = []\n\
          \    # for (file, section), root_ids in merged_experiments.items():\n  \
          \  #     # å»é‡ä½†ä¿æŒé¡ºåº\n    #     unique_root_ids = []\n    #     seen = set()\n\
          \    #     for root_id in root_ids:\n    #         if root_id not in seen:\n\
          \    #             unique_root_ids.append(root_id)\n    #             seen.add(root_id)\n\
          \    #     if len(unique_root_ids):\n    #         experiment_infos.append({\n\
          \    #             'file': file,\n    #             'section': section,\n\
          \    #             'experiment_root_ids': unique_root_ids\n    #       \
          \  })\n    \n    # âœ… æŒ‰åŸå§‹ JSON å­—ç¬¦ä¸²ç²’åº¦åˆå¹¶ï¼Œä¸æ‹† ID\n    experiment_infos = []\n\
          \    for item in experiment_data:\n        if item['experiment_root_ids']:\
          \  # åªä¿ç•™éç©ºçš„\n            experiment_infos.append({\n                'file':\
          \ item['file'],\n                'section': item['section'],\n         \
          \       'experiment_root_ids': item['experiment_root_ids']  # ä¿æŒåŸæ ·ï¼Œä¸æ‹†\n\
          \            })\n\n    # âœ… æŒ‰ (file, section, tuple(root_ids)) å»é‡\n    seen\
          \ = set()\n    unique_experiments = []\n    for exp in experiment_infos:\n\
          \        key = (exp['file'], exp['section'], tuple(exp['experiment_root_ids']))\n\
          \        if key not in seen:\n            seen.add(key)\n            unique_experiments.append(exp)\n\
          \n    experiment_infos = unique_experiments\n\n    # 1. å…ˆæŠŠ solved_tree æŒ‰\
          \ (file, section) åšåˆ†ç»„\n    merged_chapters = defaultdict(list)\n    for\
          \ node in solved_tree:\n        key = (node['file'], node['section'])\n\
          \        merged_chapters[key].extend(node['chapters'])\n\n    # 2. åœ¨æ¯ä¸ªåˆ†ç»„å†…å¯¹\
          \ chapters å»é‡ä¸”ä¿æŒåŸé¡ºåº\n    final_tree = []\n    for (file, section), chapters\
          \ in merged_chapters.items():\n        seen_id = set()\n        unique_chapters\
          \ = []\n        for c in chapters:\n            cid = c.get(\"chapter_id\"\
          )\n            if cid not in seen_id:\n                unique_chapters.append(c)\n\
          \                seen_id.add(cid)\n        if unique_chapters:\n       \
          \     final_tree.append({\n                \"file\": file,\n           \
          \     \"section\": section,\n                \"chapters\": unique_chapters\n\
          \            })\n\n    # è§£æç« èŠ‚æ ‘\n    regulation_tree = parse_input(arg2)\n\
          \    \n    # print(experiment_infos)\n\n\n    # å¤„ç†æ¯ä¸ªå®éªŒä¿¡æ¯ï¼Œæå–å¯¹åº”çš„ç« èŠ‚\n    result\
          \ = []\n    for i, experiment_info in enumerate(experiment_infos):\n   \
          \     chapters_with_hierarchy = extract_experiment_chapters(regulation_tree,\
          \ experiment_info)\n        \n        result.append(json.dumps(chapters_with_hierarchy,\
          \ ensure_ascii=False))\n    \n    # å»æ‰æ‰©å±•åï¼Œå†æ‹¼ .json\n    base_name = os.path.splitext(file_name)[0]\
          \          # å¾—åˆ°æ— åç¼€çš„çº¯æ–‡ä»¶å\n    path = f\"/tmp/mydata/{base_name}/first_version.json\"\
          \n\n\n    os.makedirs(os.path.dirname(path), exist_ok=True)\n    with open(path,\
          \ \"w\", encoding=\"utf-8\") as f:\n        json.dump(final_tree, f, ensure_ascii=False,\
          \ indent=2)\n\n    return {\n        \"chapters\":result,\n        \"experiment_infos\"\
          :json.dumps(experiment_infos, ensure_ascii=False),\n        \"final_tree\"\
          :json.dumps(final_tree, ensure_ascii=False),\n    }"
        code_language: python3
        desc: ''
        outputs:
          chapters:
            children: null
            type: array[string]
          experiment_infos:
            children: null
            type: string
          final_tree:
            children: null
            type: string
        selected: false
        title: ä»£ç æ‰§è¡Œ 3
        type: code
        variables:
        - value_selector:
          - '1758096294198'
          - result
          value_type: array[string]
          variable: arg1
        - value_selector:
          - '1756550411122'
          - tree
          value_type: string
          variable: arg2
        - value_selector:
          - '1756550268945'
          - file
          - name
          value_type: file
          variable: file_name
      height: 53
      id: '1756563307317'
      position:
        x: 3022.491463305665
        y: 352
      positionAbsolute:
        x: 3022.491463305665
        y: 352
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        code: "\ndef main(arg1: str) -> dict:\n    return {\n        \"array\": [arg1],\n\
          \    }\n"
        code_language: python3
        desc: ''
        outputs:
          array:
            children: null
            type: array[string]
        selected: false
        title: ä»£ç æ‰§è¡Œ 4
        type: code
        variables:
        - value_selector:
          - '1756550268945'
          - array
          value_type: string
          variable: arg1
      height: 53
      id: '1756567749405'
      position:
        x: 30
        y: 747
      positionAbsolute:
        x: 30
        y: 747
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        code: "\ndef main(arg1: list[str]) -> dict:\n    return {\n        \"result\"\
          :json.dumps(arg1, ensure_ascii=False)\n    }\n"
        code_language: python3
        desc: ''
        outputs:
          result:
            children: null
            type: string
        selected: false
        title: ä»£ç æ‰§è¡Œ 5
        type: code
        variables:
        - value_selector:
          - '1756557801310'
          - output
          value_type: array[string]
          variable: arg1
      height: 53
      id: '1756568067080'
      position:
        x: 30
        y: 840
      positionAbsolute:
        x: 30
        y: 840
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        desc: ''
        error_handle_mode: continue-on-error
        height: 290
        is_parallel: true
        iterator_input_type: array[string]
        iterator_selector:
        - '1756563307317'
        - chapters
        output_selector:
        - '1757762184699'
        - output
        output_type: array[string]
        parallel_nums: 10
        selected: false
        start_node_id: 1756613329065start
        title: è¿­ä»£ 2
        type: iteration
        width: 1724
      height: 290
      id: '1756613329065'
      position:
        x: 2722.491463305665
        y: 783.5500134032299
      positionAbsolute:
        x: 2722.491463305665
        y: 783.5500134032299
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 1724
      zIndex: 1
    - data:
        desc: ''
        isInIteration: true
        selected: false
        title: ''
        type: iteration-start
      draggable: false
      height: 48
      id: 1756613329065start
      parentId: '1756613329065'
      position:
        x: 60
        y: 100.5
      positionAbsolute:
        x: 2782.491463305665
        y: 884.0500134032299
      selectable: false
      sourcePosition: right
      targetPosition: left
      type: custom-iteration-start
      width: 44
      zIndex: 1002
    - data:
        context:
          enabled: true
          variable_selector:
          - '1756550411122'
          - context
        desc: ''
        isInIteration: true
        isInLoop: false
        iteration_id: '1756613329065'
        model:
          completion_params: {}
          mode: chat
          name: deepseek-reasoner
          provider: langgenius/deepseek/deepseek
        prompt_template:
        - id: 8e7bb797-e961-48ec-9029-47c638adcc67
          role: system
          text: "# è§’è‰²ï¼ˆRoleï¼‰\nä½ æ˜¯ä¸€å **æ ‡å‡†æµ‹è¯•ç”¨ä¾‹è®¾è®¡ä¸“å®¶**ï¼Œç²¾é€šæ±½è½¦åŠç›¸å…³é¢†åŸŸçš„æ ‡å‡†å’Œæ³•è§„æ–‡æ¡£ã€‚  \nä½ çš„èŒè´£æ˜¯å°†æ ‡å‡†æˆ–æ³•è§„æ–‡ä»¶ä¸­çš„æ¡æ¬¾å’Œè¯•éªŒæ–¹æ³•\
            \ **ä¸¥æ ¼æ‹†è§£ä¸ºç»“æ„åŒ–æµ‹è¯•ç”¨ä¾‹**ï¼Œè¾“å‡ºç»“æœç”¨äºï¼š  \n- æ ‡å‡†å’Œæ³•è§„çš„æ¯”å¯¹åˆ†æ  \n- æµ‹è¯•æœºæ„è®¾å¤‡èƒ½åŠ›éªŒè¯å’Œè¦†ç›–ç‡è¯„ä¼°  \n-\
            \ è‡ªåŠ¨åŒ–æµ‹è¯•ç”¨ä¾‹ç®¡ç†ç³»ç»Ÿ  \nè¦æ±‚ä¿¡æ¯å®Œæ•´ã€æ¡ç†æ¸…æ™°ã€å­—æ®µè§„èŒƒåŒ–ï¼Œç¡®ä¿ç»“æ„åŒ–ç»“æœå…·å¤‡å¯è¿½æº¯æ€§å’Œå¯æ‰§è¡Œæ€§ã€‚\n\n---\n\n# è¾“å…¥æ ¼å¼ï¼ˆInput\
            \ Formatï¼‰\nè¾“å…¥ä¸ºä¸€ä¸ª JSON æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ è¡¨ç¤ºä¸€ä¸ªç« èŠ‚ï¼š\n```json\n[\n  {\n    \"chapter_id\"\
            : \"string\",      // ç« èŠ‚ç¼–å·\n    \"chapter_title\": \"string\",   // ç« èŠ‚æ ‡é¢˜\n\
            \    \"raw_text\": \"string\",        // æœ¬ç« èŠ‚æ­£æ–‡\n    \"children\": [  \
            \              // å­ç« èŠ‚æ•°ç»„ï¼ˆé€’å½’ç»“æ„ï¼‰\n      { ...åŒç»“æ„... }\n    ],\n    \"full_path\"\
            : \"string\"        // ä»æ ¹åˆ°è¯¥ç« èŠ‚çš„è·¯å¾„\n  }\n]\n```\nè¯´æ˜ï¼š\n- ä½ ä¹Ÿå¯ä»¥ä» {{#context#}}\
            \ ä¸­ç†è§£æ­¤æ ‡å‡†çš„ç›¸å…³ä¿¡æ¯\n- `raw_text` å¯èƒ½ä¸ºç©ºï¼Œæ­£æ–‡å¯èƒ½åœ¨ `children` å†…ã€‚\n- åŒä¸€ç« èŠ‚å¯åŒ…å«å¤šä¸ªå®éªŒæ–¹æ³•ï¼›æ¯ä¸ªå®éªŒæ–¹æ³•éœ€å•ç‹¬æ‹†è§£æˆå¯¹è±¡ã€‚ä¸åŒç« èŠ‚é—´å„è‡ªç‹¬ç«‹ï¼Œä¸è¦ç›¸äº’æ¨æ–­\n\
            - ä¸€ä¸ªå®éªŒæ–¹æ³•ä¸­å­˜åœ¨å¤šç§è¯•éªŒæ–¹æ¡ˆæ—¶ï¼Œä¹Ÿè¦åˆ†åˆ«æ‹†è§£ä¸ºå¤šä¸ªå¯¹è±¡ï¼ˆå‘½åæ—¶åœ¨ `test_name` åæ·»åŠ æ–¹æ¡ˆæ ‡è¯†ï¼Œå¦‚â€œ(XXæ–¹æ¡ˆ)â€ï¼‰ã€‚\n\n\
            ---\n\n# è¾“å‡ºæ ¼å¼ï¼ˆOutput Formatï¼‰\nè¾“å‡ºä¸ºä¸€ä¸ª JSON æ•°ç»„ï¼Œå¯¹åº”è¾“å…¥çš„ç« èŠ‚æ•°ç»„ï¼Œæ¯ä¸ªç« èŠ‚å¯¹åº”ä¸€ä¸ªæ•°ç»„ï¼Œæ•°ç»„ä¸­çš„æ¯ä¸ªå¯¹è±¡ä»£è¡¨ä¸€ä¸ªç‹¬ç«‹çš„å®éªŒæˆ–å®éªŒæ–¹æ¡ˆï¼š\n\
            \n```json\n[\n  [\n    {\n      \"chapter_id\": \"string\",    // æ¥æºç« èŠ‚çš„chapter_idï¼Œä¿è¯å¯è¿½æº¯æ€§\n\
            \      \"test_name\": \"string\",     // æµ‹è¯•åç§°ï¼Œä»ç« èŠ‚æ ‡é¢˜æˆ–è·¯å¾„æç‚¼ï¼Œç®€æ´ä¸”å”¯ä¸€\n     \
            \ \"conditions\": [            // æµ‹è¯•æ¡ä»¶ï¼šè¯•ä»¶çŠ¶æ€ã€å®‰è£…ã€ç¯å¢ƒç­‰ï¼Œå®Œæ•´åˆ†ç‚¹åˆ—å‡º\n        \"\
            string\"\n      ],\n      \"criteria\": [              // åˆ¤å®šæ ‡å‡†ï¼šæ˜ç¡®æ£€æµ‹é¡¹+è¾¾æ ‡æ¡ä»¶ï¼Œåˆ†ç‚¹åˆ—å‡º\n\
            \        \"string\"\n      ],\n      \"equipment\": [             // æ‰€éœ€è®¾å¤‡åŠè§„æ ¼ï¼Œä¾¿äºè®¾å¤‡èƒ½åŠ›éªŒè¯\n\
            \        {\n          \"name\": \"string\",          // è®¾å¤‡åç§°\n       \
            \   \"specification\": \"string\"  // è®¾å¤‡è§„æ ¼/æ€§èƒ½\n        }\n      ],\n \
            \     \"parameters\": [            // æå–çš„å…³é”®å‚æ•°\n        {\n          \"\
            item\": \"string\",          // å‚æ•°é¡¹\n          \"constraint\": \"string\"\
            ,    // çº¦æŸï¼š<=|<|=|>=|>|range_closed|range_open|enum|boolean\n        \
            \  \"value\": \"string|array|null\", // å‚æ•°å€¼ï¼Œå¯ä¸ºæ•°å­—ã€èŒƒå›´æˆ–æšä¸¾\n          \"unit\"\
            : \"string|null\",     // å•ä½ï¼Œè‹¥æ— åˆ™null\n          \"source_text\": \"string\"\
            \    // åŸæ–‡å®Œæ•´ç‰‡æ®µï¼Œä¿ç•™ç¬¦å·ï¼Œä¾¿äºè¿½æº¯\n        }\n      ],\n      \"refs\": [     \
            \             // å¼•ç”¨ä¿¡æ¯\n        {\n          \"ref_type\": \"internal|external\"\
            , // å†…éƒ¨/å¤–éƒ¨å¼•ç”¨\n          \"doc_id\": \"string|null\",         // å¤–éƒ¨æ ‡å‡†ç¼–å·ï¼›å†…éƒ¨å¼•ç”¨å¡«null\n\
            \          \"target_id\": \"string\",           // å¼•ç”¨ç›®æ ‡ç¼–å·ï¼ˆå¦‚â€œB.2.1.1â€ã€â€œè¡¨B.1â€ï¼‰\n\
            \          \"anchor_text\": \"string\"          // å¼•ç”¨çš„ç®€è¦ä¸Šä¸‹æ–‡æè¿°\n      \
            \  }\n      ]\n    }\n  ]\n]\n```\n\n---\n\n# å­—æ®µè¯´æ˜\n- **chapter_id**ï¼šæ ‡è®°æ¥æºç« èŠ‚ï¼Œä¿è¯å¯è¿½æº¯æ€§ã€‚\
            \  \n- **test_name**ï¼šå”¯ä¸€æ ‡è¯†è¯¥å®éªŒæ–¹æ³•ï¼Œè‹¥åŒä¸€å®éªŒæœ‰å¤šæ–¹æ¡ˆï¼Œåœ¨ååŠ æ‹¬å·æ ‡æ˜æ–¹æ¡ˆï¼Œå¦‚â€œ(æ–¹æ¡ˆA)â€ã€‚  \n- **conditions**ï¼šå®Œæ•´åˆ†ç‚¹åˆ—å‡ºæµ‹è¯•å‰ç½®æ¡ä»¶ã€å®‰è£…çŠ¶æ€ã€ç¯å¢ƒè¦æ±‚ç­‰ã€‚\
            \  \n- **criteria**ï¼šæ¯æ¡å¿…é¡»åŒ…å«æ£€æµ‹å¯¹è±¡ä¸åˆ¤å®šæ¡ä»¶ï¼Œé¿å…ç¬¼ç»Ÿæè¿°ã€‚  \n- **equipment**ï¼šå®Œæ•´åˆ—å‡ºæ‰€æœ‰è®¾å¤‡åŠå…³é”®è§„æ ¼ã€‚\
            \  \n- **parameters**ï¼šæå–æ‰€æœ‰å®šé‡æŒ‡æ ‡ï¼Œä¸­æ–‡æ•°å­—è½¬é˜¿æ‹‰ä¼¯æ•°å­—ã€‚ ä¸åŒ…å«è¡¨æ ¼å†…çš„å‚æ•°\n- **refs**ï¼šæå–ç« èŠ‚å†…çš„æ ‡å‡†å¼•ç”¨ï¼š\n\
            \  - å†…éƒ¨å¼•ç”¨ï¼šæŒ‡å‘æœ¬æ ‡å‡†çš„ç« èŠ‚ã€è¡¨ã€å›¾ï¼Œ`ref_type=internal`ï¼Œ`doc_id=null`ã€‚\n  - å¤–éƒ¨å¼•ç”¨ï¼šæŒ‡å‘å¤–éƒ¨æ ‡å‡†ï¼Œ`ref_type=external`ï¼Œå¡«å†™æ ‡å‡†ç¼–å·ã€‚\n\
            \  - å¤šä¸ªå¼•ç”¨å¯åˆå¹¶æˆä¸€æ¡è®°å½•ï¼ˆå¦‚â€œè¡¨B.1ã€å›¾B.1â€å¯ä¸ºä¸€ä¸ªæ¡ç›®ï¼‰ã€‚\n\n---\n\n# å¤„ç†é€»è¾‘ï¼ˆChain-of-Thoughtï¼‰\n\
            1. **è§£æç« èŠ‚å†…å®¹**ï¼šé€’å½’éå†`children`ï¼Œåˆ¤æ–­æ˜¯å¦å­˜åœ¨çœŸå®å®éªŒ\n   - è‹¥ä¸»ä½“æ˜¯è¦æ±‚ï¼Œä¾‹å¦‚ï¼šâ€œ4.4. 1.1 æŒ‰é™„å½•Bè¿›è¡Œè¯•éªŒå,AECSåº”è¢«è‡ªåŠ¨è§¦å‘,ä¸”å‘é€çš„MSDä¸­è§¦å‘ç±»å‹åº”ä¸ºè‡ªåŠ¨è§¦å‘â€ï¼Œç»“åˆä¸Šä¸‹æ–‡ç»¼åˆè¯„ä¼°\n\
            \   - è‹¥ä¸»ä½“æ˜¯å®éªŒï¼Œå³å®é™…ä¸Šæ­¤å¤„ç¡®å®è¿›è¡Œå®éªŒï¼Œä½†æ˜¯å¹¶ä¸å®Œæ•´ï¼ˆæ¯”å¦‚å®éªŒæ­¥éª¤å‚è§ä»–å¤„ï¼‰ï¼Œä»åº”æå–å®éªŒ\n   - ç½®ä¿¡åº¦ååˆ†é«˜è¦æ±‚ä¸»ä½“ï¼ˆä¾‹å¦‚ï¼šThis\
            \ Appendix describes the procedure to be used to verify the production\
            \ conformity for the Type I test when the manufacturer's production standard\
            \ deviation issatisfactoryæ˜æ˜¾å¹¶ä¸æ˜¯ä¸€ä¸ªå®éªŒä¸»ä½“ï¼‰ï¼Œä¸å†è¿›è¡Œå®éªŒæå–ï¼Œç›´æ¥è¿”å›[]\n2. **æ‹†åˆ†å®éªŒæ–¹æ³•ä¸æ–¹æ¡ˆ**ï¼šæ ¹æ®æè¿°é€»è¾‘ã€åºå·ã€a)/b)åˆ†é¡¹ç­‰æ‹†æˆå¤šä¸ªå¯¹è±¡ã€‚\
            \  \n3. **æå–ç« èŠ‚ID**ï¼šå†™å…¥æ¯ä¸ªå¯¹è±¡çš„`chapter_id`å­—æ®µã€‚  \n4. **æå–å®éªŒåç§°**ï¼šä¼˜å…ˆç« èŠ‚æ ‡é¢˜ï¼Œç»“åˆè·¯å¾„ç®€åŒ–ï¼Œå¿…è¦æ—¶æ ‡æ˜æ–¹æ¡ˆã€‚\
            \  \n5. **è§£æå®éªŒæ¡ä»¶**ï¼š\n   - åˆ—å‡ºå®‰è£…æ–¹å¼ã€è¯•ä»¶çŠ¶æ€ã€ç¯å¢ƒæ¡ä»¶ã€ç”µæºçŠ¶æ€ç­‰ã€‚\n   - å³ä½¿ä¸å‚æ•°é‡å¤ï¼Œä¹Ÿéœ€å®Œæ•´ä¿ç•™ï¼Œä¿è¯ä¸Šä¸‹æ–‡å®Œæ•´æ€§ã€‚\
            \  \n6. **æå–åˆ¤å®šæ ‡å‡†**ï¼š\n   - åˆ†ç‚¹æè¿°æ£€æµ‹é¡¹+åˆ¤å®šæ¡ä»¶ï¼Œé¿å…â€œç¬¦åˆè¦æ±‚â€è¿™ç§æ¨¡ç³Šè¡¨è¿°ã€‚  \n7. **æå–è®¾å¤‡**ï¼š\n\
            \   - æ•æ‰æ‰€æœ‰æåŠçš„è®¾å¤‡ä¸å…³é”®è§„æ ¼ã€‚  \n8. **æå–å‚æ•°**ï¼š\n   - æ‰€æœ‰å®šé‡å€¼ã€èŒƒå›´ã€ç²¾åº¦è¦æ±‚ç­‰ï¼Œä¸¥æ ¼æ ‡æ³¨çº¦æŸç±»å‹å’Œå•ä½ã€‚\n\
            \   - ä¸­æ–‡æ•°å­—éœ€è½¬é˜¿æ‹‰ä¼¯æ•°å­—ã€‚  \n9. **æå–å¼•ç”¨**ï¼š\n   - å°†å†…éƒ¨å’Œå¤–éƒ¨å¼•ç”¨å•ç‹¬è®°å½•åœ¨`refs`æ•°ç»„ä¸­ã€‚  \n10.\
            \ **è¾“å‡ºæ ‡å‡†åŒ–**ï¼š\n    - ç¼ºå¤±é¡¹ä¿æŒå­—æ®µä½†å¡«ç©ºå€¼ï¼ˆå¦‚ç©ºæ•°ç»„æˆ–nullï¼‰ã€‚  \n    - ä¸¥æ ¼JSONç»“æ„ï¼Œé”®åå…¨éƒ¨å°å†™è‹±æ–‡ã€‚\
            \ \n\n\n---\n\n# æ³¨æ„äº‹é¡¹ï¼ˆNotesï¼‰\n\n- è¾“å‡ºå¿…é¡»ä¸¥æ ¼ç¬¦åˆJSONæ ¼å¼ï¼Œé”®åç»Ÿä¸€ä¸ºè‹±æ–‡ï¼Œ**å€¼åˆ™ä¸è¾“å…¥ä¿æŒä¸€è‡´**ï¼ˆè¾“å…¥è‹±æ–‡ï¼Œåˆ™ä¿ç•™è‹±æ–‡è¡¨è¾¾ï¼‰\n\
            - è¾“å…¥ç« èŠ‚å¯èƒ½åŒ…å«å¤šä¸ªå®éªŒæ–¹æ³•ï¼Œæ¯ä¸ªæ–¹æ³•ç‹¬ç«‹æˆå¯¹è±¡ã€‚\n- éœ€è¦ç»¼åˆåˆ¤æ–­è¾“å…¥å†…å®¹æ˜¯å®éªŒè¿˜æ˜¯è¦æ±‚ï¼Œä¸€èˆ¬æ¥è¯´æ— å­©å­æ— æ ‡é¢˜çš„äºŒçº§ä»¥ä¸‹ç« èŠ‚æ˜¯è¦æ±‚çš„æ¦‚ç‡è¾ƒå¤§ï¼›å¯ä»¥å…è®¸ä¸»ä½“æ˜¯å®éªŒä½†å†…å®¹ä¸å®Œæ•´ï¼Œä¸å…è®¸ä¸»ä½“æ˜¯è¦æ±‚ï¼Œåˆ¤æ–­ä¸»ä½“æ˜¯è¦æ±‚æ—¶éœ€å…·æœ‰è¾ƒé«˜çš„ç½®ä¿¡åº¦ï¼Œå¦åˆ™ä¿ç•™ä¸ºå®éªŒã€‚\n\
            - ç¼ºå¤±ä¿¡æ¯æ—¶è¿”å›ç©ºæ•°ç»„`[]`æˆ–`null`ï¼Œä¸çœç•¥å­—æ®µã€‚\n- ä¸è§£é‡Šæˆ–å±•å¼€å¼•ç”¨çš„å¤–éƒ¨æ ‡å‡†ï¼Œä»…åŸæ–‡ä¿ç•™åœ¨`source_text`ã€‚\n\
            \n# Few-shotç¤ºä¾‹\n\n### è¾“å…¥\n```json\n[\n  {\n    \"chapter_id\": \"B.2.1.1\"\
            ,\n    \"chapter_title\": \"æ»‘å°æ­£é¢ç¢°æ’è¯•éªŒ\",\n    \"raw_text\": \"\",\n   \
            \ \"children\": [\n      {\n        \"chapter_id\": \"B.2.1.1.1\",\n \
            \       \"chapter_title\": \"\",\n        \"raw_text\": \"å°†ç™½è½¦èº«æˆ–å·¥è£…å›ºå®šåœ¨ç¢°æ’è¯•éªŒæ»‘å°ä¸Š,å®‰è£…æ–¹å‘æ¨¡æ‹Ÿæ­£é¢ç¢°æ’ã€‚\"\
            ,\n        \"children\": [],\n        \"full_path\": \"B.2 è¯•éªŒé¡¹ç›®/B.2.1\
            \ æ­£é¢ç¢°æ’/B.2.1.1 æ»‘å°æ­£é¢ç¢°æ’è¯•éªŒ/B.2.1.1.1 \"\n      },\n      {\n        \"chapter_id\"\
            : \"B.2.1.1.2\",\n        \"chapter_title\": \"\",\n        \"raw_text\"\
            : \"æ»‘å°æŒ‰ç…§ä»¥ä¸‹åŠ é€Ÿåº¦æ³¢å½¢ä¹‹ä¸€è¿›è¡Œç¢°æ’è¯•éªŒã€‚ a) ä½¿ç”¨åˆ¶é€ å•†æŒ‡å®šçš„åŠ é€Ÿåº¦æ³¢å½¢è¿›è¡Œè¯•éªŒ,æŒ‡å®šçš„åŠ é€Ÿåº¦æ³¢å½¢åº”ä¸ºåœ¨B.2.1.2ä¸­æè¿°çš„å®è½¦ç¢°æ’è¯•éªŒæ¡ä»¶ä¸­,è½¦èº«éå˜å½¢åŒºåŸŸé‡‡é›†çš„åŠ é€Ÿåº¦-æ—¶é—´æ›²çº¿,å¹¶ç»è¿‡æ»¤æ³¢ç­‰çº§CFC60\
            \ æ»¤æ³¢æˆ–100Hzä½é€šæ»¤æ³¢ã€‚å®é™…è¯•éªŒç»“æœæ³¢å½¢çš„ç§¯åˆ†é€Ÿåº¦å˜åŒ–é‡Î”vs(t)åº”åœ¨ä»»æ„æ—¶åˆ»,ä¸è¶…è¿‡æŒ‡å®šæ³¢å½¢çš„ç§¯åˆ†é€Ÿåº¦å˜åŒ–é‡[Î”vt(t)Â±1]km/hçš„èŒƒå›´ã€‚\\\
            nb) æŒ‰å›¾B.1 çš„æ ‡å‡†åŠ é€Ÿåº¦é€šé“èŒƒå›´å’Œè¡¨B.1 çš„å‚æ•°è¿›è¡ŒåŠ é€Ÿæˆ–å‡é€Ÿ,å…¶é€Ÿåº¦å˜åŒ–é‡Î”v ä¸º(25Â±1)km/hã€‚\",\n      \
            \  \"children\": [],\n        \"full_path\": \"B.2 è¯•éªŒé¡¹ç›®/B.2.1 æ­£é¢ç¢°æ’/B.2.1.1\
            \ æ»‘å°æ­£é¢ç¢°æ’è¯•éªŒ/B.2.1.1.2 \"\n      }\n    ],\n    \"full_path\": \"B.2 è¯•éªŒé¡¹ç›®/B.2.1\
            \ æ­£é¢ç¢°æ’/B.2.1.1 æ»‘å°æ­£é¢ç¢°æ’è¯•éªŒ\"\n  }\n]\n```\n\n### è¾“å‡º\n```json\n[\n  [\n  \
            \  {\n      \"chapter_id\": \"B.2.1.1\",\n      \"test_name\": \"æ»‘å°æ­£é¢ç¢°æ’è¯•éªŒ(æ–¹æ¡ˆA)\"\
            ,\n      \"conditions\": [\n        \"å°†ç™½è½¦èº«æˆ–å·¥è£…å›ºå®šåœ¨ç¢°æ’è¯•éªŒæ»‘å°ä¸Šï¼Œå®‰è£…æ–¹å‘æ¨¡æ‹Ÿæ­£é¢ç¢°æ’ã€‚\"\
            ,\n        \"è¯•éªŒæ»‘å°æŒ‰åˆ¶é€ å•†æŒ‡å®šçš„åŠ é€Ÿåº¦æ³¢å½¢è¿›è¡Œè¯•éªŒã€‚\",\n        \"åŠ é€Ÿåº¦æ³¢å½¢æ¥æºï¼šå®è½¦ç¢°æ’è¯•éªŒæ¡ä»¶ä¸‹ï¼Œè½¦èº«éå˜å½¢åŒºåŸŸé‡‡é›†çš„åŠ é€Ÿåº¦-æ—¶é—´æ›²çº¿ã€‚\"\
            \n      ],\n      \"criteria\": [\n        \"æ£€æŸ¥ç¢°æ’è§¦å‘ä¿¡å·ï¼Œç¡®ä¿ä¸MSDä¸­è®°å½•çš„è§¦å‘ç±»å‹ä¸€è‡´ã€‚\"\
            ,\n        \"ç§¯åˆ†é€Ÿåº¦å˜åŒ–é‡Î”vs(t)åœ¨ä»»æ„æ—¶åˆ»ä¸å¾—è¶…å‡ºæŒ‡å®šæ³¢å½¢Î”vt(t)Â±1 km/hã€‚\"\n      ],\n  \
            \    \"equipment\": [\n        {\n          \"name\": \"ç¢°æ’è¯•éªŒæ»‘å°\",\n  \
            \        \"specification\": \"å¯æ¨¡æ‹Ÿæ­£é¢ç¢°æ’ï¼›æ”¯æŒè‡ªå®šä¹‰åŠ é€Ÿåº¦æ³¢å½¢è¾“å…¥\"\n        }\n    \
            \  ],\n      \"parameters\": [\n        {\n          \"item\": \"ç§¯åˆ†é€Ÿåº¦å˜åŒ–é‡Î”vs(t)\"\
            ,\n          \"constraint\": \"range_closed\",\n          \"value\": [\"\
            Î”vt(t)-1\", \"Î”vt(t)+1\"],\n          \"unit\": \"km/h\",\n          \"\
            source_text\": \"ç§¯åˆ†é€Ÿåº¦å˜åŒ–é‡Î”vs(t)åº”åœ¨ä»»æ„æ—¶åˆ»,ä¸è¶…è¿‡æŒ‡å®šæ³¢å½¢çš„ç§¯åˆ†é€Ÿåº¦å˜åŒ–é‡[Î”vt(t)Â±1]km/h\"\n\
            \        },\n        {\n          \"item\": \"æ»¤æ³¢ç­‰çº§\",\n          \"constraint\"\
            : \"enum\",\n          \"value\": [\"CFC60\", \"100Hzä½é€š\"],\n        \
            \  \"unit\": null,\n          \"source_text\": \"å¹¶ç»è¿‡æ»¤æ³¢ç­‰çº§CFC60æ»¤æ³¢æˆ–100Hzä½é€šæ»¤æ³¢\"\
            \n        }\n      ],\n      \"refs\": [\n        {\n          \"ref_type\"\
            : \"internal\",\n          \"doc_id\": null,\n          \"target_id\"\
            : \"B.2.1.2\",\n          \"anchor_text\": \"å®è½¦ç¢°æ’è¯•éªŒæ¡ä»¶\"\n        }\n \
            \     ]\n    },\n    {\n      \"chapter_id\": \"B.2.1.1\",\n      \"test_name\"\
            : \"æ»‘å°æ­£é¢ç¢°æ’è¯•éªŒ(æ–¹æ¡ˆB)\",\n      \"conditions\": [\n        \"å°†ç™½è½¦èº«æˆ–å·¥è£…å›ºå®šåœ¨ç¢°æ’è¯•éªŒæ»‘å°ä¸Šï¼Œå®‰è£…æ–¹å‘æ¨¡æ‹Ÿæ­£é¢ç¢°æ’ã€‚\"\
            ,\n        \"æŒ‰å›¾B.1æ ‡å‡†åŠ é€Ÿåº¦é€šé“å’Œè¡¨B.1å‚æ•°è¿›è¡Œæ»‘å°åŠ é€Ÿæˆ–å‡é€Ÿã€‚\"\n      ],\n      \"criteria\"\
            : [\n        \"æ£€æŸ¥ç¢°æ’è§¦å‘ä¿¡å·ï¼Œç¡®ä¿ä¸MSDä¸­è®°å½•çš„è§¦å‘ç±»å‹ä¸€è‡´ã€‚\",\n        \"é€Ÿåº¦å˜åŒ–é‡Î”våº”ä¸º25Â±1\
            \ km/hã€‚\"\n      ],\n      \"equipment\": [\n        {\n          \"name\"\
            : \"ç¢°æ’è¯•éªŒæ»‘å°\",\n          \"specification\": \"æ»¡è¶³å›¾B.1ã€è¡¨B.1åŠ é€Ÿåº¦æ›²çº¿å‚æ•°\"\n \
            \       }\n      ],\n      \"parameters\": [\n        {\n          \"\
            item\": \"é€Ÿåº¦å˜åŒ–é‡Î”v\",\n          \"constraint\": \"range_closed\",\n  \
            \        \"value\": [\"24\", \"26\"],\n          \"unit\": \"km/h\",\n\
            \          \"source_text\": \"é€Ÿåº¦å˜åŒ–é‡Î”vä¸º(25Â±1)km/h\"\n        },\n     \
            \ ],\n      \"refs\": [\n        {\n          \"ref_type\": \"internal\"\
            ,\n          \"doc_id\": null,\n          \"target_id\": \"å›¾B.1\",\n \
            \         \"anchor_text\": \"æ ‡å‡†åŠ é€Ÿåº¦é€šé“\"\n        },\n        {\n      \
            \    \"ref_type\": \"internal\",\n          \"doc_id\": null,\n      \
            \    \"target_id\": \"è¡¨B.1\",\n          \"anchor_text\": \"æ ‡å‡†åŠ é€Ÿåº¦å‚æ•°\"\n\
            \        }\n      ]\n    }\n  ]\n]\n```"
        - id: 8ee6a7aa-9eb1-44fd-8cf5-1d32d1a65fea
          role: user
          text: '{{#1756613329065.item#}}'
        selected: false
        title: LLM 2
        type: llm
        variables: []
        vision:
          enabled: false
      height: 89
      id: '1756613344845'
      parentId: '1756613329065'
      position:
        x: 205.75426834716745
        y: 81.75426834716768
      positionAbsolute:
        x: 2928.2457316528325
        y: 865.3042817503975
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
      zIndex: 1002
    - data:
        code: "import json\nimport ast\nimport re\nfrom typing import Dict, List,\
          \ Any\n\ndef parse_input(src: str) -> List[Dict[str, Any]]:\n    \"\"\"\
          é¢„å¤„ç†å¹¶è§£æè¾“å…¥å­—ç¬¦ä¸²ä¸ºåˆ—è¡¨å­—å…¸ç»“æ„\"\"\"\n    try:\n        # 1. æŒ‰ </think> åˆ‡åˆ†å¹¶å–ååŠæ®µ\n  \
          \      src = src.rsplit('</think>', 1)[-1].strip()\n\n        # 2. å…ˆå¹²æ‰ deepseek\
          \ V3 å¯èƒ½æ’å…¥çš„â€œæé€Ÿâ€æˆ–å•ç‹¬çš„â€œæâ€\n        src = re.sub(r'æé€Ÿ\\s*', '', src)   # å»æ‰â€œæé€Ÿâ€åŠå…¶åå¯èƒ½çš„å¤šä½™ç©ºç™½\n\
          \        src = re.sub(r'(?<!\\w)æ(?!\\w)', '', src)  # å»æ‰å­¤ç«‹å‡ºç°çš„â€œæâ€\n\n  \
          \      # 3. å°è¯•å¤šç§è§£æè·¯å¾„\n        try:\n            data = json.loads(src)\n\
          \        except json.JSONDecodeError:\n            try:\n              \
          \  if src.startswith('\\ufeff'):\n                    src = src[1:]\n  \
          \              data = ast.literal_eval(src)\n            except (SyntaxError,\
          \ ValueError):\n                try:\n                    processed = src.replace('\\\
          \\\"', '\"').replace('\\\\n', '\\n')\n                    data = json.loads(processed)\n\
          \                except Exception as e:\n                    raise ValueError(f\"\
          æ— æ³•è§£æï¼š{str(e)}\")\n\n        # 4. ä¿è¯è¿”å› list\n        if isinstance(data,\
          \ dict):\n            return [data]\n        elif isinstance(data, list):\n\
          \            return data\n        else:\n            return []\n    except\
          \ Exception as e:\n        raise ValueError(f\"é¢„å¤„ç†å¤±è´¥: {str(e)}\")\n\n\n\
          def main(arg1: str) -> dict:\n    data = parse_input(arg1)\n    return {\n\
          \        \"result\": json.dumps(data, ensure_ascii=False),\n    }"
        code_language: python3
        desc: ''
        error_strategy: fail-branch
        isInIteration: true
        isInLoop: false
        iteration_id: '1756613329065'
        outputs:
          result:
            children: null
            type: string
        selected: false
        title: ä»£ç æ‰§è¡Œ 6
        type: code
        variables:
        - value_selector:
          - '1756613344845'
          - text
          value_type: string
          variable: arg1
      height: 89
      id: '1756613456815'
      parentId: '1756613329065'
      position:
        x: 509.75426834716745
        y: 81.75426834716768
      positionAbsolute:
        x: 3232.2457316528325
        y: 865.3042817503975
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
      zIndex: 1002
    - data:
        code: "import os, json, traceback, subprocess, platform\n\ndef main(arg1:\
          \ list[str], arg2: str, file_name) -> dict:\n    \"\"\"\n    å°†è¯•éªŒæ•°æ®ä¸ç« èŠ‚ä¿¡æ¯ä¸€ä¸€å¯¹åº”ï¼Œç”Ÿæˆç›®æ ‡æ ¼å¼\n\
          \    [\n        {\n            \"file\": \"regulation\",\n            \"\
          section\": \"é™„å½•B\",\n            \"experiments\": [...]\n        },\n  \
          \      ...\n    ]\n    \"\"\"\n    # 1. å…ˆæŠŠ arg2 çš„å­—ç¬¦ä¸²ååºåˆ—åŒ–æˆåˆ—è¡¨\n    sec_list\
          \ = json.loads(arg2)\n\n    # 2. ä¾æ¬¡å¤„ç† arg1 ä¸­çš„æ¯ä¸ªå…ƒç´ \n    result = []\n   \
          \ for sec, exp_str in zip(sec_list, arg1):\n        # æŠŠå½“å‰è¯•éªŒå­—ç¬¦ä¸²ååºåˆ—åŒ–æˆåˆ—è¡¨\n\
          \        experiments = json.loads(exp_str)\n\n        # 3. æ‹¼æˆç›®æ ‡å­—å…¸\n    \
          \    result.append({\n            \"file\": sec[\"file\"],\n           \
          \ \"section\": sec[\"section\"],\n            \"experiments\": experiments\n\
          \        })\n        \n\n\n    # å»æ‰æ‰©å±•åï¼Œå†æ‹¼ .json\n    base_name = os.path.splitext(file_name)[0]\
          \          # å¾—åˆ°æ— åç¼€çš„çº¯æ–‡ä»¶å\n    path = f\"/tmp/mydata/{base_name}/experiment.json\"\
          \n\n    # 3. è°ƒè¯•ä¿¡æ¯\n    log = []\n    log.append(f\"[PWD] å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}\"\
          )\n    log.append(f\"[PATH] ç»å¯¹è·¯å¾„: {os.path.abspath(path)}\")\n    log.append(f\"\
          [DIR] çˆ¶ç›®å½•æ˜¯å¦å­˜åœ¨: {os.path.exists(os.path.dirname(path))}\")\n    log.append(f\"\
          [VOLUMES] æŒ‚è½½ä¿¡æ¯: {subprocess.getoutput('mount | grep mydata')}\")\n\n   \
          \ # 4. å†™æ–‡ä»¶\n    try:\n        os.makedirs(os.path.dirname(path), exist_ok=True)\n\
          \        with open(path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(result,\
          \ f, ensure_ascii=False, indent=2)\n        log.append(\"[SUCCESS] æ–‡ä»¶å·²å†™å…¥\"\
          )\n    except Exception as e:\n        log.append(f\"[ERROR] {type(e).__name__}:\
          \ {e}\")\n        log.append(traceback.format_exc())\n\n    # 5. å†æ£€æŸ¥ä¸€æ¬¡\n\
          \    log.append(f\"[EXISTS] æ–‡ä»¶æ˜¯å¦å­˜åœ¨: {os.path.exists(path)}\")\n    if os.path.exists(path):\n\
          \        log.append(f\"[SIZE] æ–‡ä»¶å¤§å°: {os.path.getsize(path)} å­—èŠ‚\")\n\n  \
          \  return {\"debug\": \"\\n\".join(log), \"result\": json.dumps(result,\
          \ ensure_ascii=False)}"
        code_language: python3
        desc: ''
        outputs:
          debug:
            children: null
            type: string
          result:
            children: null
            type: string
        selected: false
        title: ä»£ç æ‰§è¡Œ 7
        type: code
        variables:
        - value_selector:
          - '1756613329065'
          - output
          value_type: array[string]
          variable: arg1
        - value_selector:
          - '1756563307317'
          - experiment_infos
          value_type: string
          variable: arg2
        - value_selector:
          - '1756550268945'
          - file
          - name
          value_type: file
          variable: file_name
      height: 53
      id: '1756618044071'
      position:
        x: 4506.491463305665
        y: 783.5500134032299
      positionAbsolute:
        x: 4506.491463305665
        y: 783.5500134032299
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        code: "import json\nimport os\nfrom collections import defaultdict\n\ndef\
          \ build_chapter_index(tree_data):\n    \"\"\"æ„å»ºchapterç´¢å¼•ï¼Œkeyä¸º(file, section,\
          \ chapter_id)ï¼Œvalueä¸ºchapterå¯¹è±¡çš„å¼•ç”¨\"\"\"\n    chapter_index = {}\n    section_index\
          \ = {}\n    \n    def add_chapter_recursive(chapter, file_name, section_name):\n\
          \        \"\"\"é€’å½’æ·»åŠ ç« èŠ‚åŠå…¶å­ç« èŠ‚åˆ°ç´¢å¼•ä¸­\"\"\"\n        if 'chapter_id' in chapter:\n\
          \            chapter_id = chapter['chapter_id']\n            chapter_key\
          \ = (file_name, section_name, chapter_id)\n            chapter_index[chapter_key]\
          \ = chapter\n        \n        # é€’å½’å¤„ç†å­ç« èŠ‚\n        if 'children' in chapter:\n\
          \            for child_chapter in chapter['children']:\n               \
          \ add_chapter_recursive(child_chapter, file_name, section_name)\n    \n\
          \    for file_data in tree_data:\n        file_name = file_data['file']\n\
          \        for section in file_data['sections']:\n            section_name\
          \ = section['section']\n            section_key = (file_name, section_name)\n\
          \            section_index[section_key] = section\n            \n      \
          \      # å¤„ç†æ¯ä¸ªé¡¶çº§ç« èŠ‚\n            for chapter in section['chapters']:\n   \
          \             add_chapter_recursive(chapter, file_name, section_name)\n\
          \    \n    return chapter_index, section_index\n\ndef merge_final_tree_data(chapter_index,\
          \ final_tree_data):\n    \"\"\"å°†final_treeçš„æ•°æ®èåˆåˆ°treeä¸­\"\"\"\n    merged_count\
          \ = 0\n    not_found_count = 0\n    \n    for final_section in final_tree_data:\n\
          \        file_name = final_section['file']\n        section_name = final_section['section']\n\
          \        \n        for final_chapter in final_section['chapters']:\n   \
          \         if 'chapter_id' not in final_chapter:\n                continue\n\
          \                \n            chapter_id = final_chapter['chapter_id']\n\
          \            chapter_key = (file_name, section_name, chapter_id)\n     \
          \       \n            # O(1)æŸ¥æ‰¾\n            tree_chapter = chapter_index.get(chapter_key)\n\
          \            \n            if tree_chapter:\n                # ç›´æ¥ä¿®æ”¹å¼•ç”¨çš„å¯¹è±¡\n\
          \                tree_chapter['parameters'] = final_chapter.get('paramaters',\
          \ [])\n                tree_chapter['topic_keywords'] = final_chapter.get('topic_keywords',\
          \ [])\n                tree_chapter['context_keywords'] = final_chapter.get('context_keywords',\
          \ [])\n                tree_chapter['refs'] = final_chapter.get('refs',\
          \ [])\n                tree_chapter['table_headers'] = final_chapter.get('table_headers',\
          \ [])\n                merged_count += 1\n            else:\n          \
          \      not_found_count += 1\n    \n    return merged_count, not_found_count\n\
          \ndef merge_result_data(chapter_index, result_data):\n    \"\"\"å°†resultçš„experimentsæ•°æ®èåˆåˆ°treeä¸­\"\
          \"\"\n    merged_count = 0\n    not_found_count = 0\n    \n    for result_section\
          \ in result_data:\n        file_name = result_section['file']\n        section_name\
          \ = result_section['section']\n        experiments_groups = result_section.get('experiments',\
          \ [])\n        \n        # éå†æ¯ä¸ªå®éªŒç»„\n        for experiments_group in experiments_groups:\n\
          \            if not experiments_group:\n                continue\n     \
          \           \n            # è·å–ç¬¬ä¸€ä¸ªå®éªŒçš„chapter_idä½œä¸ºè¯¥ç»„çš„å®šä½æ ‡è¯†\n            first_experiment\
          \ = experiments_group[0]\n            chapter_id = first_experiment.get('chapter_id')\n\
          \            \n            if not chapter_id:\n                not_found_count\
          \ += 1\n                continue\n            \n            chapter_key\
          \ = (file_name, section_name, chapter_id)\n            # O(1)æŸ¥æ‰¾å¯¹åº”çš„ç« èŠ‚\n \
          \           chapter = chapter_index.get(chapter_key)\n            \n   \
          \         if chapter:\n                # å°†è¯¥ç»„å®éªŒæ·»åŠ åˆ°å¯¹åº”ç« èŠ‚çš„experimentså­—æ®µ\n  \
          \              if 'experiments' not in chapter:\n                    chapter['experiments']\
          \ = []\n                chapter['experiments'].extend(experiments_group)\n\
          \                merged_count += 1\n            else:\n                not_found_count\
          \ += 1\n    \n    return merged_count, not_found_count\n\ndef main(tree:\
          \ str, final_tree: str, file_name, result: str) -> dict:\n    # åŠ è½½JSONæ•°æ®\n\
          \    tree_data = json.loads(tree)\n    final_tree_data = json.loads(final_tree)\n\
          \    result_data = json.loads(result)\n    # result_data = []\n    \n  \
          \  # æ„å»ºç´¢å¼•\n    chapter_index, section_index = build_chapter_index(tree_data)\n\
          \    \n    # èåˆæ•°æ®\n    chapter_merged, chapter_not_found = merge_final_tree_data(chapter_index,\
          \ final_tree_data)\n    experiment_merged, experiment_not_found = merge_result_data(chapter_index,\
          \ result_data)\n    \n    # ç»Ÿè®¡ä¿¡æ¯\n    total_chapters = len(chapter_index)\n\
          \    chapters_with_params = 0\n    chapters_with_keywords = 0\n    chapters_with_experiments\
          \ = 0\n    \n    for chapter in chapter_index.values():\n        if chapter.get('parameters'):\n\
          \            chapters_with_params += 1\n        if chapter.get('topic_keywords'):\n\
          \            chapters_with_keywords += 1\n        if chapter.get('experiments'):\n\
          \            chapters_with_experiments += 1\n    \n    # åˆå¹¶åçš„æ•°æ®å°±æ˜¯ä¿®æ”¹åçš„tree_data\n\
          \    merged_tree = tree_data\n\n    # å»æ‰æ‰©å±•åï¼Œå†æ‹¼ .json\n    base_name = os.path.splitext(file_name)[0]\
          \          # å¾—åˆ°æ— åç¼€çš„çº¯æ–‡ä»¶å\n    path = f\"/tmp/mydata/{base_name}/{base_name}.json\"\
          \n    os.makedirs(os.path.dirname(path), exist_ok=True)\n    with open(path,\
          \ \"w\", encoding=\"utf-8\") as f:\n        json.dump(merged_tree, f, ensure_ascii=False,\
          \ indent=2)\n    \n    return {\n        \"result\": f\"èåˆå®Œæˆï¼ç»Ÿè®¡ä¿¡æ¯: æ€»ç« èŠ‚æ•°:\
          \ {total_chapters}, å‚æ•°ç« èŠ‚: {chapters_with_params}, å…³é”®è¯ç« èŠ‚: {chapters_with_keywords},\
          \ è¯•éªŒç« èŠ‚: {chapters_with_experiments}, èåˆæˆåŠŸ: ç« èŠ‚æ•°æ®{chapter_merged}ä¸ª, è¯•éªŒæ•°æ®{experiment_merged}ä¸ª\"\
          \n    }"
        code_language: python3
        desc: ''
        outputs:
          result:
            children: null
            type: string
        selected: false
        title: ä»£ç æ‰§è¡Œ 8
        type: code
        variables:
        - value_selector:
          - '1756550411122'
          - tree
          value_type: string
          variable: tree
        - value_selector:
          - '1756563307317'
          - final_tree
          value_type: string
          variable: final_tree
        - value_selector:
          - '1756550268945'
          - file
          - name
          value_type: file
          variable: file_name
        - value_selector:
          - '1756618044071'
          - result
          value_type: string
          variable: result
      height: 53
      id: '1756629493937'
      position:
        x: 4810.491463305665
        y: 783.5500134032299
      positionAbsolute:
        x: 4810.491463305665
        y: 783.5500134032299
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        context:
          enabled: false
          variable_selector: []
        desc: ''
        isInIteration: true
        isInLoop: false
        iteration_id: '1756557801310'
        model:
          completion_params: {}
          mode: chat
          name: deepseek-reasoner
          provider: langgenius/deepseek/deepseek
        prompt_template:
        - id: fb74cb72-3859-4cad-ac1e-cdbb9834f569
          role: system
          text: "# è§’è‰²ï¼ˆRoleï¼‰\nä½ æ˜¯ä¸€åä¸¥æ ¼çš„ JSON è¾“å‡ºä¿®å¤å™¨ï¼ˆJSON Fixerï¼‰ã€‚  \nä½ çš„èŒè´£æ˜¯ï¼š\n- æ¥æ”¶ä¸€ä¸ªå¯èƒ½æ ¼å¼é”™è¯¯çš„\
            \ JSON å­—ç¬¦ä¸²\n- ä¸¥æ ¼ä¿®å¤å…¶ä¸­çš„è¯­æ³•é—®é¢˜æˆ–ç»“æ„é—®é¢˜ã€‚\n- è¾“å‡ºå®Œå…¨ç¬¦åˆé¢„æœŸæ ¼å¼çš„ JSON å¯¹è±¡ã€‚\n\n---\n\n# è¾“å…¥ï¼ˆInputï¼‰\n\
            ä½ å°†æ”¶åˆ°ï¼š\n**raw_output**ï¼šæ¨¡å‹åŸå§‹è¾“å‡ºï¼Œä½ éœ€è¦å¿½ç•¥ä»â€<think>â€œåˆ°\"</think>\"çš„éƒ¨åˆ†ï¼Œåªä¿ç•™é”™è¯¯ JSONã€‚\n\
            \nç¤ºä¾‹è¾“å…¥ï¼š\nerror: Expecting property name enclosed in double quotes: line\
            \ 3 column 5 (char 25)\nraw_text: {file: \"doc1\", section: \"main\",\
            \ chapters: [ ... ]}\n\n---\n\n# è¾“å‡ºæ ¼å¼ï¼ˆOutputï¼‰\nè¯·è¾“å‡ºä¸€ä¸ª **å®Œæ•´ JSON å¯¹è±¡**ï¼Œå¿…é¡»ä¸¥æ ¼ç¬¦åˆä»¥ä¸‹ç»“æ„ï¼ˆä¸è¦è¾“å‡ºå¤šä½™çš„è§£é‡Šæ€§æ–‡å­—ï¼‰ï¼š\
            \  \n```json\n{\n  \"file\": \"string\",\n  \"section\": \"string\",\n\
            \  \"chapters\":[\n    {\n      \"chapter_id\": \"string\",\n      \"\
            scope\":\"string\",\n      \"topic_keywords\": [\"string\", \"...\"],\n\
            \      \"context_keywords\": [\"string\", \"...\"]\n    }\n  ]\n}\n```\n\
            \n# æ³¨æ„äº‹é¡¹\n\n1. ä½ éœ€è¦ä¸¥æ ¼æŒ‰ç…§è¾“å‡ºæ ¼å¼è¦æ±‚è¿›è¡Œä¿®å¤ï¼Œå¯é€šè¿‡è¾“å…¥jsonè¿›è¡Œæ¨æ–­ï¼Œç¡®ä¿ JSON å­—ç¬¦ä¸²å¯è¢« `json.loads()`\
            \ è§£æé€šè¿‡å³å¯\n2. ä¸¥æ ¼è¾“å‡ºä¸€ä¸ªåˆæ³• JSON å¯¹è±¡ï¼Œä¸å¾—è¾“å‡ºå¤šä½™æ–‡å­—ã€æ³¨é‡Šæˆ– Markdownã€‚\n3. å¦‚æœæŸå­—æ®µæ— å†…å®¹ï¼Œè¯·ä½¿ç”¨\
            \ `null` æˆ–ç©ºæ•°ç»„ `[]`ã€‚\n4. å¿…é¡»è¡¥å…¨ç¼ºå¤±å­—æ®µï¼Œä¸å…è®¸çœç•¥ä»»ä½•å­—æ®µã€‚"
        - id: c68f56a1-5429-4a52-b480-a030d0c3f431
          role: user
          text: 'error_type:

            {{#1756557929661.error_type#}}


            error_message:

            {{#1756557929661.error_message#}}


            raw_text:

            {{#1756557804693.text#}}

            '
        selected: false
        title: LLM 3
        type: llm
        variables: []
        vision:
          enabled: false
      height: 89
      id: '1756698267184'
      parentId: '1756557801310'
      position:
        x: 812
        y: 154.5
      positionAbsolute:
        x: 1450
        y: 506.5
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
      zIndex: 1002
    - data:
        desc: ''
        isInIteration: true
        isInLoop: false
        iteration_id: '1756557801310'
        output_type: string
        selected: false
        title: å˜é‡èšåˆå™¨
        type: variable-aggregator
        variables:
        - - '1756557929661'
          - result
        - - '1756698848564'
          - result
      height: 129
      id: '1756698812908'
      parentId: '1756557801310'
      position:
        x: 1420
        y: 65
      positionAbsolute:
        x: 2058
        y: 417
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
      zIndex: 1002
    - data:
        code: "import json\nimport ast\nimport re\nfrom typing import Dict, List,\
          \ Any\n\ndef parse_input(src: str) -> List[Dict[str, Any]]:\n    \"\"\"\
          é¢„å¤„ç†å¹¶è§£æè¾“å…¥å­—ç¬¦ä¸²ä¸ºåˆ—è¡¨å­—å…¸ç»“æ„\"\"\"\n    try:\n        # 1. æŒ‰ </think> åˆ‡åˆ†å¹¶å–ååŠæ®µ\n  \
          \      src = src.rsplit('</think>', 1)[-1].strip()\n\n        # 2. å…ˆå¹²æ‰ deepseek\
          \ V3 å¯èƒ½æ’å…¥çš„â€œæé€Ÿâ€æˆ–å•ç‹¬çš„â€œæâ€\n        src = re.sub(r'æé€Ÿ\\s*', '', src)   # å»æ‰â€œæé€Ÿâ€åŠå…¶åå¯èƒ½çš„å¤šä½™ç©ºç™½\n\
          \        src = re.sub(r'(?<!\\w)æ(?!\\w)', '', src)  # å»æ‰å­¤ç«‹å‡ºç°çš„â€œæâ€\n\n  \
          \      # 3. å°è¯•å¤šç§è§£æè·¯å¾„\n        try:\n            data = json.loads(src)\n\
          \        except json.JSONDecodeError:\n            try:\n              \
          \  if src.startswith('\\ufeff'):\n                    src = src[1:]\n  \
          \              data = ast.literal_eval(src)\n            except (SyntaxError,\
          \ ValueError):\n                try:\n                    processed = src.replace('\\\
          \\\"', '\"').replace('\\\\n', '\\n')\n                    data = json.loads(processed)\n\
          \                except Exception as e:\n                    raise ValueError(f\"\
          æ— æ³•è§£æï¼š{str(e)}\")\n\n        # 4. ä¿è¯è¿”å› list\n        if isinstance(data,\
          \ dict):\n            return [data]\n        elif isinstance(data, list):\n\
          \            return data\n        else:\n            return []\n    except\
          \ Exception as e:\n        raise ValueError(f\"é¢„å¤„ç†å¤±è´¥: {str(e)}\")\n\n\n\
          def main(arg1: str) -> dict:\n    data = parse_input(arg1)\n    return {\n\
          \        \"result\": json.dumps(data, ensure_ascii=False),\n    }"
        code_language: python3
        desc: ''
        isInIteration: true
        isInLoop: false
        iteration_id: '1756557801310'
        outputs:
          result:
            children: null
            type: string
        selected: false
        title: ä»£ç æ‰§è¡Œ 9
        type: code
        variables:
        - value_selector:
          - '1756698267184'
          - text
          value_type: string
          variable: arg1
      height: 53
      id: '1756698848564'
      parentId: '1756557801310'
      position:
        x: 1116
        y: 172.5
      positionAbsolute:
        x: 1754
        y: 524.5
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
      zIndex: 1002
    - data:
        context:
          enabled: false
          variable_selector: []
        desc: ''
        isInIteration: true
        isInLoop: false
        iteration_id: '1756613329065'
        model:
          completion_params: {}
          mode: chat
          name: deepseek-reasoner
          provider: langgenius/deepseek/deepseek
        prompt_template:
        - id: 3b1b8e69-21ca-4e8c-9eb0-e45279111fc6
          role: system
          text: "# è§’è‰²ï¼ˆRoleï¼‰\nä½ æ˜¯ä¸€åä¸¥æ ¼çš„ JSON è¾“å‡ºä¿®å¤å™¨ï¼ˆJSON Fixerï¼‰ã€‚  \nä½ çš„èŒè´£æ˜¯ï¼š\n- æ¥æ”¶ä¸€ä¸ªå¯èƒ½æ ¼å¼é”™è¯¯çš„\
            \ JSON å­—ç¬¦ä¸²\n- ä¸¥æ ¼ä¿®å¤å…¶ä¸­çš„è¯­æ³•é—®é¢˜æˆ–ç»“æ„é—®é¢˜ã€‚\n- è¾“å‡ºå®Œå…¨ç¬¦åˆé¢„æœŸæ ¼å¼çš„ JSON å¯¹è±¡ã€‚\n\n---\n\n# è¾“å…¥ï¼ˆInputï¼‰\n\
            ä½ å°†æ”¶åˆ°ï¼š\n**raw_output**ï¼šæ¨¡å‹åŸå§‹è¾“å‡ºï¼Œä½ éœ€è¦å¿½ç•¥ä»â€<think>â€œåˆ°\"</think>\"çš„éƒ¨åˆ†ï¼Œåªä¿ç•™é”™è¯¯ JSONã€‚\n\
            \nç¤ºä¾‹è¾“å…¥ï¼š\nerror: Expecting property name enclosed in double quotes: line\
            \ 3 column 5 (char 25)\nraw_text: {file: \"doc1\", section: \"main\",\
            \ chapters: [ ... ]}\n\n---\n\n# è¾“å‡ºæ ¼å¼ï¼ˆOutputï¼‰\nè¯·è¾“å‡ºä¸€ä¸ª **å®Œæ•´ JSON å¯¹è±¡**ï¼Œå¿…é¡»ä¸¥æ ¼ç¬¦åˆä»¥ä¸‹ç»“æ„ï¼ˆä¸è¦è¾“å‡ºå¤šä½™çš„è§£é‡Šæ€§æ–‡å­—ï¼‰ï¼š\
            \  \n```json\n[\n  [\n    {\n      \"chapter_id\": \"string\",    // æ¥æºç« èŠ‚çš„chapter_idï¼Œä¿è¯å¯è¿½æº¯æ€§\n\
            \      \"test_name\": \"string\",     // æµ‹è¯•åç§°ï¼Œä»ç« èŠ‚æ ‡é¢˜æˆ–è·¯å¾„æç‚¼ï¼Œç®€æ´ä¸”å”¯ä¸€\n     \
            \ \"conditions\": [            // æµ‹è¯•æ¡ä»¶ï¼šè¯•ä»¶çŠ¶æ€ã€å®‰è£…ã€ç¯å¢ƒç­‰ï¼Œå®Œæ•´åˆ†ç‚¹åˆ—å‡º\n        \"\
            string\"\n      ],\n      \"criteria\": [              // åˆ¤å®šæ ‡å‡†ï¼šæ˜ç¡®æ£€æµ‹é¡¹+è¾¾æ ‡æ¡ä»¶ï¼Œåˆ†ç‚¹åˆ—å‡º\n\
            \        \"string\"\n      ],\n      \"equipment\": [             // æ‰€éœ€è®¾å¤‡åŠè§„æ ¼ï¼Œä¾¿äºè®¾å¤‡èƒ½åŠ›éªŒè¯\n\
            \        {\n          \"name\": \"string\",          // è®¾å¤‡åç§°\n       \
            \   \"specification\": \"string\"  // è®¾å¤‡è§„æ ¼/æ€§èƒ½\n        }\n      ],\n \
            \     \"parameters\": [            // æå–çš„å…³é”®å‚æ•°\n        {\n          \"\
            item\": \"string\",          // å‚æ•°é¡¹\n          \"constraint\": \"string\"\
            ,    // çº¦æŸï¼š<=|<|=|>=|>|range_closed|range_open|enum|boolean\n        \
            \  \"value\": \"string|array|null\", // å‚æ•°å€¼ï¼Œå¯ä¸ºæ•°å­—ã€èŒƒå›´æˆ–æšä¸¾\n          \"unit\"\
            : \"string|null\",     // å•ä½ï¼Œè‹¥æ— åˆ™null\n          \"source_text\": \"string\"\
            \    // åŸæ–‡å®Œæ•´ç‰‡æ®µï¼Œä¿ç•™ç¬¦å·ï¼Œä¾¿äºè¿½æº¯\n        }\n      ],\n      \"refs\": [     \
            \             // å¼•ç”¨ä¿¡æ¯\n        {\n          \"ref_type\": \"internal|external\"\
            , // å†…éƒ¨/å¤–éƒ¨å¼•ç”¨\n          \"doc_id\": \"string|null\",         // å¤–éƒ¨æ ‡å‡†ç¼–å·ï¼›å†…éƒ¨å¼•ç”¨å¡«null\n\
            \          \"target_id\": \"string\",           // å¼•ç”¨ç›®æ ‡ç¼–å·ï¼ˆå¦‚â€œB.2.1.1â€ã€â€œè¡¨B.1â€ï¼‰\n\
            \          \"anchor_text\": \"string\"          // å¼•ç”¨çš„ç®€è¦ä¸Šä¸‹æ–‡æè¿°\n      \
            \  }\n      ]\n    }\n  ]\n]\n```\n\n# æ³¨æ„äº‹é¡¹\n\n1. ä½ éœ€è¦ä¸¥æ ¼æŒ‰ç…§è¾“å‡ºæ ¼å¼è¦æ±‚è¿›è¡Œä¿®å¤ï¼Œå¯é€šè¿‡è¾“å…¥jsonè¿›è¡Œæ¨æ–­ï¼Œç¡®ä¿\
            \ JSON å­—ç¬¦ä¸²å¯è¢« `json.loads()` è§£æé€šè¿‡å³å¯\n2. ä¸¥æ ¼è¾“å‡ºä¸€ä¸ªåˆæ³• JSON å¯¹è±¡ï¼Œä¸å¾—è¾“å‡ºå¤šä½™æ–‡å­—ã€æ³¨é‡Šæˆ– Markdownã€‚\n\
            3. å¦‚æœæŸå­—æ®µæ— å†…å®¹ï¼Œè¯·ä½¿ç”¨ `null` æˆ–ç©ºæ•°ç»„ `[]`ã€‚\n4. å¿…é¡»è¡¥å…¨ç¼ºå¤±å­—æ®µï¼Œä¸å…è®¸çœç•¥ä»»ä½•å­—æ®µã€‚"
        - id: e9d87d24-8cf0-41a1-b1d5-c976fc1c181e
          role: user
          text: 'error_type:

            {{#1756613456815.error_type#}}


            error_message:

            {{#1756613456815.error_message#}}


            raw_text:

            {{#1756613344845.text#}}

            '
        selected: false
        title: LLM 4
        type: llm
        variables: []
        vision:
          enabled: false
      height: 89
      id: '1757762170810'
      parentId: '1756613329065'
      position:
        x: 805.8389388549558
        y: 178.32128815214674
      positionAbsolute:
        x: 3528.330402160621
        y: 961.8713015553766
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
      zIndex: 1002
    - data:
        code: "import json\nimport ast\nimport re\nfrom typing import Dict, List,\
          \ Any\n\ndef parse_input(src: str) -> List[Dict[str, Any]]:\n    \"\"\"\
          é¢„å¤„ç†å¹¶è§£æè¾“å…¥å­—ç¬¦ä¸²ä¸ºåˆ—è¡¨å­—å…¸ç»“æ„\"\"\"\n    try:\n        # 1. æŒ‰ </think> åˆ‡åˆ†å¹¶å–ååŠæ®µ\n  \
          \      src = src.rsplit('</think>', 1)[-1].strip()\n\n        # 2. å…ˆå¹²æ‰ deepseek\
          \ V3 å¯èƒ½æ’å…¥çš„â€œæé€Ÿâ€æˆ–å•ç‹¬çš„â€œæâ€\n        src = re.sub(r'æé€Ÿ\\s*', '', src)   # å»æ‰â€œæé€Ÿâ€åŠå…¶åå¯èƒ½çš„å¤šä½™ç©ºç™½\n\
          \        src = re.sub(r'(?<!\\w)æ(?!\\w)', '', src)  # å»æ‰å­¤ç«‹å‡ºç°çš„â€œæâ€\n\n  \
          \      # 3. å°è¯•å¤šç§è§£æè·¯å¾„\n        try:\n            data = json.loads(src)\n\
          \        except json.JSONDecodeError:\n            try:\n              \
          \  if src.startswith('\\ufeff'):\n                    src = src[1:]\n  \
          \              data = ast.literal_eval(src)\n            except (SyntaxError,\
          \ ValueError):\n                try:\n                    processed = src.replace('\\\
          \\\"', '\"').replace('\\\\n', '\\n')\n                    data = json.loads(processed)\n\
          \                except Exception as e:\n                    raise ValueError(f\"\
          æ— æ³•è§£æï¼š{str(e)}\")\n\n        # 4. ä¿è¯è¿”å› list\n        if isinstance(data,\
          \ dict):\n            return [data]\n        elif isinstance(data, list):\n\
          \            return data\n        else:\n            return []\n    except\
          \ Exception as e:\n        raise ValueError(f\"é¢„å¤„ç†å¤±è´¥: {str(e)}\")\n\n\n\
          def main(arg1: str) -> dict:\n    data = parse_input(arg1)\n    return {\n\
          \        \"result\": json.dumps(data, ensure_ascii=False),\n    }"
        code_language: python3
        desc: ''
        isInIteration: true
        isInLoop: false
        iteration_id: '1756613329065'
        outputs:
          result:
            children: null
            type: string
        selected: false
        title: ä»£ç æ‰§è¡Œ 10
        type: code
        variables:
        - value_selector:
          - '1757762170810'
          - text
          value_type: string
          variable: arg1
      height: 53
      id: '1757762176397'
      parentId: '1756613329065'
      position:
        x: 1114.5881365502833
        y: 180.4906291677239
      positionAbsolute:
        x: 3837.0795998559483
        y: 964.0406425709538
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
      zIndex: 1002
    - data:
        desc: ''
        isInIteration: true
        isInLoop: false
        iteration_id: '1756613329065'
        output_type: string
        selected: false
        title: å˜é‡èšåˆå™¨ 2
        type: variable-aggregator
        variables:
        - - '1757762176397'
          - result
        - - '1756613456815'
          - result
      height: 129
      id: '1757762184699'
      parentId: '1756613329065'
      position:
        x: 1374.2622913938985
        y: 82.33412502691726
      positionAbsolute:
        x: 4096.753754699564
        y: 865.8841384301471
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
      zIndex: 1002
    - data:
        author: Dale
        desc: ''
        height: 88
        selected: false
        showAuthor: true
        text: '{"root":{"children":[{"children":[{"detail":0,"format":0,"mode":"normal","style":"","text":"æå–å…³é”®è¯ç­‰å­—æ®µ","type":"text","version":1}],"direction":"ltr","format":"","indent":0,"type":"paragraph","version":1,"textFormat":0,"textStyle":""}],"direction":"ltr","format":"","indent":0,"type":"root","version":1}}'
        theme: blue
        title: ''
        type: ''
        width: 240
      height: 88
      id: '1757850678552'
      position:
        x: 1299.8917635483372
        y: 203.92833256073843
      positionAbsolute:
        x: 1299.8917635483372
        y: 203.92833256073843
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom-note
      width: 240
    - data:
        author: Dale
        desc: ''
        height: 88
        selected: false
        showAuthor: true
        text: '{"root":{"children":[{"children":[{"detail":0,"format":0,"mode":"normal","style":"","text":"é¢„å¤„ç†pdfä¸ºjsonæ ¼å¼","type":"text","version":1}],"direction":"ltr","format":"","indent":0,"type":"paragraph","version":1,"textFormat":0,"textStyle":""}],"direction":"ltr","format":"","indent":0,"type":"root","version":1}}'
        theme: blue
        title: ''
        type: ''
        width: 240
      height: 88
      id: '1757850708484'
      position:
        x: 334.15313855200105
        y: 224.57661618703446
      positionAbsolute:
        x: 334.15313855200105
        y: 224.57661618703446
      sourcePosition: right
      targetPosition: left
      type: custom-note
      width: 240
    - data:
        author: Dale
        desc: ''
        height: 88
        selected: false
        showAuthor: true
        text: '{"root":{"children":[{"children":[{"detail":0,"format":0,"mode":"normal","style":"","text":"æå–å®éªŒ","type":"text","version":1}],"direction":"ltr","format":"","indent":0,"type":"paragraph","version":1,"textFormat":0,"textStyle":""}],"direction":"ltr","format":"","indent":0,"type":"root","version":1}}'
        theme: blue
        title: ''
        type: ''
        width: 240
      height: 88
      id: '1757850722503'
      position:
        x: 3469.8642784687045
        y: 643.462102402726
      positionAbsolute:
        x: 3469.8642784687045
        y: 643.462102402726
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom-note
      width: 240
    - data:
        code: "import json\nimport os\nfrom collections import defaultdict\n\ndef\
          \ build_chapter_index(tree_data):\n    \"\"\"æ„å»ºchapterç´¢å¼•ï¼Œkeyä¸º(file, section,\
          \ chapter_id)ï¼Œvalueä¸ºchapterå¯¹è±¡çš„å¼•ç”¨\"\"\"\n    chapter_index = {}\n    section_index\
          \ = {}\n    \n    def add_chapter_recursive(chapter, file_name, section_name):\n\
          \        \"\"\"é€’å½’æ·»åŠ ç« èŠ‚åŠå…¶å­ç« èŠ‚åˆ°ç´¢å¼•ä¸­\"\"\"\n        if 'chapter_id' in chapter:\n\
          \            chapter_id = chapter['chapter_id']\n            chapter_key\
          \ = (file_name, section_name, chapter_id)\n            chapter_index[chapter_key]\
          \ = chapter\n        \n        # é€’å½’å¤„ç†å­ç« èŠ‚\n        if 'children' in chapter:\n\
          \            for child_chapter in chapter['children']:\n               \
          \ add_chapter_recursive(child_chapter, file_name, section_name)\n    \n\
          \    for file_data in tree_data:\n        file_name = file_data['file']\n\
          \        for section in file_data['sections']:\n            section_name\
          \ = section['section']\n            section_key = (file_name, section_name)\n\
          \            section_index[section_key] = section\n            \n      \
          \      # å¤„ç†æ¯ä¸ªé¡¶çº§ç« èŠ‚\n            for chapter in section['chapters']:\n   \
          \             add_chapter_recursive(chapter, file_name, section_name)\n\
          \    \n    return chapter_index, section_index\n\ndef merge_final_tree_data(chapter_index,\
          \ final_tree_data):\n    \"\"\"å°†final_treeçš„æ•°æ®èåˆåˆ°treeä¸­\"\"\"\n    merged_count\
          \ = 0\n    not_found_count = 0\n    \n    for final_section in final_tree_data:\n\
          \        file_name = final_section['file']\n        section_name = final_section['section']\n\
          \        \n        for final_chapter in final_section['chapters']:\n   \
          \         if 'chapter_id' not in final_chapter:\n                continue\n\
          \                \n            chapter_id = final_chapter['chapter_id']\n\
          \            chapter_key = (file_name, section_name, chapter_id)\n     \
          \       \n            # O(1)æŸ¥æ‰¾\n            tree_chapter = chapter_index.get(chapter_key)\n\
          \            \n            if tree_chapter:\n                # ç›´æ¥ä¿®æ”¹å¼•ç”¨çš„å¯¹è±¡\n\
          \                tree_chapter['scope'] = final_chapter.get('scope', \"\"\
          )\n                tree_chapter['topic_keywords'] = final_chapter.get('topic_keywords',\
          \ [])\n                tree_chapter['context_keywords'] = final_chapter.get('context_keywords',\
          \ [])\n                tree_chapter['parameters'] = final_chapter.get('parameters',\
          \ [])\n                tree_chapter['refs'] = final_chapter.get('refs',\
          \ [])\n                tree_chapter['table_headers'] = final_chapter.get('table_headers',\
          \ [])\n                merged_count += 1\n            else:\n          \
          \      not_found_count += 1\n    \n    return merged_count, not_found_count\n\
          \ndef merge_result_data(chapter_index, result_data):\n    \"\"\"å°†resultçš„experimentsæ•°æ®èåˆåˆ°treeä¸­\"\
          \"\"\n    merged_count = 0\n    not_found_count = 0\n    \n    for result_section\
          \ in result_data:\n        file_name = result_section['file']\n        section_name\
          \ = result_section['section']\n        experiments_groups = result_section.get('experiments',\
          \ [])\n        \n        # éå†æ¯ä¸ªå®éªŒç»„\n        for experiments_group in experiments_groups:\n\
          \            if not experiments_group:\n                continue\n     \
          \           \n            # è·å–ç¬¬ä¸€ä¸ªå®éªŒçš„chapter_idä½œä¸ºè¯¥ç»„çš„å®šä½æ ‡è¯†\n            first_experiment\
          \ = experiments_group[0]\n            chapter_id = first_experiment.get('chapter_id')\n\
          \            \n            if not chapter_id:\n                not_found_count\
          \ += 1\n                continue\n            \n            chapter_key\
          \ = (file_name, section_name, chapter_id)\n            # O(1)æŸ¥æ‰¾å¯¹åº”çš„ç« èŠ‚\n \
          \           chapter = chapter_index.get(chapter_key)\n            \n   \
          \         if chapter:\n                # å°†è¯¥ç»„å®éªŒæ·»åŠ åˆ°å¯¹åº”ç« èŠ‚çš„experimentså­—æ®µ\n  \
          \              if 'experiments' not in chapter:\n                    chapter['experiments']\
          \ = []\n                chapter['experiments'].extend(experiments_group)\n\
          \                merged_count += 1\n            else:\n                not_found_count\
          \ += 1\n    \n    return merged_count, not_found_count\n\ndef main(tree:\
          \ str, final_tree: str, file_name) -> dict:\n    # åŠ è½½JSONæ•°æ®\n    tree_data\
          \ = json.loads(tree)\n    final_tree_data = json.loads(final_tree)\n   \
          \ # result_data = json.loads(result)\n    result_data = []\n    \n    #\
          \ æ„å»ºç´¢å¼•\n    chapter_index, section_index = build_chapter_index(tree_data)\n\
          \    \n    # èåˆæ•°æ®\n    chapter_merged, chapter_not_found = merge_final_tree_data(chapter_index,\
          \ final_tree_data)\n    experiment_merged, experiment_not_found = merge_result_data(chapter_index,\
          \ result_data)\n    \n    # ç»Ÿè®¡ä¿¡æ¯\n    total_chapters = len(chapter_index)\n\
          \    chapters_with_params = 0\n    chapters_with_keywords = 0\n    chapters_with_experiments\
          \ = 0\n    \n    for chapter in chapter_index.values():\n        if chapter.get('parameters'):\n\
          \            chapters_with_params += 1\n        if chapter.get('topic_keywords'):\n\
          \            chapters_with_keywords += 1\n        if chapter.get('experiments'):\n\
          \            chapters_with_experiments += 1\n    \n    # åˆå¹¶åçš„æ•°æ®å°±æ˜¯ä¿®æ”¹åçš„tree_data\n\
          \    merged_tree = tree_data\n\n    # å»æ‰æ‰©å±•åï¼Œå†æ‹¼ .json\n    base_name = os.path.splitext(file_name)[0]\
          \          # å¾—åˆ°æ— åç¼€çš„çº¯æ–‡ä»¶å\n    path = f\"/tmp/mydata/{base_name}/{base_name}.json\"\
          \n    os.makedirs(os.path.dirname(path), exist_ok=True)\n    with open(path,\
          \ \"w\", encoding=\"utf-8\") as f:\n        json.dump(merged_tree, f, ensure_ascii=False,\
          \ indent=2)\n    \n    return {\n        \"result\": f\"èåˆå®Œæˆï¼ç»Ÿè®¡ä¿¡æ¯: æ€»ç« èŠ‚æ•°:\
          \ {total_chapters}, å‚æ•°ç« èŠ‚: {chapters_with_params}, å…³é”®è¯ç« èŠ‚: {chapters_with_keywords},\
          \ è¯•éªŒç« èŠ‚: {chapters_with_experiments}, èåˆæˆåŠŸ: ç« èŠ‚æ•°æ®{chapter_merged}ä¸ª, è¯•éªŒæ•°æ®{experiment_merged}ä¸ª\"\
          \n    }"
        code_language: python3
        desc: ''
        outputs:
          result:
            children: null
            type: string
        selected: false
        title: ä»£ç æ‰§è¡Œ 8 (1)
        type: code
        variables:
        - value_selector:
          - '1756550411122'
          - tree
          value_type: string
          variable: tree
        - value_selector:
          - '1756563307317'
          - final_tree
          value_type: string
          variable: final_tree
        - value_selector:
          - '1756550268945'
          - file
          - name
          value_type: file
          variable: file_name
      height: 53
      id: '17580960600210'
      position:
        x: 5410.491463305665
        y: 352
      positionAbsolute:
        x: 5410.491463305665
        y: 352
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        desc: ''
        error_handle_mode: continue-on-error
        height: 304
        is_parallel: true
        iterator_input_type: array[string]
        iterator_selector:
        - '1756550411122'
        - array
        output_selector:
        - '1758105758637'
        - output
        output_type: array[string]
        parallel_nums: 10
        selected: false
        start_node_id: 1758096258752start
        title: è¿­ä»£ 1B
        type: iteration
        width: 1724
      height: 304
      id: '17580962587520'
      position:
        x: 638
        y: 731.7304907470502
      positionAbsolute:
        x: 638
        y: 731.7304907470502
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 1724
      zIndex: 1
    - data:
        context:
          enabled: false
          variable_selector: []
        desc: ''
        isInIteration: true
        isInLoop: false
        iteration_id: '17580962587520'
        model:
          completion_params: {}
          mode: chat
          name: deepseek-reasoner
          provider: langgenius/deepseek/deepseek
        prompt_template:
        - id: b8df94d2-1037-41d9-8649-be6e2790a7d5
          role: system
          text: "# è§’è‰²\n\nä½ æ˜¯ä¸€åæŠ€æœ¯æ ‡å‡†çŸ¥è¯†å·¥ç¨‹ä¸“å®¶ï¼Œä¸“æ³¨äºå°†æ±½è½¦åŠç›¸å…³é¢†åŸŸçš„æ ‡å‡†å’Œæ³•è§„æ–‡æ¡£è½¬åŒ–ä¸ºå¯ç»“æ„åŒ–è§£æçš„æ•°æ®èµ„äº§ï¼Œç”¨äºçŸ¥è¯†å›¾è°±å»ºç«‹ã€‚\n\
            \nä½ çš„ä»»åŠ¡æ˜¯åŸºäºè¾“å…¥çš„æ ‡å‡†æ–‡æ¡£ JSON æ ‘ç»“æ„ï¼Œç²¾å‡†æŠ½å–ç›¸å…³ä¿¡æ¯ã€‚ \n\n---\n\n# è¾“å…¥å†…å®¹\n\nä½ å°†æ”¶åˆ°ä¸€ä¸ª JSON æ•°ç»„ï¼ŒåŒ…å«ä¸€ä¸ªå¯¹è±¡ï¼Œå…¶ç»“æ„å¦‚ä¸‹ï¼š\n\
            \n- `file`: æ–‡ä»¶æ ‡è¯†ï¼ˆå¦‚ \"regulation\"ã€\"ANNEX 1\"ï¼‰\n- `sections`: å—æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼š\n\
            \  - `section`: æ ‡è¯†å½“å‰å—ï¼ˆå¦‚ \"MAIN\", \"APPENDIX 1\"ï¼‰\n  - `chapters`: ç« èŠ‚æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼š\n\
            \    - `chapter_id`: ç« èŠ‚ç¼–å·ï¼ˆä¿æŒåŸæ ·ï¼‰\n    - `chapter_title`: ç« èŠ‚æ ‡é¢˜\n    - `raw_text`:\
            \ è¯¥èŠ‚çš„çº¯æ–‡æœ¬å†…å®¹\n    - `children`: å­æ¡æ¬¾æ•°ç»„ï¼ˆç»“æ„ä¸çˆ¶çº§ç›¸åŒï¼‰\n    - `full_path`: å®Œæ•´è·¯å¾„ï¼ˆå¯é€‰ï¼‰\n\
            \n---\n\n# è¾“å‡ºæ ¼å¼\n\nè¾“å‡ºå¿…é¡»æ˜¯çº¯å‡€ã€å®Œæ•´çš„JSONå¯¹è±¡ï¼Œç¡®ä¿å¯ä»¥è¢«`json.loads()`ç›´æ¥è§£æï¼Œä¸”å¿…é¡»å®Œå…¨ç¬¦åˆä»¥ä¸‹ç»“æ„ï¼š\n\
            \n```json\n{\n  \"file\": \"string\",\n  \"section\": \"string\",\n  \"\
            chapters\":[\n    {\n      \"chapter_id\": \"string\",\n      \"parameters\"\
            : [\n        {\n          \"item\": \"string\",\n          \"constraint\"\
            : \"<|<=|>|>=|=|Â±|range\",\n          \"value\": \"string|null\",\n  \
            \        \"relative_to\": \"string|null\",\n          \"unit\": \"string|null\"\
            ,\n        }\n      ],\n      \"refs\": [\n        {\n          \"ref_type\"\
            : \"table|graph|clause|external\",\n          \"doc_id\": \"string|null\"\
            ,\n          \"target_id\": \"string\",\n          \"anchor_text\": \"\
            string\"\n        }\n      ],\n      \"table_headers\": [\"string\", \"\
            ...\"]\n    }\n  ],\n  \"experiment_root_ids\": [\"string\", \"...\"],\n\
            }\n\n```\n\n# å¤„ç†é€»è¾‘ï¼ˆé“¾å¼æ€è€ƒï¼‰\n\n1. **éå†ç« èŠ‚æ ‘**\n\n   éå†æ¯ä¸ª `chapter_id` åŠå…¶å­ç« èŠ‚ï¼Œåˆ†åˆ«å¤„ç†ï¼Œç¡®ä¿è¾“å‡ºä¸­**æ¯ä¸ªç« èŠ‚**éƒ½ç‹¬ç«‹æˆæ¡ã€‚æœ€åæ‰§è¡Œæ­¥éª¤5\n\
            \n2. **å‚æ•°æå– (`parameters`)**\n\n   - æå–æ ‡å‡†ä¸­çš„å®šé‡æŒ‡æ ‡æˆ–çº¦æŸæ¡ä»¶ï¼Œæ¯æ¡å‚æ•°ç‹¬ç«‹æˆå¯¹è±¡ï¼›**åªè¦æ²¡æœ‰å­©å­ä¸”æ²¡æœ‰æ ‡é¢˜çš„ç« èŠ‚ä¸­ï¼Œå‡ºç°å‚æ•°(æ•°å­—å½¢å¼)ï¼Œåˆ™å¿…æå–ï¼Œè¡¨æ ¼å†…å‚æ•°é™¤å¤–**\n\
            \   - `item`ï¼šä¿æŒåŸæ–‡æè¿°ï¼ŒæŠ½å–çº¦æŸçš„å…³é”®å¯¹è±¡ï¼Œæ¯”å¦‚ï¼šâ€œcarbon monoxideâ€\n   - `constraint`ï¼šæ”¯æŒ\
            \ `<`, `<=`, `>`, `>=`, `=`,`Â±`ç­‰å¸¸è§æ•°å­¦è¡¨è¾¾ï¼Œ`range`è¡¨ç¤ºåŒºé—´ã€‚\n   - `value`ï¼šä¿æŒæ•°å­—æˆ–èŒƒå›´ï¼ˆ`[1.0,1.5]`ç­‰ï¼‰ï¼Œä¸å¸¦å•ä½ï¼Œå€æ•°å’Œç™¾åˆ†æ¯”ç­‰ç»Ÿä¸€ç”¨å°æ•°ï¼ˆæ¯”å¦‚å°äº1.1å€å’Œä¸è¶…è¿‡ç™¾åˆ†ä¹‹åï¼Œåœ¨æ­¤å¤„å‡ç”¨1.1ï¼‰\n\
            \   - `relative_to`ï¼šåŸºå‡†å€¼ï¼Œæ¯”å¦‚1.5Â±0.1ä¸­çš„1.5ï¼Œå¯ä¸ºnull\n   - `unit`ï¼šç»Ÿä¸€è‹±æ–‡å•ä½\n\n\
            3. **å¼•ç”¨æå– (`refs`)**\n\n   - æå–å†…éƒ¨å¼•ç”¨ï¼ˆæœ¬æ ‡å‡†å†…ç« èŠ‚/è¡¨æ ¼/å›¾ç‰‡ï¼‰ä¸å¤–éƒ¨å¼•ç”¨ï¼ˆå¤–éƒ¨æ ‡å‡†ç¼–å·ï¼‰ï¼Œå¹¶åŒºåˆ† `ref_type`ã€‚\n\
            \   - `doc_id` å¡«å†™æ ‡å‡†ç¼–å·ï¼ˆå¦‚æœ‰ï¼‰ï¼Œå†…éƒ¨å¼•ç”¨å¡« `null`ã€‚\n   - `target_id` ä¿ç•™å¼•ç”¨ç›®æ ‡ç¼–å·ï¼ˆå¦‚â€œB.2.1.1â€ã€â€œè¡¨B.1â€ï¼‰ã€‚\n\
            \   - `anchor_text` åœ¨ä¸Šä¸‹æ–‡æå–ç®€è¦æ–‡æœ¬ï¼Œè¯´æ˜å¼•ç”¨çš„å†…å®¹ï¼Œç¡®ä¿å¯ç²¾ç¡®å®šä½ã€‚\n   - åŸæ–‡ç›¸åŒä½ç½®å‡ºç°çš„å¼•ç”¨ï¼Œå¯åªç”Ÿæˆä¸€æ¡å†…å®¹ï¼ŒåŒæ ·ä¿æŒå¹¶åˆ—å³å¯ã€‚ä¾‹å¦‚ï¼šæŒ‰Aã€Bè¿›è¡Œå®éªŒï¼Œæå–æ—¶å¯å°†Aã€Bå¹¶åˆ—ï¼Œè€Œä¸ç”¨ç”Ÿæˆä¸¤æ¡å†…å®¹ã€‚\n\
            \n4. **è¡¨æ ¼è¡¨å¤´ (`table_headers`)**\n\n   - å¦‚æœç« èŠ‚åŒ…å«è¡¨æ ¼å¼•ç”¨ï¼Œä»…æå–è¡¨å¤´å­—æ®µã€‚\n\n5. **å®éªŒç« èŠ‚è¯†åˆ«\
            \ (`experiment_root_ids`)**\n\n   - åç»­ä¼šæ ¹æ®æ­¤ç»“æœï¼Œä»å¯¹åº”çš„ç« èŠ‚å†…å®¹ä¸­æå–å®éªŒï¼Œæ•…æ ¹èŠ‚ç‚¹å¯ä»¥å…è®¸å‘ä¸Šå±‚å¦¥åï¼Œä¸å¯è¿‡åº¦ç»†åŒ–\n\
            \     \n   - åˆ¤æ–­ç« èŠ‚å†…å®¹æ˜¯å¦ä¸ºå®éªŒç« èŠ‚ï¼Œæ ‡è®°å…¶ `chapter_id` ä¸ºå®éªŒæ ¹èŠ‚ç‚¹ã€‚æœ‰ä»¥ä¸‹ä¸‰ç§æƒ…å†µï¼š\n   \n   \
            \  - æ ‡é¢˜ä¸­ç›´æ¥åŒ…å«äº†å…³é”®å­—ä¾‹å¦‚å®éªŒã€testç­‰ï¼Œä½†éœ€æ³¨æ„åŒºåˆ†å®éªŒå’Œå®éªŒçš„éƒ¨åˆ†ï¼Œæ¯”å¦‚â€œ6.1 å®éªŒæ¡ä»¶â€ã€â€œ6.2 å®éªŒæ–¹æ³•â€ã€â€œ6 è‡ªæ£€è¯•éªŒæ–¹æ³•â€ï¼Œåˆ™â€œ6â€åº”è¯¥ä½œä¸ºå®éªŒæ ¹èŠ‚ç‚¹\n\
            \     - æ­£æ–‡ä¸­åŒ…å«äº†â€œæŒ‰XXå®éªŒâ€ç­‰å†…å®¹\n   \n       - æ ¹æ®æ¨æ–­ï¼Œè¯¥ç« èŠ‚åŠå…¶å­ç« èŠ‚å‡å›´ç»•æŸä¸ªå®éªŒå±•å¼€\n   \n\
            \   \n      - è‹¥ä¸€ä¸ªç« èŠ‚æ˜¯å®éªŒç« èŠ‚ï¼Œåˆ™æ‰€æœ‰å­ç« èŠ‚éƒ½ç®—å®éªŒå†…å®¹ï¼Œæ— éœ€å•ç‹¬é‡å¤æ ‡æ³¨ã€‚ä½†è‹¥æ˜¯å­ç« èŠ‚åŒ…å«äº†ä¸åŒçš„å®éªŒï¼Œåˆ™éœ€è¦å¯¹æ¯ä¸ªå®éªŒè¿›è¡Œæ ‡è®°ï¼Œè€Œä¸æ˜¯å½“å‰ç« èŠ‚\n\
            \   \n   - è‹¥ç»¼åˆåˆ¤å®šä¸€ä¸ªsectionä»…å›´ç»•ä¸€ä¸ªå®éªŒå±•å¼€ï¼ˆä¾‹å¦‚ï¼šé™„å½•D è‡ªæ£€å®éªŒæ–¹æ³•ï¼‰ï¼Œåˆ™ä»¥ç‰¹æ®Šæ ‡è®°ALLä½œä¸ºç»“æœï¼ˆä¾‹å¦‚ï¼šexperiment_root_idsï¼š[\"\
            ALL\"]ï¼Œç¦æ­¢ä»…ä¿ç•™â€œDâ€å¯¼è‡´ä»£ç æ— æ³•åŒ¹é…ï¼‰\n   \n\n------\n\n# æ³¨æ„äº‹é¡¹\n\n- å¯¹äºæ²¡æœ‰å­©å­ï¼Œæ²¡æœ‰æ ‡é¢˜çš„ç« èŠ‚ï¼Œ**åªè¦å‡ºç°å‚æ•°(æ•°å­—ç­‰å½¢å¼ï¼Œæ¯”å¦‚1.1å€ï¼Œ10\
            \ percentç­‰)/è¡¨æ ¼/å¼•ç”¨ï¼Œåˆ™å¿…æå–ï¼Œè¡¨æ ¼å†…å‚æ•°é™¤å¤–**\n- æ— æ³•æå–çš„å­—æ®µå¯ä¸åšè¾“å‡ºï¼›å½“ä¸”ä»…å½“æ‰€æœ‰å­—æ®µå‡æ— æ³•æå–æ—¶ï¼Œå¿½ç•¥è¯¥ç« èŠ‚ã€‚\n\
            - æ‰€æœ‰å¼•ç”¨å¿…é¡»èƒ½åœ¨åŸæ–‡ä¸­ç²¾å‡†å®šä½ï¼Œä¸”å¿…é¡»æ˜¯å®é™…å¼•ç”¨æ‰èƒ½æå–refså­—æ®µï¼Œå³ä¸Šä¸‹æ–‡æœ‰ç±»ä¼¼äºâ€œå‚è§XXâ€æˆ–è€…â€œæŒ‰XXå®éªŒâ€ç­‰è¡¨è¿°ï¼Œè‹¥æ˜¯å­¤ç«‹ã€çªå…€å‡ºç°åˆ™å¯ä»¥ç†è§£ä¸ºé¡µçœ‰è¢«é”™è¯¯è§£æï¼Œæˆ–è€…â€œè§„èŒƒæ€§å¼•ç”¨æ–‡ä»¶â€ç« èŠ‚ä¸­å¯¹å¼•ç”¨æ–‡ä»¶çš„ç½—åˆ—\n\
            \n\n# Few-shot ç¤ºä¾‹\n\n## è¾“å…¥ç¤ºä¾‹1\n\n```\n[\n  {\n    \"file\": \"regulation\"\
            ,\n    \"sections\": [\n      {\n        \"section\": \"é™„å½•B\",\n     \
            \   \"context\": \"(è§„èŒƒæ€§)è‡ªåŠ¨è§¦å‘è¯•éªŒæ–¹æ³•\",\n        \"chapters\": [\n       \
            \   {\n          {\n            \"chapter_id\": \"B.2\",\n           \
            \ \"chapter_title\": \"è¯•éªŒé¡¹ç›®\",\n            \"raw_text\": \"\",\n    \
            \        \"children\": [\n              {\n                \"chapter_id\"\
            : \"B.2.1\",\n                \"chapter_title\": \"æ­£é¢ç¢°æ’\",\n         \
            \       \"raw_text\": \"\",\n                \"children\": [\n       \
            \           {\n                    \"chapter_id\": \"B.2.1.1\",\n    \
            \                \"chapter_title\": \"æ»‘å°æ­£é¢ç¢°æ’è¯•éªŒ\",\n                  \
            \  \"raw_text\": \"\",\n                    \"children\": [\n        \
            \              {\n                        \"chapter_id\": \"B.2.1.1.1\"\
            ,\n                        \"chapter_title\": \"\",\n                \
            \        \"raw_text\": \"å°†ç™½è½¦èº«æˆ–å·¥è£…å›ºå®šåœ¨ç¢°æ’è¯•éªŒæ»‘å°ä¸Š,å®‰è£…æ–¹å‘æ¨¡æ‹Ÿæ­£é¢ç¢°æ’ã€‚ \",\n         \
            \               \"children\": [],\n                        \"full_path\"\
            : \"B.2 è¯•éªŒé¡¹ç›®/B.2.1 æ­£é¢ç¢°æ’/B.2.1.1 æ»‘å°æ­£é¢ç¢°æ’è¯•éªŒ/B.2.1.1.1 \"\n              \
            \        },\n                      {\n                        \"chapter_id\"\
            : \"B.2.1.1.2\",\n                        \"chapter_title\": \"\",\n \
            \                       \"raw_text\": \"æ»‘å°æŒ‰ç…§ä»¥ä¸‹åŠ é€Ÿåº¦æ³¢å½¢ä¹‹ä¸€è¿›è¡Œç¢°æ’è¯•éªŒã€‚ a) ä½¿ç”¨åˆ¶é€ å•†æŒ‡å®šçš„åŠ é€Ÿåº¦æ³¢å½¢è¿›è¡Œè¯•éªŒ,æŒ‡å®šçš„åŠ é€Ÿåº¦æ³¢å½¢åº”ä¸ºåœ¨B.2.1.2ä¸­æè¿°çš„å®è½¦ç¢°æ’è¯•éªŒæ¡ä»¶ä¸­,è½¦èº«éå˜å½¢åŒºåŸŸé‡‡é›†çš„åŠ é€Ÿåº¦-æ—¶é—´æ›²çº¿,å¹¶ç»è¿‡æ»¤æ³¢ç­‰çº§CFC60\
            \ æ»¤æ³¢æˆ–100Hzä½é€šæ»¤æ³¢ã€‚å®é™…è¯•éªŒç»“æœæ³¢å½¢çš„ç§¯åˆ†é€Ÿåº¦å˜åŒ–é‡Î”vs( t)åº”åœ¨ä»»æ„æ—¶åˆ»,ä¸è¶…è¿‡æŒ‡å®šæ³¢å½¢çš„ç§¯åˆ†é€Ÿåº¦å˜åŒ–é‡[Î”vt( t)Â±1]km/hçš„èŒƒå›´ã€‚\\\
            nb) æŒ‰å›¾B.1 çš„æ ‡å‡†åŠ é€Ÿåº¦é€šé“èŒƒå›´å’Œè¡¨B.1 çš„å‚æ•°è¿›è¡ŒåŠ é€Ÿæˆ–å‡é€Ÿ,å…¶é€Ÿåº¦å˜åŒ–é‡Î”v ä¸º\\n(25Â±1)km/hã€‚\\nGB45672â€”2025å›¾B.1\
            \ æ­£é¢ç¢°æ’è‡ªåŠ¨è§¦å‘åŠ é€Ÿåº¦é€šé“è¡¨B.1 æ­£é¢ç¢°æ’è‡ªåŠ¨è§¦å‘åŠ é€Ÿåº¦å‚æ•°\\nç‚¹\\næ—¶é—´t\\nms\\nåŠ é€Ÿåº¦ä¸‹é™(Ã—g) ç‚¹ æ—¶é—´tms\\\
            nåŠ é€Ÿåº¦ä¸Šé™(Ã—g) A 15 0 E 0 3 B 45 10 F 40 17 C 60 10 G 63 17 D 85 0 H 105 0\"\
            ,\n                        \"children\": [],\n                       \
            \ \"full_path\": \"B.2 è¯•éªŒé¡¹ç›®/B.2.1 æ­£é¢ç¢°æ’/B.2.1.1 æ»‘å°æ­£é¢ç¢°æ’è¯•éªŒ/B.2.1.1.2 \"\n\
            \                      }\n                    ],\n                   \
            \ \"full_path\": \"B.2 è¯•éªŒé¡¹ç›®/B.2.1 æ­£é¢ç¢°æ’/B.2.1.1 æ»‘å°æ­£é¢ç¢°æ’è¯•éªŒ\"\n          \
            \        },\n                ],\n              },\n            ],\n  \
            \        }\n        ]\n      }\n    ]\n  }\n]\n```\n\n## è¾“å‡ºç¤ºä¾‹1\n\n```\n\
            \  {\n      \"file\": \"regulation\",\n      \"section\": \"é™„å½•B\",\n \
            \     \"experiment_root_ids\": [\"B.2.1.1\"]\n      \"chapters\":[\n \
            \    {\n      \"chapter_id\": \"B.2.1.1.2\",\n      \"paramaters\": [\n\
            \        {\n          \"item\": \"é€Ÿåº¦å˜åŒ–é‡Î”v\",\n          \"constraint\"\
            : \"=\",\n          \"value\": \"25\",\n          \"unit\": \"km/h\",\n\
            \          \"source_text\": \"å…¶é€Ÿåº¦å˜åŒ–é‡Î”vä¸º(25Â±1)km/h\"\n        }\n     \
            \ ],\n      \"refs\": [\n        {\n          \"ref_type\": \"clause\"\
            ,\n          \"doc_id\": null,\n          \"target_id\": \"B.2.1.2\",\n\
            \          \"anchor_text\": \"å®è½¦ç¢°æ’è¯•éªŒæ¡ä»¶-åŠ é€Ÿåº¦æ³¢å½¢\"\n        },\n        {\n\
            \          \"ref_type\": \"graph\",\n          \"doc_id\": null,\n   \
            \       \"target_id\": \"å›¾B.1\",\n          \"anchor_text\": \"\"\n  \
            \      },\n        {\n          \"ref_type\": \"table\",\n          \"\
            doc_id\": null,\n          \"target_id\": \"è¡¨B.1\",\n          \"anchor_text\"\
            : \"\"\n        },\n      ],\n      \"table_headers\": [\"ç‚¹\", \"æ—¶é—´t(ms)\"\
            , \"åŠ é€Ÿåº¦ä¸‹é™(Ã—g)\", \"åŠ é€Ÿåº¦ä¸Šé™(Ã—g)\"]\n    }\n      ]\n  }\n\n```\n\n## è¾“å…¥ç¤ºä¾‹2\n\
            \n```\n[\n  {\n    \"file\": \"regulation\",\n    \"sections\": [\n  \
            \    {\n        \"section\": \"MAIN\",\n        \"context\": \"\",\n \
            \       \"chapters\": [\n          {\n            \"chapter_id\": \"7-\"\
            ,\n            \"chapter_title\": \"CRITERIA OF TECHNICAL CONFORMITY\"\
            ,\n            \"raw_text\": \"\",\n            \"children\": [\n    \
            \         {\n                \"chapter_id\": \"7.2\",\n              \
            \  \"chapter_title\": \"Vehicle subjected to type test\",\n          \
            \      \"raw_text\": \"\",\n                \"children\": [\n        \
            \          {\n                    \"chapter_id\": \"7.2.1\",\n       \
            \             \"chapter_title\": \"\",\n                    \"raw_text\"\
            : \"The vehicle shall be considered complying with the requirement specified\
            \ in item \\n4.1of this standard, if the measured mass of carbon monoxide\
            \ and the combined mass of hydrocarbon and oxides of nitrogen, are less\
            \ than or  equal to 0.70 of theallowahle limits mentioned in Tahle (1\
            \ ).\",\n                    \"children\": [],\n                    \"\
            full_path\": \"7- CRITERIA OF TECHNICAL CONFORMITY/7.2 Vehicle subjected\
            \ to type test/7.2.1 \"\n                  },\n                  {\n \
            \                   \"chapter_id\": \"7.2.2\",\n                    \"\
            chapter_title\": \"\",\n                    \"raw_text\": \"The test shall\
            \  be repeated  if in the initial test, the measured masses of both the\
            \ carbon monoxide and the combined value of hydrocarbons and oxides of\
            \ nitrogenare less than or equal to 0.85 of their allowable limits and\
            \ one of these values isgreater than 0.70 of its allowable limit.\",\n\
            \                    \"children\": [],\n                    \"full_path\"\
            : \"7- CRITERIA OF TECHNICAL CONFORMITY/7.2 Vehicle subjected to type\
            \ test/7.2.2 \"\n                  },\n                  {\n         \
            \           \"chapter_id\": \"7.2.6\",\n                    \"chapter_title\"\
            : \"\",\n                    \"raw_text\": \"The vehicle shall be considered\
            \ non-complying with the requirement mentioned in item 4.1  if more than\
            \ one of the measured masses of both the carbon monoxide and the combined\
            \ value of hydrocarbons and oxides of nitrogen in the three tests exceed\
            \ 1.1 times the allowable limit for the pollutant.\",\n              \
            \      \"children\": [],\n                    \"full_path\": \"7- CRITERIA\
            \ OF TECHNICAL CONFORMITY/7.2 Vehicle subjected to type test/7.2.6 \"\
            ,\n                  }\n                ],\n                \"full_path\"\
            : \"7- CRITERIA OF TECHNICAL CONFORMITY/7.2 Vehicle subjected to type\
            \ test\"\n              },\n            ],\n          },\n        ],\n\
            \      },\n    ],\n  }\n]\n```\n\n## è¾“å‡ºç¤ºä¾‹2\n\n```\n{\n\"file\": \"regulation\"\
            ,\n\"section\": \"MAIN\",\n\"experiment_root_ids\": [\n\"7.2\"\n],\n\"\
            chapters\": [\n{\n\"chapter_id\": \"7.2.1\",\n\"parameters\": [\n{\n\"\
            item\": \"measured mass of carbon monoxide\",\n\"constraint\": \"<=\"\
            ,\n\"value\": \"0.70\",\n\"relative_to\": \"allowable limit\",\n\"unit\"\
            : \"times\"\n},\n{\n\"item\": \"combined mass of hydrocarbons and oxides\
            \ of nitrogen\",\n\"constraint\": \"<=\",\n\"value\": \"0.70\",\n\"relative_to\"\
            : \"allowable limit\",\n\"unit\": \"times\"\n}\n],\n\"refs\": [\n{\n\"\
            ref_type\": \"clause\",\n\"doc_id\": null,\n\"target_id\": \"4.1\",\n\"\
            anchor_text\": \"item 4.1 of this standard\"\n}\n],\n},\n{\n\"chapter_id\"\
            : \"7.2.2\",\n\"parameters\": [\n{\n\"item\": \"measured mass of carbon\
            \ monoxide\",\n\"constraint\": \"<=\",\n\"value\": \"0.85\",\n\"relative_to\"\
            : \"allowable limit\",\n\"unit\": \"times\"\n},\n{\n\"item\": \"combined\
            \ mass of hydrocarbons and oxides of nitrogen\",\n\"constraint\": \"<=\"\
            ,\n\"value\": \"0.85\",\n\"relative_to\": \"allowable limit\",\n\"unit\"\
            : \"times\"\n},\n{\n\"item\": \"one of the measured masses (carbon monoxide\
            \ or combined hydrocarbons+NOx)\",\n\"constraint\": \">\",\n\"value\"\
            : \"0.70\",\n\"relative_to\": \"allowable limit\",\n\"unit\": \"times\"\
            \n}\n],\n},\n{\n\"chapter_id\": \"7.2.6\",\n\"parameters\": [\n{\n\"item\"\
            : \"measured mass of carbon monoxide (per test)\",\n\"constraint\": \"\
            >\",\n\"value\": \"1.1\",\n\"relative_to\": \"allowable limit\",\n\"unit\"\
            : \"times\"\n},\n{\n\"item\": \"combined value of hydrocarbons and oxides\
            \ of nitrogen (per test)\",\n\"constraint\": \">\",\n\"value\": \"1.1\"\
            ,\n\"relative_to\": \"allowable limit\",\n\"unit\": \"times\"\n}\n],\n\
            }\n}\n]\n}\n```"
        - id: 2e89206c-ded1-462d-9db7-2832e587c8ad
          role: user
          text: '{{#17580962587520.item#}}'
        selected: false
        title: LLM 1B
        type: llm
        variables: []
        vision:
          enabled: false
      height: 89
      id: '1758096258752017580962587520'
      parentId: '17580962587520'
      position:
        x: 204
        y: 78.24573165283243
      positionAbsolute:
        x: 842
        y: 809.9762223998827
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
      zIndex: 1002
    - data:
        code: "import json\nimport ast\nimport re\nfrom typing import Dict, List,\
          \ Any\n\ndef parse_input(src: str) -> List[Dict[str, Any]]:\n    \"\"\"\
          é¢„å¤„ç†å¹¶è§£æè¾“å…¥å­—ç¬¦ä¸²ä¸ºåˆ—è¡¨å­—å…¸ç»“æ„\"\"\"\n    try:\n        # 1. æŒ‰ </think> åˆ‡åˆ†å¹¶å–ååŠæ®µ\n  \
          \      src = src.rsplit('</think>', 1)[-1].strip()\n\n        # 2. å…ˆå¹²æ‰ deepseek\
          \ V3 å¯èƒ½æ’å…¥çš„â€œæé€Ÿâ€æˆ–å•ç‹¬çš„â€œæâ€\n        src = re.sub(r'æé€Ÿ\\s*', '', src)   # å»æ‰â€œæé€Ÿâ€åŠå…¶åå¯èƒ½çš„å¤šä½™ç©ºç™½\n\
          \        src = re.sub(r'(?<!\\w)æ(?!\\w)', '', src)  # å»æ‰å­¤ç«‹å‡ºç°çš„â€œæâ€\n\n  \
          \      # 3. å°è¯•å¤šç§è§£æè·¯å¾„\n        try:\n            data = json.loads(src)\n\
          \        except json.JSONDecodeError:\n            try:\n              \
          \  if src.startswith('\\ufeff'):\n                    src = src[1:]\n  \
          \              data = ast.literal_eval(src)\n            except (SyntaxError,\
          \ ValueError):\n                try:\n                    processed = src.replace('\\\
          \\\"', '\"').replace('\\\\n', '\\n')\n                    data = json.loads(processed)\n\
          \                except Exception as e:\n                    raise ValueError(f\"\
          æ— æ³•è§£æï¼š{str(e)}\")\n\n        # 4. ä¿è¯è¿”å› list\n        if isinstance(data,\
          \ dict):\n            return [data]\n        elif isinstance(data, list):\n\
          \            return data\n        else:\n            return []\n    except\
          \ Exception as e:\n        raise ValueError(f\"é¢„å¤„ç†å¤±è´¥: {str(e)}\")\n\n\n\
          def main(arg1: str) -> dict:\n    data = parse_input(arg1)\n    return {\n\
          \        \"resultB\": json.dumps(data, ensure_ascii=False),\n    }"
        code_language: python3
        desc: ''
        error_strategy: fail-branch
        isInIteration: true
        isInLoop: false
        iteration_id: '17580962587520'
        outputs:
          resultB:
            children: null
            type: string
        selected: false
        title: ä»£ç æ‰§è¡Œ 12
        type: code
        variables:
        - value_selector:
          - '1758096258752017580962587520'
          - text
          value_type: string
          variable: arg1
      height: 89
      id: '1758096258752017580962587521'
      parentId: '17580962587520'
      position:
        x: 508
        y: 78.24573165283243
      positionAbsolute:
        x: 1146
        y: 809.9762223998827
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
      zIndex: 1002
    - data:
        desc: ''
        isInIteration: true
        selected: false
        title: ''
        type: iteration-start
      draggable: false
      height: 48
      id: 1758096258752start
      parentId: '17580962587520'
      position:
        x: 24
        y: 68
      positionAbsolute:
        x: 662
        y: 799.7304907470502
      selectable: false
      sourcePosition: right
      targetPosition: left
      type: custom-iteration-start
      width: 44
      zIndex: 1002
    - data:
        code: "import json\nfrom typing import List, Dict, Any\n\ndef merge_item(base:\
          \ Dict[str, Any], extra: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"\
          å°† extra åˆå¹¶åˆ° base\"\"\"\n    merged = dict(base)\n\n    # åˆå¹¶ experiment_root_ids\n\
          \    if \"experiment_root_ids\" in extra:\n        merged_ids = merged.get(\"\
          experiment_root_ids\", [])\n        merged[\"experiment_root_ids\"] = list(set(merged_ids\
          \ + extra[\"experiment_root_ids\"]))\n\n    # åˆå¹¶ chapters\n    base_chapters\
          \ = {c[\"chapter_id\"]: c for c in merged.get(\"chapters\", [])}\n    for\
          \ c in extra.get(\"chapters\", []):\n        cid = c[\"chapter_id\"]\n \
          \       if cid in base_chapters:\n            chapter = base_chapters[cid]\n\
          \            # åˆå¹¶åˆ—è¡¨å­—æ®µ\n            for k in [\"parameters\", \"refs\", \"\
          table_headers\"]:\n                if k in c:\n                    chapter.setdefault(k,\
          \ [])\n                    # å¦‚æœæ˜¯ list of dict\n                    if c[k]\
          \ and isinstance(c[k][0], dict):\n                        existing_keys\
          \ = [json.dumps(x, sort_keys=True) for x in chapter[k]]\n              \
          \          for x in c[k]:\n                            sx = json.dumps(x,\
          \ sort_keys=True)\n                            if sx not in existing_keys:\n\
          \                                chapter[k].append(x)\n                \
          \    else:\n                        for x in c[k]:\n                   \
          \         if x not in chapter[k]:\n                                chapter[k].append(x)\n\
          \        else:\n            base_chapters[cid] = c\n    merged[\"chapters\"\
          ] = list(base_chapters.values())\n    return merged\n\ndef main(arg1: list[str],\
          \ arg2: list[str], global_scope: str) -> dict:\n    if len(arg1) != len(arg2):\n\
          \        raise ValueError(\"arg1 ä¸ arg2 é•¿åº¦å¿…é¡»ä¸€è‡´\")\n\n    result = []\n \
          \   for s1, s2 in zip(arg1, arg2):\n        try:\n            data1 = json.loads(s1)\n\
          \        except:\n            data1 = []\n        try:\n            data2\
          \ = json.loads(s2)\n        except:\n            data2 = []\n\n        #\
          \ å»ºç«‹ file+section çš„æ˜ å°„\n        base_map = {(d[\"file\"], d[\"section\"]):\
          \ d for d in data1}\n        for d in data2:\n            key = (d[\"file\"\
          ], d[\"section\"])\n            if key in base_map:\n                base_map[key]\
          \ = merge_item(base_map[key], d)\n            else:\n                base_map[key]\
          \ = d\n\n        # result.append(json.dumps(list(base_map.values()), ensure_ascii=False))\n\
          \n\n        merged_list = list(base_map.values())\n        for sec in merged_list:\n\
          \            for ch in sec.get(\"chapters\", []):\n                if \"\
          scope\" in ch:\n                    ch[\"scope\"] = '[' + global_scope +\
          \ ']-' + ch[\"scope\"]\n\n        result.append(json.dumps(merged_list,\
          \ ensure_ascii=False))\n\n    return {\"result\": result}\n"
        code_language: python3
        desc: ''
        outputs:
          result:
            children: null
            type: array[string]
        selected: false
        title: ä»£ç æ‰§è¡Œ 14
        type: code
        variables:
        - value_selector:
          - '1756557801310'
          - output
          value_type: array[string]
          variable: arg1
        - value_selector:
          - '17580962587520'
          - output
          value_type: array[string]
          variable: arg2
        - value_selector:
          - '1758622742335'
          - global_scope
          value_type: string
          variable: global_scope
      height: 53
      id: '1758096294198'
      position:
        x: 2569.0205524344196
        y: 352
      positionAbsolute:
        x: 2569.0205524344196
        y: 352
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        context:
          enabled: false
          variable_selector: []
        desc: ''
        isInIteration: true
        isInLoop: false
        iteration_id: '17580962587520'
        model:
          completion_params: {}
          mode: chat
          name: deepseek-reasoner
          provider: langgenius/deepseek/deepseek
        prompt_template:
        - id: 988d6e78-9e92-4ede-bab7-9dcc0073feb4
          role: system
          text: "# è§’è‰²ï¼ˆRoleï¼‰\nä½ æ˜¯ä¸€åä¸¥æ ¼çš„ JSON è¾“å‡ºä¿®å¤å™¨ï¼ˆJSON Fixerï¼‰ã€‚  \nä½ çš„èŒè´£æ˜¯ï¼š\n- æ¥æ”¶ä¸€ä¸ªå¯èƒ½æ ¼å¼é”™è¯¯çš„\
            \ JSON å­—ç¬¦ä¸²\n- ä¸¥æ ¼ä¿®å¤å…¶ä¸­çš„è¯­æ³•é—®é¢˜æˆ–ç»“æ„é—®é¢˜ã€‚\n- è¾“å‡ºå®Œå…¨ç¬¦åˆé¢„æœŸæ ¼å¼çš„ JSON å¯¹è±¡ã€‚\n\n---\n\n# è¾“å…¥ï¼ˆInputï¼‰\n\
            ä½ å°†æ”¶åˆ°ï¼š\n**raw_output**ï¼šæ¨¡å‹åŸå§‹è¾“å‡ºï¼Œä½ éœ€è¦å¿½ç•¥ä»â€<think>â€œåˆ°\"</think>\"çš„éƒ¨åˆ†ï¼Œåªä¿ç•™é”™è¯¯ JSONã€‚\n\
            \nç¤ºä¾‹è¾“å…¥ï¼š\nerror: Expecting property name enclosed in double quotes: line\
            \ 3 column 5 (char 25)\nraw_text: {file: \"doc1\", section: \"main\",\
            \ chapters: [ ... ]}\n\n---\n\n# è¾“å‡ºæ ¼å¼ï¼ˆOutputï¼‰\nè¯·è¾“å‡ºä¸€ä¸ª **å®Œæ•´ JSON å¯¹è±¡**ï¼Œå¿…é¡»ä¸¥æ ¼ç¬¦åˆä»¥ä¸‹ç»“æ„ï¼ˆä¸è¦è¾“å‡ºå¤šä½™çš„è§£é‡Šæ€§æ–‡å­—ï¼‰ï¼š\
            \  \n```json\n{\n  \"file\": \"string\",\n  \"section\": \"string\",\n\
            \  \"experiment_root_ids\": [\"string\", \"...\"],\n  \"chapters\":[\n\
            \    {\n      \"chapter_id\": \"string\",\n      \"parameters\": [\n \
            \       {\n          \"item\": \"string\",\n          \"constraint\":\
            \ \"<=|<|=|>=|>|range_closed|range_open|enum|boolean\",\n          \"\
            value\": \"string|array|null\",\n          \"unit\": \"string|null\",\n\
            \          \"source_text\": \"string\"\n        }\n      ],\n      \"\
            refs\": [\n        {\n          \"ref_type\": \"table|graph|clause|external\"\
            ,\n          \"doc_id\": \"string|null\",\n          \"target_id\": \"\
            string\",\n          \"anchor_text\": \"string\"\n        }\n      ],\n\
            \      \"table_headers\": [\"string\", \"...\"]\n    }\n  ]\n}\n\n```\n\
            \n# æ³¨æ„äº‹é¡¹\n\n1. ä½ éœ€è¦ä¸¥æ ¼æŒ‰ç…§è¾“å‡ºæ ¼å¼è¦æ±‚è¿›è¡Œä¿®å¤ï¼Œå¯é€šè¿‡è¾“å…¥jsonè¿›è¡Œæ¨æ–­ï¼Œç¡®ä¿ JSON å­—ç¬¦ä¸²å¯è¢« `json.loads()`\
            \ è§£æé€šè¿‡å³å¯\n2. ä¸¥æ ¼è¾“å‡ºä¸€ä¸ªåˆæ³• JSON å¯¹è±¡ï¼Œä¸å¾—è¾“å‡ºå¤šä½™æ–‡å­—ã€æ³¨é‡Šæˆ– Markdownã€‚\n3. å¦‚æœæŸå­—æ®µæ— å†…å®¹ï¼Œè¯·ä½¿ç”¨\
            \ `null` æˆ–ç©ºæ•°ç»„ `[]`ã€‚\n4. å¿…é¡»è¡¥å…¨ç¼ºå¤±å­—æ®µï¼Œä¸å…è®¸çœç•¥ä»»ä½•å­—æ®µã€‚"
        - id: 8fd45a2e-1ede-4756-bf64-fbfe42302cff
          role: user
          text: 'error_type:

            {{#1758096258752017580962587521.error_type#}}


            error_message:

            {{#1758096258752017580962587521.error_message#}}


            raw_text:

            {{#1758096258752017580962587520.text#}}

            '
        - id: f02c2f0b-66b2-4a3f-8960-ea7c573f4a35
          role: assistant
          text: ''
        selected: false
        title: LLM 6
        type: llm
        variables: []
        vision:
          enabled: false
      height: 89
      id: '1758105741313'
      parentId: '17580962587520'
      position:
        x: 812
        y: 188.76463752439133
      positionAbsolute:
        x: 1450
        y: 920.4951282714416
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
      zIndex: 1002
    - data:
        code: "import json\nimport ast\nimport re\nfrom typing import Dict, List,\
          \ Any\n\ndef parse_input(src: str) -> List[Dict[str, Any]]:\n    \"\"\"\
          é¢„å¤„ç†å¹¶è§£æè¾“å…¥å­—ç¬¦ä¸²ä¸ºåˆ—è¡¨å­—å…¸ç»“æ„\"\"\"\n    try:\n        # 1. æŒ‰ </think> åˆ‡åˆ†å¹¶å–ååŠæ®µ\n  \
          \      src = src.rsplit('</think>', 1)[-1].strip()\n\n        # 2. å…ˆå¹²æ‰ deepseek\
          \ V3 å¯èƒ½æ’å…¥çš„â€œæé€Ÿâ€æˆ–å•ç‹¬çš„â€œæâ€\n        src = re.sub(r'æé€Ÿ\\s*', '', src)   # å»æ‰â€œæé€Ÿâ€åŠå…¶åå¯èƒ½çš„å¤šä½™ç©ºç™½\n\
          \        src = re.sub(r'(?<!\\w)æ(?!\\w)', '', src)  # å»æ‰å­¤ç«‹å‡ºç°çš„â€œæâ€\n\n  \
          \      # 3. å°è¯•å¤šç§è§£æè·¯å¾„\n        try:\n            data = json.loads(src)\n\
          \        except json.JSONDecodeError:\n            try:\n              \
          \  if src.startswith('\\ufeff'):\n                    src = src[1:]\n  \
          \              data = ast.literal_eval(src)\n            except (SyntaxError,\
          \ ValueError):\n                try:\n                    processed = src.replace('\\\
          \\\"', '\"').replace('\\\\n', '\\n')\n                    data = json.loads(processed)\n\
          \                except Exception as e:\n                    raise ValueError(f\"\
          æ— æ³•è§£æï¼š{str(e)}\")\n\n        # 4. ä¿è¯è¿”å› list\n        if isinstance(data,\
          \ dict):\n            return [data]\n        elif isinstance(data, list):\n\
          \            return data\n        else:\n            return []\n    except\
          \ Exception as e:\n        raise ValueError(f\"é¢„å¤„ç†å¤±è´¥: {str(e)}\")\n\n\n\
          def main(arg1: str) -> dict:\n    data = parse_input(arg1)\n    return {\n\
          \        \"resultB\": json.dumps(data, ensure_ascii=False),\n    }"
        code_language: python3
        desc: ''
        isInIteration: true
        isInLoop: false
        iteration_id: '17580962587520'
        outputs:
          resultB:
            children: null
            type: string
        selected: false
        title: ä»£ç æ‰§è¡Œ 14
        type: code
        variables:
        - value_selector:
          - '1758105741313'
          - text
          value_type: string
          variable: arg1
      height: 53
      id: '1758105753732'
      parentId: '17580962587520'
      position:
        x: 1107.228658264162
        y: 192.27317421872647
      positionAbsolute:
        x: 1745.228658264162
        y: 924.0036649657767
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
      zIndex: 1002
    - data:
        desc: ''
        isInIteration: true
        isInLoop: false
        iteration_id: '17580962587520'
        output_type: string
        selected: false
        title: å˜é‡èšåˆå™¨ 3
        type: variable-aggregator
        variables:
        - - '1758105753732'
          - resultB
        - - '1758096258752017580962587521'
          - resultB
      height: 129
      id: '1758105758637'
      parentId: '17580962587520'
      position:
        x: 1418.2457316528325
        y: 65
      positionAbsolute:
        x: 2056.2457316528325
        y: 796.7304907470502
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
      zIndex: 1002
    - data:
        context:
          enabled: true
          variable_selector:
          - '1756550411122'
          - context
        desc: ''
        model:
          completion_params: {}
          mode: chat
          name: deepseek-reasoner
          provider: langgenius/deepseek/deepseek
        prompt_template:
        - id: 57cf1789-3bae-4a65-87a4-dbbd9959758f
          role: system
          text: '# è§’è‰²

            ä½ æ˜¯ä¸€ä½æ·±è€•äºæŠ€æœ¯æ ‡å‡†é¢†åŸŸçš„èµ„æ·±ä¸“å®¶ï¼Œæ‹¥æœ‰å¤šå¹´çš„æ–‡æ¡£åˆ†æç»éªŒï¼Œæ“…é•¿ä»å¤æ‚çš„ã€å……æ»¡è¡Œä¸šæœ¯è¯­çš„æ–‡æœ¬ä¸­ï¼Œç²¾å‡†åœ°æç‚¼å‡ºæœ€æ ¸å¿ƒçš„ä¸»é¢˜ã€‚


            ä½ çš„ä»»åŠ¡æ˜¯åˆ†ææˆ‘æä¾›çš„æ±½è½¦é¢†åŸŸçš„æŠ€æœ¯æ ‡å‡†æ–‡æ¡£çš„å¼•è¨€æˆ–èŒƒå›´ï¼ˆScopeï¼‰éƒ¨åˆ†ï¼Œå¹¶è¯†åˆ«å‡ºè¿™ä»½æ–‡æ¡£æœ€æ ¸å¿ƒã€æœ€é¦–è¦çš„è§„å®šå¯¹è±¡æˆ–ä¸»é¢˜ã€‚


            # è¾“å‡ºè¦æ±‚

            1.  **é«˜åº¦æ¦‚æ‹¬**: ç»“æœå¿…é¡»æ˜¯ä¸€ä¸ªç®€æ´çš„åè¯æ€§çŸ­è¯­ã€‚

            2.  **æ ¸å¿ƒä¸»é¢˜**: è¿™ä¸ªçŸ­è¯­å¿…é¡»èƒ½ä½œä¸ºæ•´ä¸ªæ–‡æ¡£çš„â€œå…¨å±€èŒƒå›´â€æ ‡ç­¾ã€‚

            3.  **æ ¼å¼ä¸¥æ ¼**: **ä»…è¾“å‡ºè¿™ä¸ªåè¯æ€§çŸ­è¯­**ï¼Œç»å¯¹ä¸è¦åŒ…å«ä»»ä½•è§£é‡Šã€å‰ç¼€ï¼ˆå¦‚â€œä¸»é¢˜æ˜¯ï¼šâ€ï¼‰æˆ–ä»»ä½•å…¶ä»–å¤šä½™çš„æ–‡å­—ã€‚


            # fewshot


            **ç¤ºä¾‹ 1:**


            **è¾“å…¥æ–‡æœ¬:**

            ```

            This Regulation applies to:

            Front and rear position lamps and stop lamps for vehicles of categories
            L, M, N, O and T1; and,

            End-outline marker lamps for vehicles of categories M, N, O and T.

            ```


            **ä½ çš„è¾“å‡º:**

            ```

            Front and rear position lamps, stop lamps and End-outline marker lamps

            ```


            ---


            **ç¤ºä¾‹ 2:**


            **è¾“å…¥æ–‡æœ¬:**

            ```

            This Regulation applies to vehicles of categories M, N, and to their trailers
            (category O)1 with regard to the installation of lighting and light-signalling
            devices.

            ```


            **ä½ çš„è¾“å‡º:**

            ```

            Lighting and light-signalling devices installation

            ```


            ---


            **ç¤ºä¾‹ 3:**


            **è¾“å…¥æ–‡æœ¬:**

            ```

            æœ¬æ–‡ä»¶è§„å®šäº†è½¦è½½äº‹æ•…ç´§æ€¥å‘¼å«ç³»ç»Ÿçš„æŠ€æœ¯è¦æ±‚ã€åŒä¸€å‹å¼åˆ¤å®šè¦æ±‚,æè¿°äº†ç›¸åº”çš„è¯•éªŒæ–¹æ³•ã€‚

            æœ¬æ–‡ä»¶é€‚ç”¨äºM1ç±»åŠN1ç±»è½¦è¾†çš„è½¦è½½äº‹æ•…ç´§æ€¥å‘¼å«ç³»ç»Ÿã€‚

            ```


            **ä½ çš„è¾“å‡º:**

            ```

            è½¦è½½äº‹æ•…ç´§æ€¥å‘¼å«ç³»ç»Ÿ

            ```


            ---


            # å·¥ä½œ

            ç°åœ¨ï¼Œè¯·å¤„ç†ä»¥ä¸‹è¾“å…¥å†…å®¹ï¼š

            {{#context#}}'
        selected: false
        title: LLM 7
        type: llm
        variables: []
        vision:
          enabled: false
      height: 89
      id: '1758595213024'
      position:
        x: 653.8702599223778
        y: 1086.85928286599
      positionAbsolute:
        x: 653.8702599223778
        y: 1086.85928286599
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        code: "import json\nimport ast\nimport re\nfrom typing import Dict, List,\
          \ Any\n\n\ndef main(arg1: str) -> dict:\n    return {\n        \"global_scope\"\
          : arg1.rsplit('</think>', 1)[-1].strip(),\n    }"
        code_language: python3
        desc: ''
        outputs:
          global_scope:
            children: null
            type: string
        selected: false
        title: ä»£ç æ‰§è¡Œ 15
        type: code
        variables:
        - value_selector:
          - '1758595213024'
          - text
          value_type: string
          variable: arg1
      height: 53
      id: '1758622742335'
      position:
        x: 2116.2512368817224
        y: 1075.504816742838
      positionAbsolute:
        x: 2116.2512368817224
        y: 1075.504816742838
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    viewport:
      x: 349.04847037377806
      y: 142.26830271156803
      zoom: 0.5963849333908374
