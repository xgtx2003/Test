import fitz  # PyMuPDF
import re
import json
import os
from typing import List, Dict, Tuple
from collections import defaultdict

# chapter_patterns = [
#     re.compile(r'^(é™„\s*å½•\s*[A-Z])\s+(.+)$'),
#     re.compile(r'^([A-Z](?:\.\d+)+)\s+(.+)$'),
#     re.compile(r'^(\d+(?:\.\d+)*)(\s+)(.+)$'),
# ]

# chapter_patterns = [
#     re.compile(r'^(APPENDIX\s+[A-Z0-9]+)$', re.I),          # APPENDIX A / APPENDIX 1
#     re.compile(r'^([A-Z](?:\.\d+)+)\s+(.+)$'),
#     re.compile(r'^(\d+(?:\.\d+)*\.?)\s+(.+)$'),                     # 1.1. Title
# ]

# æ—§çš„ç« èŠ‚æ¨¡å¼ï¼ˆå·²æ³¨é‡Šï¼‰
# chapter_patterns = [
#     re.compile(r'^(é™„\s*å½•\s*[A-Z0-9])$'), # é™„ å½• B
#     re.compile(r'^((APPENDIX|ANNEX|ATTACHMENT)\s+(?:[A-Z0-9]+|\([A-Z0-9]+\)))$', re.I),  # ANNEX A / ANNEX 1
#     re.compile(r'^([A-Z]\.)\s+(.+)$'),                                # A. Title (å•ç‹¬å­—æ¯ç« èŠ‚)
#     re.compile(r'^([A-Z](?:\.\d+)+\.?)\s+(.+)$'),                     # A.1. Title / A.1.1. Title
#     re.compile(r'^(\d+(?:\.\d+)*\.?)\s+(.+)$'),                       # 1.1. Title
#     re.compile(r'^(\d+(?:-\d+)*-)\s+(.+)$'),                          # 1- Title / 1-2- Title
# ]

# æ–°çš„åˆå¹¶åçš„ç« èŠ‚æ¨¡å¼
chapter_patterns = [
    # 1. ä¸­æ–‡é™„å½•ï¼šé™„å½•A, é™„ å½• B
    re.compile(r'^(é™„\s*å½•\s*[A-Z0-9])$'),
    
    # 2. è‹±æ–‡é™„å½•ï¼šAPPENDIX A, ANNEX A, ATTACHMENT A
    re.compile(r'^((APPENDIX|ANNEX|ATTACHMENT)\s+(?:[A-Z0-9]+|\([A-Z0-9]+\)))$', re.I),
    
    # 3. å­—æ¯ç« èŠ‚ï¼ˆæ”¯æŒç‚¹å’Œæ¨ªçº¿åˆ†éš”ç¬¦ï¼‰ï¼šA. Title, A.1. Title, A-1- Title
    re.compile(r'^([A-Z](?:[.\-]\d+)*[.\-]?)\s+(.+)$'),
    
    # 4. æ•°å­—ç« èŠ‚ï¼ˆæ”¯æŒç‚¹å’Œæ¨ªçº¿åˆ†éš”ç¬¦ï¼‰ï¼š1. Title, 1.1. Title, 1- Title, 1-2- Title
    re.compile(r'^(\d+(?:[.\-]\d+)*[.\-]?)\s+(.+)$'),
]

def detect_document_language(lines: List[str]) -> str:
    """
    æ£€æµ‹æ–‡æ¡£è¯­è¨€ï¼šä¸­æ–‡æˆ–è‹±æ–‡
    :param lines: æ–‡æ¡£çš„æ‰€æœ‰è¡Œ
    :return: 'zh' è¡¨ç¤ºä¸­æ–‡ï¼Œ'en' è¡¨ç¤ºè‹±æ–‡
    """
    chinese_char_count = 0
    total_chars = 0
    
    # é‡‡æ ·å‰1000è¡Œæˆ–å…¨éƒ¨è¡Œ
    sample_lines = lines[:1000] if len(lines) > 1000 else lines
    
    for line in sample_lines:
        for char in line:
            total_chars += 1
            if '\u4e00' <= char <= '\u9fff':  # ä¸­æ–‡å­—ç¬¦
                chinese_char_count += 1
    
    # åªè¦æœ‰ä¸­æ–‡å­—ç¬¦å°±è®¤ä¸ºæ˜¯ä¸­æ–‡æ–‡æ¡£
    if chinese_char_count > 0:
        return 'zh'
    else:
        return 'en'

# ä¸­æ–‡ç« èŠ‚max_chapter_num=50
# å…¨æ–‡é¦–å…ˆæ£€æµ‹æ˜¯ä¸­æ–‡è¿˜æ˜¯è‹±æ–‡
def detect_chapter(line: str, max_chapter_num=1000, language='en', number_analysis=None):
    clean_line = line.strip()
    if not clean_line:
        return None

    for pattern in chapter_patterns:
        m = pattern.match(clean_line)
        if m:
            chapter_id = m.group(1).strip()
            chapter_title = m.group(len(m.groups())).strip() if m.group(len(m.groups())) else ""
            if re.match(r'^(é™„\s*å½•\s*[A-Z0-9])$', chapter_id):
                # å»æ‰ä¸­é—´çš„ç©ºæ ¼
                chapter_id = chapter_id.replace(" ", "")
                # chapter_id = chapter_id[-1]
            # ---- åŸºç¡€è¿‡æ»¤ ----
            first_num = None
            if chapter_id.upper().startswith("APPENDIX"):
                suffix = chapter_id[len("APPENDIX"):].strip(" ()")
                if suffix.isdigit():
                    first_num = int(suffix)
            else:
                m_num = re.match(r'^(\d+)', chapter_id)
                if m_num:
                    first_num = int(m_num.group(1))

            if first_num is not None and number_analysis is not None:
                # ä½¿ç”¨æ™ºèƒ½æ•°å­—èŒƒå›´åˆ¤æ–­
                min_reasonable = number_analysis.get("min_reasonable", 1)
                max_reasonable = number_analysis.get("max_reasonable", max_chapter_num)
                
                # ç‰¹æ®Šå¤„ç†æ³•è§„ç¼–å·æ¨¡å¼
                if number_analysis.get("regulation_mode", False):
                    regulation_number = number_analysis.get("regulation_number")
                    if first_num != regulation_number:
                        return None  # ä¸æ˜¯æ³•è§„ç¼–å·ï¼Œè¿‡æ»¤æ‰
                else:
                    # æ­£å¸¸ç« èŠ‚ç¼–å·èŒƒå›´æ£€æŸ¥
                    if first_num < min_reasonable or first_num > max_reasonable:
                        return None  # æ•°å­—èŒƒå›´ä¸åˆç†
            elif first_num is not None:
                # å…œåº•é€»è¾‘ï¼šä½¿ç”¨ä¼ ç»Ÿçš„max_chapter_num
                if first_num < 1 or first_num > max_chapter_num:
                    return None  # æ•°å­—èŒƒå›´ä¸åˆç†

            # ---- å†…å®¹ç‰¹å¾è¿‡æ»¤ ----
            # 1) æ ‡é¢˜å¿…é¡»åŒ…å«å­—æ¯æˆ–ä¸­æ–‡
            if not re.search(r'[A-Za-z\u4e00-\u9fff]', chapter_title):
                return None

            # 2) å»æ‰çº¯æ•°å­—è¡¨æ ¼è¡Œ
            if re.fullmatch(r'[\d\s\.\-]+', chapter_title):
                return None

            # 3) è¡¨æ ¼å†…å®¹è¿‡æ»¤ - æ£€æµ‹æ˜æ˜¾çš„è¡¨æ ¼æ•°æ®æ¨¡å¼
            # å¦‚æœæ ‡é¢˜åŒ…å«å¤§é‡æ•°å­—ã€ç©ºæ ¼å’Œå°‘é‡å­—æ¯çš„ç»„åˆï¼Œå¯èƒ½æ˜¯è¡¨æ ¼æ•°æ®
            if re.search(r'^\d+\s+\d+.*[A-Z]\s+\d+\s+\d+', chapter_title):  # å¦‚ "10 0 E 0 16"
                return None
            
            # æ£€æµ‹è¡¨æ ¼è¡Œæ¨¡å¼ï¼šå•ä¸ªå­—æ¯ + æ•°å­—ç»„åˆ
            if re.fullmatch(r'[A-Z]\s*\d+.*', chapter_title) and len(chapter_title.split()) >= 3:
                # å¦‚æœæ ‡é¢˜æ˜¯ "A 10 0" è¿™æ ·çš„æ ¼å¼ï¼Œå¾ˆå¯èƒ½æ˜¯è¡¨æ ¼æ•°æ®
                parts = chapter_title.split()
                if len(parts) >= 3 and all(part.isdigit() or part in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ' for part in parts[:3]):
                    return None

            # 4) æ£€æµ‹åæ ‡ç‚¹æˆ–å‚æ•°è¡¨æ ¼ï¼šå¦‚ "A15 0 E 0 3"
            if re.search(r'^[A-Z]\d+\s+\d+\s+[A-Z]\s+\d+\s+\d+', chapter_title):
                return None

            # 5) è¡Œå¤ªçŸ­
            if len(clean_line) < 4 and not chapter_id.upper().startswith("APPENDIX") and not chapter_id.startswith("é™„å½•"):
                return None

            # 6) è¿‡æ»¤æ˜æ˜¾çš„è¡¨æ ¼æ ‡é¢˜ç»„åˆ
            if len(chapter_id) == 1 and chapter_id.isupper():
                # å•ä¸ªå¤§å†™å­—æ¯ä½œä¸ºç« èŠ‚IDï¼Œæ£€æŸ¥æ ‡é¢˜æ˜¯å¦åƒè¡¨æ ¼æ•°æ®
                if re.search(r'\d+.*\d+', chapter_title) and len(chapter_title.split()) <= 6:
                    return None

            return {
                "chapter_id": chapter_id,
                "chapter_title": chapter_title
            }

    return None

def build_tree(chapter_list: List[Dict]) -> List[Dict]:
    id_map = {}
    root = []

    # å…ˆæ³¨å†Œæ‰€æœ‰èŠ‚ç‚¹
    for chap in chapter_list:
        chap["children"] = []
        # ç»Ÿä¸€å»æ‰æœ«å°¾ç‚¹å’Œæ¨ªçº¿ä½œä¸º key
        key = chap["chapter_id"].rstrip('.-')
        id_map[key] = chap

    # ä¸ºæ¯ä¸ªèŠ‚ç‚¹åˆ›å»ºç¼ºå¤±çš„çˆ¶èŠ‚ç‚¹ï¼ˆåªé’ˆå¯¹ä¸‰çº§åŠä»¥ä¸Šæ ‡é¢˜ï¼‰
    for chap in chapter_list:
        cid = chap["chapter_id"].rstrip('.')
        parts = cid.split('.')
        
        # åªæœ‰ä¸‰çº§åŠä»¥ä¸Šæ ‡é¢˜æ‰åˆ›å»ºä¸­é—´çˆ¶èŠ‚ç‚¹
        if len(parts) >= 3:
            # åˆ›å»ºæ‰€æœ‰ç¼ºå¤±çš„ä¸­é—´çˆ¶çº§èŠ‚ç‚¹ï¼ˆä½†ä¸åŒ…æ‹¬é¡¶çº§çˆ¶èŠ‚ç‚¹ï¼‰
            for i in range(2, len(parts)):  # ä»ç¬¬äºŒçº§å¼€å§‹åˆ›å»ºï¼Œè·³è¿‡é¡¶çº§
                parent_key = '.'.join(parts[:i])
                if parent_key not in id_map:
                    # åˆ›å»ºç¼ºå¤±çš„çˆ¶èŠ‚ç‚¹
                    parent_node = {
                        "chapter_id": parent_key + ".",
                        "chapter_title": "",
                        "raw_text": "",
                        "children": []
                    }
                    id_map[parent_key] = parent_node

    # æ„å»ºæ ‘ç»“æ„
    for chap in chapter_list:
        cid = chap["chapter_id"].rstrip('.')
        parts = cid.split('.')

        # æ ¹èŠ‚ç‚¹åˆ¤æ–­
        if cid.startswith("APPENDIX"):
            root.append(chap)
        elif cid.startswith("é™„å½•") or len(parts) == 1:
            root.append(chap)
        else:
            parent_key = '.'.join(parts[:-1])
            parent = id_map.get(parent_key)
            if parent:
                parent["children"].append(chap)
            else:
                # å¦‚æœçˆ¶èŠ‚ç‚¹ä¸å­˜åœ¨ï¼Œå¯¹äºäºŒçº§æ ‡é¢˜ï¼Œç›´æ¥ä½œä¸ºæ ¹èŠ‚ç‚¹
                if len(parts) == 2:
                    root.append(chap)
                # ä¸‰çº§åŠä»¥ä¸Šæ ‡é¢˜æ²¡æœ‰çˆ¶èŠ‚ç‚¹æ—¶ï¼Œä¸åšå¤„ç†ï¼ˆå› ä¸ºå‰é¢å·²ç»åˆ›å»ºäº†çˆ¶èŠ‚ç‚¹ï¼‰


    # å°†åˆ›å»ºçš„ä¸­é—´èŠ‚ç‚¹ä¹Ÿæ·»åŠ åˆ°æœ€ç»ˆçš„ç« èŠ‚åˆ—è¡¨ä¸­ï¼Œä½†åªæœ‰é‚£äº›æœ‰å­èŠ‚ç‚¹çš„
    created_parents = []
    for key, node in id_map.items():
        if node not in chapter_list and len(node["children"]) > 0:
            created_parents.append(node)
    
    # å¯¹åˆ›å»ºçš„çˆ¶èŠ‚ç‚¹ä¹Ÿè¿›è¡Œæ ‘ç»“æ„æ„å»º
    for parent in created_parents:
        cid = parent["chapter_id"].rstrip('.')
        parts = cid.split('.')
        
        if len(parts) == 1:
            root.append(parent)
        else:
            parent_key = '.'.join(parts[:-1])
            grandparent = id_map.get(parent_key)
            if grandparent and parent not in grandparent["children"]:
                grandparent["children"].append(parent)
            elif len(parts) == 1:  # è¿™æ˜¯ä¸€çº§ç« èŠ‚
                if parent not in root:
                    root.append(parent)
    
    return root

def build_full_path(chapters: List[Dict], path_prefix=""):
    for chap in chapters:
        if path_prefix:
            chap["full_path"] = f"{path_prefix}/{chap['chapter_id']} {chap['chapter_title']}"
        else:
            chap["full_path"] = f"{chap['chapter_id']} {chap['chapter_title']}"
        if chap.get("children"):
            build_full_path(chap["children"], chap["full_path"])

def fullwidth_to_halfwidth(text: str) -> str:
    result = []
    for char in text:
        code = ord(char)
        if 0xFF01 <= code <= 0xFF5E:
            result.append(chr(code - 0xFEE0))
        else:
            result.append(char)
    return ''.join(result)

def build_term_dict(raw_text: str) -> Dict[str, str]:
    text = re.sub(r'\n+', '\n', raw_text.strip())
    pattern = re.compile(
        r'^\d+\.\d+\n'
        r'(?P<cn>[^\n]*?)\s*'
        r'(?P<en>[A-Za-z].*?)\s*(?=\n)',
        re.MULTILINE
    )

    term_map = {}
    for m in pattern.finditer(text):
        cn = m.group("cn").strip()
        en = m.group("en").strip()
        if cn and en:
            term_map[cn] = en
    return term_map

def extract_terms_with_abbr_from_terms_section(raw_text: str) -> Dict[str, Dict[str, str]]:
    """
    æå–æœ¯è¯­ç« èŠ‚ä¸­çš„ä¸­è‹±æ–‡æœ¯è¯­åŠç¼©å†™
    è¿”å›æ ¼å¼ï¼š
    {
      "ä¸­æ–‡æœ¯è¯­": {
         "en": "è‹±æ–‡æœ¯è¯­",
         "abbr": "ç¼©å†™ï¼ˆå¦‚æœ‰ï¼‰"
      }
    }
    """
    term_map = {}
    text = re.sub(r'\n+', '\n', raw_text.strip())

    pattern = re.compile(
        r'(?P<cn>[\u4e00-\u9fffï¼ˆï¼‰()Â·\s]{2,})'        # ä¸­æ–‡éƒ¨åˆ†
        r'\s*'                                         # å¯é€‰ç©ºæ ¼
        r'(?P<en>[A-Za-z][A-Za-z\s\-/]*)'              # è‹±æ–‡æœ¯è¯­
        r'(?:[;ï¼›:ï¼š]?\s*(?P<abbr>[A-Z0-9Â·]+))?',       # å¯é€‰ç¼©å†™
        re.MULTILINE
    )


    for m in pattern.finditer(text):
        cn = m.group("cn").strip()
        en = m.group("en").strip()
        abbr = m.group("abbr").strip() if m.group("abbr") else ""

        term_map[cn] = {"en": en}
        if abbr:
            term_map[cn]["abbr"] = abbr

    return term_map

def extract_abbr_terms_from_symbols_section(raw_text: str) -> Dict[str, Dict[str, str]]:
    """
    æå–â€œç¬¦å·å’Œç¼©ç•¥è¯­â€ç« èŠ‚çš„ä¸­è‹±ç¼©å†™æ˜ å°„ï¼Œè¿”å›ä»¥ä¸­æ–‡ä¸ºé”®çš„ç»“æ„ï¼š
    {
        "ä¸­æ–‡": {
            "abbr": "ç¼©å†™",
            "en": "è‹±æ–‡é‡Šä¹‰"
        }
    }
    """
    abbr_map = {}
    # æ¸…ç†æ–‡æœ¬
    text = re.sub(r'\n+', '\n', raw_text.strip())

    # åŒ¹é…æ¨¡å¼ï¼šACLR: é‚»é“æ³„æ¼åŠŸç‡æ¯” (Adjacent Channel Leakage Power Ratio)
    pattern = re.compile(
        r'(?P<abbr>[A-Za-z0-9Â·\-_]+)\s*[:ï¼š]?\s*'
        r'(?P<cn>[\u4e00-\u9fffÂ·]+)'
        r'(?:[ï¼ˆï¼‰()]*\s*(?P<en>[A-Za-z\s/\-]+)\s*[ï¼ˆï¼‰()]*)?'
    )

    for m in pattern.finditer(text):
        abbr = m.group("abbr").strip()
        cn = m.group("cn").strip("ï¼ˆï¼‰()").strip()
        en = m.group("en").strip() if m.group("en") else ""

        if cn:
            abbr_map[cn] = {}
            if abbr:
                abbr_map[cn]["abbr"] = abbr
            if en:
                abbr_map[cn]["en"] = en

    return abbr_map

def should_merge_crossline(prev_text, curr_text, prev_bbox, curr_bbox):
    """
    åˆ¤æ–­æ˜¯å¦éœ€è¦æŠŠå½“å‰è¡Œåˆå¹¶åˆ°ä¸Šä¸€è¡Œ
    """
    text_stripped = curr_text.strip()

    # æ¨¡å¼åŒ¹é…ï¼šè¡¨æ ¼æ ‡é¢˜ã€ç¼–å·æ ‡é¢˜ç­‰
    if re.match(r'^è¡¨\s*\d+', text_stripped):
        return True

    # å‚ç›´è·ç¦»å¾ˆå°ï¼ˆè¯´æ˜æ˜¯è§†è§‰ä¸Šçš„åŒä¸€è¡Œï¼‰
    prev_y = prev_bbox[1]
    curr_y = curr_bbox[1]
    line_height = prev_bbox[3] - prev_bbox[1]
    if abs(curr_y - prev_y) < 0.3 * line_height:
        return True

    return False

def fix_broken_chapters(lines: list[str]) -> list[str]:
    def normalize_chapter_spaces(s: str) -> str:
        line = s.strip()
        
        # 1. ä¿ç•™åŸæ¥çš„é€»è¾‘ï¼šä¿®å¤ç‚¹åé¢çš„ç©ºæ ¼ï¼Œé€‚ç”¨äºæ‰€æœ‰æƒ…å†µ (A. 1, 7. 1)
        line = re.sub(r'\.\s+(?=\d)', '.', line)
        
        # 2. ä¿®å¤æ•°å­—/å­—æ¯å’Œç‚¹ä¹‹é—´çš„ç©ºæ ¼ï¼š7 .1 -> 7.1, A .1 -> A.1
        line = re.sub(r'([A-Za-z0-9]+)\s+(\.\d+)', r'\1\2', line)
        
        # 3. ä¿®å¤å¤æ‚çš„å¤šçº§ç©ºæ ¼ï¼š7 . 1 . 2 -> 7.1.2
        # éœ€è¦å¾ªç¯å¤„ç†ï¼Œç›´åˆ°æ²¡æœ‰æ›´å¤šå˜åŒ–
        max_iterations = 10  # é˜²æ­¢æ— é™å¾ªç¯
        iterations = 0
        prev_line = ""
        while prev_line != line and iterations < max_iterations:
            prev_line = line
            # å¤„ç†å„ç§ç©ºæ ¼ç»„åˆï¼Œæ”¯æŒå­—æ¯å’Œæ•°å­—å¼€å¤´
            line = re.sub(r'([A-Za-z0-9]+)\s*\.\s*(\d+)', r'\1.\2', line)
            iterations += 1
        
        # 4. ä¿®å¤OCRå¸¸è§é”™è¯¯ï¼šæ•°å­—å¼€å¤´çš„ç« èŠ‚
        line = re.sub(r'(\d+\.\d+)\.\s*l\b', r'\1.1', line)
        line = re.sub(r'([A-Za-z0-9]+)\.l\.(\d+)', r'\1.1.\2', line)
        line = re.sub(r'^l\.(\d+)', r'1.\1', line)
        
        # 5. ä¿®å¤å­—æ¯å¼€å¤´ç« èŠ‚çš„OCRé”™è¯¯ï¼šB.l -> B.1, A.O -> A.0, C.I -> C.1
        line = re.sub(r'^([A-Z])\.l\b', r'\1.1', line)
        line = re.sub(r'^([A-Z])\.l\.(\d+)', r'\1.1.\2', line)
        line = re.sub(r'^([A-Z])\.O\.(\d+)', r'\1.0.\2', line)
        line = re.sub(r'^([A-Z])\.I\.(\d+)', r'\1.1.\2', line)
        
        # 6. ä¿®å¤å…¶ä»–OCRé”™è¯¯ï¼šO -> 0, I -> 1
        line = re.sub(r'([A-Za-z0-9]+)\.O\.(\d+)', r'\1.0.\2', line)
        line = re.sub(r'([A-Za-z0-9]+)\.I\.(\d+)', r'\1.1.\2', line)
        
        return line

    lines = [normalize_chapter_spaces(line) for line in lines]

    return lines

def process_gb_terms_format(lines: List[str]) -> List[str]:
    """
    å¤„ç†å›½æ ‡æœ¯è¯­å®šä¹‰æ ¼å¼ï¼š
    å°† "3.1" (ä¸‹ä¸€è¡Œ) "ä¸­æ–‡æœ¯è¯­ è‹±æ–‡æœ¯è¯­" åˆå¹¶ä¸º "3.1 ä¸­æ–‡æœ¯è¯­ è‹±æ–‡æœ¯è¯­"
    """
    result = []
    i = 0
    
    while i < len(lines):
        current_line = lines[i].strip()
        
        # æ£€æµ‹æ˜¯å¦æ˜¯æœ¯è¯­å®šä¹‰ç¼–å·ï¼šçº¯æ•°å­—.æ•°å­—æ ¼å¼ï¼Œä¸”ä¸‹ä¸€è¡ŒåŒ…å«ä¸­æ–‡+è‹±æ–‡ï¼Œæˆ–è€…ç¬¬äºŒè¡Œæ˜¯ä¸­æ–‡ï¼Œç¬¬ä¸‰è¡Œæ˜¯è‹±æ–‡
        if (i + 1 < len(lines) and 
            re.match(r'^\d+\.\d+$', current_line) and
            current_line.startswith('3.')):  # é€šå¸¸æœ¯è¯­ç« èŠ‚æ˜¯ç¬¬3ç« 
            
            next_line = lines[i + 1].strip()
            
            # æ£€æŸ¥ä¸‹ä¸€è¡Œæ˜¯å¦ç¬¦åˆ: ä¸­æ–‡ + ç©ºæ ¼ + è‹±æ–‡ çš„æ¨¡å¼
            if re.search(r'[\u4e00-\u9fa5].*[A-Za-z]', next_line):
                # åˆå¹¶æˆæ ‡é¢˜æ ¼å¼
                merged_line = f"{current_line} {next_line}"
                result.append(merged_line)
                i += 2  # è·³è¿‡ä¸‹ä¸€è¡Œ
                continue

            # æ£€æŸ¥ç¬¬äºŒè¡Œæ˜¯å¦æ˜¯ä¸­æ–‡ï¼Œç¬¬ä¸‰è¡Œæ˜¯å¦æ˜¯è‹±æ–‡
            if (i + 2 < len(lines) and
                re.search(r'[\u4e00-\u9fa5]', lines[i + 1].strip()) and
                re.search(r'[A-Za-z]', lines[i + 2].strip())):
                merged_line = f"{current_line} {lines[i + 1].strip()} {lines[i + 2].strip()}"
                result.append(merged_line)
                i += 3  # è·³è¿‡åä¸¤è¡Œ
                continue

        result.append(current_line)
        i += 1
    
    return result

def extract_full_text_with_filter(pdf_path: str, top_crop=0.08, bottom_crop=0.08):
    doc = fitz.open(pdf_path)
    all_lines = []

    prev_line_text = None
    prev_bbox = None



    for page in doc:

        h = page.rect.height
        clip_rect = fitz.Rect(0, h * top_crop, page.rect.width, h * (1 - bottom_crop))
        page_dict = page.get_text("dict", clip=clip_rect)

        for block in page_dict["blocks"]:
            if block["type"] != 0:  # åªå¤„ç†æ–‡æœ¬
                continue

            for line in block["lines"]:
                # 1. æŒ‰xåæ ‡åˆå¹¶åŒä¸€è¡Œçš„span
                spans = sorted(line["spans"], key=lambda s: s["bbox"][0])
                merged = ""
                last_x = None
                for sp in spans:
                    x0, x1 = sp["bbox"][0], sp["bbox"][2]
                    width = max(1.0, x1 - x0)
                    avg_char_w = width / max(len(sp["text"]), 1)

                    if last_x is not None:
                        gap = x0 - last_x
                        if gap > max(avg_char_w * 0.5, 3.0):
                            merged += " "
                    merged += sp["text"]
                    last_x = x1

                merged = merged.strip()
                curr_bbox = line["bbox"]

                # 2. è·¨è¡Œæ™ºèƒ½åˆå¹¶åˆ¤å®š
                if prev_line_text is not None:
                    if should_merge_crossline(prev_line_text, merged, prev_bbox, curr_bbox):
                        prev_line_text += " " + merged
                        prev_bbox = (
                            prev_bbox[0],
                            prev_bbox[1],
                            max(prev_bbox[2], curr_bbox[2]),
                            max(prev_bbox[3], curr_bbox[3])
                        )
                        continue
                    else:
                        all_lines.append(prev_line_text)

                prev_line_text = merged
                prev_bbox = curr_bbox

    # æœ€åä¸€è¡Œ
    if prev_line_text:
        all_lines.append(prev_line_text)

    # è¿›è¡Œå…¨è§’å­—ç¬¦è½¬åŠè§’å­—ç¬¦
    all_lines = [fullwidth_to_halfwidth(line.strip()) for line in all_lines]

    # è¿›è¡Œç« èŠ‚ç¼–å·ä¿®å¤
    normalized = fix_broken_chapters(all_lines)
    
    # ğŸ†• å›½æ ‡æœ¯è¯­å®šä¹‰æ ¼å¼å¤„ç†
    normalized = process_gb_terms_format(normalized)

    # å†™å‡ºæ–‡ä»¶ä¸è¿”å›
    with open('extracted_full_text.txt', "w", encoding="utf-8") as f:
        f.write("\n".join(normalized))

    return normalized

def detect_chapter_pattern(chapters: List[Dict]) -> str:
    """
    æ£€æµ‹æ–‡æ¡£çš„ç« èŠ‚æ¨¡å¼ï¼š
    - 'alpha_first': å­—æ¯ç« èŠ‚åœ¨å‰ (A, A.1, A.2, B, B.1, 1, 2, ...)
    - 'numeric_first': æ•°å­—ç« èŠ‚åœ¨å‰ (1, 2, ..., A, A.1, A.2, B, B.1, ...)
    """
    alpha_indices = []
    numeric_indices = []
    
    for i, ch in enumerate(chapters):
        chapter_id = ch["chapter_id"].strip()
        if re.match(r'^[A-Z](\.\d+)*\.?$', chapter_id):
            alpha_indices.append(i)
        elif re.match(r'^\d+(\.\d+)*\.?$', chapter_id):
            numeric_indices.append(i)
    
    if not alpha_indices or not numeric_indices:
        return 'numeric_first'  # é»˜è®¤æ•°å­—ä¼˜å…ˆ
    
    # æ¯”è¾ƒç¬¬ä¸€ä¸ªå­—æ¯ç« èŠ‚å’Œç¬¬ä¸€ä¸ªæ•°å­—ç« èŠ‚çš„ä½ç½®
    first_alpha = min(alpha_indices)
    first_numeric = min(numeric_indices)
    
    if first_alpha < first_numeric:
        return 'alpha_first'
    else:
        return 'numeric_first'

def parse_chapter_id(chapter_id: str, pattern: str = 'numeric_first') -> List[int]:
    """
    æ ¹æ®æ–‡æ¡£æ¨¡å¼è§£æç« èŠ‚ID
    :param chapter_id: ç« èŠ‚IDå­—ç¬¦ä¸²
    :param pattern: æ–‡æ¡£æ¨¡å¼ ('alpha_first' æˆ– 'numeric_first')
    """
    chapter_id = chapter_id.strip()

    # å­—æ¯ç« èŠ‚æ ¼å¼ - æ”¯æŒç‚¹å’Œæ¨ªçº¿åˆ†éš”ç¬¦
    if re.fullmatch(r'[A-Z](?:[.\-]\d+)*[.\-]?', chapter_id):
        # ç»Ÿä¸€å¤„ç†ç‚¹å’Œæ¨ªçº¿åˆ†éš”ç¬¦
        normalized = re.sub(r'[.\-]+', '.', chapter_id).rstrip('.')
        parts = normalized.split('.')
        letter = parts[0]
        
        if pattern == 'alpha_first':
            # å­—æ¯åœ¨å‰æ¨¡å¼ï¼šA=1, B=2, C=3, ...
            letter_value = ord(letter) - ord('A') + 1
        else:
            # æ•°å­—åœ¨å‰æ¨¡å¼ï¼šå­—æ¯ç« èŠ‚æ”¾åœ¨æ•°å­—ç« èŠ‚ä¹‹å
            # å‡è®¾æœ€å¤šæœ‰100ä¸ªæ•°å­—ç« èŠ‚ï¼Œå­—æ¯ä»101å¼€å§‹
            letter_value = ord(letter) - ord('A') + 101
        
        try:
            rest = [int(p) for p in parts[1:]] if len(parts) > 1 else []
            return [letter_value] + rest
        except ValueError:
            return []

    # æ•°å­—ç« èŠ‚æ ¼å¼ - æ”¯æŒç‚¹å’Œæ¨ªçº¿åˆ†éš”ç¬¦
    elif re.fullmatch(r'\d+(?:[.\-]\d+)*[.\-]?', chapter_id):
        try:
            # ç»Ÿä¸€å¤„ç†ç‚¹å’Œæ¨ªçº¿åˆ†éš”ç¬¦
            normalized = re.sub(r'[.\-]+', '.', chapter_id).rstrip('.')
            parts = normalized.split('.')
            numeric_parts = [int(p) for p in parts]
            
            if pattern == 'alpha_first':
                # å­—æ¯åœ¨å‰æ¨¡å¼ï¼šæ•°å­—ç« èŠ‚æ”¾åœ¨å­—æ¯ç« èŠ‚ä¹‹å
                # å‡è®¾æœ€å¤šæœ‰26ä¸ªå­—æ¯ç« èŠ‚ï¼Œæ•°å­—ä»27å¼€å§‹
                numeric_parts[0] += 26
            # æ•°å­—åœ¨å‰æ¨¡å¼ï¼šä¿æŒåŸæœ‰æ•°å­—
            
            return numeric_parts
        except ValueError:
            return []

    return []

def is_chapter_a_before_b(a: list[int], b: list[int]) -> bool:
    for i in range(min(len(a), len(b))):
        if a[i] < b[i]:
            return True
        elif a[i] > b[i]:
            return False
    return len(a) < len(b)

def is_reasonable_chapter_jump(prev_id: List[int], curr_id: List[int]) -> bool:
    """
    åˆ¤æ–­ç« èŠ‚è·³è·ƒæ˜¯å¦åˆç†ï¼Œæ›´å®½æ¾çš„ç­–ç•¥ï¼š
    ä¸»è¦è¿‡æ»¤æ‰æ˜æ˜¾ä¸åˆç†çš„è·³è·ƒï¼Œä½†å…è®¸æ­£å¸¸çš„ç« èŠ‚ç»“æ„
    å¯¹å­—æ¯é“¾å’Œæ•°å­—é“¾éƒ½è¿›è¡Œåˆç†æ€§åˆ¤æ–­
    """
    if not prev_id or not curr_id:
        return True  # å¦‚æœæ— æ³•è§£æï¼Œé»˜è®¤å…è®¸
    
    # å¦‚æœæ˜¯ä¸åŒå±‚çº§ï¼Œä¸€èˆ¬éƒ½æ˜¯åˆç†çš„ï¼ˆå¦‚ 1. -> 1.1 æˆ– 1.1 -> 2.ï¼‰
    if len(prev_id) != len(curr_id):
        return True
    
    # åŒå±‚çº§çš„æƒ…å†µä¸‹ï¼Œæ£€æŸ¥è·³è·ƒå¹…åº¦
    if len(prev_id) == 1:  # ä¸€çº§ç« èŠ‚
        prev_num = prev_id[0]
        curr_num = curr_id[0]
        diff = curr_num - prev_num
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºå­—æ¯ç« èŠ‚ï¼ˆç¼–ç èŒƒå›´101-126å¯¹åº”A-Zï¼‰
        if prev_num >= 101 and curr_num >= 101:  # å­—æ¯ç« èŠ‚
            # å­—æ¯è·³è·ƒæ£€æŸ¥ï¼šä¸å…è®¸è·¨è¶Šè¶…è¿‡2ä¸ªå­—æ¯ï¼ˆå¦‚Bè·³åˆ°Eä»¥ä¸Šï¼‰
            return 1 <= diff <= 2
        else:  # æ•°å­—ç« èŠ‚
            return 1 <= diff <= 5  # å…è®¸è·³è·ƒ1-5ç« ï¼ˆè¿‡æ»¤æ‰ä»5è·³åˆ°100è¿™ç§æ˜æ˜¾é”™è¯¯çš„ï¼‰
    
    elif len(prev_id) == 2:  # äºŒçº§ç« èŠ‚
        # å¦‚æœç¬¬ä¸€çº§ç›¸åŒï¼Œæ£€æŸ¥ç¬¬äºŒçº§çš„è·³è·ƒ
        if prev_id[0] == curr_id[0]:
            prev_num = prev_id[1]
            curr_num = curr_id[1]
            diff = curr_num - prev_num
            
            # åˆ¤æ–­ç¬¬ä¸€çº§æ˜¯å¦ä¸ºå­—æ¯ç« èŠ‚
            if prev_id[0] >= 101:  # å­—æ¯ç« èŠ‚çš„å­çº§
                return 1 <= diff <= 5  # å­—æ¯ç« èŠ‚çš„å­çº§è·³è·ƒç¨å¾®å®½æ¾ä¸€äº›
            else:  # æ•°å­—ç« èŠ‚çš„å­çº§
                return 1 <= diff <= 10  # äºŒçº§ç« èŠ‚å…è®¸æ›´å¤§è·³è·ƒ
        else:
            # ä¸åŒçš„ä¸€çº§ç« èŠ‚ï¼Œéƒ½åˆç†
            return True
    
    else:  # ä¸‰çº§åŠä»¥ä¸Šç« èŠ‚
        # å¯¹äºæ·±å±‚æ¬¡ç« èŠ‚ï¼Œæ›´å®½æ¾ä¸€äº›
        return True

def analyze_chapter_number_distribution(chapters: List[Dict]) -> Dict[str, int]:
    """
    åˆ†æç« èŠ‚ç¼–å·çš„æ•°å­—åˆ†å¸ƒï¼Œç¡®å®šåˆç†çš„æ•°å­—èŒƒå›´
    è¿”å›: {"min_reasonable": æœ€å°åˆç†æ•°å­—, "max_reasonable": æœ€å¤§åˆç†æ•°å­—, "primary_range": ä¸»è¦æ•°å­—èŒƒå›´}
    """
    first_numbers = []
    
    for ch in chapters:
        chapter_id = ch["chapter_id"].strip()
        # æå–ç¬¬ä¸€ä¸ªæ•°å­—
        m_num = re.match(r'^(\d+)', chapter_id)
        if m_num:
            first_numbers.append(int(m_num.group(1)))
        # å¤„ç†APPENDIXåè·Ÿæ•°å­—çš„æƒ…å†µ
        elif chapter_id.upper().startswith("APPENDIX"):
            suffix = chapter_id[len("APPENDIX"):].strip(" ()")
            if suffix.isdigit():
                first_numbers.append(int(suffix))
    
    if not first_numbers:
        return {"min_reasonable": 1, "max_reasonable": 50, "primary_range": (1, 50)}
    
    first_numbers.sort()
    
    # åˆ†ææ•°å­—åˆ†å¸ƒæ¨¡å¼
    from collections import Counter
    counter = Counter(first_numbers)
    
    # å¦‚æœå¤§å¤šæ•°ç« èŠ‚éƒ½æ˜¯åŒä¸€ä¸ªæ•°å­—å¼€å¤´ï¼ˆå¦‚60.1, 60.2, 60.3...ï¼‰ï¼Œè¿™å¯èƒ½æ˜¯æ³•è§„ç¼–å·
    most_common = counter.most_common(1)[0]
    most_common_num, most_common_count = most_common
    
    # å¦‚æœæŸä¸ªæ•°å­—å‡ºç°æ¬¡æ•°è¶…è¿‡æ€»æ•°çš„60%ï¼Œä¸”è¿™ä¸ªæ•°å­—å¤§äº30ï¼Œå¯èƒ½æ˜¯æ³•è§„ç¼–å·æ¨¡å¼
    if most_common_count > len(first_numbers) * 0.6 and most_common_num > 30:
        print(f"æ£€æµ‹åˆ°å¯èƒ½çš„æ³•è§„ç¼–å·æ¨¡å¼: {most_common_num}.x (å‡ºç°{most_common_count}æ¬¡)")
        # åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå…è®¸è¿™ä¸ªç‰¹å®šçš„æ³•è§„ç¼–å·
        return {
            "min_reasonable": most_common_num, 
            "max_reasonable": most_common_num, 
            "primary_range": (most_common_num, most_common_num),
            "regulation_mode": True,
            "regulation_number": most_common_num
        }
    
    # æ­£å¸¸çš„ç« èŠ‚ç¼–å·æ¨¡å¼
    min_num = min(first_numbers)
    max_num = max(first_numbers)
    
    # å¦‚æœæ•°å­—èŒƒå›´å¾ˆå°ï¼ˆ<= 50ï¼‰ï¼Œè®¤ä¸ºæ˜¯æ­£å¸¸ç« èŠ‚
    if max_num <= 50:
        return {
            "min_reasonable": max(1, min_num), 
            "max_reasonable": min(50, max_num + 5),  # å…è®¸å°‘é‡è¶…å‡º
            "primary_range": (min_num, max_num)
        }
    
    # å¦‚æœæ•°å­—èŒƒå›´å¾ˆå¤§ï¼Œå¯èƒ½åŒ…å«é¡µç ç­‰å¹²æ‰°ï¼Œé‡‡ç”¨æ›´ä¿å®ˆç­–ç•¥
    # æ‰¾åˆ°æœ€å¯†é›†çš„æ•°å­—åŒºé—´
    gaps = []
    for i in range(len(first_numbers) - 1):
        gaps.append(first_numbers[i + 1] - first_numbers[i])
    
    # å¦‚æœæœ‰æ˜æ˜¾çš„å¤§è·³è·ƒï¼ˆ>20ï¼‰ï¼Œå¯èƒ½å‰é¢æ˜¯æ­£å¸¸ç« èŠ‚ï¼Œåé¢æ˜¯é¡µç ç­‰
    large_gap_idx = -1
    for i, gap in enumerate(gaps):
        if gap > 20:
            large_gap_idx = i
            break
    
    if large_gap_idx != -1:
        # å–è·³è·ƒå‰çš„æ•°å­—ä½œä¸ºåˆç†èŒƒå›´
        reasonable_max = first_numbers[large_gap_idx]
        return {
            "min_reasonable": max(1, min_num), 
            "max_reasonable": reasonable_max,
            "primary_range": (min_num, reasonable_max)
        }
    
    # é»˜è®¤ä¿å®ˆç­–ç•¥
    return {"min_reasonable": 1, "max_reasonable": 50, "primary_range": (1, 50)}

def find_longest_chapter_chain_with_append(chapters: List[Dict], language: str = 'en') -> Tuple[List[Dict], str]:
    # å…ˆæ£€æµ‹ç« èŠ‚æ¨¡å¼
    pattern = detect_chapter_pattern(chapters)
    print(f"æ£€æµ‹åˆ°ç« èŠ‚æ¨¡å¼: {pattern}")
    
    # ğŸ†• åˆ†æç« èŠ‚æ•°å­—åˆ†å¸ƒ
    number_analysis = analyze_chapter_number_distribution(chapters)
    print(f"ç« èŠ‚æ•°å­—åˆ†æç»“æœ: {number_analysis}")
    
    # ç”¨æ£€æµ‹åˆ°çš„æ¨¡å¼é‡æ–°è§£æç« èŠ‚ID
    parsed_ids = [parse_chapter_id(ch["chapter_id"], pattern) for ch in chapters]
    # print(f'ç¬¬ä¸€ä¸ªç« èŠ‚: {chapters[0]}')
    n = len(chapters)

    # ç¬¬ä¸€æ­¥ï¼šè¿‡æ»¤æ‰æ˜æ˜¾ä¸åˆç†çš„ç« èŠ‚ï¼ˆå¦‚è¯¯è¯†åˆ«çš„æ•°å­—ï¼‰
    valid_indices = []
    for i in range(n):
        if not parsed_ids[i]:
            continue
            
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ˜æ˜¾çš„è¯¯è¯†åˆ«
        chapter_text = chapters[i]["chapter_id"] + " " + chapters[i]["chapter_title"]
        
        # è¿‡æ»¤æ˜æ˜¾çš„æµ‹é‡å•ä½ã€é¢‘ç‡èŒƒå›´ã€çº¯æ•°å­—ç­‰
        if re.search(r'\b\d+\s*(MHz|GHz|Hz|kHz|dB|V|mV|ÂµV|A|mA|ÂµA|W|mW|Î©|%|Â°C|Â°F|mm|cm|m|km|kg|g|mg|ms|s|min|h|rpm|bar|Pa|kPa|MPa)\b', chapter_text, re.I):
            continue
        if re.search(r'\d+\s*MHz\s*[~-]\s*\d+\s*MHz', chapter_text, re.I):
            continue
        if re.match(r'^\d+\s*$', chapters[i]["chapter_title"].strip()):  # æ ‡é¢˜æ˜¯çº¯æ•°å­—
            continue
        if len(chapters[i]["chapter_title"].strip()) < 2:  # æ ‡é¢˜å¤ªçŸ­
            continue
            
        # ğŸ†• è¡¨æ ¼æ•°æ®ç‰¹å¾è¿‡æ»¤
        chapter_title = chapters[i]["chapter_title"].strip()
        chapter_id = chapters[i]["chapter_id"].strip()
        
        # æ£€æµ‹è¡¨æ ¼è¡Œæ¨¡å¼ï¼šå•ä¸ªå­—æ¯ + ä¸»è¦æ˜¯æ•°å­—çš„æ ‡é¢˜
        if (len(chapter_id) == 1 and chapter_id.isupper() and 
            re.search(r'^\d+.*\d+', chapter_title) and 
            len([x for x in chapter_title.split() if x.isdigit()]) >= 2):
            continue
            
        # æ£€æµ‹åæ ‡ç‚¹æ ¼å¼ï¼šå¦‚ "10 0 E 0 16"
        if re.match(r'^\d+\s+\d+\s+[A-Z]\s+\d+\s+\d+', chapter_title):
            continue
            
        # æ£€æµ‹å‚æ•°è¡¨æ ¼æ ¼å¼ï¼šå¦‚ "34 65 F 25 77"
        title_parts = chapter_title.split()
        if (len(title_parts) >= 4 and 
            sum(1 for part in title_parts if part.isdigit()) >= 3 and
            sum(1 for part in title_parts if len(part) == 1 and part.isupper()) >= 1):
            continue
            
        # æ£€æµ‹å›¾è¡¨æ ‡æ³¨è¯´æ˜ï¼šå•ä¸ªå­—æ¯ + ä»¥ç ´æŠ˜å·å¼€å¤´çš„æ ‡é¢˜
        if (len(chapter_id) == 1 and chapter_id.isupper() and 
            chapter_title.startswith('â€”â€”â€”')):
            continue
            
        valid_indices.append(i)
    
    # ç¬¬äºŒæ­¥ï¼šéªŒè¯å­—æ¯ç« èŠ‚çš„åˆç†æ€§ï¼ˆé’ˆå¯¹ä¸­è‹±æ–‡å·®å¼‚åŒ–å¤„ç†ï¼‰
    if valid_indices:
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å­—æ¯ç« èŠ‚
        alpha_chapters = []
        for i, idx in enumerate(valid_indices):
            chapter_id = chapters[idx]["chapter_id"].strip()
            if re.match(r'^[A-Z](?:\.\d+)*\.?$', chapter_id):
                alpha_chapters.append((i, idx, chapter_id[0]))  # (åœ¨valid_indicesä¸­çš„ä½ç½®, åŸå§‹ç´¢å¼•, é¦–å­—æ¯)
        
        # å¦‚æœæœ‰å­—æ¯ç« èŠ‚ï¼Œè¿›è¡Œåˆç†æ€§éªŒè¯
        if alpha_chapters:
            if language == 'en':
                # è‹±æ–‡æ–‡æ¡£ï¼šè¦æ±‚å­—æ¯ç« èŠ‚å¿…é¡»ä»Aå¼€å¤´
                first_alpha_letter = alpha_chapters[0][2]
                if first_alpha_letter != 'A':
                    print(f"è‹±æ–‡æ–‡æ¡£å­—æ¯ç« èŠ‚ä¸ä»¥Aå¼€å¤´ï¼Œè·³è¿‡: ç¬¬ä¸€ä¸ªå­—æ¯ç« èŠ‚æ˜¯ {alpha_chapters[0][2]}")
                    # ç§»é™¤æ‰€æœ‰å­—æ¯ç« èŠ‚
                    alpha_indices_set = {item[1] for item in alpha_chapters}
                    valid_indices = [idx for idx in valid_indices if idx not in alpha_indices_set]
            else:
                # ä¸­æ–‡æ–‡æ¡£ï¼šæ£€æŸ¥å­—æ¯ç« èŠ‚çš„ä¸€è‡´æ€§ï¼ˆåŒä¸€é™„å½•åº”è¯¥ä»¥åŒä¸€å­—æ¯å¼€å¤´ï¼‰
                from collections import Counter
                alpha_letters = [item[2] for item in alpha_chapters]
                letter_counter = Counter(alpha_letters)
                most_common_letter, most_common_count = letter_counter.most_common(1)[0]
                
                # å¦‚æœæŸä¸ªå­—æ¯å‡ºç°æ¬¡æ•°è¶…è¿‡60%ï¼Œè®¤ä¸ºè¿™æ˜¯ä¸»è¦çš„é™„å½•å­—æ¯
                if most_common_count > len(alpha_chapters) * 0.6:
                    print(f"ä¸­æ–‡æ–‡æ¡£æ£€æµ‹åˆ°ä¸»è¦é™„å½•å­—æ¯: {most_common_letter} (å‡ºç°{most_common_count}æ¬¡)")
                    # ä¿ç•™ä¸ä¸»è¦å­—æ¯ä¸€è‡´çš„ç« èŠ‚ï¼Œç§»é™¤å…¶ä»–å­—æ¯ç« èŠ‚
                    keep_alpha_indices = {item[1] for item in alpha_chapters if item[2] == most_common_letter}
                    remove_alpha_indices = {item[1] for item in alpha_chapters if item[2] != most_common_letter}
                    valid_indices = [idx for idx in valid_indices if idx not in remove_alpha_indices]
                    if remove_alpha_indices:
                        removed_letters = {chapters[idx]["chapter_id"].strip()[0] for idx in remove_alpha_indices}
                        print(f"ç§»é™¤ä¸ä¸€è‡´çš„å­—æ¯ç« èŠ‚: {removed_letters}")
                else:
                    # å¦‚æœæ²¡æœ‰æ˜æ˜¾çš„ä¸»è¦å­—æ¯ï¼Œä¿æŒåŸæœ‰é€»è¾‘ï¼ˆå¯èƒ½æ˜¯æ··åˆæƒ…å†µï¼‰
                    print(f"ä¸­æ–‡æ–‡æ¡£å­—æ¯ç« èŠ‚åˆ†å¸ƒè¾ƒå‡åŒ€: {dict(letter_counter)}")
                    # ä¸åšç‰¹æ®Šå¤„ç†ï¼Œä¿ç•™æ‰€æœ‰å­—æ¯ç« èŠ‚

    # ç¬¬ä¸‰æ­¥ï¼šä»åå¾€å‰æ„å»ºæœ€é•¿é“¾
    dp = [1] * len(valid_indices)
    next_link = [-1] * len(valid_indices)  # æ”¹ä¸ºè®°å½•ä¸‹ä¸€ä¸ªèŠ‚ç‚¹
    max_len = 0
    max_idx = -1

    # ä»åå¾€å‰éå†
    for i in range(len(valid_indices) - 1, -1, -1):
        curr_idx = valid_indices[i]
        curr_parsed = parsed_ids[curr_idx]
        
        # æ‰¾åœ¨å½“å‰èŠ‚ç‚¹ä¹‹åçš„æ‰€æœ‰èŠ‚ç‚¹
        for j in range(i + 1, len(valid_indices)):
            next_idx = valid_indices[j]
            next_parsed = parsed_ids[next_idx]
            
            # æ£€æŸ¥å½“å‰èŠ‚ç‚¹æ˜¯å¦å¯ä»¥è¿åˆ°ä¸‹ä¸€ä¸ªèŠ‚ç‚¹
            if (is_chapter_a_before_b(curr_parsed, next_parsed) and 
                is_reasonable_chapter_jump(curr_parsed, next_parsed)):
                if dp[j] + 1 > dp[i]:
                    dp[i] = dp[j] + 1
                    next_link[i] = j
        
        if dp[i] > max_len:
            max_len = dp[i]
            max_idx = i

    # ç¬¬å››æ­¥ï¼šå¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆç†çš„é“¾ï¼Œé€€å›åˆ°ç®€å•çš„é¡ºåºè¿‡æ»¤
    if max_len < 2:
        # ç®€å•æŒ‰ç« èŠ‚ç¼–å·é¡ºåºè¿‡æ»¤
        filtered_chapters = simple_chapter_filter(chapters)
        
        # å¦‚æœè¿‡æ»¤åè¿˜æ˜¯æ²¡æœ‰ç« èŠ‚ï¼Œå°†æ‰€æœ‰å†…å®¹æ”¾å…¥è·³è¿‡çš„å†…å®¹ä¸­
        if not filtered_chapters:
            all_content = []
            for ch in chapters:
                content = f"{ch['chapter_id']} {ch['chapter_title']}"
                if ch.get('raw_text'):
                    content += " " + ch['raw_text']
                all_content.append(content)
            skipped_text = "\n".join(all_content)
            return [], skipped_text
        
        return filtered_chapters, ""

    # å›æº¯å‡ºä¸»é“¾ç´¢å¼•ï¼ˆä»å‰å¾€åçš„æ­£ç¡®é¡ºåºï¼‰
    chain_indices = []
    idx = max_idx
    while idx != -1:
        chain_indices.append(valid_indices[idx])
        idx = next_link[idx]
    
    print(f"ä»åå¾€å‰ç”Ÿæˆçš„æœ€é•¿é“¾: é•¿åº¦={len(chain_indices)}, ä½ç½®={chain_indices[:5]}{'...' if len(chain_indices)>5 else ''}")
    
    # æœ€é•¿é“¾çš„ç¬¬ä¸€ä¸ªç« èŠ‚ç´¢å¼•
    first_chain_idx = chain_indices[0]
    
    # ç”Ÿæˆè·³è¿‡çš„å†…å®¹ï¼ˆæœ€é•¿é“¾ç¬¬ä¸€ä¸ªç« èŠ‚ä¹‹å‰çš„æ‰€æœ‰å†…å®¹ï¼‰
    skipped_chapters = chapters[:first_chain_idx]
    skipped_text = "\n".join([f"{ch['chapter_id']} {ch['chapter_title']} {ch.get('raw_text','')}" for ch in skipped_chapters])
    
    chain_set = set(chain_indices)

    # æœ€ç»ˆç»“æœæ„å»º
    result = []
    last_valid = None
    for i, chap in enumerate(chapters):
        if i in chain_set:
            result.append(chap)
            last_valid = chap
        elif i >= first_chain_idx:  # åªå¤„ç†æœ€é•¿é“¾å¼€å§‹ä¹‹åçš„ç« èŠ‚
            if last_valid:
                content_to_add = "\n" + chap["chapter_id"] + chap["chapter_title"]
                if chap.get("raw_text"):
                    content_to_add += " " + chap["raw_text"]
                last_valid["raw_text"] += content_to_add

    # åˆ¤æ–­ç« èŠ‚æ ‡é¢˜æ˜¯å¦åº”è¯¥åˆå¹¶åˆ°æ­£æ–‡ä¸­
    for chap in result:
        should_merge = False
        
        # ä¸­æ–‡å¤„ç†ï¼šåŒ…å«ä¸­æ–‡ä¸”æœ‰ä¸­æ–‡é€—å·ï¼Œå¥å·ï¼Œå†’å·ï¼Œæˆ–è€…é•¿åº¦å¤§äº30å­—ç¬¦
        if re.search(r'[\u4e00-\u9fa5]', chap["chapter_title"]):
            # è·å–ç« èŠ‚ç¼–å·çš„ç¬¬ä¸€ä¸ªæ•°å­—ï¼Œå‰ä¸‰ç« è·³è¿‡åˆå¹¶åˆ¤æ–­
            first_num = None
            chapter_id = chap["chapter_id"].strip('.-')
            if re.match(r'^\d+', chapter_id):
                first_num = int(re.match(r'^\d+', chapter_id).group())
            # å‰ä¸‰ç« è·³è¿‡åˆå¹¶åˆ¤æ–­
            if first_num is not None and first_num <= 3:
                continue          
            if re.search(r'[ï¼Œã€‚ï¼š,:]', chap["chapter_title"]) or len(chap["chapter_title"]) > 30:
                should_merge = True
        
        # è‹±æ–‡å¤„ç†ï¼šæ›´æ™ºèƒ½çš„åˆ¤æ–­é€»è¾‘
        else:
            # å¦‚æœå…¨å¤§å†™ï¼Œåˆ™è‚¯å®šæ˜¯æ ‡é¢˜
            if chap["chapter_title"].isupper():
                continue
            # 1. å¦‚æœraw_textä»¥å°å†™å­—æ¯å¼€å¤´ï¼Œå¯èƒ½æ˜¯æ ‡é¢˜çš„å»¶ç»­
            if len(chap["raw_text"]) and chap["raw_text"][0].islower():
                should_merge = True
            # 2. å¦‚æœchapter_titleåŒ…å«å®Œæ•´å¥å­çš„ç‰¹å¾
            elif re.search(r'[,;!?]', chap["chapter_title"]):
                should_merge = True
            # 3. å¦‚æœchapter_titleå¾ˆé•¿ï¼ˆè¶…è¿‡50ä¸ªå­—ç¬¦ï¼‰ï¼Œå¯èƒ½æ˜¯æ®µè½æ–‡æœ¬
            elif len(chap["chapter_title"]) > 50:
                should_merge = True
        
        if should_merge:
            chap["raw_text"] = chap["chapter_title"] + ' ' + chap["raw_text"]
            chap["chapter_title"] = ""

    return result, skipped_text

def simple_chapter_filter(chapters: List[Dict]) -> List[Dict]:
    """
    ç®€å•çš„ç« èŠ‚è¿‡æ»¤ç­–ç•¥ï¼šå½“æœ€é•¿é“¾ç®—æ³•å¤±æ•ˆæ—¶çš„å¤‡ç”¨æ–¹æ¡ˆ
    """
    # æ£€æµ‹ç« èŠ‚æ¨¡å¼
    pattern = detect_chapter_pattern(chapters)
    
    result = []
    parsed_ids = [parse_chapter_id(ch["chapter_id"], pattern) for ch in chapters]
    
    for i, chap in enumerate(chapters):
        parsed_id = parsed_ids[i]
        
        # åŸºæœ¬åˆç†æ€§æ£€æŸ¥
        if not parsed_id:
            # æ— æ³•è§£æçš„ç« èŠ‚ï¼Œè¿½åŠ åˆ°ä¸Šä¸€ä¸ªæœ‰æ•ˆç« èŠ‚
            if result:
                content_to_add = "\n" + chap["chapter_id"] + chap["chapter_title"]
                if chap.get("raw_text"):
                    content_to_add += " " + chap["raw_text"]
                result[-1]["raw_text"] += content_to_add
            continue
        
        # æ£€æŸ¥ç« èŠ‚ç¼–å·æ˜¯å¦åœ¨åˆç†èŒƒå›´å†…
        first_num = parsed_id[0]
        
        # æ ¹æ®æ¨¡å¼è°ƒæ•´åˆç†æ€§æ£€æŸ¥
        if pattern == 'alpha_first':
            # å­—æ¯åœ¨å‰ï¼šA=1, B=2, ..., 1=27, 2=28, ...
            if 1 <= first_num <= 50:  # åˆç†èŒƒå›´ï¼š26ä¸ªå­—æ¯ + 20ä¸ªæ•°å­—ç« èŠ‚
                result.append(chap)
            else:
                # ä¸åˆç†çš„ç« èŠ‚ï¼Œè¿½åŠ åˆ°ä¸Šä¸€ä¸ªæœ‰æ•ˆç« èŠ‚
                if result:
                    content_to_add = "\n" + chap["chapter_id"] + chap["chapter_title"]
                    if chap.get("raw_text"):
                        content_to_add += " " + chap["raw_text"]
                    result[-1]["raw_text"] += content_to_add
        else:
            # æ•°å­—åœ¨å‰ï¼š1, 2, ..., A=101, B=102, ...
            if (1 <= first_num <= 20) or (101 <= first_num <= 126):  # æ•°å­—ç« èŠ‚æˆ–å­—æ¯ç« èŠ‚
                result.append(chap)
            else:
                # ä¸åˆç†çš„ç« èŠ‚ï¼Œè¿½åŠ åˆ°ä¸Šä¸€ä¸ªæœ‰æ•ˆç« èŠ‚
                if result:
                    content_to_add = "\n" + chap["chapter_id"] + chap["chapter_title"]
                    if chap.get("raw_text"):
                        content_to_add += " " + chap["raw_text"]
                    result[-1]["raw_text"] += content_to_add
    
    return result
    
    return result

def split_sections_by_attachment(chapters: List[Dict]) -> List[Dict]:
    """
    å°†æ•´ä¸ªæ–‡æ¡£æŒ‰é™„ä»¶ï¼ˆANNEXï¼‰åˆ‡åˆ†ã€‚
    é¡¶å±‚ file: regulation / ANNEX n
    æ”¹è¿›ï¼šåˆå¹¶è¿ç»­çš„ç›¸åŒé™„ä»¶æ ‡é¢˜
    """
    sections = []
    current_section = {
        "section": "regulation",  # é»˜è®¤ä¸»æ–‡æ¡£
        "chapters": []
    }

    annex_pattern = re.compile(r'^(ANNEX|ATTACHMENT)\s+([A-Z0-9]+)', re.I)

    for chap in chapters:
        match = annex_pattern.match(chap['chapter_id'])
        if match:
            annex_name = match.group(1).upper() + " " + match.group(2)  # æ ‡å‡†åŒ–åç§°ï¼Œå¦‚ "ANNEX 1"
            
            # æ£€æŸ¥æ˜¯å¦ä¸å½“å‰ section çš„åç§°ç›¸åŒ
            if current_section["section"] != "regulation" and current_section["section"].upper() == annex_name:
                # ç›¸åŒçš„é™„ä»¶ï¼Œç›´æ¥æ·»åŠ åˆ°å½“å‰ sectionï¼Œè·³è¿‡é‡å¤çš„æ ‡é¢˜ç« èŠ‚
                if chap.get('chapter_title') or chap.get('raw_text', '').strip():
                    current_section["chapters"].append(chap)
                # å¦‚æœæ˜¯ç©ºçš„é‡å¤æ ‡é¢˜ç« èŠ‚ï¼ˆåªæœ‰chapter_idæ²¡æœ‰å†…å®¹ï¼‰ï¼Œåˆ™è·³è¿‡
            else:
                # ä¸åŒçš„é™„ä»¶ï¼Œä¿å­˜å½“å‰å—å¹¶æ–°å»º
                if current_section["chapters"]:
                    sections.append(current_section)
                # æ–°å»ºé™„ä»¶å—
                current_section = {
                    "section": annex_name,
                    "chapters": [chap] if (chap.get('chapter_title') or chap.get('raw_text', '').strip()) else []
                }
        else:
            current_section["chapters"].append(chap)

    if current_section["chapters"]:
        sections.append(current_section)

    return sections

def split_sections_by_appendix(chapters):
    sections = []
    current_section = {"section": "MAIN", "chapters": []}

    for ch in chapters:
        # æ£€æµ‹ APPENDIX å¼€å¤´çš„é¡¶å±‚æ ‡é¢˜ï¼Œæˆ–è€…é™„å½•
        appendix_match = re.match(r'^(APPENDIX\s+(?:[A-Z0-9]+|\([A-Z0-9]+\)))$', ch['chapter_id'], re.IGNORECASE)
        annex_match = ch["chapter_id"].startswith("é™„å½•")
        
        if appendix_match or annex_match:
            # æ ‡å‡†åŒ–é™„å½•åç§°
            if appendix_match:
                appendix_name = appendix_match.group(1).upper()
            else:
                appendix_name = ch['chapter_id'].strip()
            
            # æ£€æŸ¥æ˜¯å¦ä¸å½“å‰ section çš„åç§°ç›¸åŒ
            if current_section["section"] != "MAIN" and current_section["section"].upper() == appendix_name:
                # ç›¸åŒçš„é™„å½•ï¼Œç›´æ¥æ·»åŠ åˆ°å½“å‰ sectionï¼ˆå¦‚æœæœ‰å®é™…å†…å®¹ï¼‰
                if ch.get('chapter_title') or ch.get('raw_text', '').strip():
                    current_section["chapters"].append(ch)
                # å¦‚æœæ˜¯ç©ºçš„é‡å¤æ ‡é¢˜ç« èŠ‚ï¼Œåˆ™è·³è¿‡
            else:
                # ä¸åŒçš„é™„å½•ï¼Œå…ˆä¿å­˜å½“å‰å—
                if current_section["chapters"]:
                    sections.append(current_section)
                # æ–°å»ºé™„å½•å—
                current_section = {
                    "section": appendix_name,
                    "chapters": [ch] if (ch.get('chapter_title') or ch.get('raw_text', '').strip()) else []
                }
        else:
            current_section["chapters"].append(ch)

    # æœ«å°¾å—åŠ å…¥
    if current_section["chapters"]:
        sections.append(current_section)

    # # æ‰“å°æå–çš„æ‰€æœ‰ç« èŠ‚æ ‡é¢˜
    # for sec in sections:
    #     print(f"Section: {sec['section']}")
    #     for chap in sec["chapters"]:
    #         print(f"  Chapter ID: {chap['chapter_id']}, Title: {chap['chapter_title']}")

    return sections

def process_sections_with_lis(chapters, language='en'):
    # å…ˆæ‹†åˆ†æˆæ­£æ–‡å’Œå¤šä¸ªé™„å½•
    sections = split_sections_by_appendix(chapters)

    # æ¯ä¸ªéƒ¨åˆ†å†…éƒ¨å•ç‹¬è·‘æœ€é•¿é“¾
    processed_sections = []
    for sec in sections:
        valid_chaps, skipped_content = find_longest_chapter_chain_with_append(sec["chapters"], language)
        processed_sections.append({
            "section": sec["section"],
            "context": skipped_content,  # æ·»åŠ è¢«è·³è¿‡çš„å†…å®¹
            "chapters": valid_chaps
        })

    return processed_sections

def filter_start_of_main(chapters: List[Dict]) -> Tuple[List[Dict], str]:
    """
    æ‰¾åˆ°ç¬¬ä¸€ä¸ªæ­£æ–‡ç« èŠ‚ä½œä¸ºèµ·ç‚¹ï¼Œè·³è¿‡ç›®å½•
    """
    start_index = 0
    for i, chap in enumerate(chapters):
        chapter_id = chap.get("chapter_id", "").strip()
        # æ­£æ–‡ä¸»é“¾æˆ–é™„ä»¶å†…éƒ¨ç« èŠ‚ï¼šæ•°å­—å¼€å¤´æˆ–å­—æ¯å¼€å¤´
        if chapter_id in {"1", "1-", "1.", "A", "A.", "A.1"}:
            # SCOPE / GENERAL / INTRO ç­‰éƒ½ç®—æ­£æ–‡èµ·ç‚¹
            title_upper = chap.get("chapter_title", "").upper()
            if any(k in title_upper for k in ["SCOPE", "GENERAL", "INTRO", "æ€»åˆ™", "èŒƒå›´", "LEGISLATIVE", "FUNCTION"]):
                start_index = i
                break
                
        # ä¹Ÿæ£€æŸ¥æ ‡å‡†çš„ç« èŠ‚å¼€å¤´æ¨¡å¼
        if re.match(r'^[A-Z](\.\d+)*\.?$', chapter_id) or re.match(r'^\d+(\.\d+)*\.?$', chapter_id):
            title_upper = chap.get("chapter_title", "").upper()
            if any(k in title_upper for k in ["SCOPE", "GENERAL", "INTRO", "æ€»åˆ™", "èŒƒå›´", "LEGISLATIVE", "FUNCTION"]):
                start_index = i
                break
                
    # print(f'chapters[str]: {chapters[start_index]}')
    filtered_chapters = chapters[start_index:]
    skipped_content = chapters[:start_index]
    skipped_text = "\n".join([f"{ch['chapter_id']} {ch['chapter_title']} {ch.get('raw_text','')}" for ch in skipped_content])

    return filtered_chapters, skipped_text


def smart_paragraph_join(lines: List[str]) -> str:
    """
    æ™ºèƒ½æ®µè½åˆå¹¶ï¼šåªåœ¨æ®µè½ç»“æŸæ—¶æ¢è¡Œ
    """
    if not lines:
        return ""
    
    result = []
    current_paragraph = []
    
    for i, line in enumerate(lines):
        line = line.strip()
        if not line:  # ç©ºè¡Œç›´æ¥è·³è¿‡
            continue
            
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ®µè½ç»“æŸçš„æ ‡å¿—
        is_paragraph_end = False
        
        # 1. ä»¥æ ‡ç‚¹ç¬¦å·ç»“å°¾ï¼ˆä¸­è‹±æ–‡ï¼‰
        if re.search(r'[ã€‚ï¼ï¼Ÿï¼›ï¼š.!?;:]$', line):
            is_paragraph_end = True
            
        # 2. æ£€æŸ¥ä¸‹ä¸€è¡Œæ˜¯å¦æ˜¯æ–°æ®µè½çš„å¼€å§‹
        if i + 1 < len(lines):
            next_line = lines[i + 1].strip()
            # ä¸‹ä¸€è¡Œæ˜¯ç« èŠ‚æ ‡é¢˜ã€åˆ—è¡¨é¡¹ã€æˆ–æ˜æ˜¾çš„æ®µè½å¼€å§‹
            if (detect_chapter(next_line) or
                re.match(r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ã€\.\)]', next_line) or  # åˆ—è¡¨é¡¹
                re.match(r'^[ï¼ˆ(]\d+[ï¼‰)]', next_line) or  # ç¼–å·é¡¹
                re.match(r'^[â€”â€”\-â€”]+', next_line)):  # ç ´æŠ˜å·å¼€å¤´
                is_paragraph_end = True
        
        # 3. è¡¨æ ¼ç›¸å…³å†…å®¹ä¿æŒåŸæœ‰æ¢è¡Œ
        if ('è¡¨' in line and re.search(r'è¡¨\s*[A-Z0-9]', line)) or \
           re.match(r'^[|\s]*[A-Za-z0-9\u4e00-\u9fa5]+[|\s]*$', line):  # ç®€å•è¡¨æ ¼è¡Œæ£€æµ‹
            current_paragraph.append(line)
            is_paragraph_end = True
        else:
            current_paragraph.append(line)
        
        # å¦‚æœæ˜¯æ®µè½ç»“æŸï¼Œå°†å½“å‰æ®µè½åˆå¹¶å¹¶åŠ å…¥ç»“æœ
        if is_paragraph_end:
            if current_paragraph:
                paragraph_text = ''.join(current_paragraph).strip()
                if paragraph_text:
                    result.append(paragraph_text)
                current_paragraph = []
    
    # å¤„ç†æœ€åå‰©ä½™çš„æ®µè½
    if current_paragraph:
        paragraph_text = ' '.join(current_paragraph).strip()
        if paragraph_text:
            result.append(paragraph_text)
    
    return '\n'.join(result)

def parse_pdf_to_chapter_tree(pdf_path: str) -> Tuple[List[Dict], Dict[str, str]]:
    """
    ä» PDF ä¸­æå–ç« èŠ‚æ ‘å’Œæœ¯è¯­æ˜ å°„
    :param pdf_path: PDF æ–‡ä»¶è·¯å¾„
    :return: (ç« èŠ‚æ ‘, æœ¯è¯­æ˜ å°„)
    """
    cleaned_lines = extract_full_text_with_filter(pdf_path)

    # ğŸ†• æ£€æµ‹æ–‡æ¡£è¯­è¨€
    language = detect_document_language(cleaned_lines)
    max_chapter_num = 50 if language == 'zh' else 1000
    print(f"æ£€æµ‹åˆ°æ–‡æ¡£è¯­è¨€: {'ä¸­æ–‡' if language == 'zh' else 'è‹±æ–‡'}, max_chapter_num={max_chapter_num}")

    # ğŸ†• ç¬¬ä¸€è½®ï¼šç²—ç•¥æå–æ‰€æœ‰å¯èƒ½çš„ç« èŠ‚ï¼Œç”¨äºåˆ†ææ•°å­—åˆ†å¸ƒ
    preliminary_chapters = []
    current = {
        "chapter_id": "",
        "chapter_title": "",
        "raw_text": ""
    }
    buffer = []

    for line in cleaned_lines:
        # ç¬¬ä¸€è½®ä½¿ç”¨å®½æ¾çš„æ•°å­—èŒƒå›´è¿›è¡Œç²—æå–
        chapter_info = detect_chapter(line, max_chapter_num=1000, language=language, number_analysis=None)

        if chapter_info:
            if current:
                current["raw_text"] = smart_paragraph_join(buffer)
                preliminary_chapters.append(current)
                buffer = []
            current = {
                "chapter_id": chapter_info["chapter_id"],
                "chapter_title": chapter_info["chapter_title"],
                "raw_text": ""
            }
        else:
            buffer.append(line)

    if current:
        current["raw_text"] = smart_paragraph_join(buffer)
        preliminary_chapters.append(current)

    # ğŸ†• åˆ†æç« èŠ‚æ•°å­—åˆ†å¸ƒ
    number_analysis = analyze_chapter_number_distribution(preliminary_chapters)
    print(f"æ•°å­—åˆ†å¸ƒåˆ†æ: {number_analysis}")

    # ğŸ†• ç¬¬äºŒè½®ï¼šä½¿ç”¨åˆ†æç»“æœé‡æ–°ç²¾ç¡®æå–ç« èŠ‚
    chapters = []
    current = {
        "chapter_id": "",
        "chapter_title": "",
        "raw_text": ""
    }
    buffer = []

    for line in cleaned_lines:
        chapter_info = detect_chapter(line, max_chapter_num=max_chapter_num, language=language, number_analysis=number_analysis)

        if chapter_info:
            if current:
                # ä½¿ç”¨æ™ºèƒ½æ®µè½åˆå¹¶è€Œä¸æ˜¯ç®€å•çš„ \n è¿æ¥
                current["raw_text"] = smart_paragraph_join(buffer)
                chapters.append(current)
                buffer = []
            current = {
                "chapter_id": chapter_info["chapter_id"],
                "chapter_title": chapter_info["chapter_title"],
                "raw_text": ""
            }
        else:
            buffer.append(line)

    if current:
        current["raw_text"] = smart_paragraph_join(buffer)
        chapters.append(current)

    # 1ï¸âƒ£ å…ˆæŒ‰é™„ä»¶åˆ‡åˆ†é¡¶å±‚
    attachment_sections = split_sections_by_attachment(chapters)

    tree = []

    for top_sec in attachment_sections:
        
        # 2ï¸âƒ£ æ¯ä¸ªé¡¶å±‚å—å†æŒ‰é™„å½•åˆ‡åˆ†
        sections = split_sections_by_appendix(top_sec["chapters"])
        section_tree_list = []

        for sec in sections:
            # filtered_chapters, skipped_text = filter_start_of_main(sec["chapters"])
            # 3ï¸âƒ£ å¯¹æ¯ä¸ªéƒ¨åˆ†å†…éƒ¨ä¿ç•™æœ€é•¿é“¾
            valid_chaps_in_sec, skipped_text = find_longest_chapter_chain_with_append(sec["chapters"], language)
            tree_in_sec = build_tree(valid_chaps_in_sec)
            build_full_path(tree_in_sec)
            # æ’å…¥é”®å€¼å¯¹ section
            section_tree_list.append({
                "section": sec["section"],
                "context": skipped_text,
                "chapters": tree_in_sec,
            })

        # 4ï¸âƒ£ æ„å»ºé¡¶å±‚æ ‘
        tree.append({
            "file": top_sec["section"],  # regulation æˆ– ANNEX n
            "sections": section_tree_list,
        })

    term_map = {}

    for chap in chapters:
        title = chap.get("chapter_title", "")
        if "æœ¯è¯­" in title:
            # æå–æœ¯è¯­
            terms = extract_terms_with_abbr_from_terms_section(chap["chapter_title"])
            term_map.update(terms)
            for child in chap.get("children", []):
                terms = extract_terms_with_abbr_from_terms_section(child["chapter_title"])
                term_map.update(terms)
        elif "ç¼©ç•¥" in title:
            # æå–ç¼©ç•¥è¯­
            abbr_terms = extract_abbr_terms_from_symbols_section(chap["chapter_title"] + chap["raw_text"])
            term_map.update(abbr_terms)
            for child in chap.get("children", []):
                abbr_terms = extract_abbr_terms_from_symbols_section(child["chapter_title"] + child["raw_text"])
                term_map.update(abbr_terms)

    return tree, term_map

import re
from collections import defaultdict
from typing import List, Dict
import pdfplumber

def _build_page_lines_from_words(page, y_tol=3):
    """
    ç”¨ page.extract_words() æ„å»ºè¡Œï¼šæŒ‰ top åˆ†æ¡¶ï¼ˆy_tol å®¹å·®ï¼‰ï¼Œæ¯è¡ŒæŒ‰ x0 æ’åºå¹¶åˆå¹¶æ–‡æœ¬ï¼Œ
    è¿”å›åˆ—è¡¨ï¼š{'text', 'y', 'x0', 'x1'}
    """
    words = page.extract_words()  # è¿”å›æ¯ä¸ª word å¸¦ x0,x1,top,bottom,text
    if not words:
        return []

    # æŒ‰ top, x0 æ’åº
    words = sorted(words, key=lambda w: (w['top'], w['x0']))

    lines = []
    cur = None
    for w in words:
        top = w['top']
        x0 = float(w['x0'])
        x1 = float(w['x1'])
        text = w['text']

        if cur is None:
            cur = {'text': text, 'y': top, 'x0': x0, 'x1': x1}
            continue

        if abs(top - cur['y']) <= y_tol:
            # åŒä¸€è¡Œï¼ŒæŒ‰ x é¡ºåºè¿æ¥ï¼ˆextract_words å·²æŒ‰ x æ’åºï¼Œä½†ä»åšä¿é™©ï¼‰
            if x0 < cur['x1'] + 1:
                # é‡å æˆ–ç´§é‚»ï¼Œç›´æ¥ç”¨ç©ºæ ¼éš”å¼€ï¼ˆé¿å…æŠŠè¯ç²˜åœ¨ä¸€èµ·ï¼‰
                cur['text'] = cur['text'] + ' ' + text
            else:
                cur['text'] = cur['text'] + ' ' + text
            cur['x1'] = max(cur['x1'], x1)
            cur['x0'] = min(cur['x0'], x0)
            # keep y as average to be robust
            cur['y'] = (cur['y'] + top) / 2.0
        else:
            lines.append(cur)
            cur = {'text': text, 'y': top, 'x0': x0, 'x1': x1}
    if cur:
        lines.append(cur)
    return lines

def _find_table_title_near_bbox(page, table_bbox, max_above=60, y_tol=4):
    """
    åœ¨è¡¨æ ¼ä¸Šæ–¹ max_above pt çš„èŒƒå›´å†…æ‰¾å¯èƒ½çš„æ ‡é¢˜ï¼š
    - å…ˆä» page.extract_words() ä¸­ç­›é€‰å‡ºè¯¥å‚ç›´å¸¦å†…å¹¶ä¸”ä¸è¡¨æ ¼æ°´å¹³æœ‰é‡å çš„ words
    - æŒ‰ top/x0 åˆ†ç»„æˆè¡Œå¹¶æ‹¼æ¥ï¼Œè¿”å›æ‹¼å®Œçš„å­—ç¬¦ä¸²ï¼ˆå¯èƒ½åŒ…å«ç¼–å·ï¼‰
    """
    table_top = float(table_bbox[1])
    table_x0, table_x1 = float(table_bbox[0]), float(table_bbox[2])
    words = page.extract_words()
    if not words:
        return None

    # ç­›é€‰ï¼šå‚ç›´åœ¨ (table_top - max_above, table_top + 10) èŒƒå›´å†…ï¼Œ
    # åŒæ—¶æ°´å¹³ä¸Šè‡³å°‘ä¸è¡¨æ ¼å·¦å³æ‰©å±• 50pt æœ‰é‡å ï¼ˆé˜²æ­¢å®Œå…¨é å·¦çš„æ ‡é¢˜è¢«å¿½ç•¥ï¼‰
    relevant = []
    margin_x = 60
    for w in words:
        w_top = float(w['top'])
        if not (table_top - max_above <= w_top <= table_top + 10):
            continue
        w_x0 = float(w['x0']); w_x1 = float(w['x1'])
        # ä¸è¡¨æ ¼æ°´å¹³æŠ•å½±æœ‰é‡å  æˆ– åœ¨è¡¨æ ¼å·¦ä¾§æ¥è¿‘ä½ç½®
        if (w_x1 >= table_x0 - margin_x and w_x0 <= table_x1 + margin_x) or w_x0 < table_x0:
            relevant.append(w)

    if not relevant:
        return None

    # å°†è¿™äº› words æŒ‰ top/x0 æ’åºï¼Œåˆ†è¡Œå¹¶åˆå¹¶
    relevant = sorted(relevant, key=lambda w: (w['top'], w['x0']))
    lines = []
    cur = None
    for w in relevant:
        top = w['top']; x0 = float(w['x0']); x1 = float(w['x1']); txt = w['text']
        if cur is None:
            cur = {'text': txt, 'y': top, 'x0': x0, 'x1': x1}
        else:
            if abs(top - cur['y']) <= y_tol:
                cur['text'] = cur['text'] + ' ' + txt
                cur['x1'] = max(cur['x1'], x1)
                cur['x0'] = min(cur['x0'], x0)
                cur['y'] = (cur['y'] + top) / 2.0
            else:
                lines.append(cur)
                cur = {'text': txt, 'y': top, 'x0': x0, 'x1': x1}
    if cur:
        lines.append(cur)

    # ç°åœ¨æŠŠè¿™äº›è¡Œæ‹¼æˆæœ€ç»ˆæ ‡é¢˜ï¼šæŒ‰ y ä»ä¸Šåˆ°ä¸‹ã€æŒ‰ x0 ä»å·¦åˆ°å³è¿æ¥
    # ä½†ä¼˜å…ˆé€‰æ‹©åŒ…å« "è¡¨" çš„è¡Œæˆ–ä»¥ "è¡¨" å¼€å¤´çš„è¡ŒåŠå…¶ç›¸é‚»è¡Œ
    full = " ".join([ln['text'] for ln in lines]).strip()

    # ä¼˜å…ˆç­–ç•¥ï¼šæ‰¾åˆ°åŒ…å«"è¡¨"çš„è¡Œï¼ˆæœ€é è¿‘è¡¨æ ¼çš„é‚£ä¸€è¡Œä¼˜å…ˆï¼‰
    cand = None
    candidates = []
    for ln in lines:
        if 'è¡¨' in ln['text'] or 'Table' in ln['text']:
            candidates.append((abs(table_top - ln['y']), ln))
    if candidates:
        # å–è·ç¦»æœ€å°çš„é‚£ä¸€è¡Œä½œä¸ºæ ¸å¿ƒï¼Œç„¶åæŠŠåŒä¸€ y å¸¦å†…çš„å…¶ä»–è¡Œåˆå¹¶ï¼ˆå·¦å³æ‰©å±•ï¼‰
        candidates.sort(key=lambda x: x[0])
        core_y = candidates[0][1]['y']
        # åˆå¹¶ä¸ core_y æ¥è¿‘çš„æ‰€æœ‰è¡Œ
        merge_parts = [ln['text'] for ln in lines if abs(ln['y'] - core_y) <= max(y_tol, 10)]
        cand = " ".join(merge_parts).strip()
    else:
        # æœªæ‰¾åˆ°åŒ…å«"è¡¨"çš„æ˜ç¡®è¡Œï¼Œå°±é€€å›ç”¨ fullï¼ˆå¯èƒ½å°±æ˜¯æ•´æ®µæ ‡é¢˜ï¼‰
        cand = full if full else None

    # print(f"æ‰¾åˆ°æ ‡é¢˜å€™é€‰: {cand}")
    # è¿›ä¸€æ­¥æ¸…ç†ï¼šæŠŠå¤šä½™ç©ºæ ¼ã€è¿ç»­å¤šä½™æ ‡ç‚¹æ•´ç†ä¸€ä¸‹
    if cand:
        cand = re.sub(r'\s+', ' ', cand).strip()
        cand = re.sub(r'\s*([ï¼Œ,ï¼š:ï¼›;])\s*', r'\1 ', cand)
    return cand

def extract_tables_from_pdf(pdf_path: str) -> List[Dict]:
    """
    æ”¹è¿›ç‰ˆï¼šä»PDFä¸­æå–æ‰€æœ‰è¡¨æ ¼åŠå…¶æ ‡è¯†ï¼Œå°½é‡æ¢å¤ 'è¡¨F.1' è¿™ç±»ç¼–å·
    è¿”å›æ ¼å¼: [{"table_id": "è¡¨X.x æ ‡é¢˜", "table_content": äºŒç»´æ•°ç»„}, ...]
    """
    all_tables = []

    with pdfplumber.open(pdf_path) as pdf:
        for page_num, page in enumerate(pdf.pages):
            # ä½¿ç”¨ extract_words æ„å»ºé¡µé¢è¡Œï¼ˆä¹Ÿä¿ç•™åŸå§‹ words ç”¨äºæ›´ç²¾ç»†åˆ¤æ–­ï¼‰
            page_lines = _build_page_lines_from_words(page, y_tol=3)

            # æå–å¹¶æ’åºè¡¨æ ¼
            tables = page.find_tables()
            if not tables:
                continue
            tables = sorted(tables, key=lambda t: t.bbox[1])

            for table_idx, table in enumerate(tables):
                # è¿‡æ»¤ï¼šåªæœ‰ 1 åˆ—çš„ç›´æ¥ä¸¢å¼ƒ
                table_data = table.extract()
                if not table_data:
                    continue
                sample_row = table_data[0]
                if len(sample_row) <= 1:
                    continue

                cleaned_data = [
                    [cell.replace('\n', ' ').strip() if cell else "" for cell in row]
                    for row in table_data
                ]

                # ä¼˜å…ˆé€šè¿‡ page_lines æ‰¾æ ‡é¢˜
                table_top = table.bbox[1]
                best_line = None
                min_gap = float('inf')

                # å…ˆå°è¯•æ›´ç¨³å¥çš„æ–¹å¼ï¼šä» words åŒºåŸŸæ”¶é›†å¹¶æ‹¼æ¥æ ‡é¢˜
                cand = _find_table_title_near_bbox(page, table.bbox, max_above=60, y_tol=4)
                if cand:
                    best_line = cand
                else:
                    # é€€å›åˆ°åŸå…ˆé€»è¾‘ï¼šåœ¨ page_lines ä¸­æ‰¾åŒ…å« "è¡¨" çš„è¡Œï¼ˆè·ç¦»æœ€è¿‘çš„ï¼‰
                    for item in page_lines:
                        if ("è¡¨" in item['text'] or 'Table' in item['text']) and 0 < (table_top - item['y']) < 60:
                            gap = table_top - item['y']
                            if gap < min_gap:
                                min_gap = gap
                                best_line = item['text']

                # å…œåº•ï¼šå¦‚æœè¿˜æ˜¯æ²¡æœ‰ç¼–å·ï¼Œæ£€æŸ¥è¡¨æ ¼ç¬¬ä¸€è¡Œçš„å•å…ƒæ ¼é‡Œæ˜¯å¦æœ‰â€œè¡¨Xâ€æ ·å¼
                if best_line is None:
                    first_row = cleaned_data[0]
                    # æŠŠç¬¬ä¸€è¡Œæ‰€æœ‰å•å…ƒæ ¼æ‹¼èµ·æ¥æŸ¥æ‰¾â€œè¡¨â€å…³é”®è¯
                    joined_first = " ".join(first_row).strip()
                    if re.search(r'è¡¨\s*[A-Z0-9]\.?\d*', joined_first) or joined_first.startswith('è¡¨'):
                        best_line = joined_first

                # å¦‚æœæ‰¾åˆ°äº†æ ‡é¢˜åˆ™æ–°å¢ï¼Œå¦åˆ™ç”¨å…œåº• id
                if best_line:
                    # è¿›ä¸€æ­¥åšå°æ¸…æ´—ï¼šå°† "è¡¨  F.1" ç­‰ä¸­é—´å¤šä½™ç©ºæ ¼å»æ‰ï¼ˆä¿ç•™è¡¨å­—å’Œç¼–å·ï¼‰
                    best_line = re.sub(r'è¡¨\s+([A-Za-z0-9])', r'è¡¨\1', best_line)
                    all_tables.append({
                        "table_id": best_line,
                        "table_content": cleaned_data
                    })
                else:
                    table_id = f"è¡¨-é¡µ{page_num + 1}-è¡¨{table_idx + 1}"
                    all_tables.append({
                        "table_id": table_id,
                        "table_content": cleaned_data
                    })

    return all_tables


def main():
    import argparse
    parser = argparse.ArgumentParser()
    # parser.add_argument("--pdf_path", default="C:/Users/chenhuaji/OneDrive/æ¡Œé¢/DAO2016-23-1.pdf")
    # parser.add_argument("--pdf_path", default="C:/Users/chenhuaji/OneDrive/æ¡Œé¢/CELEX_42012X0920(02)_EN_TXT.pdf")
    
    # parser.add_argument("--pdf_path", default="å›½æ ‡_è½¦è½½äº‹æ•…ç´§æ€¥å‘¼å«ç³»ç»Ÿ-å¾æ±‚æ„è§ç¨¿.pdf")
    # parser.add_argument("--pdf_path", default="D:\\Documents\\çŸ¥è¯†å›¾è°±agent\\ç¤ºä¾‹æ–‡ä»¶\\GBT+43187-2023 å¤„ç†\\ç»„åˆ 1.pdf")
    
    # parser.add_argument("--pdf_path", default="GBâˆ•T 38997-2020 è½»å°å‹å¤šæ—‹ç¿¼æ— äººæœºé£è¡Œæ§åˆ¶ä¸å¯¼èˆªç³»ç»Ÿé€šç”¨è¦æ±‚.pdf")
    # parser.add_argument("--pdf_path", default="å›½æ ‡_è½¦è½½äº‹æ•…ç´§æ€¥å‘¼å«ç³»ç»Ÿ-å¾æ±‚æ„è§ç¨¿_å¯è¯†åˆ«æ–‡å­—.pdf")
    # parser.add_argument("--pdf_path", default="GBâˆ•T 38930-2020 æ°‘ç”¨è½»å°å‹æ— äººæœºç³»ç»ŸæŠ—é£æ€§è¦æ±‚åŠè¯•éªŒæ–¹æ³•.pdf")
    # parser.add_argument("--pdf_path", default="GB 7258-2017 ã€ŠæœºåŠ¨è½¦è¿è¡Œå®‰å…¨æŠ€æœ¯æ¡ä»¶ã€‹.pdf")
    # parser.add_argument("--pdf_path", default="../ç¤ºä¾‹æ–‡ä»¶/GB+11551-2014.pdf")
    # parser.add_argument("--pdf_path", default="../ç¤ºä¾‹æ–‡ä»¶/GB+20071-2025.pdf")
    # parser.add_argument("--pdf_path", default="../ç¤ºä¾‹æ–‡ä»¶/GB+20072-2024.pdf")
    # parser.add_argument("--pdf_path", default="../ç¤ºä¾‹æ–‡ä»¶/GB+34660-2017.pdf")
    # parser.add_argument("--pdf_path", default="../ç¤ºä¾‹æ–‡ä»¶/GBT+43187-2023.pdf")
    # parser.add_argument("--pdf_path", default="../ç¤ºä¾‹æ–‡ä»¶/GBT+43187-2023_OCR.pdf")
    # parser.add_argument("--pdf_path", default="C:/Users/chenhuaji/OneDrive/æ¡Œé¢/test/GBT+43187-2023_page-0001_chrome.pdf")
    # parser.add_argument("--pdf_path", default="D:\\Documents\\çŸ¥è¯†å›¾è°±agent\\ç¤ºä¾‹æ–‡ä»¶\\GB+34660-2017 å¤„ç†\\ç»„åˆ 1.pdf")
    # parser.add_argument("--pdf_path", default="D:\\Documents\\çŸ¥è¯†å›¾è°±agent\\ç¤ºä¾‹æ–‡ä»¶\\GBT+43187-2023 å¤„ç†\\ç»„åˆ 1.pdf")
    # parser.add_argument("--pdf_path", default="GB+45672-2025.pdf")
    parser.add_argument("--pdf_path", default="D:/Documents/çŸ¥è¯†å›¾è°±agent/æ ‡å‡†å·®å¼‚åˆ†æ/GSO-1040/GSO-1040-2000_OCR/GSO-1040-2000-EæœºåŠ¨è½¦- è½»å‹æŸ´æ²¹å¼•æ“è½¦è¾†å¤§æ°”æ±¡æŸ“ç‰©æ’æ”¾å…è®¸é™å€¼.pdf")
    # parser.add_argument("--pdf_path", default="D:/Documents/çŸ¥è¯†å›¾è°±agent/æ ‡å‡†å·®å¼‚åˆ†æ/GSO-1040/CELEX_42006X1124(01)_EN_TXT.pdf")
    # parser.add_argument("--pdf_path", default="D:/Documents/çŸ¥è¯†å›¾è°±agent/æ ‡å‡†å·®å¼‚åˆ†æ/GSO-1040/CELEX_42006X1227(06)_EN_TXT.pdf")
    # parser.add_argument("--pdf_path", default="D:/Documents/çŸ¥è¯†å›¾è°±agent/æ ‡å‡†å·®å¼‚åˆ†æ/ADR60/R048r12e.pdf")
    # parser.add_argument("--pdf_path", default="D:/Documents/çŸ¥è¯†å›¾è°±agent/æ ‡å‡†å·®å¼‚åˆ†æ/ADR60/R007r6e.pdf")
    # parser.add_argument("--pdf_path", default="D:/Documents/çŸ¥è¯†å›¾è°±agent/æ ‡å‡†å·®å¼‚åˆ†æ/ADR60/F2023C00147.pdf")


    parser.add_argument("--output", help="è¾“å‡º JSON æ–‡ä»¶è·¯å¾„", default="output.json")
    args = parser.parse_args()

    chapter_tree, term_map = parse_pdf_to_chapter_tree(args.pdf_path)

    # # æå–è¡¨æ ¼
    # tables = extract_tables_from_pdf(args.pdf_path)
    
    # # åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„
    # output_data = {
    #     "chapters": chapter_tree,
    #     "tables": tables
    # }

    output_data = chapter_tree
    # # output_data["terms"] = term_map
    # output_data["tables"] = tables

    print(f'term_map: {term_map}')

    # ä¿å­˜ç»“æœ
    with open(args.output, "w", encoding="utf-8") as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    # # fileä¹‹é—´æ¢è¡Œéš”å¼€ï¼Œsectionç›´æ¥æ¢è¡Œéš”å¼€
    # with open(args.output, "w", encoding="utf-8") as f:
    #     for file in output_data:
    #         for section in file["sections"]:
    #             for chapter in section["chapters"]:
    #                 # å°†chapter_titleå­—æ®µåˆå¹¶åˆ°rawtextï¼Œåˆ æ‰chapter_titleå­—æ®µ
    #                 # chapter["raw_text"] = chapter["chapter_title"] + " " + chapter["raw_text"]
    #                 # del chapter["chapter_title"]
    #                 f.write(json.dumps(chapter, ensure_ascii=False) + "\n")
    #             f.write("\n")
    #         f.write("\n")
    #     # json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… æå–å®Œæˆï¼Œç« èŠ‚å’Œè¡¨æ ¼å·²ä¿å­˜è‡³ {args.output}")
    print(f"   - å…±æå– {len(chapter_tree)} ä¸ªç« èŠ‚")
    # print(f"   - å…±æå– {len(tables)} ä¸ªè¡¨æ ¼")

if __name__ == "__main__":
    main()
