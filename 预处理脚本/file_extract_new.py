# # #!/usr/bin/env python3
# # # -*- coding: utf-8 -*-
# # """
# # æŒ‰å­—ç¬¦çº§ç²—ä½“ä¿¡æ¯ç²¾ç¡®æ‹†åˆ†æ ‡é¢˜ä¸æ­£æ–‡
# # è¦æ±‚ï¼š
# # 1. åªæœ‰ç« èŠ‚å·ï¼ˆå«ç©ºæ ¼ï¼‰æ˜¯ç²—ä½“ â†’ å…¶ä½™éƒ¨åˆ†ç®—æ­£æ–‡
# # 2. ç¬¬ä¸€å±‚èŠ‚ç‚¹è‹¥æ— å­èŠ‚ç‚¹ä¸”æ— æ­£æ–‡ â†’ ç›´æ¥åˆ é™¤
# # """

# # import pdfplumber
# # import re
# # import json
# # import argparse
# # from typing import List, Dict, Tuple, Optional

# # # -------------- æ­£åˆ™åˆ—è¡¨ --------------
# # PATTERNS = [
# #     re.compile(r'^(é™„å½•\s*[A-Z])\s+(.+)$'),          # é™„å½•A xxx
# #     re.compile(r'^([A-Z](?:\.\d+)+)\s+(.+)$'),       # A.1ã€B.2.3
# #     re.compile(r'^(\d+(?:\.\d+)*)\s+(.+)$'),         # 4ã€4.1ã€4.1.2
# # ]

# # # -------------- ç²—ä½“åˆ¤å®š --------------
# # def is_bold(char: Dict) -> bool:
# #     font = (char.get("fontname") or "").upper()
# #     return any(k in font for k in ("BOLD", "HEAVY", "BLACK"))

# # # -------------- æ‹†åˆ†ä¸€è¡Œ --------------
# # def split_line(line: str, page) -> Tuple[Optional[str], Optional[str], str]:
# #     """
# #     è¿”å› (chapter_id, chapter_title, extra)
# #     extra ä¸ºâ€œéç²—ä½“â€éƒ¨åˆ†ï¼Œç›´æ¥æ”¾åˆ° raw_text æœ€å‰é¢
# #     """
# #     # 1. å…ˆå°è¯•æ­£åˆ™
# #     for pat in PATTERNS:
# #         m = pat.match(line)
# #         if m:
# #             cid, tail = m.group(1), m.group(2)
# #             break
# #     else:
# #         return None, None, ""

# #     # 2. æ‰¾åˆ°è¿™ä¸€è¡Œçš„æ‰€æœ‰å­—ç¬¦
# #     y_top = page.height
# #     y_bottom = 0
# #     for c in page.chars:
# #         if c["text"] and c["text"] in line:
# #             y_top = min(y_top, c["y0"])
# #             y_bottom = max(y_bottom, c["y1"])
# #     chars = [c for c in page.chars if y_top <= c["y0"] <= y_bottom and c["text"]]

# #     # 3. è®¡ç®—ç²—ä½“ç»“æŸä½ç½®
# #     bold_end = 0
# #     for i, c in enumerate(chars):
# #         if not is_bold(c):
# #             bold_end = i
# #             break
# #     else:
# #         bold_end = len(chars)

# #     bold_part = "".join([c["text"] for c in chars[:bold_end]]).strip()
# #     rest_part = "".join([c["text"] for c in chars[bold_end:]]).strip()

# #     # 4. å†å¯¹ç²—ä½“éƒ¨åˆ†åšä¸€æ¬¡æ­£åˆ™ï¼Œé˜²æ­¢åªåŒ¹é…åˆ°åºå·
# #     for pat in PATTERNS:
# #         m2 = pat.match(bold_part)
# #         if m2:
# #             return m2.group(1), "", rest_part  # æ ‡é¢˜åªä¿ç•™ç« èŠ‚å·

# #     # å¦‚æœæ•´è¡Œéƒ½æ²¡åŒ¹é…åˆ°ï¼Œç›´æ¥è¿”å›æ•´è¡Œ
# #     return cid, tail, ""

# # # -------------- æ„é€ æ ‘ --------------
# # def build_tree(chapters: List[Dict]) -> List[Dict]:
# #     id_map = {c["chapter_id"]: c for c in chapters}
# #     root = []
# #     for c in chapters:
# #         c.setdefault("children", [])
# #     for c in chapters:
# #         parts = c["chapter_id"].split(".")
# #         if len(parts) == 1 or c["chapter_id"].startswith("é™„å½•"):
# #             root.append(c)
# #         else:
# #             parent_id = ".".join(parts[:-1])
# #             if parent_id in id_map:
# #                 id_map[parent_id]["children"].append(c)
# #             else:
# #                 root.append(c)  # fallback
# #     return root

# # def build_full_path(nodes: List[Dict], prefix: str = ""):
# #     for n in nodes:
# #         if prefix:
# #             n["full_path"] = f"{prefix}/{n['chapter_id']}"
# #         else:
# #             n["full_path"] = n["chapter_id"]
# #         build_full_path(n.get("children", []), n["full_path"])

# # # -------------- ä¸»è§£æ --------------
# # def parse(pdf_path: str) -> List[Dict]:
# #     chapters, current, buffer = [], None, []

# #     with pdfplumber.open(pdf_path) as pdf:
# #         for page in pdf.pages:
# #             if not page.extract_text():
# #                 continue
# #             for line in page.extract_text().splitlines():
# #                 line = line.strip()
# #                 if not line:
# #                     continue

# #                 cid, title, extra = split_line(line, page)
# #                 if cid is not None:
# #                     if current:
# #                         current["raw_text"] = "\n".join(buffer).strip()
# #                         chapters.append(current)
# #                         buffer = []
# #                     current = {
# #                         "chapter_id": cid,
# #                         "chapter_title": title or "",  # åªä¿ç•™ç« èŠ‚å·
# #                         "raw_text": extra or ""
# #                     }
# #                 else:
# #                     buffer.append(line)

# #     if current:
# #         current["raw_text"] = "\n".join(buffer).strip()
# #         chapters.append(current)

# #     tree = build_tree(chapters)
# #     build_full_path(tree)

# #     # åˆ é™¤ç¬¬ä¸€å±‚ç©ºèŠ‚ç‚¹
# #     tree = [n for n in tree if n.get("raw_text") or n.get("children")]
# #     return tree

# # # -------------- CLI --------------
# # def main():
# #     parser = argparse.ArgumentParser()
# #     parser.add_argument("--pdf", default="å›½æ ‡_è½¦è½½äº‹æ•…ç´§æ€¥å‘¼å«ç³»ç»Ÿ-å¾æ±‚æ„è§ç¨¿.pdf")
# #     parser.add_argument("--out", default="output.json")
# #     args = parser.parse_args()

# #     tree = parse(args.pdf)
# #     with open(args.out, "w", encoding="utf-8") as f:
# #         json.dump(tree, f, ensure_ascii=False, indent=2)
# #     print("âœ… å®Œæˆ ->", args.out)

# # if __name__ == "__main__":
# #     main()

import pdfplumber
import re
import json
import os
from collections import defaultdict
from typing import List, Dict, Tuple, Optional

# ç« èŠ‚æ ‡é¢˜æ­£åˆ™ï¼ˆæ”¯æŒ 1ã€1.1ã€1.1.1ã€é™„å½•Aã€A.1 ç­‰ï¼‰
chapter_patterns = [
    re.compile(r'^(é™„\s*å½•\s*[A-Z])\s+(.+)$'),                        # é™„å½•A xxx
    re.compile(r'^([A-Z](?:\.\d+)+)\s+(.+)$'),                        # A.1ã€B.2.3
    re.compile(r'^(\d+(?:\.\d+)*)(\s+)(.+)$'),                        # 4ã€4.1ã€4.1.2
]

# è¡¨æ ¼æ ‡é¢˜æ­£åˆ™ï¼ˆè¡¨X.xå½¢å¼ï¼‰
table_id_regex = re.compile(r'^\s*è¡¨\s*([A-Z]*\d+(?:\.[\d]+)*)\s*$')  # åŒ¹é…è¡¨1.1, è¡¨A.1ç­‰

def detect_chapter(line: str):
    for pattern in chapter_patterns:
        m = pattern.match(line)
        if m:
            return {
                "chapter_id": m.group(1).strip(),
                "chapter_title": m.group(len(m.groups())).strip()
            }
    return None

def build_tree(chapter_list: List[Dict]) -> List[Dict]:
    id_map = {}
    root = []

    # åˆå§‹åŒ– id_map
    for chap in chapter_list:
        chap["children"] = []
        id_map[chap["chapter_id"]] = chap

    # æ„å»ºæ™®é€šçˆ¶å­å…³ç³»
    for chap in chapter_list:
        parts = chap["chapter_id"].split(".")
        if len(parts) == 1 or chap["chapter_id"].startswith("é™„å½•"):
            root.append(chap)
        else:
            parent_id = ".".join(parts[:-1])
            parent = id_map.get(parent_id)
            if parent:
                parent["children"].append(chap)
            else:
                # æ£€æŸ¥æ˜¯å¦æ˜¯ A.1, B.2 è¿™ç±»æ ¼å¼
                if len(parts) == 2 and len(parts[0]) == 1 and parts[0].isupper():
                    appendix_id = f"é™„å½•{parts[0]}"
                    appendix_title = f"é™„å½•{parts[0]}"
                    if appendix_id not in id_map:
                        appendix_node = {
                            "chapter_id": appendix_id,
                            "chapter_title": appendix_title,
                            "raw_text": "",
                            "children": []
                        }
                        id_map[appendix_id] = appendix_node
                        root.append(appendix_node)
                    else:
                        appendix_node = id_map[appendix_id]
                    appendix_node["children"].append(chap)
                else:
                    root.append(chap)  # fallback

    return root

def build_full_path(chapters: List[Dict], path_prefix=""):
    for chap in chapters:
        if path_prefix:
            chap["full_path"] = f"{path_prefix}/{chap['chapter_id']} {chap['chapter_title']}"
        else:
            chap["full_path"] = f"{chap['chapter_id']} {chap['chapter_title']}"
        if chap.get("children"):
            build_full_path(chap["children"], chap["full_path"])

def fullwidth_to_halfwidth(text: str) -> str:
    result = []
    for char in text:
        code = ord(char)
        if 0xFF01 <= code <= 0xFF5E:  # å…¨è§’å­—ç¬¦èŒƒå›´
            result.append(chr(code - 0xFEE0))
        else:
            result.append(char)
    return ''.join(result)


def is_valid_next_chapter(prev_parts: list, chapter_id: str) -> bool:
    """
    åˆ¤æ–­ chapter_id æ˜¯å¦å¯èƒ½ä¸ºå‰ä¸€ä¸ªç« èŠ‚å·çš„â€œä¸‹ä¸€ä¸ªâ€åˆæ³•ç¼–å·ã€‚
    prev_parts: å‰ä¸€ä¸ªæœ‰æ•ˆç« èŠ‚å·æŒ‰'.'åˆ†å‰²åçš„ int åˆ—è¡¨ï¼Œä¾‹å¦‚ [4,1,10]
    chapter_id: å½“å‰å¾…æ£€æµ‹çš„ç« èŠ‚å·å­—ç¬¦ä¸²ï¼Œå¦‚ "4.2"
    è¿”å› True è¡¨ç¤ºåˆæ³•ï¼ŒFalse è¡¨ç¤ºéæ³•
    """
    # 1. è¿‡æ»¤å¸¦å•ä½çš„ä¼ªç¼–å·
    if re.search(r'\d+\s*(mm|cm|kg|km/h|%|â„ƒ)', chapter_id, re.I):
        return False

    # 2. é™„å½•ç›´æ¥æ”¾è¿‡
    if re.match(r'^[A-Z](?:\.\d+)*$', chapter_id):
        return True

    # 3. è§£æä¸ºæ•°å­—åˆ—è¡¨
    try:
        parts = [int(p) for p in chapter_id.split('.')]
    except ValueError:
        return False

    # # 4. é¦–ç« ç›´æ¥é€šè¿‡
    # if not prev_parts:
    #     return True

    # 5. åŒå±‚é€’å¢ï¼š4.1.10 -> 4.1.11
    if len(parts) == len(prev_parts) and parts[:-1] == prev_parts[:-1] and parts[-1] == prev_parts[-1] + 1:
        return True

    # 6. å‡å±‚ï¼š4.1.10 -> 4.2
    if len(parts) < len(prev_parts) and parts[:-1] == prev_parts[:len(parts)-1] and parts[-1] == prev_parts[len(parts) - 1] + 1:
        return True

    # 7. å­å±‚ï¼š4.1.10 -> 4.1.10.1
    if len(parts) == len(prev_parts) + 1 and parts[:-1] == prev_parts and parts[-1] == 1:
        return True

    return False

def build_term_dict(raw_text: str) -> Dict[str, str]:
    """
    è¿”å›: {ä¸­æ–‡æœ¯è¯­: è‹±æ–‡æœ¯è¯­}
    """
    # æŠŠå¤šä¸ªæ¢è¡Œå‹æˆä¸€ä¸ª
    text = re.sub(r'\n+', '\n', raw_text.strip())

    # å…³é”®æ­£åˆ™ï¼š
    # 3.1\n
    # ä¸­æ–‡æœ¯è¯­è‹±æ–‡å­—ç¬¦ä¸²ï¼ˆä¸­é—´æ— æ¢è¡Œï¼‰
    # ç„¶åä¸€ä¸ªæ¢è¡Œå¼€å§‹æè¿°
    pattern = re.compile(
        r'^\d+\.\d+\n'                     # 3.1
        r'(?P<cn>[^\n]*?)\s*'             # ä¸­æ–‡æœ¯è¯­ï¼ˆæ— æ¢è¡Œï¼‰
        r'(?P<en>[A-Za-z].*?)\s*(?=\n)',  # è‹±æ–‡æœ¯è¯­ï¼ˆç›´åˆ°æ¢è¡Œï¼‰
        re.MULTILINE
    )

    term_map = {}
    for m in pattern.finditer(text):
        cn = m.group("cn").strip()
        en = m.group("en").strip()
        if cn and en:
            term_map[cn] = en
    return term_map

def parse_pdf_to_chapter_tree(pdf_path: str) -> Tuple[List[Dict], Dict[str, str]]:
    with pdfplumber.open(pdf_path) as pdf:
        lines = []
        for page in pdf.pages:
            text = page.extract_text(x_tolerance=3, y_tolerance=3)
            if text:
                lines += text.split("\n")

    chapters = []
    current = None
    buffer = []

    for line in lines:
        line = line.strip()
        line = fullwidth_to_halfwidth(line)
        chapter_info = detect_chapter(line)

        if chapter_info:
            if current:
                current["raw_text"] = "\n".join(buffer).strip()
                chapters.append(current)
                buffer = []
            current = {
                "chapter_id": chapter_info["chapter_id"],
                "chapter_title": chapter_info["chapter_title"],
                "raw_text": ""
            }
        else:
            buffer.append(line)

    if current:
        current["raw_text"] = "\n".join(buffer).strip()
        chapters.append(current)

    # âœ… æ–°å¢ï¼šè¿‡æ»¤æ‰â€œèŒƒå›´â€ä¹‹å‰çš„ç« èŠ‚
    start_index = 0
    for i, chap in enumerate(chapters):
        if chap.get("chapter_id") == "1" and chap.get("chapter_title", "") == "èŒƒå›´":
            start_index = i
            break
    print(start_index)
    filtered_chapters = chapters[start_index:]

    # # å½“raw_textä¸ºç©ºä¸”childrenéƒ½ä¸ºç©ºåˆ—è¡¨æ—¶ï¼Œå°†chapter_titleç§»åŠ¨åˆ°raw_text
    # for chap in filtered_chapters:
    #     if not chap.get("raw_text") and not chap.get("children"):
    #         chap["raw_text"] = chap.get("chapter_title", "")
    #         chap["chapter_title"] = ""


    # ğŸ” å†åšé€’å¢æ ¡éªŒ
    valid_chapters = [filtered_chapters[0]]
    prev_parts = [1,]

    for chap in filtered_chapters[1:]:
        cid = chap["chapter_id"]
        if not is_valid_next_chapter(prev_parts, cid):
            print(f"æ— æ•ˆç« èŠ‚: {cid}, æ ‡é¢˜: {chap['chapter_title']}")
            # è¿½åŠ åˆ°æœ€è¿‘æœ‰æ•ˆç« èŠ‚
            if valid_chapters:                       # æœ‰æœ‰æ•ˆç« èŠ‚
                valid_chapters[-1]["raw_text"] += "\n" + chap['chapter_id'] + chap["chapter_title"]
                print(f"è¿½åŠ åˆ°ä¸Šä¸€æœ‰æ•ˆç« èŠ‚: {valid_chapters[-1]['chapter_id']}")
            continue

        print(f"æœ‰æ•ˆç« èŠ‚: {cid}, æ ‡é¢˜: {chap['chapter_title']}")
        valid_chapters.append(chap)                  # é¦–æ¬¡åŠ å…¥åˆ—è¡¨

        try:
            prev_parts = [int(p) for p in cid.split('.')]
        except ValueError:
            # prev_parts = []   # é™„å½•
            prev_parts = [-1]  # é™„å½•ç”¨ -1 ä»£æ›¿ï¼Œé¿å…å½±å“é€’å¢åˆ¤æ–­

    # valid_chapters = filtered_chapters  # ç›´æ¥ä½¿ç”¨è¿‡æ»¤åçš„ç« èŠ‚åˆ—è¡¨

    # æ„å»º full_path
    tree = build_tree(valid_chapters)
    build_full_path(tree)
    # # åˆ é™¤ç¬¬ä¸€å±‚ç©ºèŠ‚ç‚¹
    # tree = [n for n in tree if n.get("raw_text") or n.get("children")]

    # å‡è®¾ third_chapter æ˜¯ç« èŠ‚æ ‘é‡Œ chapter_id == "3" çš„å­—å…¸
    third_chapter = valid_chapters[2]               # ä½ è§£æåˆ°çš„ç¬¬ä¸‰ç« èŠ‚ç‚¹
    raw = third_chapter["raw_text"]
    term_map = build_term_dict(raw)
    # print(term_map)

    return tree, term_map

import re
from collections import defaultdict
from typing import List, Dict
import pdfplumber


def extract_tables_from_pdf(pdf_path: str) -> List[Dict]:
    """
    ä»PDFä¸­æå–æ‰€æœ‰è¡¨æ ¼åŠå…¶æ ‡è¯†
    è¿”å›æ ¼å¼: [{"table_id": "è¡¨X.x æ ‡é¢˜", "table_content": äºŒç»´æ•°ç»„}, ...]
    """
    all_tables = []

    with pdfplumber.open(pdf_path) as pdf:
        for page_num, page in enumerate(pdf.pages):
            # ---- 1. å…ˆæ”¶é›†é¡µé¢æ–‡æœ¬è¡ŒåŠå…¶ y åæ ‡ï¼Œæ–¹ä¾¿åé¢æ‰¾æ ‡é¢˜ ----
            page_lines = []
            if page.extract_text():
                chars = page.chars
                rows = defaultdict(list)
                for c in chars:
                    rows[int(c['top'])].append(c)
                for top in sorted(rows.keys()):
                    line = ''.join([c['text'] for c in rows[top]]).strip()
                    page_lines.append({'text': line, 'y': top})

            # ---- 2. æå–å¹¶æ’åºè¡¨æ ¼ ----
            tables = page.find_tables()
            if not tables:
                continue
            tables = sorted(tables, key=lambda t: t.bbox[1])  # ä»ä¸Šåˆ°ä¸‹

            # ---- 3. å¤„ç†æ¯ä¸ªè¡¨æ ¼ ----
            for table_idx, table in enumerate(tables):
                # è¿‡æ»¤ï¼šåªæœ‰ 1 åˆ—çš„ç›´æ¥ä¸¢å¼ƒ
                sample_row = table.extract()[0] if table.extract() else []
                if len(sample_row) <= 1:
                    continue

                # è·å–è¡¨æ ¼å†…å®¹
                table_data = table.extract()
                cleaned_data = [
                    [cell.replace('\n', ' ').strip() if cell else "" for cell in row]
                    for row in table_data
                ]

                # ---- æ‰¾æ ‡é¢˜ï¼šåœ¨è¡¨æ ¼ä¸Šæ–¹ 50 pt å†…æ‰¾â€œè¡¨ X.Y â€¦â€æ•´è¡Œ ----
                table_top = table.bbox[1]
                best_line = None
                min_gap = float('inf')
                for item in page_lines:
                    if "è¡¨" in item['text'] and 0 < (table_top - item['y']) < 50:
                        gap = table_top - item['y']
                        if gap < min_gap:
                            min_gap = gap
                            best_line = item['text']

                # ã€ä¿®æ”¹ã€‘é€»è¾‘ï¼šæ‰¾ä¸åˆ°æ ‡é¢˜æ—¶ï¼Œå°è¯•åˆå¹¶åˆ°ä¸Šä¸€å¼ è¡¨
                if best_line:
                    # æ‰¾åˆ°äº†æ ‡é¢˜ -> æ–°å¢ä¸€å¼ è¡¨
                    all_tables.append({
                        "table_id": best_line,
                        "table_content": cleaned_data
                    })
                else:
                    if all_tables:
                        # ã€ä¿®æ”¹ã€‘æ‰¾ä¸åˆ°æ ‡é¢˜ -> æŠŠå½“å‰è¡¨å†…å®¹è¿½åŠ åˆ°ä¸Šä¸€å¼ è¡¨
                        all_tables[-1]["table_content"].extend(cleaned_data)
                    else:
                        # æ•´ä¸ªæ–‡æ¡£ç¬¬ä¸€å¼ è¡¨å°±æ‰¾ä¸åˆ°æ ‡é¢˜ï¼Œå…œåº•å‘½å
                        table_id = f"è¡¨-é¡µ{page_num + 1}-è¡¨{table_idx + 1}"
                        all_tables.append({
                            "table_id": table_id,
                            "table_content": cleaned_data
                        })

    return all_tables

def main():
    import argparse
    parser = argparse.ArgumentParser()
    # parser.add_argument("--pdf_path", help="PDF æ–‡ä»¶è·¯å¾„", default="GBâˆ•T 38997-2020 è½»å°å‹å¤šæ—‹ç¿¼æ— äººæœºé£è¡Œæ§åˆ¶ä¸å¯¼èˆªç³»ç»Ÿé€šç”¨è¦æ±‚.pdf")
    # parser.add_argument("--pdf_path", help="PDF æ–‡ä»¶è·¯å¾„", default="å›½æ ‡_è½¦è½½äº‹æ•…ç´§æ€¥å‘¼å«ç³»ç»Ÿ-å¾æ±‚æ„è§ç¨¿.pdf")
    # parser.add_argument("--pdf_path", help="PDF æ–‡ä»¶è·¯å¾„", default="GBâˆ•T 38930-2020 æ°‘ç”¨è½»å°å‹æ— äººæœºç³»ç»ŸæŠ—é£æ€§è¦æ±‚åŠè¯•éªŒæ–¹æ³•.pdf")
    # parser.add_argument("--pdf_path", help="PDF æ–‡ä»¶è·¯å¾„", default="GB 7258-2017 ã€ŠæœºåŠ¨è½¦è¿è¡Œå®‰å…¨æŠ€æœ¯æ¡ä»¶ã€‹.pdf")
    # parser.add_argument("--pdf_path", help="PDF æ–‡ä»¶è·¯å¾„", default="../ç¤ºä¾‹æ–‡ä»¶/GB+11551-2014.pdf")
    parser.add_argument("--pdf_path", help="PDF æ–‡ä»¶è·¯å¾„", default="GB+45672-2025.pdf")
    parser.add_argument("--output",   help="è¾“å‡º JSON æ–‡ä»¶è·¯å¾„", default="output.json")
    args = parser.parse_args()

    # æå–ç« èŠ‚ç»“æ„
    chapter_tree, term_map = parse_pdf_to_chapter_tree(args.pdf_path)

    # æå–è¡¨æ ¼
    tables = extract_tables_from_pdf(args.pdf_path)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„
    output_data = {
        "chapters": chapter_tree,
        "tables": tables
    }
    
    # ä¿å­˜ç»“æœ
    with open(args.output, "w", encoding="utf-8") as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… æå–å®Œæˆï¼Œç« èŠ‚å’Œè¡¨æ ¼å·²ä¿å­˜è‡³ {args.output}")
    print(f"   - å…±æå– {len(chapter_tree)} ä¸ªç« èŠ‚")
    print(f"   - å…±æå– {len(tables)} ä¸ªè¡¨æ ¼")


# def main(pdf_path: str) -> dict:
#     # ç« èŠ‚æ ‘
#     tree, term_map = parse_pdf_to_chapter_tree(pdf_path)
#     # è¡¨æ ¼
#     tables = extract_tables_from_pdf(pdf_path)

#     # 1-3 ç« ï¼ˆid ä¸º '1','2','3'ï¼‰
#     context = str(tree[:3])

#     # ç« èŠ‚å†…å®¹


#     # å…¶ä½™ç« èŠ‚ï¼ˆéé™„å½•ã€éå‰ä¸‰ç« ï¼‰
#     chapter_text = []
#     for c in tree:
#         if c['chapter_id'] not in {"1", "2", "3"} and not c['chapter_id'].startswith("é™„å½•"):
#             chapter_text.append(c)

#     appendix = str([c for c in tree if c['chapter_id'].startswith("é™„å½•")])

#     # è¡¨æ ¼å†…å®¹
#     tables_str = str(tables)

#     return {
#         "context": context,
#         "chapter_text": chapter_text,
#         "appendix": appendix,
#         "tables": tables_str
#     }

if __name__ == "__main__":
    # print(main("å›½æ ‡_è½¦è½½äº‹æ•…ç´§æ€¥å‘¼å«ç³»ç»Ÿ-å¾æ±‚æ„è§ç¨¿.pdf"))
    main()



import pdfplumber
import re

def extract_full_text_with_filter(pdf_path, output_txt_path):
    """
    ä½¿ç”¨ pdfplumber å®Œæ•´æå– PDF æ–‡æœ¬ï¼š
    1. è‡ªåŠ¨è¿‡æ»¤æ°´å°
    2. ä¿®å¤ç‰¹æ®Šå­—ç¬¦ç¼–ç ï¼ˆå¦‚çŠŒçŠ… â†’ GBï¼‰
    3. ä¿ç•™æ®µè½ç»“æ„
    """
    full_text = []
    total_lines = 0

    # å®šä¹‰æ°´å°è¿‡æ»¤è§„åˆ™ï¼ˆå¯æ ¹æ®å®é™…æ–‡ä»¶è°ƒæ•´ï¼‰
    watermark_patterns = [
        re.compile(r'ä¸‹è½½è€…ï¼š.*?æ‰¹å‡†è€…ï¼š.*?\d{2}:\d{2}:\d{2}'),
        re.compile(r'çŠŒçŠ…ï¼çŠœ[\dâ€”]+'),
    ]

    with pdfplumber.open(pdf_path) as pdf:
        for i, page in enumerate(pdf.pages):
            text = page.extract_text()
            if not text:
                full_text.append(f"[Page {i+1}] <no extractable text>")
                continue

            cleaned_lines = []
            for line in text.splitlines():
                line = line.strip()
                if not line:
                    continue

                # æ°´å°è¿‡æ»¤
                if any(p.search(line) for p in watermark_patterns):
                    continue

                # ç‰¹æ®Šå­—ç¬¦æ›¿æ¢
                line = line.replace('çŠŒçŠ…', 'GB').replace('ï¼', '/')

                cleaned_lines.append(line)

            total_lines += len(cleaned_lines)
            full_text.append("\n".join(cleaned_lines))

    # å†™å…¥æ–‡ä»¶
    with open(output_txt_path, "w", encoding="utf-8") as f:
        f.write("\n\n".join(full_text))

    print(f"âœ… æ–‡æœ¬å·²æå–åˆ° {output_txt_path}")
    print(f"æ€»é¡µæ•°: {len(full_text)}")
    print(f"æœ‰æ•ˆæ–‡æœ¬è¡Œ: {total_lines}")

# ç¤ºä¾‹è°ƒç”¨ï¼ˆè¯·æŒ‰éœ€ä¿®æ”¹è·¯å¾„ï¼‰
if __name__ == "__main__":
    import sys
    if len(sys.argv) != 3:
        print("â—ç”¨æ³•: python extract_full_text_with_filter_plumber.py <PDFè·¯å¾„> <è¾“å‡ºTXTè·¯å¾„>")
    else:
        extract_full_text_with_filter(sys.argv[1], sys.argv[2])
